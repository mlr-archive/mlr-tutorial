{
    "docs": [
        {
            "location": "/index.html", 
            "text": "Machine Learning in R: mlr Tutorial\n\n\nThis web page provides an in-depth introduction to\n\nMachine Learning in R: mlr\n, a framework for machine\nlearning experiments in \nR\n.\n\n\nIn this tutorial, we focus on basic functions and applications.\nMore detailed technical information can be found in the \nmanual pages\n which\nare regularly updated and reflect the documentation of the\n\ncurrent package version on CRAN\n.\n\n\nOffline versions of this tutorial are also available for download:\n\n\n\n\ncurrent mlr release on CRAN\n\n\nthe mlr devel version on GitHub\n\n\n\n\nThe tutorial aims to walkthrough basic data analysis tasks step by step. We will use simple examples\nfrom classification, regression, cluster and survival analysis to illustrate the main features of the package.\n\n\nEnjoy reading!\n\n\nQuick start\n\n\nHere we show the \nmlr\n workflow to train, make predictions, and evaluate a learner on a\nclassification problem. We walk through 5 basic steps that work on any learning problem or\nmethod supported by \nmlr\n.\n\n\nlibrary(mlr)\ndata(iris)\n\n## 1) Define the task\n## Specify the type of analysis (e.g. classification) and provide data and response variable\ntask = makeClassifTask(data = iris, target = \nSpecies\n)\n\n## 2) Define the learner\n## Choose a specific algorithm (e.g. linear discriminant analysis)\nlrn = makeLearner(\nclassif.lda\n)\n\nn = nrow(iris)\ntrain.set = sample(n, size = 2/3*n)\ntest.set = setdiff(1:n, train.set)\n\n## 3) Fit the model\n## Train the learner on the task using a random subset of the data as training set\nmodel = train(lrn, task, subset = train.set)\n\n## 4) Make predictions\n## Predict values of the response variable for new observations by the trained model\n## using the other part of the data as test set\npred = predict(model, task = task, subset = test.set)\n\n## 5) Evaluate the learner\n## Calculate the mean misclassification error and accuracy\nperformance(pred, measures = list(mmce, acc))\n#\n mmce  acc \n#\n 0.04 0.96", 
            "title": "Home"
        }, 
        {
            "location": "/index.html#machine-learning-in-r-mlr-tutorial", 
            "text": "This web page provides an in-depth introduction to Machine Learning in R: mlr , a framework for machine\nlearning experiments in  R .  In this tutorial, we focus on basic functions and applications.\nMore detailed technical information can be found in the  manual pages  which\nare regularly updated and reflect the documentation of the current package version on CRAN .  Offline versions of this tutorial are also available for download:   current mlr release on CRAN  the mlr devel version on GitHub   The tutorial aims to walkthrough basic data analysis tasks step by step. We will use simple examples\nfrom classification, regression, cluster and survival analysis to illustrate the main features of the package.  Enjoy reading!", 
            "title": "Machine Learning in R: mlr Tutorial"
        }, 
        {
            "location": "/index.html#quick-start", 
            "text": "Here we show the  mlr  workflow to train, make predictions, and evaluate a learner on a\nclassification problem. We walk through 5 basic steps that work on any learning problem or\nmethod supported by  mlr .  library(mlr)\ndata(iris)\n\n## 1) Define the task\n## Specify the type of analysis (e.g. classification) and provide data and response variable\ntask = makeClassifTask(data = iris, target =  Species )\n\n## 2) Define the learner\n## Choose a specific algorithm (e.g. linear discriminant analysis)\nlrn = makeLearner( classif.lda )\n\nn = nrow(iris)\ntrain.set = sample(n, size = 2/3*n)\ntest.set = setdiff(1:n, train.set)\n\n## 3) Fit the model\n## Train the learner on the task using a random subset of the data as training set\nmodel = train(lrn, task, subset = train.set)\n\n## 4) Make predictions\n## Predict values of the response variable for new observations by the trained model\n## using the other part of the data as test set\npred = predict(model, task = task, subset = test.set)\n\n## 5) Evaluate the learner\n## Calculate the mean misclassification error and accuracy\nperformance(pred, measures = list(mmce, acc))\n#  mmce  acc \n#  0.04 0.96", 
            "title": "Quick start"
        }, 
        {
            "location": "/task/index.html", 
            "text": "Learning Tasks\n\n\nLearning tasks allow us to encapsulate the data set and specify information about a machine\nlearning task.\n\n\nTask types and creation\n\n\nThe tasks are organized in a hierarchy, with the generic \nTask\n at the top.\nThe following tasks can be instantiated and inherit from the virtual superclass \nTask\n:\n\n\n\n\nRegrTask\n for regression problems\n\n\nClassifTask\n for binary and multi-class classification problems\n\n\nSurvTask\n for survival analysis\n\n\nClusterTask\n for cluster analysis\n\n\nMultilabelTask\n for multilabel classification problems\n\n\nCostSensTask\n for general \ncost-sensitive classification\n\n  (with example-specific costs)\n\n\n\n\nTo create a task, just call \nmake\nTaskType\n, e.g., \nmakeClassifTask\n.\nAll tasks have an identifier (argument \nid\n) and a \ndata.frame\n (argument \ndata\n).\nIf no ID is provided, it is automatically generated using the variable name of the data.\nThe ID will be later used to label results (for example\n\nbenchmark experiments\n) and to annotate plots.\nDepending on the nature of the learning problem, additional arguments may be required and\nare discussed in the following sections.\n\n\nRegression\n\n\nFor supervised learning like regression (as well as classification and\nsurvival analysis), \ndata\n and \ntarget\n must be specified.\n\n\ndata(BostonHousing, package = \nmlbench\n)\nregr.task = makeRegrTask(id = \nbh\n, data = BostonHousing, target = \nmedv\n)\nregr.task\n#\n Supervised task: bh\n#\n Type: regr\n#\n Target: medv\n#\n Observations: 506\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n          12           1           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n\n\n\n\nAs you can see, the \nTask\n records the type of the learning problem and basic information about\nthe data set, e.g., the types of the features (\nnumeric\n vectors, \nfactors\n or\n\nordered factors\n), the number of observations, or whether missing values are present.\n\n\nCreating tasks for classification and survival analysis follows the same scheme,\nthe data type of the target variables included in \ndata\n is simply different.\nFor each of these learning problems, some specifics are described below.\n\n\nClassification\n\n\nFor classification, the target column must be a \nfactor\n.\n\n\nIn the following example we define a classification task for the\n\nBreastCancer\n data set and exclude the variable \nId\n from all further\nmodel fitting and evaluation.\n\n\ndata(BreastCancer, package = \nmlbench\n)\ndf = BreastCancer\ndf$Id = NULL\nclassif.task = makeClassifTask(id = \nBreastCancer\n, data = df, target = \nClass\n)\nclassif.task\n#\n Supervised task: BreastCancer\n#\n Type: classif\n#\n Target: Class\n#\n Observations: 699\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n           0           4           5           0 \n#\n Missings: TRUE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 2\n#\n    benign malignant \n#\n       458       241 \n#\n Positive class: benign\n\n\n\n\nIn binary classification, the two classes are usually referred to as \npositive\n and\n\nnegative\n class with the positive class being the category of greater interest.\nThis is relevant for many \nperformance measures\n\nlike the \ntrue positive rate\n or \nROC curves\n.\nMoreover, \nmlr\n allows the user to set options like the \ndecision threshold\n\nor \nclass weights\n and returns and plots results (like class\nposterior probabilities) for the positive class only.\n\n\nmakeClassifTask\n by default selects the first factor level of the target variable\nas the positive class, \nbenign\n in the example above.\nClass \nmalignant\n can be manually selected as follows:\n\n\nclassif.task = makeClassifTask(id = \nBreastCancer\n, data = df, target = \nClass\n, positive = \nmalignant\n)\n\n\n\n\nSurvival analysis\n\n\nSurvival tasks use two target columns.\nFor left and right censored problems these consist of the survival time and a binary event indicator.\nFor interval censored data the two target columns must be specified in the \n\"interval2\"\n format (see \nSurv\n).\n\n\ndata(lung, package = \nsurvival\n)\nlung$status = (lung$status == 2) # convert to logical\nsurv.task = makeSurvTask(data = lung, target = c(\ntime\n, \nstatus\n))\nsurv.task\n#\n Supervised task: lung\n#\n Type: surv\n#\n Target: time,status\n#\n Events: 165\n#\n Observations: 228\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n           8           0           0           0 \n#\n Missings: TRUE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n\n\n\n\nThe type of censoring can be specified via the argument \ncensoring\n, which defaults to\n\n\"rcens\"\n for right censored data.\n\n\nMultilabel classification\n\n\nIn multilabel classification, each object can belong to more than one category at the same time.\n\n\nThe \ndata\n are expected to contain as many target columns as there are class labels.\nThe target columns should be logical vectors that indicate which class labels are present.\nThe names of the target columns are taken as class labels and need to be passed to the \ntarget\n\nargument of \nmakeMultilabelTask\n.\n\n\nIn the following example, we extract the label names and pass them to the \ntarget\n argument in\n\nmakeMultilabelTask\n.\n\n\nyeast = getTaskData(yeast.task)\n\nlabels = colnames(yeast)[1:14]\nyeast.task = makeMultilabelTask(id = \nmulti\n, data = yeast, target = labels)\nyeast.task\n#\n Supervised task: multi\n#\n Type: multilabel\n#\n Target: label1,label2,label3,label4,label5,label6,label7,label8,label9,label10,label11,label12,label13,label14\n#\n Observations: 2417\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n         103           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 14\n#\n  label1  label2  label3  label4  label5  label6  label7  label8  label9 \n#\n     762    1038     983     862     722     597     428     480     178 \n#\n label10 label11 label12 label13 label14 \n#\n     253     289    1816    1799      34\n\n\n\n\nSee also the tutorial page on \nmultilabel classification\n.\n\n\nCluster analysis\n\n\nAs cluster analysis is unsupervised, the only mandatory argument to construct a cluster analysis\ntask is the \ndata\n.\nWe create a learning task from \nmtcars\n below:\n\n\ndata(mtcars, package = \ndatasets\n)\ncluster.task = makeClusterTask(data = mtcars)\ncluster.task\n#\n Unsupervised task: mtcars\n#\n Type: cluster\n#\n Observations: 32\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n          11           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n\n\n\n\nCost-sensitive classification\n\n\nThe standard objective in classification is to obtain a high prediction accuracy, i.e., to\nminimize the number of errors our classifier makes. This implies that all types of errors are treated equally.\nIn many applications, such as healthcare or finance, different kinds of errors incur asymmetric costs.\n\n\nFor \nclass-dependent costs\n that solely depend on the actual and predicted class labels, use\n\nClassifTask\n.\n\n\nFor \nexample-specific costs\n, use \nCostSensTask\n.\nIn this scenario, each example \n(x, y)\n is associated with an individual cost vector of length\n\nK\n with \nK\n denoting the number of classes. The \nk\n-th component indicates the cost of assigning\n\nx\n to class \nk\n. Naturally, it is assumed that the cost of the intended class label \ny\n is\nminimal.\n\n\nAs the cost vector contains all relevant information about the intended class \ny\n, only\nthe feature values \nx\n and a \ncost\n matrix, which contains the cost vectors for all examples in the data\nset, are required to create the \nCostSensTask\n.\n\n\nIn the following example we use the \niris\n data and an artificial cost matrix\n(as proposed by \nBeygelzimer et al., 2005\n):\n\n\ndf = iris\ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,]\ndf$Species = NULL\n\ncostsens.task = makeCostSensTask(data = df, cost = cost)\ncostsens.task\n#\n Supervised task: df\n#\n Type: costsens\n#\n Observations: 150\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n           4           0           0           0 \n#\n Missings: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 3\n#\n y1, y2, y3\n\n\n\n\nFor more details, see the page about \ncost-sensitive classification\n.\n\n\nFurther settings\n\n\nThe \nTask\n help page also lists several other arguments to provide further specifications of the\nlearning problem.\n\n\nFor example, we could include a \nblocking\n factor in the task.\nThis would indicate that some observations \"belong together\" and should\nnot be separated when splitting the data into training and test sets for \nresampling\n.\n\n\nAnother option is to assign \nweights\n to observations.\nThese can indicate observation frequencies or result from the sampling\nscheme used to collect the data.\nNote that you should use this option only if the weights really belong to the task.\nIf you plan to train some learning algorithms with different weights on the same \nTask\n,\n\nmlr\n offers several other ways to set observation or class weights (for supervised\nclassification). See the \ntraining tutorial\n or\n\nmakeWeightedClassesWrapper\n.\n\n\nAccessing a learning task\n\n\nWe provide many operators to access the elements stored in a \nTask\n.\nThe most important ones are listed in the documentation of \nTask\n and \ngetTaskData\n.\n\n\nTo access the \ntask description\n that contains basic information about\nthe task you can use:\n\n\ngetTaskDesc(classif.task)\n#\n $id\n#\n [1] \nBreastCancer\n\n#\n \n#\n $type\n#\n [1] \nclassif\n\n#\n \n#\n $target\n#\n [1] \nClass\n\n#\n \n#\n $size\n#\n [1] 699\n#\n \n#\n $n.feat\n#\n    numerics     factors     ordered functionals \n#\n           0           4           5           0 \n#\n \n#\n $has.missings\n#\n [1] TRUE\n#\n \n#\n $has.weights\n#\n [1] FALSE\n#\n \n#\n $has.blocking\n#\n [1] FALSE\n#\n \n#\n $is.spatial\n#\n [1] FALSE\n#\n \n#\n $class.levels\n#\n [1] \nbenign\n    \nmalignant\n\n#\n \n#\n $positive\n#\n [1] \nmalignant\n\n#\n \n#\n $negative\n#\n [1] \nbenign\n\n#\n \n#\n $class.distribution\n#\n \n#\n    benign malignant \n#\n       458       241 \n#\n \n#\n attr(,\nclass\n)\n#\n [1] \nClassifTaskDesc\n    \nSupervisedTaskDesc\n \nTaskDesc\n\n\n\n\n\nNote that \ntask descriptions\n have slightly different elements for different\ntypes of \nTask\ns.\nFrequently required elements can also be accessed directly.\n\n\n## Get the ID\ngetTaskId(classif.task)\n#\n [1] \nBreastCancer\n\n\n## Get the type of task\ngetTaskType(classif.task)\n#\n [1] \nclassif\n\n\n## Get the names of the target columns\ngetTaskTargetNames(classif.task)\n#\n [1] \nClass\n\n\n## Get the number of observations\ngetTaskSize(classif.task)\n#\n [1] 699\n\n## Get the number of input variables\ngetTaskNFeats(classif.task)\n#\n [1] 9\n\n## Get the class levels in classif.task\ngetTaskClassLevels(classif.task)\n#\n [1] \nbenign\n    \nmalignant\n\n\n\n\n\nFor convenience, \nmlr\n provides several functions to extract data from a \nTask\n.\n\n\n## Accessing the data set in classif.task\nstr(getTaskData(classif.task))\n#\n 'data.frame':    699 obs. of  10 variables:\n#\n  $ Cl.thickness   : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 5 5 3 6 4 8 1 2 2 4 ...\n#\n  $ Cell.size      : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 1 4 1 8 1 10 1 1 1 2 ...\n#\n  $ Cell.shape     : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 1 4 1 8 1 10 1 2 1 1 ...\n#\n  $ Marg.adhesion  : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 1 5 1 1 3 8 1 1 1 1 ...\n#\n  $ Epith.c.size   : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 2 7 2 3 2 7 2 2 2 2 ...\n#\n  $ Bare.nuclei    : Factor w/ 10 levels \n1\n,\n2\n,\n3\n,\n4\n,..: 1 10 2 4 1 10 10 1 1 1 ...\n#\n  $ Bl.cromatin    : Factor w/ 10 levels \n1\n,\n2\n,\n3\n,\n4\n,..: 3 3 3 3 3 9 3 3 1 2 ...\n#\n  $ Normal.nucleoli: Factor w/ 10 levels \n1\n,\n2\n,\n3\n,\n4\n,..: 1 2 1 7 1 7 1 1 1 1 ...\n#\n  $ Mitoses        : Factor w/ 9 levels \n1\n,\n2\n,\n3\n,\n4\n,..: 1 1 1 1 1 1 1 1 5 1 ...\n#\n  $ Class          : Factor w/ 2 levels \nbenign\n,\nmalignant\n: 1 1 1 1 1 2 1 1 1 1 ...\n\n## Get the names of the input variables in cluster.task\ngetTaskFeatureNames(cluster.task)\n#\n  [1] \nmpg\n  \ncyl\n  \ndisp\n \nhp\n   \ndrat\n \nwt\n   \nqsec\n \nvs\n   \nam\n   \ngear\n\n#\n [11] \ncarb\n\n\n## Get the values of the target variables in surv.task\nhead(getTaskTargets(surv.task))\n#\n   time status\n#\n 1  306   TRUE\n#\n 2  455   TRUE\n#\n 3 1010  FALSE\n#\n 4  210   TRUE\n#\n 5  883   TRUE\n#\n 6 1022  FALSE\n\n## Get the cost matrix in costsens.task\nhead(getTaskCosts(costsens.task))\n#\n      y1        y2         y3\n#\n [1,]  0 1589.5664  674.44434\n#\n [2,]  0 1173.4364  828.40682\n#\n [3,]  0  942.7611 1095.33713\n#\n [4,]  0 1049.5562  477.82496\n#\n [5,]  0 1121.8899   90.85237\n#\n [6,]  0 1819.9830  841.06686\n\n\n\n\nNote that \ngetTaskData\n offers many options for converting the data set into a convenient format.\nThis especially comes in handy when you \nintegrate a new learner\n from another \nR\n\npackage into \nmlr\n.\nIn this regard function \ngetTaskFormula\n is also useful.\n\n\nModifying a learning task\n\n\nmlr\n provides several functions to alter an existing \nTask\n, which is often more\nconvenient than creating a new \nTask\n from scratch.\nHere are some examples:\n\n\n## Select observations and/or features\ncluster.task = subsetTask(cluster.task, subset = 4:17)\n\n## It may happen, especially after selecting observations, that features are constant.\n## These should be removed.\nremoveConstantFeatures(cluster.task)\n#\n Removing 1 columns: am\n#\n Unsupervised task: mtcars\n#\n Type: cluster\n#\n Observations: 14\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n          10           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n\n## Remove selected features\ndropFeatures(surv.task, c(\nmeal.cal\n, \nwt.loss\n))\n#\n Supervised task: lung\n#\n Type: surv\n#\n Target: time,status\n#\n Events: 165\n#\n Observations: 228\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n           6           0           0           0 \n#\n Missings: TRUE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n\n## Standardize numerical features\ntask = normalizeFeatures(cluster.task, method = \nrange\n)\nsummary(getTaskData(task))\n#\n       mpg              cyl              disp              hp        \n#\n  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#\n  1st Qu.:0.3161   1st Qu.:0.5000   1st Qu.:0.1242   1st Qu.:0.2801  \n#\n  Median :0.5107   Median :1.0000   Median :0.4076   Median :0.6311  \n#\n  Mean   :0.4872   Mean   :0.7143   Mean   :0.4430   Mean   :0.5308  \n#\n  3rd Qu.:0.6196   3rd Qu.:1.0000   3rd Qu.:0.6618   3rd Qu.:0.7473  \n#\n  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#\n       drat              wt              qsec              vs        \n#\n  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#\n  1st Qu.:0.2672   1st Qu.:0.1275   1st Qu.:0.2302   1st Qu.:0.0000  \n#\n  Median :0.3060   Median :0.1605   Median :0.3045   Median :0.0000  \n#\n  Mean   :0.4544   Mean   :0.3268   Mean   :0.3752   Mean   :0.4286  \n#\n  3rd Qu.:0.7026   3rd Qu.:0.3727   3rd Qu.:0.4908   3rd Qu.:1.0000  \n#\n  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#\n        am           gear             carb       \n#\n  Min.   :0.5   Min.   :0.0000   Min.   :0.0000  \n#\n  1st Qu.:0.5   1st Qu.:0.0000   1st Qu.:0.3333  \n#\n  Median :0.5   Median :0.0000   Median :0.6667  \n#\n  Mean   :0.5   Mean   :0.2857   Mean   :0.6429  \n#\n  3rd Qu.:0.5   3rd Qu.:0.7500   3rd Qu.:1.0000  \n#\n  Max.   :0.5   Max.   :1.0000   Max.   :1.0000\n\n\n\n\nFor more functions and detailed explanations of common preprocessing tasks, see\n\ndata preprocessing tutorial\n.\n\n\nExample tasks and convenience functions\n\n\nFor your convenience, \nmlr\n provides pre-defined \nTask\ns for each type of learning problem.\nThese are also used throughout this tutorial.\nA list of all \nTask\ns can be found in the \nAppendix\n.\n\n\nUse \nmlr\n's function \nconvertMLBenchObjToTask\n to generate \nTask\ns from the data\nsets and data generating functions in package \nmlbench\n.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\ndata(BostonHousing, package = \nmlbench\n) \nregr.task = makeRegrTask(id = \nbh\n, data = BostonHousing, target = \nmedv\n) \nregr.task \ndata(BreastCancer, package = \nmlbench\n) \ndf = BreastCancer \ndf$Id = NULL \nclassif.task = makeClassifTask(id = \nBreastCancer\n, data = df, target = \nClass\n) \nclassif.task \nclassif.task = makeClassifTask(id = \nBreastCancer\n, data = df, target = \nClass\n, positive = \nmalignant\n) \ndata(lung, package = \nsurvival\n) \nlung$status = (lung$status == 2) # convert to logical \nsurv.task = makeSurvTask(data = lung, target = c(\ntime\n, \nstatus\n)) \nsurv.task \nyeast = getTaskData(yeast.task) \n\nlabels = colnames(yeast)[1:14] \nyeast.task = makeMultilabelTask(id = \nmulti\n, data = yeast, target = labels) \nyeast.task \ndata(mtcars, package = \ndatasets\n) \ncluster.task = makeClusterTask(data = mtcars) \ncluster.task \ndf = iris \ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] \ndf$Species = NULL \n\ncostsens.task = makeCostSensTask(data = df, cost = cost) \ncostsens.task \ngetTaskDesc(classif.task) \n## Get the ID \ngetTaskId(classif.task) \n\n## Get the type of task \ngetTaskType(classif.task) \n\n## Get the names of the target columns \ngetTaskTargetNames(classif.task) \n\n## Get the number of observations \ngetTaskSize(classif.task) \n\n## Get the number of input variables \ngetTaskNFeats(classif.task) \n\n## Get the class levels in classif.task \ngetTaskClassLevels(classif.task) \n## Accessing the data set in classif.task \nstr(getTaskData(classif.task)) \n\n## Get the names of the input variables in cluster.task \ngetTaskFeatureNames(cluster.task) \n\n## Get the values of the target variables in surv.task \nhead(getTaskTargets(surv.task)) \n\n## Get the cost matrix in costsens.task \nhead(getTaskCosts(costsens.task)) \n## Select observations and/or features \ncluster.task = subsetTask(cluster.task, subset = 4:17) \n\n## It may happen, especially after selecting observations, that features are constant. \n## These should be removed. \nremoveConstantFeatures(cluster.task) \n\n## Remove selected features \ndropFeatures(surv.task, c(\nmeal.cal\n, \nwt.loss\n)) \n\n## Standardize numerical features \ntask = normalizeFeatures(cluster.task, method = \nrange\n) \nsummary(getTaskData(task))", 
            "title": "Tasks"
        }, 
        {
            "location": "/task/index.html#learning-tasks", 
            "text": "Learning tasks allow us to encapsulate the data set and specify information about a machine\nlearning task.", 
            "title": "Learning Tasks"
        }, 
        {
            "location": "/task/index.html#task-types-and-creation", 
            "text": "The tasks are organized in a hierarchy, with the generic  Task  at the top.\nThe following tasks can be instantiated and inherit from the virtual superclass  Task :   RegrTask  for regression problems  ClassifTask  for binary and multi-class classification problems  SurvTask  for survival analysis  ClusterTask  for cluster analysis  MultilabelTask  for multilabel classification problems  CostSensTask  for general  cost-sensitive classification \n  (with example-specific costs)   To create a task, just call  make TaskType , e.g.,  makeClassifTask .\nAll tasks have an identifier (argument  id ) and a  data.frame  (argument  data ).\nIf no ID is provided, it is automatically generated using the variable name of the data.\nThe ID will be later used to label results (for example benchmark experiments ) and to annotate plots.\nDepending on the nature of the learning problem, additional arguments may be required and\nare discussed in the following sections.", 
            "title": "Task types and creation"
        }, 
        {
            "location": "/task/index.html#regression", 
            "text": "For supervised learning like regression (as well as classification and\nsurvival analysis),  data  and  target  must be specified.  data(BostonHousing, package =  mlbench )\nregr.task = makeRegrTask(id =  bh , data = BostonHousing, target =  medv )\nregr.task\n#  Supervised task: bh\n#  Type: regr\n#  Target: medv\n#  Observations: 506\n#  Features:\n#     numerics     factors     ordered functionals \n#           12           1           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE  As you can see, the  Task  records the type of the learning problem and basic information about\nthe data set, e.g., the types of the features ( numeric  vectors,  factors  or ordered factors ), the number of observations, or whether missing values are present.  Creating tasks for classification and survival analysis follows the same scheme,\nthe data type of the target variables included in  data  is simply different.\nFor each of these learning problems, some specifics are described below.", 
            "title": "Regression"
        }, 
        {
            "location": "/task/index.html#classification", 
            "text": "For classification, the target column must be a  factor .  In the following example we define a classification task for the BreastCancer  data set and exclude the variable  Id  from all further\nmodel fitting and evaluation.  data(BreastCancer, package =  mlbench )\ndf = BreastCancer\ndf$Id = NULL\nclassif.task = makeClassifTask(id =  BreastCancer , data = df, target =  Class )\nclassif.task\n#  Supervised task: BreastCancer\n#  Type: classif\n#  Target: Class\n#  Observations: 699\n#  Features:\n#     numerics     factors     ordered functionals \n#            0           4           5           0 \n#  Missings: TRUE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 2\n#     benign malignant \n#        458       241 \n#  Positive class: benign  In binary classification, the two classes are usually referred to as  positive  and negative  class with the positive class being the category of greater interest.\nThis is relevant for many  performance measures \nlike the  true positive rate  or  ROC curves .\nMoreover,  mlr  allows the user to set options like the  decision threshold \nor  class weights  and returns and plots results (like class\nposterior probabilities) for the positive class only.  makeClassifTask  by default selects the first factor level of the target variable\nas the positive class,  benign  in the example above.\nClass  malignant  can be manually selected as follows:  classif.task = makeClassifTask(id =  BreastCancer , data = df, target =  Class , positive =  malignant )", 
            "title": "Classification"
        }, 
        {
            "location": "/task/index.html#survival-analysis", 
            "text": "Survival tasks use two target columns.\nFor left and right censored problems these consist of the survival time and a binary event indicator.\nFor interval censored data the two target columns must be specified in the  \"interval2\"  format (see  Surv ).  data(lung, package =  survival )\nlung$status = (lung$status == 2) # convert to logical\nsurv.task = makeSurvTask(data = lung, target = c( time ,  status ))\nsurv.task\n#  Supervised task: lung\n#  Type: surv\n#  Target: time,status\n#  Events: 165\n#  Observations: 228\n#  Features:\n#     numerics     factors     ordered functionals \n#            8           0           0           0 \n#  Missings: TRUE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE  The type of censoring can be specified via the argument  censoring , which defaults to \"rcens\"  for right censored data.", 
            "title": "Survival analysis"
        }, 
        {
            "location": "/task/index.html#multilabel-classification", 
            "text": "In multilabel classification, each object can belong to more than one category at the same time.  The  data  are expected to contain as many target columns as there are class labels.\nThe target columns should be logical vectors that indicate which class labels are present.\nThe names of the target columns are taken as class labels and need to be passed to the  target \nargument of  makeMultilabelTask .  In the following example, we extract the label names and pass them to the  target  argument in makeMultilabelTask .  yeast = getTaskData(yeast.task)\n\nlabels = colnames(yeast)[1:14]\nyeast.task = makeMultilabelTask(id =  multi , data = yeast, target = labels)\nyeast.task\n#  Supervised task: multi\n#  Type: multilabel\n#  Target: label1,label2,label3,label4,label5,label6,label7,label8,label9,label10,label11,label12,label13,label14\n#  Observations: 2417\n#  Features:\n#     numerics     factors     ordered functionals \n#          103           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 14\n#   label1  label2  label3  label4  label5  label6  label7  label8  label9 \n#      762    1038     983     862     722     597     428     480     178 \n#  label10 label11 label12 label13 label14 \n#      253     289    1816    1799      34  See also the tutorial page on  multilabel classification .", 
            "title": "Multilabel classification"
        }, 
        {
            "location": "/task/index.html#cluster-analysis", 
            "text": "As cluster analysis is unsupervised, the only mandatory argument to construct a cluster analysis\ntask is the  data .\nWe create a learning task from  mtcars  below:  data(mtcars, package =  datasets )\ncluster.task = makeClusterTask(data = mtcars)\ncluster.task\n#  Unsupervised task: mtcars\n#  Type: cluster\n#  Observations: 32\n#  Features:\n#     numerics     factors     ordered functionals \n#           11           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE", 
            "title": "Cluster analysis"
        }, 
        {
            "location": "/task/index.html#cost-sensitive-classification", 
            "text": "The standard objective in classification is to obtain a high prediction accuracy, i.e., to\nminimize the number of errors our classifier makes. This implies that all types of errors are treated equally.\nIn many applications, such as healthcare or finance, different kinds of errors incur asymmetric costs.  For  class-dependent costs  that solely depend on the actual and predicted class labels, use ClassifTask .  For  example-specific costs , use  CostSensTask .\nIn this scenario, each example  (x, y)  is associated with an individual cost vector of length K  with  K  denoting the number of classes. The  k -th component indicates the cost of assigning x  to class  k . Naturally, it is assumed that the cost of the intended class label  y  is\nminimal.  As the cost vector contains all relevant information about the intended class  y , only\nthe feature values  x  and a  cost  matrix, which contains the cost vectors for all examples in the data\nset, are required to create the  CostSensTask .  In the following example we use the  iris  data and an artificial cost matrix\n(as proposed by  Beygelzimer et al., 2005 ):  df = iris\ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,]\ndf$Species = NULL\n\ncostsens.task = makeCostSensTask(data = df, cost = cost)\ncostsens.task\n#  Supervised task: df\n#  Type: costsens\n#  Observations: 150\n#  Features:\n#     numerics     factors     ordered functionals \n#            4           0           0           0 \n#  Missings: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 3\n#  y1, y2, y3  For more details, see the page about  cost-sensitive classification .", 
            "title": "Cost-sensitive classification"
        }, 
        {
            "location": "/task/index.html#further-settings", 
            "text": "The  Task  help page also lists several other arguments to provide further specifications of the\nlearning problem.  For example, we could include a  blocking  factor in the task.\nThis would indicate that some observations \"belong together\" and should\nnot be separated when splitting the data into training and test sets for  resampling .  Another option is to assign  weights  to observations.\nThese can indicate observation frequencies or result from the sampling\nscheme used to collect the data.\nNote that you should use this option only if the weights really belong to the task.\nIf you plan to train some learning algorithms with different weights on the same  Task , mlr  offers several other ways to set observation or class weights (for supervised\nclassification). See the  training tutorial  or makeWeightedClassesWrapper .", 
            "title": "Further settings"
        }, 
        {
            "location": "/task/index.html#accessing-a-learning-task", 
            "text": "We provide many operators to access the elements stored in a  Task .\nThe most important ones are listed in the documentation of  Task  and  getTaskData .  To access the  task description  that contains basic information about\nthe task you can use:  getTaskDesc(classif.task)\n#  $id\n#  [1]  BreastCancer \n#  \n#  $type\n#  [1]  classif \n#  \n#  $target\n#  [1]  Class \n#  \n#  $size\n#  [1] 699\n#  \n#  $n.feat\n#     numerics     factors     ordered functionals \n#            0           4           5           0 \n#  \n#  $has.missings\n#  [1] TRUE\n#  \n#  $has.weights\n#  [1] FALSE\n#  \n#  $has.blocking\n#  [1] FALSE\n#  \n#  $is.spatial\n#  [1] FALSE\n#  \n#  $class.levels\n#  [1]  benign      malignant \n#  \n#  $positive\n#  [1]  malignant \n#  \n#  $negative\n#  [1]  benign \n#  \n#  $class.distribution\n#  \n#     benign malignant \n#        458       241 \n#  \n#  attr(, class )\n#  [1]  ClassifTaskDesc      SupervisedTaskDesc   TaskDesc   Note that  task descriptions  have slightly different elements for different\ntypes of  Task s.\nFrequently required elements can also be accessed directly.  ## Get the ID\ngetTaskId(classif.task)\n#  [1]  BreastCancer \n\n## Get the type of task\ngetTaskType(classif.task)\n#  [1]  classif \n\n## Get the names of the target columns\ngetTaskTargetNames(classif.task)\n#  [1]  Class \n\n## Get the number of observations\ngetTaskSize(classif.task)\n#  [1] 699\n\n## Get the number of input variables\ngetTaskNFeats(classif.task)\n#  [1] 9\n\n## Get the class levels in classif.task\ngetTaskClassLevels(classif.task)\n#  [1]  benign      malignant   For convenience,  mlr  provides several functions to extract data from a  Task .  ## Accessing the data set in classif.task\nstr(getTaskData(classif.task))\n#  'data.frame':    699 obs. of  10 variables:\n#   $ Cl.thickness   : Ord.factor w/ 10 levels  1 2 3 4 ..: 5 5 3 6 4 8 1 2 2 4 ...\n#   $ Cell.size      : Ord.factor w/ 10 levels  1 2 3 4 ..: 1 4 1 8 1 10 1 1 1 2 ...\n#   $ Cell.shape     : Ord.factor w/ 10 levels  1 2 3 4 ..: 1 4 1 8 1 10 1 2 1 1 ...\n#   $ Marg.adhesion  : Ord.factor w/ 10 levels  1 2 3 4 ..: 1 5 1 1 3 8 1 1 1 1 ...\n#   $ Epith.c.size   : Ord.factor w/ 10 levels  1 2 3 4 ..: 2 7 2 3 2 7 2 2 2 2 ...\n#   $ Bare.nuclei    : Factor w/ 10 levels  1 , 2 , 3 , 4 ,..: 1 10 2 4 1 10 10 1 1 1 ...\n#   $ Bl.cromatin    : Factor w/ 10 levels  1 , 2 , 3 , 4 ,..: 3 3 3 3 3 9 3 3 1 2 ...\n#   $ Normal.nucleoli: Factor w/ 10 levels  1 , 2 , 3 , 4 ,..: 1 2 1 7 1 7 1 1 1 1 ...\n#   $ Mitoses        : Factor w/ 9 levels  1 , 2 , 3 , 4 ,..: 1 1 1 1 1 1 1 1 5 1 ...\n#   $ Class          : Factor w/ 2 levels  benign , malignant : 1 1 1 1 1 2 1 1 1 1 ...\n\n## Get the names of the input variables in cluster.task\ngetTaskFeatureNames(cluster.task)\n#   [1]  mpg    cyl    disp   hp     drat   wt     qsec   vs     am     gear \n#  [11]  carb \n\n## Get the values of the target variables in surv.task\nhead(getTaskTargets(surv.task))\n#    time status\n#  1  306   TRUE\n#  2  455   TRUE\n#  3 1010  FALSE\n#  4  210   TRUE\n#  5  883   TRUE\n#  6 1022  FALSE\n\n## Get the cost matrix in costsens.task\nhead(getTaskCosts(costsens.task))\n#       y1        y2         y3\n#  [1,]  0 1589.5664  674.44434\n#  [2,]  0 1173.4364  828.40682\n#  [3,]  0  942.7611 1095.33713\n#  [4,]  0 1049.5562  477.82496\n#  [5,]  0 1121.8899   90.85237\n#  [6,]  0 1819.9830  841.06686  Note that  getTaskData  offers many options for converting the data set into a convenient format.\nThis especially comes in handy when you  integrate a new learner  from another  R \npackage into  mlr .\nIn this regard function  getTaskFormula  is also useful.", 
            "title": "Accessing a learning task"
        }, 
        {
            "location": "/task/index.html#modifying-a-learning-task", 
            "text": "mlr  provides several functions to alter an existing  Task , which is often more\nconvenient than creating a new  Task  from scratch.\nHere are some examples:  ## Select observations and/or features\ncluster.task = subsetTask(cluster.task, subset = 4:17)\n\n## It may happen, especially after selecting observations, that features are constant.\n## These should be removed.\nremoveConstantFeatures(cluster.task)\n#  Removing 1 columns: am\n#  Unsupervised task: mtcars\n#  Type: cluster\n#  Observations: 14\n#  Features:\n#     numerics     factors     ordered functionals \n#           10           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n\n## Remove selected features\ndropFeatures(surv.task, c( meal.cal ,  wt.loss ))\n#  Supervised task: lung\n#  Type: surv\n#  Target: time,status\n#  Events: 165\n#  Observations: 228\n#  Features:\n#     numerics     factors     ordered functionals \n#            6           0           0           0 \n#  Missings: TRUE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n\n## Standardize numerical features\ntask = normalizeFeatures(cluster.task, method =  range )\nsummary(getTaskData(task))\n#        mpg              cyl              disp              hp        \n#   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#   1st Qu.:0.3161   1st Qu.:0.5000   1st Qu.:0.1242   1st Qu.:0.2801  \n#   Median :0.5107   Median :1.0000   Median :0.4076   Median :0.6311  \n#   Mean   :0.4872   Mean   :0.7143   Mean   :0.4430   Mean   :0.5308  \n#   3rd Qu.:0.6196   3rd Qu.:1.0000   3rd Qu.:0.6618   3rd Qu.:0.7473  \n#   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#        drat              wt              qsec              vs        \n#   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#   1st Qu.:0.2672   1st Qu.:0.1275   1st Qu.:0.2302   1st Qu.:0.0000  \n#   Median :0.3060   Median :0.1605   Median :0.3045   Median :0.0000  \n#   Mean   :0.4544   Mean   :0.3268   Mean   :0.3752   Mean   :0.4286  \n#   3rd Qu.:0.7026   3rd Qu.:0.3727   3rd Qu.:0.4908   3rd Qu.:1.0000  \n#   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#         am           gear             carb       \n#   Min.   :0.5   Min.   :0.0000   Min.   :0.0000  \n#   1st Qu.:0.5   1st Qu.:0.0000   1st Qu.:0.3333  \n#   Median :0.5   Median :0.0000   Median :0.6667  \n#   Mean   :0.5   Mean   :0.2857   Mean   :0.6429  \n#   3rd Qu.:0.5   3rd Qu.:0.7500   3rd Qu.:1.0000  \n#   Max.   :0.5   Max.   :1.0000   Max.   :1.0000  For more functions and detailed explanations of common preprocessing tasks, see data preprocessing tutorial .", 
            "title": "Modifying a learning task"
        }, 
        {
            "location": "/task/index.html#example-tasks-and-convenience-functions", 
            "text": "For your convenience,  mlr  provides pre-defined  Task s for each type of learning problem.\nThese are also used throughout this tutorial.\nA list of all  Task s can be found in the  Appendix .  Use  mlr 's function  convertMLBenchObjToTask  to generate  Task s from the data\nsets and data generating functions in package  mlbench .", 
            "title": "Example tasks and convenience functions"
        }, 
        {
            "location": "/task/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  data(BostonHousing, package =  mlbench ) \nregr.task = makeRegrTask(id =  bh , data = BostonHousing, target =  medv ) \nregr.task \ndata(BreastCancer, package =  mlbench ) \ndf = BreastCancer \ndf$Id = NULL \nclassif.task = makeClassifTask(id =  BreastCancer , data = df, target =  Class ) \nclassif.task \nclassif.task = makeClassifTask(id =  BreastCancer , data = df, target =  Class , positive =  malignant ) \ndata(lung, package =  survival ) \nlung$status = (lung$status == 2) # convert to logical \nsurv.task = makeSurvTask(data = lung, target = c( time ,  status )) \nsurv.task \nyeast = getTaskData(yeast.task) \n\nlabels = colnames(yeast)[1:14] \nyeast.task = makeMultilabelTask(id =  multi , data = yeast, target = labels) \nyeast.task \ndata(mtcars, package =  datasets ) \ncluster.task = makeClusterTask(data = mtcars) \ncluster.task \ndf = iris \ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] \ndf$Species = NULL \n\ncostsens.task = makeCostSensTask(data = df, cost = cost) \ncostsens.task \ngetTaskDesc(classif.task) \n## Get the ID \ngetTaskId(classif.task) \n\n## Get the type of task \ngetTaskType(classif.task) \n\n## Get the names of the target columns \ngetTaskTargetNames(classif.task) \n\n## Get the number of observations \ngetTaskSize(classif.task) \n\n## Get the number of input variables \ngetTaskNFeats(classif.task) \n\n## Get the class levels in classif.task \ngetTaskClassLevels(classif.task) \n## Accessing the data set in classif.task \nstr(getTaskData(classif.task)) \n\n## Get the names of the input variables in cluster.task \ngetTaskFeatureNames(cluster.task) \n\n## Get the values of the target variables in surv.task \nhead(getTaskTargets(surv.task)) \n\n## Get the cost matrix in costsens.task \nhead(getTaskCosts(costsens.task)) \n## Select observations and/or features \ncluster.task = subsetTask(cluster.task, subset = 4:17) \n\n## It may happen, especially after selecting observations, that features are constant. \n## These should be removed. \nremoveConstantFeatures(cluster.task) \n\n## Remove selected features \ndropFeatures(surv.task, c( meal.cal ,  wt.loss )) \n\n## Standardize numerical features \ntask = normalizeFeatures(cluster.task, method =  range ) \nsummary(getTaskData(task))", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/learner/index.html", 
            "text": "Learners\n\n\nThe following classes provide a unified interface to all popular machine learning methods in \nR\n:\n(cost-sensitive) classification, regression, survival analysis, and clustering.\nMany are already integrated, and \nmlr\n is specifically designed to make extensions simple.\n\n\nSee the \nintegrated learners page\n to reference already\nimplemented machine learning methods and their properties.\nIf your favorite method is missing, either \nopen an issue\n or\n\nintegrate a learning method yourself\n.\n\n\nThis basic introduction demonstrates how to use already implemented learners.\n\n\nConstructing a learner\n\n\nA learner in \nmlr\n is generated by calling \nmakeLearner\n.\nIn the constructor, you need to specify which learning method you want to use.\nYou may also:\n\n\n\n\nSet hyperparameters\n\n\nControl the prediction output type, e.g., for classification\n  whether you want a factor of predicted class labels or probabilities.\n\n\nSet an ID to name the object (some methods will later use this ID to label results or annotate plots).\n\n\n\n\n## Classification tree, set it up for predicting probabilities\nclassif.lrn = makeLearner(\nclassif.randomForest\n, predict.type = \nprob\n, fix.factors.prediction = TRUE)\n\n## Regression gradient boosting machine, specify hyperparameters via a list\nregr.lrn = makeLearner(\nregr.gbm\n, par.vals = list(n.trees = 500, interaction.depth = 3))\n\n## Cox proportional hazards model with custom name\nsurv.lrn = makeLearner(\nsurv.coxph\n, id = \ncph\n)\n\n## K-means with 5 clusters\ncluster.lrn = makeLearner(\ncluster.kmeans\n, centers = 5)\n\n## Multilabel Random Ferns classification algorithm\nmultilabel.lrn = makeLearner(\nmultilabel.rFerns\n)\n\n\n\n\nThe first argument specifies which algorithm to use.\nThe naming convention is \nclassif.\nR_method_name\n for\nclassification methods, \nregr.\nR_method_name\n for regression methods,\n\nsurv.\nR_method_name\n for survival analysis, \ncluster.\nR_method_name\n\nfor clustering methods, and \nmultilabel.\nR_method_name\n for multilabel classification.\n\n\nHyperparameter values can be specified either via the \n...\n argument or as a \nlist\n\nvia \npar.vals\n.\n\n\nOccasionally, \nfactor\n features may cause problems when fewer levels are present in the\ntest data set than in the training data.\nWe can avoid this by setting \nfix.factors.prediction = TRUE\n to add a factor level for missing data in the test data set.\n\n\nLet's have a look at two of the learners created above:\n\n\nclassif.lrn\n#\n Learner classif.randomForest from package randomForest\n#\n Type: classif\n#\n Name: Random Forest; Short name: rf\n#\n Class: classif.randomForest\n#\n Properties: twoclass,multiclass,numerics,factors,ordered,prob,class.weights,oobpreds,featimp\n#\n Predict-Type: prob\n#\n Hyperparameters:\n\nsurv.lrn\n#\n Learner cph from package survival\n#\n Type: surv\n#\n Name: Cox Proportional Hazard Model; Short name: coxph\n#\n Class: surv.coxph\n#\n Properties: numerics,factors,weights\n#\n Predict-Type: response\n#\n Hyperparameters:\n\n\n\n\nAll generated learners are objects of class \nLearner\n.\nThis class contains the properties of the method, e.g., which types of features it can handle,\nwhat kind of output is possible during prediction, and whether multi-class problems,\nobservations weights or missing values are supported.\n\n\nThere is currently no special learner class for cost-sensitive classification.\nFor ordinary misclassification costs, you can use standard classification methods.\nFor example-dependent costs, there are several ways to generate cost-sensitive learners from ordinary\nregression and classification learners. This is explained in greater detail in the\n\ncost-sensitive classification tutorial\n.\n\n\nAccessing a learner\n\n\nThe \nLearner\n object is a \nlist\n and the following elements contain\ninformation regarding the hyperparameters and the type of prediction.\n\n\n## Get the configured hyperparameter settings that deviate from the defaults\ncluster.lrn$par.vals\n#\n $centers\n#\n [1] 5\n\n## Get the set of hyperparameters\nclassif.lrn$par.set\n#\n                      Type  len   Def   Constr Req Tunable Trafo\n#\n ntree             integer    -   500 1 to Inf   -    TRUE     -\n#\n mtry              integer    -     - 1 to Inf   -    TRUE     -\n#\n replace           logical    -  TRUE        -   -    TRUE     -\n#\n classwt     numericvector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n cutoff      numericvector \nNA\n     -   0 to 1   -    TRUE     -\n#\n strata            untyped    -     -        -   -   FALSE     -\n#\n sampsize    integervector \nNA\n     - 1 to Inf   -    TRUE     -\n#\n nodesize          integer    -     1 1 to Inf   -    TRUE     -\n#\n maxnodes          integer    -     - 1 to Inf   -    TRUE     -\n#\n importance        logical    - FALSE        -   -    TRUE     -\n#\n localImp          logical    - FALSE        -   -    TRUE     -\n#\n proximity         logical    - FALSE        -   -   FALSE     -\n#\n oob.prox          logical    -     -        -   Y   FALSE     -\n#\n norm.votes        logical    -  TRUE        -   -   FALSE     -\n#\n do.trace          logical    - FALSE        -   -   FALSE     -\n#\n keep.forest       logical    -  TRUE        -   -   FALSE     -\n#\n keep.inbag        logical    - FALSE        -   -   FALSE     -\n\n## Get the type of prediction\nregr.lrn$predict.type\n#\n [1] \nresponse\n\n\n\n\n\nSlot \n$par.set\n is an object of class \nParamSet\n\ncontaining the type of hyperparameters (e.g., numeric, logical), potential\ndefault values and the range of allowed values.\n\n\nmlr\n provides function \ngetHyperPars\n or its alternative \ngetLearnerParVals\n to access the current hyperparameter setting\nof a \nLearner\n and \ngetParamSet\n to get a description of all possible settings.\n\n\nThese are particularly useful with wrapped \nLearner\ns,\nsuch as a learner fused with a feature selection strategy, where both the learner and feature selection strategy\nhave hyperparameters. For details see the \nwrapped learners tutorial\n.\n\n\n## Get current hyperparameter settings\ngetHyperPars(cluster.lrn)\n#\n $centers\n#\n [1] 5\n\n## Get a description of all possible hyperparameter settings\ngetParamSet(classif.lrn)\n#\n                      Type  len   Def   Constr Req Tunable Trafo\n#\n ntree             integer    -   500 1 to Inf   -    TRUE     -\n#\n mtry              integer    -     - 1 to Inf   -    TRUE     -\n#\n replace           logical    -  TRUE        -   -    TRUE     -\n#\n classwt     numericvector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n cutoff      numericvector \nNA\n     -   0 to 1   -    TRUE     -\n#\n strata            untyped    -     -        -   -   FALSE     -\n#\n sampsize    integervector \nNA\n     - 1 to Inf   -    TRUE     -\n#\n nodesize          integer    -     1 1 to Inf   -    TRUE     -\n#\n maxnodes          integer    -     - 1 to Inf   -    TRUE     -\n#\n importance        logical    - FALSE        -   -    TRUE     -\n#\n localImp          logical    - FALSE        -   -    TRUE     -\n#\n proximity         logical    - FALSE        -   -   FALSE     -\n#\n oob.prox          logical    -     -        -   Y   FALSE     -\n#\n norm.votes        logical    -  TRUE        -   -   FALSE     -\n#\n do.trace          logical    - FALSE        -   -   FALSE     -\n#\n keep.forest       logical    -  TRUE        -   -   FALSE     -\n#\n keep.inbag        logical    - FALSE        -   -   FALSE     -\n\n\n\n\nWe can also use \ngetParamSet\n or its alias \ngetLearnerParamSet\n to get a quick overview about the available\nhyperparameters and defaults of a learning method without explicitly constructing it (by calling \nmakeLearner\n).\n\n\ngetParamSet(\nclassif.randomForest\n)\n#\n                      Type  len   Def   Constr Req Tunable Trafo\n#\n ntree             integer    -   500 1 to Inf   -    TRUE     -\n#\n mtry              integer    -     - 1 to Inf   -    TRUE     -\n#\n replace           logical    -  TRUE        -   -    TRUE     -\n#\n classwt     numericvector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n cutoff      numericvector \nNA\n     -   0 to 1   -    TRUE     -\n#\n strata            untyped    -     -        -   -   FALSE     -\n#\n sampsize    integervector \nNA\n     - 1 to Inf   -    TRUE     -\n#\n nodesize          integer    -     1 1 to Inf   -    TRUE     -\n#\n maxnodes          integer    -     - 1 to Inf   -    TRUE     -\n#\n importance        logical    - FALSE        -   -    TRUE     -\n#\n localImp          logical    - FALSE        -   -    TRUE     -\n#\n proximity         logical    - FALSE        -   -   FALSE     -\n#\n oob.prox          logical    -     -        -   Y   FALSE     -\n#\n norm.votes        logical    -  TRUE        -   -   FALSE     -\n#\n do.trace          logical    - FALSE        -   -   FALSE     -\n#\n keep.forest       logical    -  TRUE        -   -   FALSE     -\n#\n keep.inbag        logical    - FALSE        -   -   FALSE     -\n\n\n\n\nFunctions for accessing a Learner's meta information are available in \nmlr\n. We can use \ngetLearnerId\n, \ngetLearnerShortName\n and  \ngetLearnerType\n. To show the required packages for a Learner, use\n\ngetLearnerPackages\n.\n\n\n## Get object's id\ngetLearnerId(surv.lrn)\n#\n [1] \ncph\n\n\n## Get the short name\ngetLearnerShortName(classif.lrn)\n#\n [1] \nrf\n\n\n## Get the type of the learner\ngetLearnerType(multilabel.lrn)\n#\n [1] \nmultilabel\n\n\n## Get required packages\ngetLearnerPackages(cluster.lrn)\n#\n [1] \nstats\n \nclue\n\n\n\n\n\nModifying a learner\n\n\nWe also provide functions that enable you to change certain aspects\nof a \nLearner\n without needing to create a new \nLearner\n from scratch.\nHere are some examples:\n\n\n## Change the ID\nsurv.lrn = setLearnerId(surv.lrn, \nCoxModel\n)\nsurv.lrn\n#\n Learner CoxModel from package survival\n#\n Type: surv\n#\n Name: Cox Proportional Hazard Model; Short name: coxph\n#\n Class: surv.coxph\n#\n Properties: numerics,factors,weights\n#\n Predict-Type: response\n#\n Hyperparameters:\n\n## Change the prediction type, predict a factor with class labels instead of probabilities\nclassif.lrn = setPredictType(classif.lrn, \nresponse\n)\n\n## Change hyperparameter values\ncluster.lrn = setHyperPars(cluster.lrn, centers = 4)\n\n## Go back to default hyperparameter values\nregr.lrn = removeHyperPars(regr.lrn, c(\nn.trees\n, \ninteraction.depth\n))\n\n\n\n\nListing learners\n\n\nSee the \nAppendix\n for a list of all learners integrated in \nmlr\n\nalong with their respective properties.\n\n\nIf you would like a list of available learners with certain properties or suitable for a\nparticular learning \nTask\n, use function \nlistLearners\n.\n\n\n## List everything in mlr\nlrns = listLearners()\nhead(lrns[c(\nclass\n, \npackage\n)])\n#\n                 class      package\n#\n 1         classif.ada    ada,rpart\n#\n 2  classif.adaboostm1        RWeka\n#\n 3 classif.bartMachine  bartMachine\n#\n 4    classif.binomial        stats\n#\n 5  classif.blackboost mboost,party\n#\n 6    classif.boosting adabag,rpart\n\n## List classifiers that can output probabilities\nlrns = listLearners(\nclassif\n, properties = \nprob\n)\nhead(lrns[c(\nclass\n, \npackage\n)])\n#\n                 class      package\n#\n 1         classif.ada    ada,rpart\n#\n 2  classif.adaboostm1        RWeka\n#\n 3 classif.bartMachine  bartMachine\n#\n 4    classif.binomial        stats\n#\n 5  classif.blackboost mboost,party\n#\n 6    classif.boosting adabag,rpart\n\n## List classifiers that can be applied to iris (i.e., multiclass) and output probabilities\nlrns = listLearners(iris.task, properties = \nprob\n)\nhead(lrns[c(\nclass\n, \npackage\n)])\n#\n                class      package\n#\n 1 classif.adaboostm1        RWeka\n#\n 2   classif.boosting adabag,rpart\n#\n 3        classif.C50          C50\n#\n 4    classif.cforest        party\n#\n 5      classif.ctree        party\n#\n 6   classif.cvglmnet       glmnet\n\n## The calls above return character vectors, but you can also create learner objects\nhead(listLearners(\ncluster\n, create = TRUE), 2)\n#\n [[1]]\n#\n Learner cluster.cmeans from package e1071,clue\n#\n Type: cluster\n#\n Name: Fuzzy C-Means Clustering; Short name: cmeans\n#\n Class: cluster.cmeans\n#\n Properties: numerics,prob\n#\n Predict-Type: response\n#\n Hyperparameters: centers=2\n#\n \n#\n \n#\n [[2]]\n#\n Learner cluster.Cobweb from package RWeka\n#\n Type: cluster\n#\n Name: Cobweb Clustering Algorithm; Short name: cobweb\n#\n Class: cluster.Cobweb\n#\n Properties: numerics\n#\n Predict-Type: response\n#\n Hyperparameters:\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## Classification tree, set it up for predicting probabilities \nclassif.lrn = makeLearner(\nclassif.randomForest\n, predict.type = \nprob\n, fix.factors.prediction = TRUE) \n\n## Regression gradient boosting machine, specify hyperparameters via a list \nregr.lrn = makeLearner(\nregr.gbm\n, par.vals = list(n.trees = 500, interaction.depth = 3)) \n\n## Cox proportional hazards model with custom name \nsurv.lrn = makeLearner(\nsurv.coxph\n, id = \ncph\n) \n\n## K-means with 5 clusters \ncluster.lrn = makeLearner(\ncluster.kmeans\n, centers = 5) \n\n## Multilabel Random Ferns classification algorithm \nmultilabel.lrn = makeLearner(\nmultilabel.rFerns\n) \nclassif.lrn \n\nsurv.lrn \n## Get the configured hyperparameter settings that deviate from the defaults \ncluster.lrn$par.vals \n\n## Get the set of hyperparameters \nclassif.lrn$par.set \n\n## Get the type of prediction \nregr.lrn$predict.type \n## Get current hyperparameter settings \ngetHyperPars(cluster.lrn) \n\n## Get a description of all possible hyperparameter settings \ngetParamSet(classif.lrn) \ngetParamSet(\nclassif.randomForest\n) \n## Get object's id \ngetLearnerId(surv.lrn) \n\n## Get the short name \ngetLearnerShortName(classif.lrn) \n\n## Get the type of the learner \ngetLearnerType(multilabel.lrn) \n\n## Get required packages \ngetLearnerPackages(cluster.lrn) \n## Change the ID \nsurv.lrn = setLearnerId(surv.lrn, \nCoxModel\n) \nsurv.lrn \n\n## Change the prediction type, predict a factor with class labels instead of probabilities \nclassif.lrn = setPredictType(classif.lrn, \nresponse\n) \n\n## Change hyperparameter values \ncluster.lrn = setHyperPars(cluster.lrn, centers = 4) \n\n## Go back to default hyperparameter values \nregr.lrn = removeHyperPars(regr.lrn, c(\nn.trees\n, \ninteraction.depth\n)) \n## List everything in mlr \nlrns = listLearners() \nhead(lrns[c(\nclass\n, \npackage\n)]) \n\n## List classifiers that can output probabilities \nlrns = listLearners(\nclassif\n, properties = \nprob\n) \nhead(lrns[c(\nclass\n, \npackage\n)]) \n\n## List classifiers that can be applied to iris (i.e., multiclass) and output probabilities \nlrns = listLearners(iris.task, properties = \nprob\n) \nhead(lrns[c(\nclass\n, \npackage\n)]) \n\n## The calls above return character vectors, but you can also create learner objects \nhead(listLearners(\ncluster\n, create = TRUE), 2)", 
            "title": "Learners"
        }, 
        {
            "location": "/learner/index.html#learners", 
            "text": "The following classes provide a unified interface to all popular machine learning methods in  R :\n(cost-sensitive) classification, regression, survival analysis, and clustering.\nMany are already integrated, and  mlr  is specifically designed to make extensions simple.  See the  integrated learners page  to reference already\nimplemented machine learning methods and their properties.\nIf your favorite method is missing, either  open an issue  or integrate a learning method yourself .  This basic introduction demonstrates how to use already implemented learners.", 
            "title": "Learners"
        }, 
        {
            "location": "/learner/index.html#constructing-a-learner", 
            "text": "A learner in  mlr  is generated by calling  makeLearner .\nIn the constructor, you need to specify which learning method you want to use.\nYou may also:   Set hyperparameters  Control the prediction output type, e.g., for classification\n  whether you want a factor of predicted class labels or probabilities.  Set an ID to name the object (some methods will later use this ID to label results or annotate plots).   ## Classification tree, set it up for predicting probabilities\nclassif.lrn = makeLearner( classif.randomForest , predict.type =  prob , fix.factors.prediction = TRUE)\n\n## Regression gradient boosting machine, specify hyperparameters via a list\nregr.lrn = makeLearner( regr.gbm , par.vals = list(n.trees = 500, interaction.depth = 3))\n\n## Cox proportional hazards model with custom name\nsurv.lrn = makeLearner( surv.coxph , id =  cph )\n\n## K-means with 5 clusters\ncluster.lrn = makeLearner( cluster.kmeans , centers = 5)\n\n## Multilabel Random Ferns classification algorithm\nmultilabel.lrn = makeLearner( multilabel.rFerns )  The first argument specifies which algorithm to use.\nThe naming convention is  classif. R_method_name  for\nclassification methods,  regr. R_method_name  for regression methods, surv. R_method_name  for survival analysis,  cluster. R_method_name \nfor clustering methods, and  multilabel. R_method_name  for multilabel classification.  Hyperparameter values can be specified either via the  ...  argument or as a  list \nvia  par.vals .  Occasionally,  factor  features may cause problems when fewer levels are present in the\ntest data set than in the training data.\nWe can avoid this by setting  fix.factors.prediction = TRUE  to add a factor level for missing data in the test data set.  Let's have a look at two of the learners created above:  classif.lrn\n#  Learner classif.randomForest from package randomForest\n#  Type: classif\n#  Name: Random Forest; Short name: rf\n#  Class: classif.randomForest\n#  Properties: twoclass,multiclass,numerics,factors,ordered,prob,class.weights,oobpreds,featimp\n#  Predict-Type: prob\n#  Hyperparameters:\n\nsurv.lrn\n#  Learner cph from package survival\n#  Type: surv\n#  Name: Cox Proportional Hazard Model; Short name: coxph\n#  Class: surv.coxph\n#  Properties: numerics,factors,weights\n#  Predict-Type: response\n#  Hyperparameters:  All generated learners are objects of class  Learner .\nThis class contains the properties of the method, e.g., which types of features it can handle,\nwhat kind of output is possible during prediction, and whether multi-class problems,\nobservations weights or missing values are supported.  There is currently no special learner class for cost-sensitive classification.\nFor ordinary misclassification costs, you can use standard classification methods.\nFor example-dependent costs, there are several ways to generate cost-sensitive learners from ordinary\nregression and classification learners. This is explained in greater detail in the cost-sensitive classification tutorial .", 
            "title": "Constructing a learner"
        }, 
        {
            "location": "/learner/index.html#accessing-a-learner", 
            "text": "The  Learner  object is a  list  and the following elements contain\ninformation regarding the hyperparameters and the type of prediction.  ## Get the configured hyperparameter settings that deviate from the defaults\ncluster.lrn$par.vals\n#  $centers\n#  [1] 5\n\n## Get the set of hyperparameters\nclassif.lrn$par.set\n#                       Type  len   Def   Constr Req Tunable Trafo\n#  ntree             integer    -   500 1 to Inf   -    TRUE     -\n#  mtry              integer    -     - 1 to Inf   -    TRUE     -\n#  replace           logical    -  TRUE        -   -    TRUE     -\n#  classwt     numericvector  NA      - 0 to Inf   -    TRUE     -\n#  cutoff      numericvector  NA      -   0 to 1   -    TRUE     -\n#  strata            untyped    -     -        -   -   FALSE     -\n#  sampsize    integervector  NA      - 1 to Inf   -    TRUE     -\n#  nodesize          integer    -     1 1 to Inf   -    TRUE     -\n#  maxnodes          integer    -     - 1 to Inf   -    TRUE     -\n#  importance        logical    - FALSE        -   -    TRUE     -\n#  localImp          logical    - FALSE        -   -    TRUE     -\n#  proximity         logical    - FALSE        -   -   FALSE     -\n#  oob.prox          logical    -     -        -   Y   FALSE     -\n#  norm.votes        logical    -  TRUE        -   -   FALSE     -\n#  do.trace          logical    - FALSE        -   -   FALSE     -\n#  keep.forest       logical    -  TRUE        -   -   FALSE     -\n#  keep.inbag        logical    - FALSE        -   -   FALSE     -\n\n## Get the type of prediction\nregr.lrn$predict.type\n#  [1]  response   Slot  $par.set  is an object of class  ParamSet \ncontaining the type of hyperparameters (e.g., numeric, logical), potential\ndefault values and the range of allowed values.  mlr  provides function  getHyperPars  or its alternative  getLearnerParVals  to access the current hyperparameter setting\nof a  Learner  and  getParamSet  to get a description of all possible settings.  These are particularly useful with wrapped  Learner s,\nsuch as a learner fused with a feature selection strategy, where both the learner and feature selection strategy\nhave hyperparameters. For details see the  wrapped learners tutorial .  ## Get current hyperparameter settings\ngetHyperPars(cluster.lrn)\n#  $centers\n#  [1] 5\n\n## Get a description of all possible hyperparameter settings\ngetParamSet(classif.lrn)\n#                       Type  len   Def   Constr Req Tunable Trafo\n#  ntree             integer    -   500 1 to Inf   -    TRUE     -\n#  mtry              integer    -     - 1 to Inf   -    TRUE     -\n#  replace           logical    -  TRUE        -   -    TRUE     -\n#  classwt     numericvector  NA      - 0 to Inf   -    TRUE     -\n#  cutoff      numericvector  NA      -   0 to 1   -    TRUE     -\n#  strata            untyped    -     -        -   -   FALSE     -\n#  sampsize    integervector  NA      - 1 to Inf   -    TRUE     -\n#  nodesize          integer    -     1 1 to Inf   -    TRUE     -\n#  maxnodes          integer    -     - 1 to Inf   -    TRUE     -\n#  importance        logical    - FALSE        -   -    TRUE     -\n#  localImp          logical    - FALSE        -   -    TRUE     -\n#  proximity         logical    - FALSE        -   -   FALSE     -\n#  oob.prox          logical    -     -        -   Y   FALSE     -\n#  norm.votes        logical    -  TRUE        -   -   FALSE     -\n#  do.trace          logical    - FALSE        -   -   FALSE     -\n#  keep.forest       logical    -  TRUE        -   -   FALSE     -\n#  keep.inbag        logical    - FALSE        -   -   FALSE     -  We can also use  getParamSet  or its alias  getLearnerParamSet  to get a quick overview about the available\nhyperparameters and defaults of a learning method without explicitly constructing it (by calling  makeLearner ).  getParamSet( classif.randomForest )\n#                       Type  len   Def   Constr Req Tunable Trafo\n#  ntree             integer    -   500 1 to Inf   -    TRUE     -\n#  mtry              integer    -     - 1 to Inf   -    TRUE     -\n#  replace           logical    -  TRUE        -   -    TRUE     -\n#  classwt     numericvector  NA      - 0 to Inf   -    TRUE     -\n#  cutoff      numericvector  NA      -   0 to 1   -    TRUE     -\n#  strata            untyped    -     -        -   -   FALSE     -\n#  sampsize    integervector  NA      - 1 to Inf   -    TRUE     -\n#  nodesize          integer    -     1 1 to Inf   -    TRUE     -\n#  maxnodes          integer    -     - 1 to Inf   -    TRUE     -\n#  importance        logical    - FALSE        -   -    TRUE     -\n#  localImp          logical    - FALSE        -   -    TRUE     -\n#  proximity         logical    - FALSE        -   -   FALSE     -\n#  oob.prox          logical    -     -        -   Y   FALSE     -\n#  norm.votes        logical    -  TRUE        -   -   FALSE     -\n#  do.trace          logical    - FALSE        -   -   FALSE     -\n#  keep.forest       logical    -  TRUE        -   -   FALSE     -\n#  keep.inbag        logical    - FALSE        -   -   FALSE     -  Functions for accessing a Learner's meta information are available in  mlr . We can use  getLearnerId ,  getLearnerShortName  and   getLearnerType . To show the required packages for a Learner, use getLearnerPackages .  ## Get object's id\ngetLearnerId(surv.lrn)\n#  [1]  cph \n\n## Get the short name\ngetLearnerShortName(classif.lrn)\n#  [1]  rf \n\n## Get the type of the learner\ngetLearnerType(multilabel.lrn)\n#  [1]  multilabel \n\n## Get required packages\ngetLearnerPackages(cluster.lrn)\n#  [1]  stats   clue", 
            "title": "Accessing a learner"
        }, 
        {
            "location": "/learner/index.html#modifying-a-learner", 
            "text": "We also provide functions that enable you to change certain aspects\nof a  Learner  without needing to create a new  Learner  from scratch.\nHere are some examples:  ## Change the ID\nsurv.lrn = setLearnerId(surv.lrn,  CoxModel )\nsurv.lrn\n#  Learner CoxModel from package survival\n#  Type: surv\n#  Name: Cox Proportional Hazard Model; Short name: coxph\n#  Class: surv.coxph\n#  Properties: numerics,factors,weights\n#  Predict-Type: response\n#  Hyperparameters:\n\n## Change the prediction type, predict a factor with class labels instead of probabilities\nclassif.lrn = setPredictType(classif.lrn,  response )\n\n## Change hyperparameter values\ncluster.lrn = setHyperPars(cluster.lrn, centers = 4)\n\n## Go back to default hyperparameter values\nregr.lrn = removeHyperPars(regr.lrn, c( n.trees ,  interaction.depth ))", 
            "title": "Modifying a learner"
        }, 
        {
            "location": "/learner/index.html#listing-learners", 
            "text": "See the  Appendix  for a list of all learners integrated in  mlr \nalong with their respective properties.  If you would like a list of available learners with certain properties or suitable for a\nparticular learning  Task , use function  listLearners .  ## List everything in mlr\nlrns = listLearners()\nhead(lrns[c( class ,  package )])\n#                  class      package\n#  1         classif.ada    ada,rpart\n#  2  classif.adaboostm1        RWeka\n#  3 classif.bartMachine  bartMachine\n#  4    classif.binomial        stats\n#  5  classif.blackboost mboost,party\n#  6    classif.boosting adabag,rpart\n\n## List classifiers that can output probabilities\nlrns = listLearners( classif , properties =  prob )\nhead(lrns[c( class ,  package )])\n#                  class      package\n#  1         classif.ada    ada,rpart\n#  2  classif.adaboostm1        RWeka\n#  3 classif.bartMachine  bartMachine\n#  4    classif.binomial        stats\n#  5  classif.blackboost mboost,party\n#  6    classif.boosting adabag,rpart\n\n## List classifiers that can be applied to iris (i.e., multiclass) and output probabilities\nlrns = listLearners(iris.task, properties =  prob )\nhead(lrns[c( class ,  package )])\n#                 class      package\n#  1 classif.adaboostm1        RWeka\n#  2   classif.boosting adabag,rpart\n#  3        classif.C50          C50\n#  4    classif.cforest        party\n#  5      classif.ctree        party\n#  6   classif.cvglmnet       glmnet\n\n## The calls above return character vectors, but you can also create learner objects\nhead(listLearners( cluster , create = TRUE), 2)\n#  [[1]]\n#  Learner cluster.cmeans from package e1071,clue\n#  Type: cluster\n#  Name: Fuzzy C-Means Clustering; Short name: cmeans\n#  Class: cluster.cmeans\n#  Properties: numerics,prob\n#  Predict-Type: response\n#  Hyperparameters: centers=2\n#  \n#  \n#  [[2]]\n#  Learner cluster.Cobweb from package RWeka\n#  Type: cluster\n#  Name: Cobweb Clustering Algorithm; Short name: cobweb\n#  Class: cluster.Cobweb\n#  Properties: numerics\n#  Predict-Type: response\n#  Hyperparameters:", 
            "title": "Listing learners"
        }, 
        {
            "location": "/learner/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## Classification tree, set it up for predicting probabilities \nclassif.lrn = makeLearner( classif.randomForest , predict.type =  prob , fix.factors.prediction = TRUE) \n\n## Regression gradient boosting machine, specify hyperparameters via a list \nregr.lrn = makeLearner( regr.gbm , par.vals = list(n.trees = 500, interaction.depth = 3)) \n\n## Cox proportional hazards model with custom name \nsurv.lrn = makeLearner( surv.coxph , id =  cph ) \n\n## K-means with 5 clusters \ncluster.lrn = makeLearner( cluster.kmeans , centers = 5) \n\n## Multilabel Random Ferns classification algorithm \nmultilabel.lrn = makeLearner( multilabel.rFerns ) \nclassif.lrn \n\nsurv.lrn \n## Get the configured hyperparameter settings that deviate from the defaults \ncluster.lrn$par.vals \n\n## Get the set of hyperparameters \nclassif.lrn$par.set \n\n## Get the type of prediction \nregr.lrn$predict.type \n## Get current hyperparameter settings \ngetHyperPars(cluster.lrn) \n\n## Get a description of all possible hyperparameter settings \ngetParamSet(classif.lrn) \ngetParamSet( classif.randomForest ) \n## Get object's id \ngetLearnerId(surv.lrn) \n\n## Get the short name \ngetLearnerShortName(classif.lrn) \n\n## Get the type of the learner \ngetLearnerType(multilabel.lrn) \n\n## Get required packages \ngetLearnerPackages(cluster.lrn) \n## Change the ID \nsurv.lrn = setLearnerId(surv.lrn,  CoxModel ) \nsurv.lrn \n\n## Change the prediction type, predict a factor with class labels instead of probabilities \nclassif.lrn = setPredictType(classif.lrn,  response ) \n\n## Change hyperparameter values \ncluster.lrn = setHyperPars(cluster.lrn, centers = 4) \n\n## Go back to default hyperparameter values \nregr.lrn = removeHyperPars(regr.lrn, c( n.trees ,  interaction.depth )) \n## List everything in mlr \nlrns = listLearners() \nhead(lrns[c( class ,  package )]) \n\n## List classifiers that can output probabilities \nlrns = listLearners( classif , properties =  prob ) \nhead(lrns[c( class ,  package )]) \n\n## List classifiers that can be applied to iris (i.e., multiclass) and output probabilities \nlrns = listLearners(iris.task, properties =  prob ) \nhead(lrns[c( class ,  package )]) \n\n## The calls above return character vectors, but you can also create learner objects \nhead(listLearners( cluster , create = TRUE), 2)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/train/index.html", 
            "text": "Training a Learner\n\n\nTraining a learner means fitting a model to a given data set.\nIn \nmlr\n this can be done by calling function \ntrain\n\non a \nLearner\n and a suitable \nTask\n.\n\n\nWe start with a classification example and perform a \nlinear discriminant analysis\n\non the \niris\n data set.\n\n\n## Generate the task\ntask = makeClassifTask(data = iris, target = \nSpecies\n)\n\n## Generate the learner\nlrn = makeLearner(\nclassif.lda\n)\n\n## Train the learner\nmod = train(lrn, task)\nmod\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = iris; obs = 150; features = 4\n#\n Hyperparameters:\n\n\n\n\nIn the above example creating the \nLearner\n explicitly is not absolutely necessary.\nAs a general rule, you have to generate the \nLearner\n yourself if you want\nto change any defaults, e.g., setting hyperparameter values or altering the predict type.\nOtherwise, \ntrain\n and many other functions also accept the class name of the learner and\ncall \nmakeLearner\n internally with default settings.\n\n\nmod = train(\nclassif.lda\n, task)\nmod\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = iris; obs = 150; features = 4\n#\n Hyperparameters:\n\n\n\n\nTraining a learner works the same way for every type of learning problem.\nBelow is a survival analysis example where a\n\nCox proportional hazards model\n is fitted to the \nlung\n\ndata set.\nNote that we use the corresponding \nlung.task\n provided by \nmlr\n.\nAll available \nTask\ns are listed in the \nAppendix\n.\n\n\nmod = train(\nsurv.coxph\n, lung.task)\nmod\n#\n Model for learner.id=surv.coxph; learner.class=surv.coxph\n#\n Trained on: task.id = lung-example; obs = 167; features = 8\n#\n Hyperparameters:\n\n\n\n\nAccessing learner models\n\n\nFunction \ntrain\n returns an object of class \nWrappedModel\n, which encapsulates\nthe fitted model, i.e., the output of the underlying \nR\n learning method. Additionally,\nit contains some information about the \nLearner\n, the \nTask\n, the features and\nobservations used for training, and the training time.\nA \nWrappedModel\n can subsequently be used to make a\n\nprediction\n for new observations.\n\n\nThe fitted model in slot \n$learner.model\n of the \nWrappedModel\n object\ncan be accessed using function \ngetLearnerModel\n.\n\n\nIn the following example we cluster the \nRuspini\n data set (which has\nfour groups and two features) by \nK\n-means with \nK = 4\n and extract the output of the\nunderlying \nkmeans\n function.\n\n\ndata(ruspini, package = \ncluster\n)\nplot(y ~ x, ruspini)\n\n\n\n\n\n\n## Generate the task\nruspini.task = makeClusterTask(data = ruspini)\n\n## Generate the learner\nlrn = makeLearner(\ncluster.kmeans\n, centers = 4)\n\n## Train the learner\nmod = train(lrn, ruspini.task)\nmod\n#\n Model for learner.id=cluster.kmeans; learner.class=cluster.kmeans\n#\n Trained on: task.id = ruspini; obs = 75; features = 2\n#\n Hyperparameters: centers=4\n\n## Peak into mod\nnames(mod)\n#\n [1] \nlearner\n       \nlearner.model\n \ntask.desc\n     \nsubset\n       \n#\n [5] \nfeatures\n      \nfactor.levels\n \ntime\n          \ndump\n\n\nmod$learner\n#\n Learner cluster.kmeans from package stats,clue\n#\n Type: cluster\n#\n Name: K-Means; Short name: kmeans\n#\n Class: cluster.kmeans\n#\n Properties: numerics,prob\n#\n Predict-Type: response\n#\n Hyperparameters: centers=4\n\nmod$features\n#\n [1] \nx\n \ny\n\n\nmod$time\n#\n [1] 0.001\n\n## Extract the fitted model\ngetLearnerModel(mod)\n#\n K-means clustering with 4 clusters of sizes 23, 17, 15, 20\n#\n \n#\n Cluster means:\n#\n          x        y\n#\n 1 43.91304 146.0435\n#\n 2 98.17647 114.8824\n#\n 3 68.93333  19.4000\n#\n 4 20.15000  64.9500\n#\n \n#\n Clustering vector:\n#\n  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n#\n  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  1  1  1  1  1 \n#\n 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n#\n  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2 \n#\n 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 \n#\n  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 \n#\n \n#\n Within cluster sum of squares by cluster:\n#\n [1] 3176.783 4558.235 1456.533 3689.500\n#\n  (between_SS / total_SS =  94.7 %)\n#\n \n#\n Available components:\n#\n \n#\n [1] \ncluster\n      \ncenters\n      \ntotss\n        \nwithinss\n    \n#\n [5] \ntot.withinss\n \nbetweenss\n    \nsize\n         \niter\n        \n#\n [9] \nifault\n\n\n\n\n\nFurther options and comments\n\n\nBy default, the whole data set in the \nTask\n is used for training.\nThe \nsubset\n argument of \ntrain\n takes a logical or integer vector that indicates which\nobservations to use, for example if you want to split your data into a training and a test\nset or if you want to fit separate models to different subgroups in the data.\n\n\nBelow we fit a \nlinear regression model\n to the \nBostonHousing\n\ndata set (\nbh.task\n) and randomly select 1/3 of the data set for training.\n\n\n## Get the number of observations\nn = getTaskSize(bh.task)\n\n## Use 1/3 of the observations for training\ntrain.set = sample(n, size = n/3)\n\n## Train the learner\nmod = train(\nregr.lm\n, bh.task, subset = train.set)\nmod\n#\n Model for learner.id=regr.lm; learner.class=regr.lm\n#\n Trained on: task.id = BostonHousing-example; obs = 168; features = 13\n#\n Hyperparameters:\n\n\n\n\nNote, for later, that all standard \nresampling strategies\n are supported.\nTherefore you usually do not have to subset the data yourself.\n\n\nMoreover, if the learner supports this, you can specify observation \nweights\n\nthat reflect the relevance of observations in the training process.\nWeights can be useful in many regards, for example to express the reliability of the training\nobservations, reduce the influence of outliers or, if the data were collected over a longer\ntime period, increase the influence of recent data.\nIn supervised classification weights can be used to incorporate misclassification costs or\naccount for class imbalance.\n\n\nFor example in the \nBreastCancer\n data set class \nbenign\n is almost\ntwice as frequent as class \nmalignant\n.\nIn order to grant both classes equal importance in training the classifier we can weight the\nexamples according to the inverse class frequencies in the data set as shown in the following\n\nR\n code.\n\n\n## Calculate the observation weights\ntarget = getTaskTargets(bc.task)\ntab = as.numeric(table(target))\nw = 1/tab[target]\n\ntrain(\nclassif.rpart\n, task = bc.task, weights = w)\n#\n Model for learner.id=classif.rpart; learner.class=classif.rpart\n#\n Trained on: task.id = BreastCancer_example; obs = 683; features = 9\n#\n Hyperparameters: xval=0\n\n\n\n\nNote, for later, that \nmlr\n offers much more functionality to deal with\n\nimbalanced classification problems\n.\n\n\nAs another side remark for more advanced readers:\nBy varying the weights in the calls to \ntrain\n, you could also implement your own variant of\na general boosting type algorithm on arbitrary \nmlr\n base learners.\n\n\nAs you may recall, it is also possible to set observation weights when creating the\n\nTask\n. As a general rule, you should specify them in \nmake*Task\n if the weights\nreally \"belong\" to the task and always should be used.\nOtherwise, pass them to \ntrain\n.\nThe weights in \ntrain\n take precedence over the weights in \nTask\n.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## Generate the task \ntask = makeClassifTask(data = iris, target = \nSpecies\n) \n\n## Generate the learner \nlrn = makeLearner(\nclassif.lda\n) \n\n## Train the learner \nmod = train(lrn, task) \nmod \nmod = train(\nclassif.lda\n, task) \nmod \nmod = train(\nsurv.coxph\n, lung.task) \nmod \ndata(ruspini, package = \ncluster\n) \nplot(y ~ x, ruspini) \n## Generate the task \nruspini.task = makeClusterTask(data = ruspini) \n\n## Generate the learner \nlrn = makeLearner(\ncluster.kmeans\n, centers = 4) \n\n## Train the learner \nmod = train(lrn, ruspini.task) \nmod \n\n## Peak into mod \nnames(mod) \n\nmod$learner \n\nmod$features \n\nmod$time \n\n## Extract the fitted model \ngetLearnerModel(mod) \n## Get the number of observations \nn = getTaskSize(bh.task) \n\n## Use 1/3 of the observations for training \ntrain.set = sample(n, size = n/3) \n\n## Train the learner \nmod = train(\nregr.lm\n, bh.task, subset = train.set) \nmod \n## Calculate the observation weights \ntarget = getTaskTargets(bc.task) \ntab = as.numeric(table(target)) \nw = 1/tab[target] \n\ntrain(\nclassif.rpart\n, task = bc.task, weights = w)", 
            "title": "Train"
        }, 
        {
            "location": "/train/index.html#training-a-learner", 
            "text": "Training a learner means fitting a model to a given data set.\nIn  mlr  this can be done by calling function  train \non a  Learner  and a suitable  Task .  We start with a classification example and perform a  linear discriminant analysis \non the  iris  data set.  ## Generate the task\ntask = makeClassifTask(data = iris, target =  Species )\n\n## Generate the learner\nlrn = makeLearner( classif.lda )\n\n## Train the learner\nmod = train(lrn, task)\nmod\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = iris; obs = 150; features = 4\n#  Hyperparameters:  In the above example creating the  Learner  explicitly is not absolutely necessary.\nAs a general rule, you have to generate the  Learner  yourself if you want\nto change any defaults, e.g., setting hyperparameter values or altering the predict type.\nOtherwise,  train  and many other functions also accept the class name of the learner and\ncall  makeLearner  internally with default settings.  mod = train( classif.lda , task)\nmod\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = iris; obs = 150; features = 4\n#  Hyperparameters:  Training a learner works the same way for every type of learning problem.\nBelow is a survival analysis example where a Cox proportional hazards model  is fitted to the  lung \ndata set.\nNote that we use the corresponding  lung.task  provided by  mlr .\nAll available  Task s are listed in the  Appendix .  mod = train( surv.coxph , lung.task)\nmod\n#  Model for learner.id=surv.coxph; learner.class=surv.coxph\n#  Trained on: task.id = lung-example; obs = 167; features = 8\n#  Hyperparameters:", 
            "title": "Training a Learner"
        }, 
        {
            "location": "/train/index.html#accessing-learner-models", 
            "text": "Function  train  returns an object of class  WrappedModel , which encapsulates\nthe fitted model, i.e., the output of the underlying  R  learning method. Additionally,\nit contains some information about the  Learner , the  Task , the features and\nobservations used for training, and the training time.\nA  WrappedModel  can subsequently be used to make a prediction  for new observations.  The fitted model in slot  $learner.model  of the  WrappedModel  object\ncan be accessed using function  getLearnerModel .  In the following example we cluster the  Ruspini  data set (which has\nfour groups and two features) by  K -means with  K = 4  and extract the output of the\nunderlying  kmeans  function.  data(ruspini, package =  cluster )\nplot(y ~ x, ruspini)   ## Generate the task\nruspini.task = makeClusterTask(data = ruspini)\n\n## Generate the learner\nlrn = makeLearner( cluster.kmeans , centers = 4)\n\n## Train the learner\nmod = train(lrn, ruspini.task)\nmod\n#  Model for learner.id=cluster.kmeans; learner.class=cluster.kmeans\n#  Trained on: task.id = ruspini; obs = 75; features = 2\n#  Hyperparameters: centers=4\n\n## Peak into mod\nnames(mod)\n#  [1]  learner         learner.model   task.desc       subset        \n#  [5]  features        factor.levels   time            dump \n\nmod$learner\n#  Learner cluster.kmeans from package stats,clue\n#  Type: cluster\n#  Name: K-Means; Short name: kmeans\n#  Class: cluster.kmeans\n#  Properties: numerics,prob\n#  Predict-Type: response\n#  Hyperparameters: centers=4\n\nmod$features\n#  [1]  x   y \n\nmod$time\n#  [1] 0.001\n\n## Extract the fitted model\ngetLearnerModel(mod)\n#  K-means clustering with 4 clusters of sizes 23, 17, 15, 20\n#  \n#  Cluster means:\n#           x        y\n#  1 43.91304 146.0435\n#  2 98.17647 114.8824\n#  3 68.93333  19.4000\n#  4 20.15000  64.9500\n#  \n#  Clustering vector:\n#   1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n#   4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  1  1  1  1  1 \n#  26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n#   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2 \n#  51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 \n#   2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 \n#  \n#  Within cluster sum of squares by cluster:\n#  [1] 3176.783 4558.235 1456.533 3689.500\n#   (between_SS / total_SS =  94.7 %)\n#  \n#  Available components:\n#  \n#  [1]  cluster        centers        totss          withinss     \n#  [5]  tot.withinss   betweenss      size           iter         \n#  [9]  ifault", 
            "title": "Accessing learner models"
        }, 
        {
            "location": "/train/index.html#further-options-and-comments", 
            "text": "By default, the whole data set in the  Task  is used for training.\nThe  subset  argument of  train  takes a logical or integer vector that indicates which\nobservations to use, for example if you want to split your data into a training and a test\nset or if you want to fit separate models to different subgroups in the data.  Below we fit a  linear regression model  to the  BostonHousing \ndata set ( bh.task ) and randomly select 1/3 of the data set for training.  ## Get the number of observations\nn = getTaskSize(bh.task)\n\n## Use 1/3 of the observations for training\ntrain.set = sample(n, size = n/3)\n\n## Train the learner\nmod = train( regr.lm , bh.task, subset = train.set)\nmod\n#  Model for learner.id=regr.lm; learner.class=regr.lm\n#  Trained on: task.id = BostonHousing-example; obs = 168; features = 13\n#  Hyperparameters:  Note, for later, that all standard  resampling strategies  are supported.\nTherefore you usually do not have to subset the data yourself.  Moreover, if the learner supports this, you can specify observation  weights \nthat reflect the relevance of observations in the training process.\nWeights can be useful in many regards, for example to express the reliability of the training\nobservations, reduce the influence of outliers or, if the data were collected over a longer\ntime period, increase the influence of recent data.\nIn supervised classification weights can be used to incorporate misclassification costs or\naccount for class imbalance.  For example in the  BreastCancer  data set class  benign  is almost\ntwice as frequent as class  malignant .\nIn order to grant both classes equal importance in training the classifier we can weight the\nexamples according to the inverse class frequencies in the data set as shown in the following R  code.  ## Calculate the observation weights\ntarget = getTaskTargets(bc.task)\ntab = as.numeric(table(target))\nw = 1/tab[target]\n\ntrain( classif.rpart , task = bc.task, weights = w)\n#  Model for learner.id=classif.rpart; learner.class=classif.rpart\n#  Trained on: task.id = BreastCancer_example; obs = 683; features = 9\n#  Hyperparameters: xval=0  Note, for later, that  mlr  offers much more functionality to deal with imbalanced classification problems .  As another side remark for more advanced readers:\nBy varying the weights in the calls to  train , you could also implement your own variant of\na general boosting type algorithm on arbitrary  mlr  base learners.  As you may recall, it is also possible to set observation weights when creating the Task . As a general rule, you should specify them in  make*Task  if the weights\nreally \"belong\" to the task and always should be used.\nOtherwise, pass them to  train .\nThe weights in  train  take precedence over the weights in  Task .", 
            "title": "Further options and comments"
        }, 
        {
            "location": "/train/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## Generate the task \ntask = makeClassifTask(data = iris, target =  Species ) \n\n## Generate the learner \nlrn = makeLearner( classif.lda ) \n\n## Train the learner \nmod = train(lrn, task) \nmod \nmod = train( classif.lda , task) \nmod \nmod = train( surv.coxph , lung.task) \nmod \ndata(ruspini, package =  cluster ) \nplot(y ~ x, ruspini) \n## Generate the task \nruspini.task = makeClusterTask(data = ruspini) \n\n## Generate the learner \nlrn = makeLearner( cluster.kmeans , centers = 4) \n\n## Train the learner \nmod = train(lrn, ruspini.task) \nmod \n\n## Peak into mod \nnames(mod) \n\nmod$learner \n\nmod$features \n\nmod$time \n\n## Extract the fitted model \ngetLearnerModel(mod) \n## Get the number of observations \nn = getTaskSize(bh.task) \n\n## Use 1/3 of the observations for training \ntrain.set = sample(n, size = n/3) \n\n## Train the learner \nmod = train( regr.lm , bh.task, subset = train.set) \nmod \n## Calculate the observation weights \ntarget = getTaskTargets(bc.task) \ntab = as.numeric(table(target)) \nw = 1/tab[target] \n\ntrain( classif.rpart , task = bc.task, weights = w)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/predict/index.html", 
            "text": "Predicting Outcomes for New Data\n\n\nPredicting the target values for new observations is\nimplemented the same way as most of the other predict methods in \nR\n.\nIn general, all you need to do is call \npredict\n on the object\nreturned by \ntrain\n and pass the data you want predictions for.\n\n\nThere are two ways to pass the data:\n\n\n\n\nEither pass the \nTask\n via the \ntask\n argument or\n\n\npass a \ndata.frame\n via the \nnewdata\n argument.\n\n\n\n\nThe first way is preferable if you want predictions for data already included in a \nTask\n.\n\n\nJust as \ntrain\n, the \npredict\n function has a \nsubset\n argument,\nso you can set aside different portions of the data in \nTask\n for training and prediction\n(more advanced methods for splitting the data in train and test set are described in the\n\nsection on resampling\n).\n\n\nIn the following example we fit a \ngradient boosting machine\n to every second\nobservation of the \nBostonHousing\n data set and make predictions\non the remaining data in \nbh.task\n.\n\n\nn = getTaskSize(bh.task)\ntrain.set = seq(1, n, by = 2)\ntest.set = seq(2, n, by = 2)\nlrn = makeLearner(\nregr.gbm\n, n.trees = 100)\nmod = train(lrn, bh.task, subset = train.set)\n\ntask.pred = predict(mod, task = bh.task, subset = test.set)\ntask.pred\n#\n Prediction: 253 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n    id truth response\n#\n 2   2  21.6 22.28539\n#\n 4   4  33.4 23.33968\n#\n 6   6  28.7 22.40896\n#\n 8   8  27.1 22.12750\n#\n 10 10  18.9 22.12750\n#\n 12 12  18.9 22.12750\n#\n ... (#rows: 253, #cols: 3)\n\n\n\n\nThe second way is useful if you want to predict data not included in the \nTask\n.\n\n\nHere we cluster the \niris\n data set without the target variable.\nAll observations with an odd index are included in the \nTask\n and used for training.\nPredictions are made for the remaining observations.\n\n\nn = nrow(iris)\niris.train = iris[seq(1, n, by = 2), -5]\niris.test = iris[seq(2, n, by = 2), -5]\ntask = makeClusterTask(data = iris.train)\nmod = train(\ncluster.kmeans\n, task)\n\nnewdata.pred = predict(mod, newdata = iris.test)\nnewdata.pred\n#\n Prediction: 75 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n    response\n#\n 2         2\n#\n 4         2\n#\n 6         2\n#\n 8         2\n#\n 10        2\n#\n 12        2\n#\n ... (#rows: 75, #cols: 1)\n\n\n\n\nNote that for supervised learning you do not have to remove the target columns from the data.\nThese columns are automatically removed prior to calling the underlying \npredict\n method of the learner.\n\n\nAccessing the prediction\n\n\nFunction \npredict\n returns a named \nlist\n of class \nPrediction\n.\nIts most important element is \n$data\n which is a \ndata.frame\n that contains\ncolumns with the true values of the target variable (in case of supervised learning problems)\nand the predictions.\nUse \nas.data.frame\n for direct access.\n\n\nIn the following the predictions on the \nBostonHousing\n and the\n\niris\n data sets are shown.\nAs you may recall, the predictions in the first case were made from a \nTask\n and in the\nsecond case from a \ndata.frame\n.\n\n\n## Result of predict with data passed via task argument\nhead(as.data.frame(task.pred))\n#\n    id truth response\n#\n 2   2  21.6 22.28539\n#\n 4   4  33.4 23.33968\n#\n 6   6  28.7 22.40896\n#\n 8   8  27.1 22.12750\n#\n 10 10  18.9 22.12750\n#\n 12 12  18.9 22.12750\n\n## Result of predict with data passed via newdata argument\nhead(as.data.frame(newdata.pred))\n#\n    response\n#\n 2         2\n#\n 4         2\n#\n 6         2\n#\n 8         2\n#\n 10        2\n#\n 12        2\n\n\n\n\nAs you can see when predicting from a \nTask\n, the resulting \ndata.frame\n\ncontains an additional column, called \nid\n, which tells us which element in the original data set\nthe prediction corresponds to.\n\n\nA direct way to access the true and predicted values of the target variable(s) is provided by\nfunctions \ngetPredictionTruth\n and \ngetPredictionResponse\n.\n\n\nhead(getPredictionTruth(task.pred))\n#\n [1] 21.6 33.4 28.7 27.1 18.9 18.9\n\nhead(getPredictionResponse(task.pred))\n#\n [1] 22.28539 23.33968 22.40896 22.12750 22.12750 22.12750\n\n\n\n\nRegression: Extracting standard errors\n\n\nSome learners provide standard errors for predictions, which can be accessed in \nmlr\n.\nAn overview is given by calling the function \nlistLearners\n and setting \nproperties = \"se\"\n.\nBy assigning \nFALSE\n to \ncheck.packages\n learners from packages which are not installed\nwill be included in the overview.\n\n\nlistLearners(\nregr\n, check.packages = FALSE, properties = \nse\n)[c(\nclass\n, \nname\n)]\n#\n          class\n#\n 1   regr.bcart\n#\n 2     regr.bgp\n#\n 3  regr.bgpllm\n#\n 4     regr.blm\n#\n 5    regr.btgp\n#\n 6 regr.btgpllm\n#\n                                                                      name\n#\n 1                                                           Bayesian CART\n#\n 2                                               Bayesian Gaussian Process\n#\n 3       Bayesian Gaussian Process with jumps to the Limiting Linear Model\n#\n 4                                                   Bayesian Linear Model\n#\n 5                                         Bayesian Treed Gaussian Process\n#\n 6 Bayesian Treed Gaussian Process with jumps to the Limiting Linear Model\n#\n ... (#rows: 16, #cols: 2)\n\n\n\n\nIn this example we train a \nlinear regression model\n on the \nBoston Housing\n\ndataset. In order to calculate standard errors set the \npredict.type\n to \n\"se\"\n:\n\n\n## Create learner and specify predict.type\nlrn.lm = makeLearner(\nregr.lm\n, predict.type = 'se')\nmod.lm = train(lrn.lm, bh.task, subset = train.set)\ntask.pred.lm = predict(mod.lm, task = bh.task, subset = test.set)\ntask.pred.lm\n#\n Prediction: 253 observations\n#\n predict.type: se\n#\n threshold: \n#\n time: 0.00\n#\n    id truth response        se\n#\n 2   2  21.6 24.83734 0.7501615\n#\n 4   4  33.4 28.38206 0.8742590\n#\n 6   6  28.7 25.16725 0.8652139\n#\n 8   8  27.1 19.38145 1.1963265\n#\n 10 10  18.9 18.66449 1.1793944\n#\n 12 12  18.9 21.25802 1.0727918\n#\n ... (#rows: 253, #cols: 4)\n\n\n\n\nThe standard errors can then be extracted using \ngetPredictionSE\n.\n\n\nhead(getPredictionSE(task.pred.lm))\n#\n [1] 0.7501615 0.8742590 0.8652139 1.1963265 1.1793944 1.0727918\n\n\n\n\nClassification and clustering: Extracting probabilities\n\n\nThe predicted probabilities can be extracted from the \nPrediction\n using function\n\ngetPredictionProbabilities\n.\nHere is another cluster analysis example. We use \nfuzzy c-means clustering\n\non the \nmtcars\n data set.\n\n\nlrn = makeLearner(\ncluster.cmeans\n, predict.type = \nprob\n)\nmod = train(lrn, mtcars.task)\n\npred = predict(mod, task = mtcars.task)\nhead(getPredictionProbabilities(pred))\n#\n                            1           2\n#\n Mazda RX4         0.97959529 0.020404714\n#\n Mazda RX4 Wag     0.97963550 0.020364495\n#\n Datsun 710        0.99265984 0.007340164\n#\n Hornet 4 Drive    0.54292079 0.457079211\n#\n Hornet Sportabout 0.01870622 0.981293776\n#\n Valiant           0.75746556 0.242534444\n\n\n\n\nFor \nclassification problems\n there are some more things worth mentioning.\nBy default, class labels are predicted.\n\n\n## Linear discriminant analysis on the iris data set\nmod = train(\nclassif.lda\n, task = iris.task)\n\npred = predict(mod, task = iris.task)\npred\n#\n Prediction: 150 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n   id  truth response\n#\n 1  1 setosa   setosa\n#\n 2  2 setosa   setosa\n#\n 3  3 setosa   setosa\n#\n 4  4 setosa   setosa\n#\n 5  5 setosa   setosa\n#\n 6  6 setosa   setosa\n#\n ... (#rows: 150, #cols: 3)\n\n\n\n\nIn order to get predicted posterior probabilities we have to create a \nLearner\n\nwith the appropriate \npredict.type\n.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, iris.task)\n\npred = predict(mod, newdata = iris)\nhead(as.data.frame(pred))\n#\n    truth prob.setosa prob.versicolor prob.virginica response\n#\n 1 setosa           1               0              0   setosa\n#\n 2 setosa           1               0              0   setosa\n#\n 3 setosa           1               0              0   setosa\n#\n 4 setosa           1               0              0   setosa\n#\n 5 setosa           1               0              0   setosa\n#\n 6 setosa           1               0              0   setosa\n\n\n\n\nIn addition to the probabilities, class labels are predicted\nby choosing the class with the maximum probability and breaking ties at random.\n\n\nAs mentioned above, the predicted posterior probabilities can be accessed via the\n\ngetPredictionProbabilities\n function.\n\n\nhead(getPredictionProbabilities(pred))\n#\n   setosa versicolor virginica\n#\n 1      1          0         0\n#\n 2      1          0         0\n#\n 3      1          0         0\n#\n 4      1          0         0\n#\n 5      1          0         0\n#\n 6      1          0         0\n\n\n\n\nClassification: Confusion matrix\n\n\nA confusion matrix can be obtained by calling \ncalculateConfusionMatrix\n. The columns represent\npredicted and the rows true class labels.\n\n\ncalculateConfusionMatrix(pred)\n#\n             predicted\n#\n true         setosa versicolor virginica -err.-\n#\n   setosa         50          0         0      0\n#\n   versicolor      0         49         1      1\n#\n   virginica       0          5        45      5\n#\n   -err.-          0          5         1      6\n\n\n\n\nYou can see the number of correctly classified observations on the diagonal of the matrix. Misclassified\nobservations are on the off-diagonal. The total number of errors for single (true and predicted)\nclasses is shown in the \n-err.-\n row and column, respectively.\n\n\nTo get relative frequencies additional to the absolute numbers we can set \nrelative = TRUE\n.\n\n\nconf.matrix = calculateConfusionMatrix(pred, relative = TRUE)\nconf.matrix\n#\n Relative confusion matrix (normalized by row/column):\n#\n             predicted\n#\n true         setosa    versicolor virginica -err.-   \n#\n   setosa     1.00/1.00 0.00/0.00  0.00/0.00 0.00     \n#\n   versicolor 0.00/0.00 0.98/0.91  0.02/0.02 0.02     \n#\n   virginica  0.00/0.00 0.10/0.09  0.90/0.98 0.10     \n#\n   -err.-          0.00      0.09       0.02 0.04     \n#\n \n#\n \n#\n Absolute confusion matrix:\n#\n             predicted\n#\n true         setosa versicolor virginica -err.-\n#\n   setosa         50          0         0      0\n#\n   versicolor      0         49         1      1\n#\n   virginica       0          5        45      5\n#\n   -err.-          0          5         1      6\n\n\n\n\nIt is possible to normalize by either row or column, therefore every element of the above\nrelative confusion matrix contains two values. The first is the relative frequency grouped by row\n(the true label) and the second value grouped by column (the predicted label).\n\n\nIf you want to access the relative values directly you can do this through the \n$relative.row\n\nand \n$relative.col\n members of the returned object \nconf.matrix\n.\nFor more details see the \nConfusionMatrix\n documentation page.\n\n\nconf.matrix$relative.row\n#\n            setosa versicolor virginica -err-\n#\n setosa          1       0.00      0.00  0.00\n#\n versicolor      0       0.98      0.02  0.02\n#\n virginica       0       0.10      0.90  0.10\n\n\n\n\nFinally, we can also add the absolute number of observations for each predicted\nand true class label to the matrix (both absolute and relative) by setting \nsums = TRUE\n.\n\n\ncalculateConfusionMatrix(pred, relative = TRUE, sums = TRUE)\n#\n Relative confusion matrix (normalized by row/column):\n#\n             predicted\n#\n true         setosa    versicolor virginica -err.-    -n- \n#\n   setosa     1.00/1.00 0.00/0.00  0.00/0.00 0.00      50  \n#\n   versicolor 0.00/0.00 0.98/0.91  0.02/0.02 0.02      54  \n#\n   virginica  0.00/0.00 0.10/0.09  0.90/0.98 0.10      46  \n#\n   -err.-          0.00      0.09       0.02 0.04      \nNA\n\n#\n   -n-        50        50         50        \nNA\n      150 \n#\n \n#\n \n#\n Absolute confusion matrix:\n#\n            setosa versicolor virginica -err.- -n-\n#\n setosa         50          0         0      0  50\n#\n versicolor      0         49         1      1  50\n#\n virginica       0          5        45      5  50\n#\n -err.-          0          5         1      6  NA\n#\n -n-            50         54        46     NA 150\n\n\n\n\nClassification: Adjusting the decision threshold\n\n\nWe can set the threshold value that is used to map the predicted posterior probabilities to class labels.\nNote that for this purpose we need to create a \nLearner\n that predicts probabilities.\nFor binary classification, the threshold determines when the \npositive\n class is predicted.\nThe default is 0.5.\nNow, we set the threshold for the positive class to 0.9 (that is, an example is assigned to the positive class if its posterior probability exceeds 0.9).\nWhich of the two classes is the positive one can be seen by accessing the \nTask\n.\nTo illustrate binary classification, we use the \nSonar\n data set from the \nmlbench\n package.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, task = sonar.task)\n\n## Label of the positive class\ngetTaskDesc(sonar.task)$positive\n#\n [1] \nM\n\n\n## Default threshold\npred1 = predict(mod, sonar.task)\npred1$threshold\n#\n   M   R \n#\n 0.5 0.5\n\n## Set the threshold value for the positive class\npred2 = setThreshold(pred1, 0.9)\npred2$threshold\n#\n   M   R \n#\n 0.9 0.1\n\npred2\n#\n Prediction: 208 observations\n#\n predict.type: prob\n#\n threshold: M=0.90,R=0.10\n#\n time: 0.01\n#\n   id truth    prob.M    prob.R response\n#\n 1  1     R 0.1060606 0.8939394        R\n#\n 2  2     R 0.7333333 0.2666667        R\n#\n 3  3     R 0.0000000 1.0000000        R\n#\n 4  4     R 0.1060606 0.8939394        R\n#\n 5  5     R 0.9250000 0.0750000        M\n#\n 6  6     R 0.0000000 1.0000000        R\n#\n ... (#rows: 208, #cols: 5)\n\n## We can also set the effect in the confusion matrix\ncalculateConfusionMatrix(pred1)\n#\n         predicted\n#\n true      M  R -err.-\n#\n   M      95 16     16\n#\n   R      10 87     10\n#\n   -err.- 10 16     26\n\ncalculateConfusionMatrix(pred2)\n#\n         predicted\n#\n true      M  R -err.-\n#\n   M      84 27     27\n#\n   R       6 91      6\n#\n   -err.-  6 27     33\n\n\n\n\nNote that in the binary case \ngetPredictionProbabilities\n by default extracts the posterior\nprobabilities of the positive class only.\n\n\nhead(getPredictionProbabilities(pred1))\n#\n [1] 0.1060606 0.7333333 0.0000000 0.1060606 0.9250000 0.0000000\n\n## But we can change that, too\nhead(getPredictionProbabilities(pred1, cl = c(\nM\n, \nR\n)))\n#\n           M         R\n#\n 1 0.1060606 0.8939394\n#\n 2 0.7333333 0.2666667\n#\n 3 0.0000000 1.0000000\n#\n 4 0.1060606 0.8939394\n#\n 5 0.9250000 0.0750000\n#\n 6 0.0000000 1.0000000\n\n\n\n\nIt works similarly for multiclass classification.\nThe threshold has to be given by a named vector specifying the values by which each probability will be divided.\nThe class with the maximum resulting value is then selected.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, iris.task)\npred = predict(mod, newdata = iris)\npred$threshold\n#\n     setosa versicolor  virginica \n#\n  0.3333333  0.3333333  0.3333333\ntable(as.data.frame(pred)$response)\n#\n \n#\n     setosa versicolor  virginica \n#\n         50         54         46\npred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1))\npred$threshold\n#\n     setosa versicolor  virginica \n#\n       0.01      50.00       1.00\ntable(as.data.frame(pred)$response)\n#\n \n#\n     setosa versicolor  virginica \n#\n         50          0        100\n\n\n\n\nIf you are interested in tuning the threshold (vector) have a look at the section about\n\nperformance curves and threshold tuning\n.\n\n\nVisualizing the prediction\n\n\nThe function \nplotLearnerPrediction\n allows to visualize predictions, e.g., for teaching purposes\nor exploring models.\nIt trains the chosen learning method for 1 or 2 selected features and then displays the\npredictions with \nggplot\n.\n\n\nFor \nclassification\n, we get a scatter plot of 2 features (by default the first 2 in the data set).\nThe type of symbol shows the true class labels of the data points.\nSymbols with white border indicate misclassified observations.\nThe posterior probabilities (if the learner under consideration supports this)\nare represented by the background color where higher saturation means larger probabilities.\n\n\nThe plot title displays the ID of the \nLearner\n (in the following example CART),\nits parameters, its training performance and its cross-validation performance.\n\nmmce\n stands for \nmean misclassification error\n, i.e., the error rate.\nSee the sections on \nperformance\n and\n\nresampling\n for further explanations.\n\n\nlrn = makeLearner(\nclassif.rpart\n, id = \nCART\n)\nplotLearnerPrediction(lrn, task = iris.task)\n\n\n\n\n\n\nFor \nclustering\n we also get a scatter plot of two selected features.\nThe color of the points indicates the predicted cluster.\n\n\nlrn = makeLearner(\ncluster.kmeans\n)\nplotLearnerPrediction(lrn, task = mtcars.task, features = c(\ndisp\n, \ndrat\n), cv = 0)\n\n\n\n\n\n\nFor \nregression\n, there are two types of plots.\nThe 1D plot shows the target values in relation to a single feature, the regression curve and,\nif the chosen learner supports this, the estimated standard error.\n\n\nplotLearnerPrediction(\nregr.lm\n, features = \nlstat\n, task = bh.task)\n\n\n\n\n\n\nThe 2D variant, as in the classification case, generates a scatter plot of 2 features.\nThe fill color of the dots illustrates the value of the target variable \n\"medv\"\n, the\nbackground colors show the estimated mean.\nThe plot does not represent the estimated standard error.\n\n\nplotLearnerPrediction(\nregr.lm\n, features = c(\nlstat\n, \nrm\n), task = bh.task)\n\n\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nn = getTaskSize(bh.task) \ntrain.set = seq(1, n, by = 2) \ntest.set = seq(2, n, by = 2) \nlrn = makeLearner(\nregr.gbm\n, n.trees = 100) \nmod = train(lrn, bh.task, subset = train.set) \n\ntask.pred = predict(mod, task = bh.task, subset = test.set) \ntask.pred \nn = nrow(iris) \niris.train = iris[seq(1, n, by = 2), -5] \niris.test = iris[seq(2, n, by = 2), -5] \ntask = makeClusterTask(data = iris.train) \nmod = train(\ncluster.kmeans\n, task) \n\nnewdata.pred = predict(mod, newdata = iris.test) \nnewdata.pred \n## Result of predict with data passed via task argument \nhead(as.data.frame(task.pred)) \n\n## Result of predict with data passed via newdata argument \nhead(as.data.frame(newdata.pred)) \nhead(getPredictionTruth(task.pred)) \n\nhead(getPredictionResponse(task.pred)) \nlistLearners(\nregr\n, check.packages = FALSE, properties = \nse\n)[c(\nclass\n, \nname\n)] \n## Create learner and specify predict.type \nlrn.lm = makeLearner(\nregr.lm\n, predict.type = 'se') \nmod.lm = train(lrn.lm, bh.task, subset = train.set) \ntask.pred.lm = predict(mod.lm, task = bh.task, subset = test.set) \ntask.pred.lm \nhead(getPredictionSE(task.pred.lm)) \nlrn = makeLearner(\ncluster.cmeans\n, predict.type = \nprob\n) \nmod = train(lrn, mtcars.task) \n\npred = predict(mod, task = mtcars.task) \nhead(getPredictionProbabilities(pred)) \n## Linear discriminant analysis on the iris data set \nmod = train(\nclassif.lda\n, task = iris.task) \n\npred = predict(mod, task = iris.task) \npred \nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n) \nmod = train(lrn, iris.task) \n\npred = predict(mod, newdata = iris) \nhead(as.data.frame(pred)) \nhead(getPredictionProbabilities(pred)) \ncalculateConfusionMatrix(pred) \nconf.matrix = calculateConfusionMatrix(pred, relative = TRUE) \nconf.matrix \nconf.matrix$relative.row \ncalculateConfusionMatrix(pred, relative = TRUE, sums = TRUE) \nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n) \nmod = train(lrn, task = sonar.task) \n\n## Label of the positive class \ngetTaskDesc(sonar.task)$positive \n\n## Default threshold \npred1 = predict(mod, sonar.task) \npred1$threshold \n\n## Set the threshold value for the positive class \npred2 = setThreshold(pred1, 0.9) \npred2$threshold \n\npred2 \n\n## We can also set the effect in the confusion matrix \ncalculateConfusionMatrix(pred1) \n\ncalculateConfusionMatrix(pred2) \nhead(getPredictionProbabilities(pred1)) \n\n## But we can change that, too \nhead(getPredictionProbabilities(pred1, cl = c(\nM\n, \nR\n))) \nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n) \nmod = train(lrn, iris.task) \npred = predict(mod, newdata = iris) \npred$threshold \ntable(as.data.frame(pred)$response) \npred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1)) \npred$threshold \ntable(as.data.frame(pred)$response) \nlrn = makeLearner(\nclassif.rpart\n, id = \nCART\n) \nplotLearnerPrediction(lrn, task = iris.task) \nlrn = makeLearner(\ncluster.kmeans\n) \nplotLearnerPrediction(lrn, task = mtcars.task, features = c(\ndisp\n, \ndrat\n), cv = 0) \nplotLearnerPrediction(\nregr.lm\n, features = \nlstat\n, task = bh.task) \nplotLearnerPrediction(\nregr.lm\n, features = c(\nlstat\n, \nrm\n), task = bh.task)", 
            "title": "Predict"
        }, 
        {
            "location": "/predict/index.html#predicting-outcomes-for-new-data", 
            "text": "Predicting the target values for new observations is\nimplemented the same way as most of the other predict methods in  R .\nIn general, all you need to do is call  predict  on the object\nreturned by  train  and pass the data you want predictions for.  There are two ways to pass the data:   Either pass the  Task  via the  task  argument or  pass a  data.frame  via the  newdata  argument.   The first way is preferable if you want predictions for data already included in a  Task .  Just as  train , the  predict  function has a  subset  argument,\nso you can set aside different portions of the data in  Task  for training and prediction\n(more advanced methods for splitting the data in train and test set are described in the section on resampling ).  In the following example we fit a  gradient boosting machine  to every second\nobservation of the  BostonHousing  data set and make predictions\non the remaining data in  bh.task .  n = getTaskSize(bh.task)\ntrain.set = seq(1, n, by = 2)\ntest.set = seq(2, n, by = 2)\nlrn = makeLearner( regr.gbm , n.trees = 100)\nmod = train(lrn, bh.task, subset = train.set)\n\ntask.pred = predict(mod, task = bh.task, subset = test.set)\ntask.pred\n#  Prediction: 253 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#     id truth response\n#  2   2  21.6 22.28539\n#  4   4  33.4 23.33968\n#  6   6  28.7 22.40896\n#  8   8  27.1 22.12750\n#  10 10  18.9 22.12750\n#  12 12  18.9 22.12750\n#  ... (#rows: 253, #cols: 3)  The second way is useful if you want to predict data not included in the  Task .  Here we cluster the  iris  data set without the target variable.\nAll observations with an odd index are included in the  Task  and used for training.\nPredictions are made for the remaining observations.  n = nrow(iris)\niris.train = iris[seq(1, n, by = 2), -5]\niris.test = iris[seq(2, n, by = 2), -5]\ntask = makeClusterTask(data = iris.train)\nmod = train( cluster.kmeans , task)\n\nnewdata.pred = predict(mod, newdata = iris.test)\nnewdata.pred\n#  Prediction: 75 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#     response\n#  2         2\n#  4         2\n#  6         2\n#  8         2\n#  10        2\n#  12        2\n#  ... (#rows: 75, #cols: 1)  Note that for supervised learning you do not have to remove the target columns from the data.\nThese columns are automatically removed prior to calling the underlying  predict  method of the learner.", 
            "title": "Predicting Outcomes for New Data"
        }, 
        {
            "location": "/predict/index.html#accessing-the-prediction", 
            "text": "Function  predict  returns a named  list  of class  Prediction .\nIts most important element is  $data  which is a  data.frame  that contains\ncolumns with the true values of the target variable (in case of supervised learning problems)\nand the predictions.\nUse  as.data.frame  for direct access.  In the following the predictions on the  BostonHousing  and the iris  data sets are shown.\nAs you may recall, the predictions in the first case were made from a  Task  and in the\nsecond case from a  data.frame .  ## Result of predict with data passed via task argument\nhead(as.data.frame(task.pred))\n#     id truth response\n#  2   2  21.6 22.28539\n#  4   4  33.4 23.33968\n#  6   6  28.7 22.40896\n#  8   8  27.1 22.12750\n#  10 10  18.9 22.12750\n#  12 12  18.9 22.12750\n\n## Result of predict with data passed via newdata argument\nhead(as.data.frame(newdata.pred))\n#     response\n#  2         2\n#  4         2\n#  6         2\n#  8         2\n#  10        2\n#  12        2  As you can see when predicting from a  Task , the resulting  data.frame \ncontains an additional column, called  id , which tells us which element in the original data set\nthe prediction corresponds to.  A direct way to access the true and predicted values of the target variable(s) is provided by\nfunctions  getPredictionTruth  and  getPredictionResponse .  head(getPredictionTruth(task.pred))\n#  [1] 21.6 33.4 28.7 27.1 18.9 18.9\n\nhead(getPredictionResponse(task.pred))\n#  [1] 22.28539 23.33968 22.40896 22.12750 22.12750 22.12750", 
            "title": "Accessing the prediction"
        }, 
        {
            "location": "/predict/index.html#regression-extracting-standard-errors", 
            "text": "Some learners provide standard errors for predictions, which can be accessed in  mlr .\nAn overview is given by calling the function  listLearners  and setting  properties = \"se\" .\nBy assigning  FALSE  to  check.packages  learners from packages which are not installed\nwill be included in the overview.  listLearners( regr , check.packages = FALSE, properties =  se )[c( class ,  name )]\n#           class\n#  1   regr.bcart\n#  2     regr.bgp\n#  3  regr.bgpllm\n#  4     regr.blm\n#  5    regr.btgp\n#  6 regr.btgpllm\n#                                                                       name\n#  1                                                           Bayesian CART\n#  2                                               Bayesian Gaussian Process\n#  3       Bayesian Gaussian Process with jumps to the Limiting Linear Model\n#  4                                                   Bayesian Linear Model\n#  5                                         Bayesian Treed Gaussian Process\n#  6 Bayesian Treed Gaussian Process with jumps to the Limiting Linear Model\n#  ... (#rows: 16, #cols: 2)  In this example we train a  linear regression model  on the  Boston Housing \ndataset. In order to calculate standard errors set the  predict.type  to  \"se\" :  ## Create learner and specify predict.type\nlrn.lm = makeLearner( regr.lm , predict.type = 'se')\nmod.lm = train(lrn.lm, bh.task, subset = train.set)\ntask.pred.lm = predict(mod.lm, task = bh.task, subset = test.set)\ntask.pred.lm\n#  Prediction: 253 observations\n#  predict.type: se\n#  threshold: \n#  time: 0.00\n#     id truth response        se\n#  2   2  21.6 24.83734 0.7501615\n#  4   4  33.4 28.38206 0.8742590\n#  6   6  28.7 25.16725 0.8652139\n#  8   8  27.1 19.38145 1.1963265\n#  10 10  18.9 18.66449 1.1793944\n#  12 12  18.9 21.25802 1.0727918\n#  ... (#rows: 253, #cols: 4)  The standard errors can then be extracted using  getPredictionSE .  head(getPredictionSE(task.pred.lm))\n#  [1] 0.7501615 0.8742590 0.8652139 1.1963265 1.1793944 1.0727918", 
            "title": "Regression: Extracting standard errors"
        }, 
        {
            "location": "/predict/index.html#classification-and-clustering-extracting-probabilities", 
            "text": "The predicted probabilities can be extracted from the  Prediction  using function getPredictionProbabilities .\nHere is another cluster analysis example. We use  fuzzy c-means clustering \non the  mtcars  data set.  lrn = makeLearner( cluster.cmeans , predict.type =  prob )\nmod = train(lrn, mtcars.task)\n\npred = predict(mod, task = mtcars.task)\nhead(getPredictionProbabilities(pred))\n#                             1           2\n#  Mazda RX4         0.97959529 0.020404714\n#  Mazda RX4 Wag     0.97963550 0.020364495\n#  Datsun 710        0.99265984 0.007340164\n#  Hornet 4 Drive    0.54292079 0.457079211\n#  Hornet Sportabout 0.01870622 0.981293776\n#  Valiant           0.75746556 0.242534444  For  classification problems  there are some more things worth mentioning.\nBy default, class labels are predicted.  ## Linear discriminant analysis on the iris data set\nmod = train( classif.lda , task = iris.task)\n\npred = predict(mod, task = iris.task)\npred\n#  Prediction: 150 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#    id  truth response\n#  1  1 setosa   setosa\n#  2  2 setosa   setosa\n#  3  3 setosa   setosa\n#  4  4 setosa   setosa\n#  5  5 setosa   setosa\n#  6  6 setosa   setosa\n#  ... (#rows: 150, #cols: 3)  In order to get predicted posterior probabilities we have to create a  Learner \nwith the appropriate  predict.type .  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, iris.task)\n\npred = predict(mod, newdata = iris)\nhead(as.data.frame(pred))\n#     truth prob.setosa prob.versicolor prob.virginica response\n#  1 setosa           1               0              0   setosa\n#  2 setosa           1               0              0   setosa\n#  3 setosa           1               0              0   setosa\n#  4 setosa           1               0              0   setosa\n#  5 setosa           1               0              0   setosa\n#  6 setosa           1               0              0   setosa  In addition to the probabilities, class labels are predicted\nby choosing the class with the maximum probability and breaking ties at random.  As mentioned above, the predicted posterior probabilities can be accessed via the getPredictionProbabilities  function.  head(getPredictionProbabilities(pred))\n#    setosa versicolor virginica\n#  1      1          0         0\n#  2      1          0         0\n#  3      1          0         0\n#  4      1          0         0\n#  5      1          0         0\n#  6      1          0         0", 
            "title": "Classification and clustering: Extracting probabilities"
        }, 
        {
            "location": "/predict/index.html#classification-confusion-matrix", 
            "text": "A confusion matrix can be obtained by calling  calculateConfusionMatrix . The columns represent\npredicted and the rows true class labels.  calculateConfusionMatrix(pred)\n#              predicted\n#  true         setosa versicolor virginica -err.-\n#    setosa         50          0         0      0\n#    versicolor      0         49         1      1\n#    virginica       0          5        45      5\n#    -err.-          0          5         1      6  You can see the number of correctly classified observations on the diagonal of the matrix. Misclassified\nobservations are on the off-diagonal. The total number of errors for single (true and predicted)\nclasses is shown in the  -err.-  row and column, respectively.  To get relative frequencies additional to the absolute numbers we can set  relative = TRUE .  conf.matrix = calculateConfusionMatrix(pred, relative = TRUE)\nconf.matrix\n#  Relative confusion matrix (normalized by row/column):\n#              predicted\n#  true         setosa    versicolor virginica -err.-   \n#    setosa     1.00/1.00 0.00/0.00  0.00/0.00 0.00     \n#    versicolor 0.00/0.00 0.98/0.91  0.02/0.02 0.02     \n#    virginica  0.00/0.00 0.10/0.09  0.90/0.98 0.10     \n#    -err.-          0.00      0.09       0.02 0.04     \n#  \n#  \n#  Absolute confusion matrix:\n#              predicted\n#  true         setosa versicolor virginica -err.-\n#    setosa         50          0         0      0\n#    versicolor      0         49         1      1\n#    virginica       0          5        45      5\n#    -err.-          0          5         1      6  It is possible to normalize by either row or column, therefore every element of the above\nrelative confusion matrix contains two values. The first is the relative frequency grouped by row\n(the true label) and the second value grouped by column (the predicted label).  If you want to access the relative values directly you can do this through the  $relative.row \nand  $relative.col  members of the returned object  conf.matrix .\nFor more details see the  ConfusionMatrix  documentation page.  conf.matrix$relative.row\n#             setosa versicolor virginica -err-\n#  setosa          1       0.00      0.00  0.00\n#  versicolor      0       0.98      0.02  0.02\n#  virginica       0       0.10      0.90  0.10  Finally, we can also add the absolute number of observations for each predicted\nand true class label to the matrix (both absolute and relative) by setting  sums = TRUE .  calculateConfusionMatrix(pred, relative = TRUE, sums = TRUE)\n#  Relative confusion matrix (normalized by row/column):\n#              predicted\n#  true         setosa    versicolor virginica -err.-    -n- \n#    setosa     1.00/1.00 0.00/0.00  0.00/0.00 0.00      50  \n#    versicolor 0.00/0.00 0.98/0.91  0.02/0.02 0.02      54  \n#    virginica  0.00/0.00 0.10/0.09  0.90/0.98 0.10      46  \n#    -err.-          0.00      0.09       0.02 0.04       NA \n#    -n-        50        50         50         NA       150 \n#  \n#  \n#  Absolute confusion matrix:\n#             setosa versicolor virginica -err.- -n-\n#  setosa         50          0         0      0  50\n#  versicolor      0         49         1      1  50\n#  virginica       0          5        45      5  50\n#  -err.-          0          5         1      6  NA\n#  -n-            50         54        46     NA 150", 
            "title": "Classification: Confusion matrix"
        }, 
        {
            "location": "/predict/index.html#classification-adjusting-the-decision-threshold", 
            "text": "We can set the threshold value that is used to map the predicted posterior probabilities to class labels.\nNote that for this purpose we need to create a  Learner  that predicts probabilities.\nFor binary classification, the threshold determines when the  positive  class is predicted.\nThe default is 0.5.\nNow, we set the threshold for the positive class to 0.9 (that is, an example is assigned to the positive class if its posterior probability exceeds 0.9).\nWhich of the two classes is the positive one can be seen by accessing the  Task .\nTo illustrate binary classification, we use the  Sonar  data set from the  mlbench  package.  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, task = sonar.task)\n\n## Label of the positive class\ngetTaskDesc(sonar.task)$positive\n#  [1]  M \n\n## Default threshold\npred1 = predict(mod, sonar.task)\npred1$threshold\n#    M   R \n#  0.5 0.5\n\n## Set the threshold value for the positive class\npred2 = setThreshold(pred1, 0.9)\npred2$threshold\n#    M   R \n#  0.9 0.1\n\npred2\n#  Prediction: 208 observations\n#  predict.type: prob\n#  threshold: M=0.90,R=0.10\n#  time: 0.01\n#    id truth    prob.M    prob.R response\n#  1  1     R 0.1060606 0.8939394        R\n#  2  2     R 0.7333333 0.2666667        R\n#  3  3     R 0.0000000 1.0000000        R\n#  4  4     R 0.1060606 0.8939394        R\n#  5  5     R 0.9250000 0.0750000        M\n#  6  6     R 0.0000000 1.0000000        R\n#  ... (#rows: 208, #cols: 5)\n\n## We can also set the effect in the confusion matrix\ncalculateConfusionMatrix(pred1)\n#          predicted\n#  true      M  R -err.-\n#    M      95 16     16\n#    R      10 87     10\n#    -err.- 10 16     26\n\ncalculateConfusionMatrix(pred2)\n#          predicted\n#  true      M  R -err.-\n#    M      84 27     27\n#    R       6 91      6\n#    -err.-  6 27     33  Note that in the binary case  getPredictionProbabilities  by default extracts the posterior\nprobabilities of the positive class only.  head(getPredictionProbabilities(pred1))\n#  [1] 0.1060606 0.7333333 0.0000000 0.1060606 0.9250000 0.0000000\n\n## But we can change that, too\nhead(getPredictionProbabilities(pred1, cl = c( M ,  R )))\n#            M         R\n#  1 0.1060606 0.8939394\n#  2 0.7333333 0.2666667\n#  3 0.0000000 1.0000000\n#  4 0.1060606 0.8939394\n#  5 0.9250000 0.0750000\n#  6 0.0000000 1.0000000  It works similarly for multiclass classification.\nThe threshold has to be given by a named vector specifying the values by which each probability will be divided.\nThe class with the maximum resulting value is then selected.  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, iris.task)\npred = predict(mod, newdata = iris)\npred$threshold\n#      setosa versicolor  virginica \n#   0.3333333  0.3333333  0.3333333\ntable(as.data.frame(pred)$response)\n#  \n#      setosa versicolor  virginica \n#          50         54         46\npred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1))\npred$threshold\n#      setosa versicolor  virginica \n#        0.01      50.00       1.00\ntable(as.data.frame(pred)$response)\n#  \n#      setosa versicolor  virginica \n#          50          0        100  If you are interested in tuning the threshold (vector) have a look at the section about performance curves and threshold tuning .", 
            "title": "Classification: Adjusting the decision threshold"
        }, 
        {
            "location": "/predict/index.html#visualizing-the-prediction", 
            "text": "The function  plotLearnerPrediction  allows to visualize predictions, e.g., for teaching purposes\nor exploring models.\nIt trains the chosen learning method for 1 or 2 selected features and then displays the\npredictions with  ggplot .  For  classification , we get a scatter plot of 2 features (by default the first 2 in the data set).\nThe type of symbol shows the true class labels of the data points.\nSymbols with white border indicate misclassified observations.\nThe posterior probabilities (if the learner under consideration supports this)\nare represented by the background color where higher saturation means larger probabilities.  The plot title displays the ID of the  Learner  (in the following example CART),\nits parameters, its training performance and its cross-validation performance. mmce  stands for  mean misclassification error , i.e., the error rate.\nSee the sections on  performance  and resampling  for further explanations.  lrn = makeLearner( classif.rpart , id =  CART )\nplotLearnerPrediction(lrn, task = iris.task)   For  clustering  we also get a scatter plot of two selected features.\nThe color of the points indicates the predicted cluster.  lrn = makeLearner( cluster.kmeans )\nplotLearnerPrediction(lrn, task = mtcars.task, features = c( disp ,  drat ), cv = 0)   For  regression , there are two types of plots.\nThe 1D plot shows the target values in relation to a single feature, the regression curve and,\nif the chosen learner supports this, the estimated standard error.  plotLearnerPrediction( regr.lm , features =  lstat , task = bh.task)   The 2D variant, as in the classification case, generates a scatter plot of 2 features.\nThe fill color of the dots illustrates the value of the target variable  \"medv\" , the\nbackground colors show the estimated mean.\nThe plot does not represent the estimated standard error.  plotLearnerPrediction( regr.lm , features = c( lstat ,  rm ), task = bh.task)", 
            "title": "Visualizing the prediction"
        }, 
        {
            "location": "/predict/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  n = getTaskSize(bh.task) \ntrain.set = seq(1, n, by = 2) \ntest.set = seq(2, n, by = 2) \nlrn = makeLearner( regr.gbm , n.trees = 100) \nmod = train(lrn, bh.task, subset = train.set) \n\ntask.pred = predict(mod, task = bh.task, subset = test.set) \ntask.pred \nn = nrow(iris) \niris.train = iris[seq(1, n, by = 2), -5] \niris.test = iris[seq(2, n, by = 2), -5] \ntask = makeClusterTask(data = iris.train) \nmod = train( cluster.kmeans , task) \n\nnewdata.pred = predict(mod, newdata = iris.test) \nnewdata.pred \n## Result of predict with data passed via task argument \nhead(as.data.frame(task.pred)) \n\n## Result of predict with data passed via newdata argument \nhead(as.data.frame(newdata.pred)) \nhead(getPredictionTruth(task.pred)) \n\nhead(getPredictionResponse(task.pred)) \nlistLearners( regr , check.packages = FALSE, properties =  se )[c( class ,  name )] \n## Create learner and specify predict.type \nlrn.lm = makeLearner( regr.lm , predict.type = 'se') \nmod.lm = train(lrn.lm, bh.task, subset = train.set) \ntask.pred.lm = predict(mod.lm, task = bh.task, subset = test.set) \ntask.pred.lm \nhead(getPredictionSE(task.pred.lm)) \nlrn = makeLearner( cluster.cmeans , predict.type =  prob ) \nmod = train(lrn, mtcars.task) \n\npred = predict(mod, task = mtcars.task) \nhead(getPredictionProbabilities(pred)) \n## Linear discriminant analysis on the iris data set \nmod = train( classif.lda , task = iris.task) \n\npred = predict(mod, task = iris.task) \npred \nlrn = makeLearner( classif.rpart , predict.type =  prob ) \nmod = train(lrn, iris.task) \n\npred = predict(mod, newdata = iris) \nhead(as.data.frame(pred)) \nhead(getPredictionProbabilities(pred)) \ncalculateConfusionMatrix(pred) \nconf.matrix = calculateConfusionMatrix(pred, relative = TRUE) \nconf.matrix \nconf.matrix$relative.row \ncalculateConfusionMatrix(pred, relative = TRUE, sums = TRUE) \nlrn = makeLearner( classif.rpart , predict.type =  prob ) \nmod = train(lrn, task = sonar.task) \n\n## Label of the positive class \ngetTaskDesc(sonar.task)$positive \n\n## Default threshold \npred1 = predict(mod, sonar.task) \npred1$threshold \n\n## Set the threshold value for the positive class \npred2 = setThreshold(pred1, 0.9) \npred2$threshold \n\npred2 \n\n## We can also set the effect in the confusion matrix \ncalculateConfusionMatrix(pred1) \n\ncalculateConfusionMatrix(pred2) \nhead(getPredictionProbabilities(pred1)) \n\n## But we can change that, too \nhead(getPredictionProbabilities(pred1, cl = c( M ,  R ))) \nlrn = makeLearner( classif.rpart , predict.type =  prob ) \nmod = train(lrn, iris.task) \npred = predict(mod, newdata = iris) \npred$threshold \ntable(as.data.frame(pred)$response) \npred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1)) \npred$threshold \ntable(as.data.frame(pred)$response) \nlrn = makeLearner( classif.rpart , id =  CART ) \nplotLearnerPrediction(lrn, task = iris.task) \nlrn = makeLearner( cluster.kmeans ) \nplotLearnerPrediction(lrn, task = mtcars.task, features = c( disp ,  drat ), cv = 0) \nplotLearnerPrediction( regr.lm , features =  lstat , task = bh.task) \nplotLearnerPrediction( regr.lm , features = c( lstat ,  rm ), task = bh.task)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/preproc/index.html", 
            "text": "Data Preprocessing\n\n\nData preprocessing refers to any transformation of the data done before applying a learning\nalgorithm.\nThis comprises for example finding and resolving inconsistencies, imputation of missing values,\nidentifying, removing or replacing outliers, discretizing numerical data or generating numerical\ndummy variables for categorical data, any kind of transformation like standardization of predictors\nor Box-Cox, dimensionality reduction and feature extraction and/or selection.\n\n\nmlr\n offers several options for data preprocessing.\nSome of the following simple methods to change a \nTask\n (or \ndata.frame\n)\nwere already mentioned on the page about \nlearning tasks\n:\n\n\n\n\ncapLargeValues\n: Convert large/infinite numeric values.\n\n\ncreateDummyFeatures\n: Generate dummy variables for factor features.\n\n\ndropFeatures\n: Remove selected features.\n\n\njoinClassLevels\n: Only for classification: Merge existing classes to\n  new, larger classes.\n\n\nmergeSmallFactorLevels\n: Merge infrequent levels of factor features.\n\n\nnormalizeFeatures\n: Normalize features by different methods, e.g.,\n  standardization or scaling to a certain range.\n\n\nremoveConstantFeatures\n: Remove constant features.\n\n\nsubsetTask\n: Remove observations and/or features from a \nTask\n.\n\n\n\n\nMoreover, there are tutorial pages devoted to\n\n\n\n\nFeature selection\n and\n\n\nImputation of missing values\n.\n\n\n\n\nFusing learners with preprocessing\n\n\nmlr\n's wrapper functionality permits to combine learners with preprocessing steps.\nThis means that the preprocessing \"belongs\" to the learner and is done any time the learner\nis trained or predictions are made.\n\n\nThis is, on the one hand, very practical.\nYou don't need to change any data or learning \nTask\ns and it's quite easy to combine\ndifferent learners with different preprocessing steps.\n\n\nOn the other hand this helps to avoid a common mistake in evaluating the performance of a\nlearner with preprocessing:\nPreprocessing is often seen as completely independent of the later applied learning algorithms.\nWhen estimating the performance of the a learner, e.g., by cross-validation all preprocessing\nis done beforehand on the full data set and only training/predicting the learner is done on the\ntrain/test sets.\nDepending on what exactly is done as preprocessing this can lead to overoptimistic results.\nFor example if imputation by the mean is done on the whole data set before evaluating the learner\nperformance you are using information from the test data during training, which can cause\noveroptimistic performance results.\n\n\nTo clarify things one should distinguish between \ndata-dependent\n and \ndata-independent\n\npreprocessing steps:\nData-dependent steps in some way learn from the data and give different results when applied to\ndifferent data sets. Data-independent steps always lead to the same results.\nClearly, correcting errors in the data or removing data columns like Ids that should\nnot be used for learning, is data-independent.\nImputation of missing values by the mean, as mentioned above, is data-dependent.\nImputation by a fixed constant, however, is not.\n\n\nTo get a honest estimate of learner performance combined with preprocessing, all data-dependent\npreprocessing steps must be included in the resampling.\nThis is automatically done when fusing a learner with preprocessing.\n\n\nTo this end \nmlr\n provides two \nwrappers\n:\n\n\n\n\nmakePreprocWrapperCaret\n is an interface to all preprocessing options offered by \ncaret\n's\n  \npreProcess\n function.\n\n\nmakePreprocWrapper\n permits to write your own custom preprocessing methods by defining\n  the actions to be taken before training and before prediction.\n\n\n\n\nAs mentioned above the specified preprocessing steps then \"belong\" to the wrapped \nLearner\n.\nIn contrast to the preprocessing options listed above like \nnormalizeFeatures\n\n\n\n\nthe \nTask\n itself remains unchanged,\n\n\nthe preprocessing is not done globally, i.e., for the whole data set, but for every pair of\n  training/test data sets in, e.g., resampling,\n\n\nany parameters controlling the preprocessing as, e.g., the percentage of outliers to be removed\n  can be \ntuned\n together with the base learner parameters.\n\n\n\n\nWe start with some examples for \nmakePreprocWrapperCaret\n.\n\n\nPreprocessing with makePreprocWrapperCaret\n\n\nmakePreprocWrapperCaret\n is an interface to \ncaret\n's \npreProcess\n\nfunction that provides many different options like imputation of missing values,\ndata transformations as scaling the features to a certain range or Box-Cox and dimensionality\nreduction via Independent or Principal Component Analysis.\nFor all possible options see the help page of function \npreProcess\n.\n\n\nNote that the usage of \nmakePreprocWrapperCaret\n is slightly different than that of\n\npreProcess\n.\n\n\n\n\nmakePreprocWrapperCaret\n takes (almost) the same formal arguments as \npreProcess\n,\n  but their names are prefixed by \nppc.\n.\n\n\nThe only exception: \nmakePreprocWrapperCaret\n does not have a \nmethod\n argument. Instead\n  all preprocessing options that would be passed to \npreProcess\n's \nmethod\n\n  argument are given as individual logical parameters to \nmakePreprocWrapperCaret\n.\n\n\n\n\nFor example the following call to \npreProcess\n\n\npreProcess(x, method = c(\nknnImpute\n, \npca\n), pcaComp = 10)\n\n\n\n\nwith \nx\n being a \nmatrix\n or \ndata.frame\n\nwould thus translate into\n\n\nmakePreprocWrapperCaret(learner, ppc.knnImpute = TRUE, ppc.pca = TRUE, ppc.pcaComp = 10)\n\n\n\n\nwhere \nlearner\n is a \nmlr\n \nLearner\n or the name of a learner class like\n\n\"classif.lda\"\n.\n\n\nIf you enable multiple preprocessing options (like knn imputation and principal component\nanalysis above) these are executed in a certain order detailed on the help page of function\n\npreProcess\n.\n\n\nIn the following we show an example where principal components analysis (PCA) is used for\ndimensionality reduction.\nThis should never be applied blindly, but can be beneficial with learners that get problems\nwith high dimensionality or those that can profit from rotating the data.\n\n\nWe consider the \nsonar.task\n, which poses a binary classification problem with 208 observations\nand 60 features.\n\n\nsonar.task\n#\n Supervised task: Sonar_example\n#\n Type: classif\n#\n Target: Class\n#\n Observations: 208\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n          60           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 2\n#\n   M   R \n#\n 111  97 \n#\n Positive class: M\n\n\n\n\nBelow we fuse \nquadratic discriminant analysis\n from package \nMASS\n with a principal\ncomponents preprocessing step.\nThe threshold is set to 0.9, i.e., the principal components necessary to explain a cumulative\npercentage of 90% of the total variance are kept.\nThe data are automatically standardized prior to PCA.\n\n\nlrn = makePreprocWrapperCaret(\nclassif.qda\n, ppc.pca = TRUE, ppc.thresh = 0.9)\nlrn\n#\n Learner classif.qda.preproc from package MASS\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: PreprocWrapperCaret\n#\n Properties: twoclass,multiclass,numerics,factors,prob\n#\n Predict-Type: response\n#\n Hyperparameters: ppc.BoxCox=FALSE,ppc.YeoJohnson=FALSE,ppc.expoTrans=FALSE,ppc.center=TRUE,ppc.scale=TRUE,ppc.range=FALSE,ppc.knnImpute=FALSE,ppc.bagImpute=FALSE,ppc.medianImpute=FALSE,ppc.pca=TRUE,ppc.ica=FALSE,ppc.spatialSign=FALSE,ppc.corr=FALSE,ppc.zv=FALSE,ppc.nzv=FALSE,ppc.thresh=0.9,ppc.na.remove=TRUE,ppc.k=5,ppc.fudge=0.2,ppc.numUnique=3,ppc.cutoff=0.9,ppc.freqCut=19,ppc.uniqueCut=10\n\n\n\n\nThe wrapped learner is trained on the \nsonar.task\n.\nBy inspecting the underlying \nqda\n model, we see that the first 22\nprincipal components have been used for training.\n\n\nmod = train(lrn, sonar.task)\nmod\n#\n Model for learner.id=classif.qda.preproc; learner.class=PreprocWrapperCaret\n#\n Trained on: task.id = Sonar_example; obs = 208; features = 60\n#\n Hyperparameters: ppc.BoxCox=FALSE,ppc.YeoJohnson=FALSE,ppc.expoTrans=FALSE,ppc.center=TRUE,ppc.scale=TRUE,ppc.range=FALSE,ppc.knnImpute=FALSE,ppc.bagImpute=FALSE,ppc.medianImpute=FALSE,ppc.pca=TRUE,ppc.ica=FALSE,ppc.spatialSign=FALSE,ppc.corr=FALSE,ppc.zv=FALSE,ppc.nzv=FALSE,ppc.thresh=0.9,ppc.na.remove=TRUE,ppc.k=5,ppc.fudge=0.2,ppc.numUnique=3,ppc.cutoff=0.9,ppc.freqCut=19,ppc.uniqueCut=10\n\ngetLearnerModel(mod)\n#\n Model for learner.id=classif.qda; learner.class=classif.qda\n#\n Trained on: task.id = Sonar_example; obs = 208; features = 22\n#\n Hyperparameters:\n\ngetLearnerModel(mod, more.unwrap = TRUE)\n#\n Call:\n#\n qda(f, data = getTaskData(.task, .subset, recode.target = \ndrop.levels\n))\n#\n \n#\n Prior probabilities of groups:\n#\n         M         R \n#\n 0.5336538 0.4663462 \n#\n \n#\n Group means:\n#\n          PC1        PC2        PC3         PC4         PC5         PC6\n#\n M  0.5976122 -0.8058235  0.9773518  0.03794232 -0.04568166 -0.06721702\n#\n R -0.6838655  0.9221279 -1.1184128 -0.04341853  0.05227489  0.07691845\n#\n          PC7         PC8        PC9       PC10        PC11          PC12\n#\n M  0.2278162 -0.01034406 -0.2530606 -0.1793157 -0.04084466 -0.0004789888\n#\n R -0.2606969  0.01183702  0.2895848  0.2051963  0.04673977  0.0005481212\n#\n          PC13       PC14        PC15        PC16        PC17        PC18\n#\n M -0.06138758 -0.1057137  0.02808048  0.05215865 -0.07453265  0.03869042\n#\n R  0.07024765  0.1209713 -0.03213333 -0.05968671  0.08528994 -0.04427460\n#\n          PC19         PC20        PC21         PC22\n#\n M -0.01192247  0.006098658  0.01263492 -0.001224809\n#\n R  0.01364323 -0.006978877 -0.01445851  0.001401586\n\n\n\n\nBelow the performances of \nqda\n with and without PCA preprocessing are compared\nin a \nbenchmark experiment\n.\nNote that we use stratified resampling to prevent errors in \nqda\n due to a too\nsmall number of observations from either class.\n\n\nrin = makeResampleInstance(\nCV\n, iters = 3, stratify = TRUE, task = sonar.task)\nres = benchmark(list(\nclassif.qda\n, lrn), sonar.task, rin, show.info = FALSE)\nres\n#\n         task.id          learner.id mmce.test.mean\n#\n 1 Sonar_example         classif.qda      0.3941339\n#\n 2 Sonar_example classif.qda.preproc      0.2643202\n\n\n\n\nPCA preprocessing in this case turns out to be really beneficial for the\nperformance of Quadratic Discriminant Analysis.\n\n\nJoint tuning of preprocessing options and learner parameters\n\n\nLet's see if we can optimize this a bit.\nThe threshold value of 0.9 above was chosen arbitrarily and led to 22 out of 60 principal\ncomponents.\nBut maybe a lower or higher number of principal components should be used.\nMoreover, \nqda\n has several options that control how the class covariance matrices\nor class probabilities are estimated.\n\n\nThose preprocessing and learner parameters can be \ntuned\n jointly.\nBefore doing this let's first get an overview of all the parameters of the wrapped learner\nusing function \ngetParamSet\n.\n\n\ngetParamSet(lrn)\n#\n                      Type len     Def                      Constr Req\n#\n ppc.BoxCox        logical   -   FALSE                           -   -\n#\n ppc.YeoJohnson    logical   -   FALSE                           -   -\n#\n ppc.expoTrans     logical   -   FALSE                           -   -\n#\n ppc.center        logical   -    TRUE                           -   -\n#\n ppc.scale         logical   -    TRUE                           -   -\n#\n ppc.range         logical   -   FALSE                           -   -\n#\n ppc.knnImpute     logical   -   FALSE                           -   -\n#\n ppc.bagImpute     logical   -   FALSE                           -   -\n#\n ppc.medianImpute  logical   -   FALSE                           -   -\n#\n ppc.pca           logical   -   FALSE                           -   -\n#\n ppc.ica           logical   -   FALSE                           -   -\n#\n ppc.spatialSign   logical   -   FALSE                           -   -\n#\n ppc.corr          logical   -   FALSE                           -   -\n#\n ppc.zv            logical   -   FALSE                           -   -\n#\n ppc.nzv           logical   -   FALSE                           -   -\n#\n ppc.thresh        numeric   -    0.95                    0 to Inf   -\n#\n ppc.pcaComp       integer   -       -                    1 to Inf   -\n#\n ppc.na.remove     logical   -    TRUE                           -   -\n#\n ppc.k             integer   -       5                    1 to Inf   -\n#\n ppc.fudge         numeric   -     0.2                    0 to Inf   -\n#\n ppc.numUnique     integer   -       3                    1 to Inf   -\n#\n ppc.n.comp        integer   -       -                    1 to Inf   -\n#\n ppc.cutoff        numeric   -     0.9                      0 to 1   -\n#\n ppc.freqCut       numeric   -      19                    1 to Inf   -\n#\n ppc.uniqueCut     numeric   -      10                    0 to Inf   -\n#\n method           discrete   -  moment            moment,mle,mve,t   -\n#\n nu                numeric   -       5                    2 to Inf   Y\n#\n predict.method   discrete   - plug-in plug-in,predictive,debiased   -\n#\n                  Tunable Trafo\n#\n ppc.BoxCox          TRUE     -\n#\n ppc.YeoJohnson      TRUE     -\n#\n ppc.expoTrans       TRUE     -\n#\n ppc.center          TRUE     -\n#\n ppc.scale           TRUE     -\n#\n ppc.range           TRUE     -\n#\n ppc.knnImpute       TRUE     -\n#\n ppc.bagImpute       TRUE     -\n#\n ppc.medianImpute    TRUE     -\n#\n ppc.pca             TRUE     -\n#\n ppc.ica             TRUE     -\n#\n ppc.spatialSign     TRUE     -\n#\n ppc.corr            TRUE     -\n#\n ppc.zv              TRUE     -\n#\n ppc.nzv             TRUE     -\n#\n ppc.thresh          TRUE     -\n#\n ppc.pcaComp         TRUE     -\n#\n ppc.na.remove       TRUE     -\n#\n ppc.k               TRUE     -\n#\n ppc.fudge           TRUE     -\n#\n ppc.numUnique       TRUE     -\n#\n ppc.n.comp          TRUE     -\n#\n ppc.cutoff          TRUE     -\n#\n ppc.freqCut         TRUE     -\n#\n ppc.uniqueCut       TRUE     -\n#\n method              TRUE     -\n#\n nu                  TRUE     -\n#\n predict.method      TRUE     -\n\n\n\n\nThe parameters prefixed by \nppc.\n belong to preprocessing. \nmethod\n, \nnu\n and \npredict.method\n\nare \nqda\n parameters.\n\n\nInstead of tuning the PCA threshold (\nppc.thresh\n) we tune the number of principal\ncomponents (\nppc.pcaComp\n) directly.\nMoreover, for \nqda\n we try two different ways to estimate the posterior probabilities\n(parameter \npredict.method\n): the usual plug-in estimates and unbiased estimates.\n\n\nWe perform a grid search and set the resolution to 10.\nThis is for demonstration. You might want to use a finer resolution.\n\n\nps = makeParamSet(\n  makeIntegerParam(\nppc.pcaComp\n, lower = 1, upper = getTaskNFeats(sonar.task)),\n  makeDiscreteParam(\npredict.method\n, values = c(\nplug-in\n, \ndebiased\n))\n)\nctrl = makeTuneControlGrid(resolution = 10)\nres = tuneParams(lrn, sonar.task, rin, par.set = ps, control = ctrl, show.info = FALSE)\nres\n#\n Tune result:\n#\n Op. pars: ppc.pcaComp=8; predict.method=plug-in\n#\n mmce.test.mean=0.1920635\n\nas.data.frame(res$opt.path)[1:3]\n#\n    ppc.pcaComp predict.method mmce.test.mean\n#\n 1            1        plug-in      0.4757074\n#\n 2            8        plug-in      0.1920635\n#\n 3           14        plug-in      0.2162871\n#\n 4           21        plug-in      0.2643202\n#\n 5           27        plug-in      0.2454106\n#\n 6           34        plug-in      0.2645273\n#\n 7           40        plug-in      0.2742581\n#\n 8           47        plug-in      0.3173223\n#\n 9           53        plug-in      0.3512767\n#\n 10          60        plug-in      0.3941339\n#\n 11           1       debiased      0.5336094\n#\n 12           8       debiased      0.2450656\n#\n 13          14       debiased      0.2403037\n#\n 14          21       debiased      0.2546584\n#\n 15          27       debiased      0.3075224\n#\n 16          34       debiased      0.3172533\n#\n 17          40       debiased      0.3125604\n#\n 18          47       debiased      0.2979986\n#\n 19          53       debiased      0.3079365\n#\n 20          60       debiased      0.3654244\n\n\n\n\nThere seems to be a preference for a lower number of principal components (\n27) for both \n\"plug-in\"\n\nand \n\"debiased\"\n with \n\"plug-in\"\n achieving slightly lower error rates.\n\n\nWriting a custom preprocessing wrapper\n\n\nIf the options offered by \nmakePreprocWrapperCaret\n are not enough, you can write your own\npreprocessing wrapper using function \nmakePreprocWrapper\n.\n\n\nAs described in the tutorial section about \nwrapped learners\n wrappers are\nimplemented using a \ntrain\n and a \npredict\n method.\nIn case of preprocessing wrappers these methods specify how to transform the data before\ntraining and before prediction and are \ncompletely user-defined\n.\n\n\nBelow we show how to create a preprocessing wrapper that centers and scales the data before\ntraining/predicting.\nSome learning methods as, e.g., k nearest neighbors, support vector machines or neural networks\nusually require scaled features.\nMany, but not all, have a built-in scaling option where the training data set is scaled before\nmodel fitting and the test data set is scaled accordingly, that is by using the scaling\nparameters from the training stage, before making predictions.\nIn the following we show how to add a scaling option to a \nLearner\n by coupling\nit with function \nscale\n.\n\n\nNote that we chose this simple example for demonstration.\nCentering/scaling the data is also possible with \nmakePreprocWrapperCaret\n.\n\n\nSpecifying the train function\n\n\nThe \ntrain\n function has to be a function with the following arguments:\n\n\n\n\ndata\n is a \ndata.frame\n with columns for all features and\n  the target variable.\n\n\ntarget\n is a string and denotes the name of the target variable in \ndata\n.\n\n\nargs\n is a \nlist\n of further arguments and parameters that influence the\n  preprocessing.\n\n\n\n\nIt must return a \nlist\n with elements \n$data\n and \n$control\n,\nwhere \n$data\n is the preprocessed data set and \n$control\n stores all information required\nto preprocess the data before prediction.\n\n\nThe \ntrain\n function for the scaling example is given below. It calls \nscale\n on the\nnumerical features and returns the scaled training data and the corresponding scaling parameters.\n\n\nargs\n contains the \ncenter\n and \nscale\n arguments of function \nscale\n\nand slot \n$control\n stores the scaling parameters to be used in the prediction stage.\n\n\nRegarding the latter note that the \ncenter\n and \nscale\n arguments of \nscale\n\ncan be either a logical value or a numeric vector of length equal to the number of the numeric\ncolumns in \ndata\n, respectively.\nIf a logical value was passed to \nargs\n we store the column means and standard deviations/\nroot mean squares in the \n$center\n and \n$scale\n slots of the returned \n$control\n object.\n\n\ntrainfun = function(data, target, args = list(center, scale)) {\n  ## Identify numerical features\n  cns = colnames(data)\n  nums = setdiff(cns[sapply(data, is.numeric)], target)\n  ## Extract numerical features from the data set and call scale\n  x = as.matrix(data[, nums, drop = FALSE])\n  x = scale(x, center = args$center, scale = args$scale)\n  ## Store the scaling parameters in control\n  ## These are needed to preprocess the data before prediction\n  control = args\n  if (is.logical(control$center) \n control$center)\n    control$center = attr(x, \nscaled:center\n)\n  if (is.logical(control$scale) \n control$scale)\n    control$scale = attr(x, \nscaled:scale\n)\n  ## Recombine the data\n  data = data[, setdiff(cns, nums), drop = FALSE]\n  data = cbind(data, as.data.frame(x))\n  return(list(data = data, control = control))\n}\n\n\n\n\nSpecifying the predict function\n\n\nThe \npredict\n function has the following arguments:\n\n\n\n\ndata\n is a \ndata.frame\n containing \nonly\n feature values\n  (as for prediction the target values naturally are not known).\n\n\ntarget\n is a string indicating the name of the target variable.\n\n\nargs\n are the \nargs\n that were passed to the \ntrain\n function.\n\n\ncontrol\n is the object returned by the \ntrain\n function.\n\n\n\n\nIt returns the preprocessed data.\n\n\nIn our scaling example the \npredict\n function scales the numerical features using the\nparameters from the training stage stored in \ncontrol\n.\n\n\npredictfun = function(data, target, args, control) {\n  ## Identify numerical features\n  cns = colnames(data)\n  nums = cns[sapply(data, is.numeric)]\n  ## Extract numerical features from the data set and call scale\n  x = as.matrix(data[, nums, drop = FALSE])\n  x = scale(x, center = control$center, scale = control$scale)\n  ## Recombine the data\n  data = data[, setdiff(cns, nums), drop = FALSE]\n  data = cbind(data, as.data.frame(x))\n  return(data)\n}\n\n\n\n\nCreating the preprocessing wrapper\n\n\nBelow we create a preprocessing wrapper with a \nregression neural network\n (which\nitself does not have a scaling option) as base learner.\n\n\nThe \ntrain\n and \npredict\n functions defined above are passed to \nmakePreprocWrapper\n via\nthe \ntrain\n and \npredict\n arguments.\n\npar.vals\n is a \nlist\n of parameter values that is relayed to the \nargs\n\nargument of the \ntrain\n function.\n\n\nlrn = makeLearner(\nregr.nnet\n, trace = FALSE, decay = 1e-02)\nlrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun,\n  par.vals = list(center = TRUE, scale = TRUE))\nlrn\n#\n Learner regr.nnet.preproc from package nnet\n#\n Type: regr\n#\n Name: ; Short name: \n#\n Class: PreprocWrapper\n#\n Properties: numerics,factors,weights\n#\n Predict-Type: response\n#\n Hyperparameters: size=3,trace=FALSE,decay=0.01\n\n\n\n\nLet's compare the cross-validated mean squared error (\nmse\n) on the\n\nBoston Housing data set\n with and without scaling.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\n\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: BostonHousing-example\n#\n Learner: regr.nnet.preproc\n#\n Aggr perf: mse.test.mean=20.6204314\n#\n Runtime: 0.214963\n\nlrn = makeLearner(\nregr.nnet\n, trace = FALSE, decay = 1e-02)\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: BostonHousing-example\n#\n Learner: regr.nnet\n#\n Aggr perf: mse.test.mean=55.0639088\n#\n Runtime: 0.103992\n\n\n\n\nJoint tuning of preprocessing and learner parameters\n\n\nOften it's not clear which preprocessing options work best with a certain learning algorithm.\nAs already shown for the number of principal components in \nmakePreprocWrapperCaret\n we can\n\ntune\n them easily together with other hyperparameters of the learner.\n\n\nIn our scaling example we can try if \nnnet\n works best with both centering and\nscaling the data or if it's better to omit one of the two operations or do no preprocessing\nat all.\nIn order to tune \ncenter\n and \nscale\n we have to add appropriate \nLearnerParam\ns\nto the \nparameter set\n of the wrapped learner.\n\n\nAs mentioned above \nscale\n allows for numeric and logical \ncenter\n and \nscale\n\narguments. As we want to use the latter option we declare \ncenter\n and \nscale\n as logical\nlearner parameters.\n\n\nlrn = makeLearner(\nregr.nnet\n, trace = FALSE)\nlrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun,\n  par.set = makeParamSet(\n    makeLogicalLearnerParam(\ncenter\n),\n    makeLogicalLearnerParam(\nscale\n)\n  ),\n  par.vals = list(center = TRUE, scale = TRUE))\n\nlrn\n#\n Learner regr.nnet.preproc from package nnet\n#\n Type: regr\n#\n Name: ; Short name: \n#\n Class: PreprocWrapper\n#\n Properties: numerics,factors,weights\n#\n Predict-Type: response\n#\n Hyperparameters: size=3,trace=FALSE,center=TRUE,scale=TRUE\n\ngetParamSet(lrn)\n#\n             Type len    Def      Constr Req Tunable Trafo\n#\n center   logical   -      -           -   -    TRUE     -\n#\n scale    logical   -      -           -   -    TRUE     -\n#\n size     integer   -      3    0 to Inf   -    TRUE     -\n#\n maxit    integer   -    100    1 to Inf   -    TRUE     -\n#\n linout   logical   -  FALSE           -   Y    TRUE     -\n#\n entropy  logical   -  FALSE           -   Y    TRUE     -\n#\n softmax  logical   -  FALSE           -   Y    TRUE     -\n#\n censored logical   -  FALSE           -   Y    TRUE     -\n#\n skip     logical   -  FALSE           -   -    TRUE     -\n#\n rang     numeric   -    0.7 -Inf to Inf   -    TRUE     -\n#\n decay    numeric   -      0    0 to Inf   -    TRUE     -\n#\n Hess     logical   -  FALSE           -   -    TRUE     -\n#\n trace    logical   -   TRUE           -   -   FALSE     -\n#\n MaxNWts  integer   -   1000    1 to Inf   -   FALSE     -\n#\n abstol   numeric   - 0.0001 -Inf to Inf   -    TRUE     -\n#\n reltol   numeric   -  1e-08 -Inf to Inf   -    TRUE     -\n\n\n\n\nNow we do a simple grid search for the \ndecay\n parameter of \nnnet\n and the\n\ncenter\n and \nscale\n parameters.\n\n\nrdesc = makeResampleDesc(\nHoldout\n)\nps = makeParamSet(\n  makeDiscreteParam(\ndecay\n, c(0, 0.05, 0.1)),\n  makeLogicalParam(\ncenter\n),\n  makeLogicalParam(\nscale\n)\n)\nctrl = makeTuneControlGrid()\nres = tuneParams(lrn, bh.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\n\nres\n#\n Tune result:\n#\n Op. pars: decay=0.05; center=FALSE; scale=TRUE\n#\n mse.test.mean=14.8430579\n\nas.data.frame(res$opt.path)\n#\n    decay center scale mse.test.mean dob eol error.message exec.time\n#\n 1      0   TRUE  TRUE      49.38128   1  NA          \nNA\n     0.045\n#\n 2   0.05   TRUE  TRUE      20.64761   2  NA          \nNA\n     0.056\n#\n 3    0.1   TRUE  TRUE      22.42986   3  NA          \nNA\n     0.057\n#\n 4      0  FALSE  TRUE      96.25474   4  NA          \nNA\n     0.028\n#\n 5   0.05  FALSE  TRUE      14.84306   5  NA          \nNA\n     0.063\n#\n 6    0.1  FALSE  TRUE      16.65383   6  NA          \nNA\n     0.051\n#\n 7      0   TRUE FALSE      40.51518   7  NA          \nNA\n     0.048\n#\n 8   0.05   TRUE FALSE      68.00069   8  NA          \nNA\n     0.060\n#\n 9    0.1   TRUE FALSE      55.42210   9  NA          \nNA\n     0.052\n#\n 10     0  FALSE FALSE      96.25474  10  NA          \nNA\n     0.025\n#\n 11  0.05  FALSE FALSE      56.25758  11  NA          \nNA\n     0.050\n#\n 12   0.1  FALSE FALSE      42.85529  12  NA          \nNA\n     0.046\n\n\n\n\nPreprocessing wrapper functions\n\n\nIf you have written a preprocessing wrapper that you might want to use from time to time\nit's a good idea to encapsulate it in an own function as shown below.\nIf you think your preprocessing method is something others might want to use as well and should\nbe integrated into \nmlr\n just \ncontact us\n.\n\n\nmakePreprocWrapperScale = function(learner, center = TRUE, scale = TRUE) {\n  trainfun = function(data, target, args = list(center, scale)) {\n    cns = colnames(data)\n    nums = setdiff(cns[sapply(data, is.numeric)], target)\n    x = as.matrix(data[, nums, drop = FALSE])\n    x = scale(x, center = args$center, scale = args$scale)\n    control = args\n    if (is.logical(control$center) \n control$center)\n      control$center = attr(x, \nscaled:center\n)\n    if (is.logical(control$scale) \n control$scale)\n      control$scale = attr(x, \nscaled:scale\n)\n    data = data[, setdiff(cns, nums), drop = FALSE]\n    data = cbind(data, as.data.frame(x))\n    return(list(data = data, control = control))\n  }\n  predictfun = function(data, target, args, control) {\n    cns = colnames(data)\n    nums = cns[sapply(data, is.numeric)]\n    x = as.matrix(data[, nums, drop = FALSE])\n    x = scale(x, center = control$center, scale = control$scale)\n    data = data[, setdiff(cns, nums), drop = FALSE]\n    data = cbind(data, as.data.frame(x))\n    return(data)\n  }\n  makePreprocWrapper(\n    learner,\n    train = trainfun,\n    predict = predictfun,\n    par.set = makeParamSet(\n      makeLogicalLearnerParam(\ncenter\n),\n      makeLogicalLearnerParam(\nscale\n)\n    ),\n    par.vals = list(center = center, scale = scale)\n  )\n}\n\nlrn = makePreprocWrapperScale(\nclassif.lda\n)\ntrain(lrn, iris.task)\n#\n Model for learner.id=classif.lda.preproc; learner.class=PreprocWrapper\n#\n Trained on: task.id = iris_example; obs = 150; features = 4\n#\n Hyperparameters: center=TRUE,scale=TRUE\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## preProcess(x, method = c(\nknnImpute\n, \npca\n), pcaComp = 10) \n## makePreprocWrapperCaret(learner, ppc.knnImpute = TRUE, ppc.pca = TRUE, ppc.pcaComp = 10) \nsonar.task \nlrn = makePreprocWrapperCaret(\nclassif.qda\n, ppc.pca = TRUE, ppc.thresh = 0.9) \nlrn \nmod = train(lrn, sonar.task) \nmod \n\ngetLearnerModel(mod) \n\ngetLearnerModel(mod, more.unwrap = TRUE) \nrin = makeResampleInstance(\nCV\n, iters = 3, stratify = TRUE, task = sonar.task) \nres = benchmark(list(\nclassif.qda\n, lrn), sonar.task, rin, show.info = FALSE) \nres \ngetParamSet(lrn) \nps = makeParamSet( \n  makeIntegerParam(\nppc.pcaComp\n, lower = 1, upper = getTaskNFeats(sonar.task)), \n  makeDiscreteParam(\npredict.method\n, values = c(\nplug-in\n, \ndebiased\n)) \n) \nctrl = makeTuneControlGrid(resolution = 10) \nres = tuneParams(lrn, sonar.task, rin, par.set = ps, control = ctrl, show.info = FALSE) \nres \n\nas.data.frame(res$opt.path)[1:3] \ntrainfun = function(data, target, args = list(center, scale)) { \n  ## Identify numerical features \n  cns = colnames(data) \n  nums = setdiff(cns[sapply(data, is.numeric)], target) \n  ## Extract numerical features from the data set and call scale \n  x = as.matrix(data[, nums, drop = FALSE]) \n  x = scale(x, center = args$center, scale = args$scale) \n  ## Store the scaling parameters in control \n  ## These are needed to preprocess the data before prediction \n  control = args \n  if (is.logical(control$center) \n control$center) \n    control$center = attr(x, \nscaled:center\n) \n  if (is.logical(control$scale) \n control$scale) \n    control$scale = attr(x, \nscaled:scale\n) \n  ## Recombine the data \n  data = data[, setdiff(cns, nums), drop = FALSE] \n  data = cbind(data, as.data.frame(x)) \n  return(list(data = data, control = control)) \n} \npredictfun = function(data, target, args, control) { \n  ## Identify numerical features \n  cns = colnames(data) \n  nums = cns[sapply(data, is.numeric)] \n  ## Extract numerical features from the data set and call scale \n  x = as.matrix(data[, nums, drop = FALSE]) \n  x = scale(x, center = control$center, scale = control$scale) \n  ## Recombine the data \n  data = data[, setdiff(cns, nums), drop = FALSE] \n  data = cbind(data, as.data.frame(x)) \n  return(data) \n} \nlrn = makeLearner(\nregr.nnet\n, trace = FALSE, decay = 1e-02) \nlrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun, \n  par.vals = list(center = TRUE, scale = TRUE)) \nlrn \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \n\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE) \nr \n\nlrn = makeLearner(\nregr.nnet\n, trace = FALSE, decay = 1e-02) \nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE) \nr \nlrn = makeLearner(\nregr.nnet\n, trace = FALSE) \nlrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun, \n  par.set = makeParamSet( \n    makeLogicalLearnerParam(\ncenter\n), \n    makeLogicalLearnerParam(\nscale\n) \n  ), \n  par.vals = list(center = TRUE, scale = TRUE)) \n\nlrn \n\ngetParamSet(lrn) \nrdesc = makeResampleDesc(\nHoldout\n) \nps = makeParamSet( \n  makeDiscreteParam(\ndecay\n, c(0, 0.05, 0.1)), \n  makeLogicalParam(\ncenter\n), \n  makeLogicalParam(\nscale\n) \n) \nctrl = makeTuneControlGrid() \nres = tuneParams(lrn, bh.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE) \n\nres \n\nas.data.frame(res$opt.path) \nmakePreprocWrapperScale = function(learner, center = TRUE, scale = TRUE) { \n  trainfun = function(data, target, args = list(center, scale)) { \n    cns = colnames(data) \n    nums = setdiff(cns[sapply(data, is.numeric)], target) \n    x = as.matrix(data[, nums, drop = FALSE]) \n    x = scale(x, center = args$center, scale = args$scale) \n    control = args \n    if (is.logical(control$center) \n control$center) \n      control$center = attr(x, \nscaled:center\n) \n    if (is.logical(control$scale) \n control$scale) \n      control$scale = attr(x, \nscaled:scale\n) \n    data = data[, setdiff(cns, nums), drop = FALSE] \n    data = cbind(data, as.data.frame(x)) \n    return(list(data = data, control = control)) \n  } \n  predictfun = function(data, target, args, control) { \n    cns = colnames(data) \n    nums = cns[sapply(data, is.numeric)] \n    x = as.matrix(data[, nums, drop = FALSE]) \n    x = scale(x, center = control$center, scale = control$scale) \n    data = data[, setdiff(cns, nums), drop = FALSE] \n    data = cbind(data, as.data.frame(x)) \n    return(data) \n  } \n  makePreprocWrapper( \n    learner, \n    train = trainfun, \n    predict = predictfun, \n    par.set = makeParamSet( \n      makeLogicalLearnerParam(\ncenter\n), \n      makeLogicalLearnerParam(\nscale\n) \n    ), \n    par.vals = list(center = center, scale = scale) \n  ) \n} \n\nlrn = makePreprocWrapperScale(\nclassif.lda\n) \ntrain(lrn, iris.task)", 
            "title": "Preprocessing"
        }, 
        {
            "location": "/preproc/index.html#data-preprocessing", 
            "text": "Data preprocessing refers to any transformation of the data done before applying a learning\nalgorithm.\nThis comprises for example finding and resolving inconsistencies, imputation of missing values,\nidentifying, removing or replacing outliers, discretizing numerical data or generating numerical\ndummy variables for categorical data, any kind of transformation like standardization of predictors\nor Box-Cox, dimensionality reduction and feature extraction and/or selection.  mlr  offers several options for data preprocessing.\nSome of the following simple methods to change a  Task  (or  data.frame )\nwere already mentioned on the page about  learning tasks :   capLargeValues : Convert large/infinite numeric values.  createDummyFeatures : Generate dummy variables for factor features.  dropFeatures : Remove selected features.  joinClassLevels : Only for classification: Merge existing classes to\n  new, larger classes.  mergeSmallFactorLevels : Merge infrequent levels of factor features.  normalizeFeatures : Normalize features by different methods, e.g.,\n  standardization or scaling to a certain range.  removeConstantFeatures : Remove constant features.  subsetTask : Remove observations and/or features from a  Task .   Moreover, there are tutorial pages devoted to   Feature selection  and  Imputation of missing values .", 
            "title": "Data Preprocessing"
        }, 
        {
            "location": "/preproc/index.html#fusing-learners-with-preprocessing", 
            "text": "mlr 's wrapper functionality permits to combine learners with preprocessing steps.\nThis means that the preprocessing \"belongs\" to the learner and is done any time the learner\nis trained or predictions are made.  This is, on the one hand, very practical.\nYou don't need to change any data or learning  Task s and it's quite easy to combine\ndifferent learners with different preprocessing steps.  On the other hand this helps to avoid a common mistake in evaluating the performance of a\nlearner with preprocessing:\nPreprocessing is often seen as completely independent of the later applied learning algorithms.\nWhen estimating the performance of the a learner, e.g., by cross-validation all preprocessing\nis done beforehand on the full data set and only training/predicting the learner is done on the\ntrain/test sets.\nDepending on what exactly is done as preprocessing this can lead to overoptimistic results.\nFor example if imputation by the mean is done on the whole data set before evaluating the learner\nperformance you are using information from the test data during training, which can cause\noveroptimistic performance results.  To clarify things one should distinguish between  data-dependent  and  data-independent \npreprocessing steps:\nData-dependent steps in some way learn from the data and give different results when applied to\ndifferent data sets. Data-independent steps always lead to the same results.\nClearly, correcting errors in the data or removing data columns like Ids that should\nnot be used for learning, is data-independent.\nImputation of missing values by the mean, as mentioned above, is data-dependent.\nImputation by a fixed constant, however, is not.  To get a honest estimate of learner performance combined with preprocessing, all data-dependent\npreprocessing steps must be included in the resampling.\nThis is automatically done when fusing a learner with preprocessing.  To this end  mlr  provides two  wrappers :   makePreprocWrapperCaret  is an interface to all preprocessing options offered by  caret 's\n   preProcess  function.  makePreprocWrapper  permits to write your own custom preprocessing methods by defining\n  the actions to be taken before training and before prediction.   As mentioned above the specified preprocessing steps then \"belong\" to the wrapped  Learner .\nIn contrast to the preprocessing options listed above like  normalizeFeatures   the  Task  itself remains unchanged,  the preprocessing is not done globally, i.e., for the whole data set, but for every pair of\n  training/test data sets in, e.g., resampling,  any parameters controlling the preprocessing as, e.g., the percentage of outliers to be removed\n  can be  tuned  together with the base learner parameters.   We start with some examples for  makePreprocWrapperCaret .", 
            "title": "Fusing learners with preprocessing"
        }, 
        {
            "location": "/preproc/index.html#preprocessing-with-makepreprocwrappercaret", 
            "text": "makePreprocWrapperCaret  is an interface to  caret 's  preProcess \nfunction that provides many different options like imputation of missing values,\ndata transformations as scaling the features to a certain range or Box-Cox and dimensionality\nreduction via Independent or Principal Component Analysis.\nFor all possible options see the help page of function  preProcess .  Note that the usage of  makePreprocWrapperCaret  is slightly different than that of preProcess .   makePreprocWrapperCaret  takes (almost) the same formal arguments as  preProcess ,\n  but their names are prefixed by  ppc. .  The only exception:  makePreprocWrapperCaret  does not have a  method  argument. Instead\n  all preprocessing options that would be passed to  preProcess 's  method \n  argument are given as individual logical parameters to  makePreprocWrapperCaret .   For example the following call to  preProcess  preProcess(x, method = c( knnImpute ,  pca ), pcaComp = 10)  with  x  being a  matrix  or  data.frame \nwould thus translate into  makePreprocWrapperCaret(learner, ppc.knnImpute = TRUE, ppc.pca = TRUE, ppc.pcaComp = 10)  where  learner  is a  mlr   Learner  or the name of a learner class like \"classif.lda\" .  If you enable multiple preprocessing options (like knn imputation and principal component\nanalysis above) these are executed in a certain order detailed on the help page of function preProcess .  In the following we show an example where principal components analysis (PCA) is used for\ndimensionality reduction.\nThis should never be applied blindly, but can be beneficial with learners that get problems\nwith high dimensionality or those that can profit from rotating the data.  We consider the  sonar.task , which poses a binary classification problem with 208 observations\nand 60 features.  sonar.task\n#  Supervised task: Sonar_example\n#  Type: classif\n#  Target: Class\n#  Observations: 208\n#  Features:\n#     numerics     factors     ordered functionals \n#           60           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 2\n#    M   R \n#  111  97 \n#  Positive class: M  Below we fuse  quadratic discriminant analysis  from package  MASS  with a principal\ncomponents preprocessing step.\nThe threshold is set to 0.9, i.e., the principal components necessary to explain a cumulative\npercentage of 90% of the total variance are kept.\nThe data are automatically standardized prior to PCA.  lrn = makePreprocWrapperCaret( classif.qda , ppc.pca = TRUE, ppc.thresh = 0.9)\nlrn\n#  Learner classif.qda.preproc from package MASS\n#  Type: classif\n#  Name: ; Short name: \n#  Class: PreprocWrapperCaret\n#  Properties: twoclass,multiclass,numerics,factors,prob\n#  Predict-Type: response\n#  Hyperparameters: ppc.BoxCox=FALSE,ppc.YeoJohnson=FALSE,ppc.expoTrans=FALSE,ppc.center=TRUE,ppc.scale=TRUE,ppc.range=FALSE,ppc.knnImpute=FALSE,ppc.bagImpute=FALSE,ppc.medianImpute=FALSE,ppc.pca=TRUE,ppc.ica=FALSE,ppc.spatialSign=FALSE,ppc.corr=FALSE,ppc.zv=FALSE,ppc.nzv=FALSE,ppc.thresh=0.9,ppc.na.remove=TRUE,ppc.k=5,ppc.fudge=0.2,ppc.numUnique=3,ppc.cutoff=0.9,ppc.freqCut=19,ppc.uniqueCut=10  The wrapped learner is trained on the  sonar.task .\nBy inspecting the underlying  qda  model, we see that the first 22\nprincipal components have been used for training.  mod = train(lrn, sonar.task)\nmod\n#  Model for learner.id=classif.qda.preproc; learner.class=PreprocWrapperCaret\n#  Trained on: task.id = Sonar_example; obs = 208; features = 60\n#  Hyperparameters: ppc.BoxCox=FALSE,ppc.YeoJohnson=FALSE,ppc.expoTrans=FALSE,ppc.center=TRUE,ppc.scale=TRUE,ppc.range=FALSE,ppc.knnImpute=FALSE,ppc.bagImpute=FALSE,ppc.medianImpute=FALSE,ppc.pca=TRUE,ppc.ica=FALSE,ppc.spatialSign=FALSE,ppc.corr=FALSE,ppc.zv=FALSE,ppc.nzv=FALSE,ppc.thresh=0.9,ppc.na.remove=TRUE,ppc.k=5,ppc.fudge=0.2,ppc.numUnique=3,ppc.cutoff=0.9,ppc.freqCut=19,ppc.uniqueCut=10\n\ngetLearnerModel(mod)\n#  Model for learner.id=classif.qda; learner.class=classif.qda\n#  Trained on: task.id = Sonar_example; obs = 208; features = 22\n#  Hyperparameters:\n\ngetLearnerModel(mod, more.unwrap = TRUE)\n#  Call:\n#  qda(f, data = getTaskData(.task, .subset, recode.target =  drop.levels ))\n#  \n#  Prior probabilities of groups:\n#          M         R \n#  0.5336538 0.4663462 \n#  \n#  Group means:\n#           PC1        PC2        PC3         PC4         PC5         PC6\n#  M  0.5976122 -0.8058235  0.9773518  0.03794232 -0.04568166 -0.06721702\n#  R -0.6838655  0.9221279 -1.1184128 -0.04341853  0.05227489  0.07691845\n#           PC7         PC8        PC9       PC10        PC11          PC12\n#  M  0.2278162 -0.01034406 -0.2530606 -0.1793157 -0.04084466 -0.0004789888\n#  R -0.2606969  0.01183702  0.2895848  0.2051963  0.04673977  0.0005481212\n#           PC13       PC14        PC15        PC16        PC17        PC18\n#  M -0.06138758 -0.1057137  0.02808048  0.05215865 -0.07453265  0.03869042\n#  R  0.07024765  0.1209713 -0.03213333 -0.05968671  0.08528994 -0.04427460\n#           PC19         PC20        PC21         PC22\n#  M -0.01192247  0.006098658  0.01263492 -0.001224809\n#  R  0.01364323 -0.006978877 -0.01445851  0.001401586  Below the performances of  qda  with and without PCA preprocessing are compared\nin a  benchmark experiment .\nNote that we use stratified resampling to prevent errors in  qda  due to a too\nsmall number of observations from either class.  rin = makeResampleInstance( CV , iters = 3, stratify = TRUE, task = sonar.task)\nres = benchmark(list( classif.qda , lrn), sonar.task, rin, show.info = FALSE)\nres\n#          task.id          learner.id mmce.test.mean\n#  1 Sonar_example         classif.qda      0.3941339\n#  2 Sonar_example classif.qda.preproc      0.2643202  PCA preprocessing in this case turns out to be really beneficial for the\nperformance of Quadratic Discriminant Analysis.", 
            "title": "Preprocessing with makePreprocWrapperCaret"
        }, 
        {
            "location": "/preproc/index.html#joint-tuning-of-preprocessing-options-and-learner-parameters", 
            "text": "Let's see if we can optimize this a bit.\nThe threshold value of 0.9 above was chosen arbitrarily and led to 22 out of 60 principal\ncomponents.\nBut maybe a lower or higher number of principal components should be used.\nMoreover,  qda  has several options that control how the class covariance matrices\nor class probabilities are estimated.  Those preprocessing and learner parameters can be  tuned  jointly.\nBefore doing this let's first get an overview of all the parameters of the wrapped learner\nusing function  getParamSet .  getParamSet(lrn)\n#                       Type len     Def                      Constr Req\n#  ppc.BoxCox        logical   -   FALSE                           -   -\n#  ppc.YeoJohnson    logical   -   FALSE                           -   -\n#  ppc.expoTrans     logical   -   FALSE                           -   -\n#  ppc.center        logical   -    TRUE                           -   -\n#  ppc.scale         logical   -    TRUE                           -   -\n#  ppc.range         logical   -   FALSE                           -   -\n#  ppc.knnImpute     logical   -   FALSE                           -   -\n#  ppc.bagImpute     logical   -   FALSE                           -   -\n#  ppc.medianImpute  logical   -   FALSE                           -   -\n#  ppc.pca           logical   -   FALSE                           -   -\n#  ppc.ica           logical   -   FALSE                           -   -\n#  ppc.spatialSign   logical   -   FALSE                           -   -\n#  ppc.corr          logical   -   FALSE                           -   -\n#  ppc.zv            logical   -   FALSE                           -   -\n#  ppc.nzv           logical   -   FALSE                           -   -\n#  ppc.thresh        numeric   -    0.95                    0 to Inf   -\n#  ppc.pcaComp       integer   -       -                    1 to Inf   -\n#  ppc.na.remove     logical   -    TRUE                           -   -\n#  ppc.k             integer   -       5                    1 to Inf   -\n#  ppc.fudge         numeric   -     0.2                    0 to Inf   -\n#  ppc.numUnique     integer   -       3                    1 to Inf   -\n#  ppc.n.comp        integer   -       -                    1 to Inf   -\n#  ppc.cutoff        numeric   -     0.9                      0 to 1   -\n#  ppc.freqCut       numeric   -      19                    1 to Inf   -\n#  ppc.uniqueCut     numeric   -      10                    0 to Inf   -\n#  method           discrete   -  moment            moment,mle,mve,t   -\n#  nu                numeric   -       5                    2 to Inf   Y\n#  predict.method   discrete   - plug-in plug-in,predictive,debiased   -\n#                   Tunable Trafo\n#  ppc.BoxCox          TRUE     -\n#  ppc.YeoJohnson      TRUE     -\n#  ppc.expoTrans       TRUE     -\n#  ppc.center          TRUE     -\n#  ppc.scale           TRUE     -\n#  ppc.range           TRUE     -\n#  ppc.knnImpute       TRUE     -\n#  ppc.bagImpute       TRUE     -\n#  ppc.medianImpute    TRUE     -\n#  ppc.pca             TRUE     -\n#  ppc.ica             TRUE     -\n#  ppc.spatialSign     TRUE     -\n#  ppc.corr            TRUE     -\n#  ppc.zv              TRUE     -\n#  ppc.nzv             TRUE     -\n#  ppc.thresh          TRUE     -\n#  ppc.pcaComp         TRUE     -\n#  ppc.na.remove       TRUE     -\n#  ppc.k               TRUE     -\n#  ppc.fudge           TRUE     -\n#  ppc.numUnique       TRUE     -\n#  ppc.n.comp          TRUE     -\n#  ppc.cutoff          TRUE     -\n#  ppc.freqCut         TRUE     -\n#  ppc.uniqueCut       TRUE     -\n#  method              TRUE     -\n#  nu                  TRUE     -\n#  predict.method      TRUE     -  The parameters prefixed by  ppc.  belong to preprocessing.  method ,  nu  and  predict.method \nare  qda  parameters.  Instead of tuning the PCA threshold ( ppc.thresh ) we tune the number of principal\ncomponents ( ppc.pcaComp ) directly.\nMoreover, for  qda  we try two different ways to estimate the posterior probabilities\n(parameter  predict.method ): the usual plug-in estimates and unbiased estimates.  We perform a grid search and set the resolution to 10.\nThis is for demonstration. You might want to use a finer resolution.  ps = makeParamSet(\n  makeIntegerParam( ppc.pcaComp , lower = 1, upper = getTaskNFeats(sonar.task)),\n  makeDiscreteParam( predict.method , values = c( plug-in ,  debiased ))\n)\nctrl = makeTuneControlGrid(resolution = 10)\nres = tuneParams(lrn, sonar.task, rin, par.set = ps, control = ctrl, show.info = FALSE)\nres\n#  Tune result:\n#  Op. pars: ppc.pcaComp=8; predict.method=plug-in\n#  mmce.test.mean=0.1920635\n\nas.data.frame(res$opt.path)[1:3]\n#     ppc.pcaComp predict.method mmce.test.mean\n#  1            1        plug-in      0.4757074\n#  2            8        plug-in      0.1920635\n#  3           14        plug-in      0.2162871\n#  4           21        plug-in      0.2643202\n#  5           27        plug-in      0.2454106\n#  6           34        plug-in      0.2645273\n#  7           40        plug-in      0.2742581\n#  8           47        plug-in      0.3173223\n#  9           53        plug-in      0.3512767\n#  10          60        plug-in      0.3941339\n#  11           1       debiased      0.5336094\n#  12           8       debiased      0.2450656\n#  13          14       debiased      0.2403037\n#  14          21       debiased      0.2546584\n#  15          27       debiased      0.3075224\n#  16          34       debiased      0.3172533\n#  17          40       debiased      0.3125604\n#  18          47       debiased      0.2979986\n#  19          53       debiased      0.3079365\n#  20          60       debiased      0.3654244  There seems to be a preference for a lower number of principal components ( 27) for both  \"plug-in\" \nand  \"debiased\"  with  \"plug-in\"  achieving slightly lower error rates.", 
            "title": "Joint tuning of preprocessing options and learner parameters"
        }, 
        {
            "location": "/preproc/index.html#writing-a-custom-preprocessing-wrapper", 
            "text": "If the options offered by  makePreprocWrapperCaret  are not enough, you can write your own\npreprocessing wrapper using function  makePreprocWrapper .  As described in the tutorial section about  wrapped learners  wrappers are\nimplemented using a  train  and a  predict  method.\nIn case of preprocessing wrappers these methods specify how to transform the data before\ntraining and before prediction and are  completely user-defined .  Below we show how to create a preprocessing wrapper that centers and scales the data before\ntraining/predicting.\nSome learning methods as, e.g., k nearest neighbors, support vector machines or neural networks\nusually require scaled features.\nMany, but not all, have a built-in scaling option where the training data set is scaled before\nmodel fitting and the test data set is scaled accordingly, that is by using the scaling\nparameters from the training stage, before making predictions.\nIn the following we show how to add a scaling option to a  Learner  by coupling\nit with function  scale .  Note that we chose this simple example for demonstration.\nCentering/scaling the data is also possible with  makePreprocWrapperCaret .", 
            "title": "Writing a custom preprocessing wrapper"
        }, 
        {
            "location": "/preproc/index.html#specifying-the-train-function", 
            "text": "The  train  function has to be a function with the following arguments:   data  is a  data.frame  with columns for all features and\n  the target variable.  target  is a string and denotes the name of the target variable in  data .  args  is a  list  of further arguments and parameters that influence the\n  preprocessing.   It must return a  list  with elements  $data  and  $control ,\nwhere  $data  is the preprocessed data set and  $control  stores all information required\nto preprocess the data before prediction.  The  train  function for the scaling example is given below. It calls  scale  on the\nnumerical features and returns the scaled training data and the corresponding scaling parameters.  args  contains the  center  and  scale  arguments of function  scale \nand slot  $control  stores the scaling parameters to be used in the prediction stage.  Regarding the latter note that the  center  and  scale  arguments of  scale \ncan be either a logical value or a numeric vector of length equal to the number of the numeric\ncolumns in  data , respectively.\nIf a logical value was passed to  args  we store the column means and standard deviations/\nroot mean squares in the  $center  and  $scale  slots of the returned  $control  object.  trainfun = function(data, target, args = list(center, scale)) {\n  ## Identify numerical features\n  cns = colnames(data)\n  nums = setdiff(cns[sapply(data, is.numeric)], target)\n  ## Extract numerical features from the data set and call scale\n  x = as.matrix(data[, nums, drop = FALSE])\n  x = scale(x, center = args$center, scale = args$scale)\n  ## Store the scaling parameters in control\n  ## These are needed to preprocess the data before prediction\n  control = args\n  if (is.logical(control$center)   control$center)\n    control$center = attr(x,  scaled:center )\n  if (is.logical(control$scale)   control$scale)\n    control$scale = attr(x,  scaled:scale )\n  ## Recombine the data\n  data = data[, setdiff(cns, nums), drop = FALSE]\n  data = cbind(data, as.data.frame(x))\n  return(list(data = data, control = control))\n}", 
            "title": "Specifying the train function"
        }, 
        {
            "location": "/preproc/index.html#specifying-the-predict-function", 
            "text": "The  predict  function has the following arguments:   data  is a  data.frame  containing  only  feature values\n  (as for prediction the target values naturally are not known).  target  is a string indicating the name of the target variable.  args  are the  args  that were passed to the  train  function.  control  is the object returned by the  train  function.   It returns the preprocessed data.  In our scaling example the  predict  function scales the numerical features using the\nparameters from the training stage stored in  control .  predictfun = function(data, target, args, control) {\n  ## Identify numerical features\n  cns = colnames(data)\n  nums = cns[sapply(data, is.numeric)]\n  ## Extract numerical features from the data set and call scale\n  x = as.matrix(data[, nums, drop = FALSE])\n  x = scale(x, center = control$center, scale = control$scale)\n  ## Recombine the data\n  data = data[, setdiff(cns, nums), drop = FALSE]\n  data = cbind(data, as.data.frame(x))\n  return(data)\n}", 
            "title": "Specifying the predict function"
        }, 
        {
            "location": "/preproc/index.html#creating-the-preprocessing-wrapper", 
            "text": "Below we create a preprocessing wrapper with a  regression neural network  (which\nitself does not have a scaling option) as base learner.  The  train  and  predict  functions defined above are passed to  makePreprocWrapper  via\nthe  train  and  predict  arguments. par.vals  is a  list  of parameter values that is relayed to the  args \nargument of the  train  function.  lrn = makeLearner( regr.nnet , trace = FALSE, decay = 1e-02)\nlrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun,\n  par.vals = list(center = TRUE, scale = TRUE))\nlrn\n#  Learner regr.nnet.preproc from package nnet\n#  Type: regr\n#  Name: ; Short name: \n#  Class: PreprocWrapper\n#  Properties: numerics,factors,weights\n#  Predict-Type: response\n#  Hyperparameters: size=3,trace=FALSE,decay=0.01  Let's compare the cross-validated mean squared error ( mse ) on the Boston Housing data set  with and without scaling.  rdesc = makeResampleDesc( CV , iters = 3)\n\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE)\nr\n#  Resample Result\n#  Task: BostonHousing-example\n#  Learner: regr.nnet.preproc\n#  Aggr perf: mse.test.mean=20.6204314\n#  Runtime: 0.214963\n\nlrn = makeLearner( regr.nnet , trace = FALSE, decay = 1e-02)\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE)\nr\n#  Resample Result\n#  Task: BostonHousing-example\n#  Learner: regr.nnet\n#  Aggr perf: mse.test.mean=55.0639088\n#  Runtime: 0.103992", 
            "title": "Creating the preprocessing wrapper"
        }, 
        {
            "location": "/preproc/index.html#joint-tuning-of-preprocessing-and-learner-parameters", 
            "text": "Often it's not clear which preprocessing options work best with a certain learning algorithm.\nAs already shown for the number of principal components in  makePreprocWrapperCaret  we can tune  them easily together with other hyperparameters of the learner.  In our scaling example we can try if  nnet  works best with both centering and\nscaling the data or if it's better to omit one of the two operations or do no preprocessing\nat all.\nIn order to tune  center  and  scale  we have to add appropriate  LearnerParam s\nto the  parameter set  of the wrapped learner.  As mentioned above  scale  allows for numeric and logical  center  and  scale \narguments. As we want to use the latter option we declare  center  and  scale  as logical\nlearner parameters.  lrn = makeLearner( regr.nnet , trace = FALSE)\nlrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun,\n  par.set = makeParamSet(\n    makeLogicalLearnerParam( center ),\n    makeLogicalLearnerParam( scale )\n  ),\n  par.vals = list(center = TRUE, scale = TRUE))\n\nlrn\n#  Learner regr.nnet.preproc from package nnet\n#  Type: regr\n#  Name: ; Short name: \n#  Class: PreprocWrapper\n#  Properties: numerics,factors,weights\n#  Predict-Type: response\n#  Hyperparameters: size=3,trace=FALSE,center=TRUE,scale=TRUE\n\ngetParamSet(lrn)\n#              Type len    Def      Constr Req Tunable Trafo\n#  center   logical   -      -           -   -    TRUE     -\n#  scale    logical   -      -           -   -    TRUE     -\n#  size     integer   -      3    0 to Inf   -    TRUE     -\n#  maxit    integer   -    100    1 to Inf   -    TRUE     -\n#  linout   logical   -  FALSE           -   Y    TRUE     -\n#  entropy  logical   -  FALSE           -   Y    TRUE     -\n#  softmax  logical   -  FALSE           -   Y    TRUE     -\n#  censored logical   -  FALSE           -   Y    TRUE     -\n#  skip     logical   -  FALSE           -   -    TRUE     -\n#  rang     numeric   -    0.7 -Inf to Inf   -    TRUE     -\n#  decay    numeric   -      0    0 to Inf   -    TRUE     -\n#  Hess     logical   -  FALSE           -   -    TRUE     -\n#  trace    logical   -   TRUE           -   -   FALSE     -\n#  MaxNWts  integer   -   1000    1 to Inf   -   FALSE     -\n#  abstol   numeric   - 0.0001 -Inf to Inf   -    TRUE     -\n#  reltol   numeric   -  1e-08 -Inf to Inf   -    TRUE     -  Now we do a simple grid search for the  decay  parameter of  nnet  and the center  and  scale  parameters.  rdesc = makeResampleDesc( Holdout )\nps = makeParamSet(\n  makeDiscreteParam( decay , c(0, 0.05, 0.1)),\n  makeLogicalParam( center ),\n  makeLogicalParam( scale )\n)\nctrl = makeTuneControlGrid()\nres = tuneParams(lrn, bh.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\n\nres\n#  Tune result:\n#  Op. pars: decay=0.05; center=FALSE; scale=TRUE\n#  mse.test.mean=14.8430579\n\nas.data.frame(res$opt.path)\n#     decay center scale mse.test.mean dob eol error.message exec.time\n#  1      0   TRUE  TRUE      49.38128   1  NA           NA      0.045\n#  2   0.05   TRUE  TRUE      20.64761   2  NA           NA      0.056\n#  3    0.1   TRUE  TRUE      22.42986   3  NA           NA      0.057\n#  4      0  FALSE  TRUE      96.25474   4  NA           NA      0.028\n#  5   0.05  FALSE  TRUE      14.84306   5  NA           NA      0.063\n#  6    0.1  FALSE  TRUE      16.65383   6  NA           NA      0.051\n#  7      0   TRUE FALSE      40.51518   7  NA           NA      0.048\n#  8   0.05   TRUE FALSE      68.00069   8  NA           NA      0.060\n#  9    0.1   TRUE FALSE      55.42210   9  NA           NA      0.052\n#  10     0  FALSE FALSE      96.25474  10  NA           NA      0.025\n#  11  0.05  FALSE FALSE      56.25758  11  NA           NA      0.050\n#  12   0.1  FALSE FALSE      42.85529  12  NA           NA      0.046", 
            "title": "Joint tuning of preprocessing and learner parameters"
        }, 
        {
            "location": "/preproc/index.html#preprocessing-wrapper-functions", 
            "text": "If you have written a preprocessing wrapper that you might want to use from time to time\nit's a good idea to encapsulate it in an own function as shown below.\nIf you think your preprocessing method is something others might want to use as well and should\nbe integrated into  mlr  just  contact us .  makePreprocWrapperScale = function(learner, center = TRUE, scale = TRUE) {\n  trainfun = function(data, target, args = list(center, scale)) {\n    cns = colnames(data)\n    nums = setdiff(cns[sapply(data, is.numeric)], target)\n    x = as.matrix(data[, nums, drop = FALSE])\n    x = scale(x, center = args$center, scale = args$scale)\n    control = args\n    if (is.logical(control$center)   control$center)\n      control$center = attr(x,  scaled:center )\n    if (is.logical(control$scale)   control$scale)\n      control$scale = attr(x,  scaled:scale )\n    data = data[, setdiff(cns, nums), drop = FALSE]\n    data = cbind(data, as.data.frame(x))\n    return(list(data = data, control = control))\n  }\n  predictfun = function(data, target, args, control) {\n    cns = colnames(data)\n    nums = cns[sapply(data, is.numeric)]\n    x = as.matrix(data[, nums, drop = FALSE])\n    x = scale(x, center = control$center, scale = control$scale)\n    data = data[, setdiff(cns, nums), drop = FALSE]\n    data = cbind(data, as.data.frame(x))\n    return(data)\n  }\n  makePreprocWrapper(\n    learner,\n    train = trainfun,\n    predict = predictfun,\n    par.set = makeParamSet(\n      makeLogicalLearnerParam( center ),\n      makeLogicalLearnerParam( scale )\n    ),\n    par.vals = list(center = center, scale = scale)\n  )\n}\n\nlrn = makePreprocWrapperScale( classif.lda )\ntrain(lrn, iris.task)\n#  Model for learner.id=classif.lda.preproc; learner.class=PreprocWrapper\n#  Trained on: task.id = iris_example; obs = 150; features = 4\n#  Hyperparameters: center=TRUE,scale=TRUE", 
            "title": "Preprocessing wrapper functions"
        }, 
        {
            "location": "/preproc/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## preProcess(x, method = c( knnImpute ,  pca ), pcaComp = 10) \n## makePreprocWrapperCaret(learner, ppc.knnImpute = TRUE, ppc.pca = TRUE, ppc.pcaComp = 10) \nsonar.task \nlrn = makePreprocWrapperCaret( classif.qda , ppc.pca = TRUE, ppc.thresh = 0.9) \nlrn \nmod = train(lrn, sonar.task) \nmod \n\ngetLearnerModel(mod) \n\ngetLearnerModel(mod, more.unwrap = TRUE) \nrin = makeResampleInstance( CV , iters = 3, stratify = TRUE, task = sonar.task) \nres = benchmark(list( classif.qda , lrn), sonar.task, rin, show.info = FALSE) \nres \ngetParamSet(lrn) \nps = makeParamSet( \n  makeIntegerParam( ppc.pcaComp , lower = 1, upper = getTaskNFeats(sonar.task)), \n  makeDiscreteParam( predict.method , values = c( plug-in ,  debiased )) \n) \nctrl = makeTuneControlGrid(resolution = 10) \nres = tuneParams(lrn, sonar.task, rin, par.set = ps, control = ctrl, show.info = FALSE) \nres \n\nas.data.frame(res$opt.path)[1:3] \ntrainfun = function(data, target, args = list(center, scale)) { \n  ## Identify numerical features \n  cns = colnames(data) \n  nums = setdiff(cns[sapply(data, is.numeric)], target) \n  ## Extract numerical features from the data set and call scale \n  x = as.matrix(data[, nums, drop = FALSE]) \n  x = scale(x, center = args$center, scale = args$scale) \n  ## Store the scaling parameters in control \n  ## These are needed to preprocess the data before prediction \n  control = args \n  if (is.logical(control$center)   control$center) \n    control$center = attr(x,  scaled:center ) \n  if (is.logical(control$scale)   control$scale) \n    control$scale = attr(x,  scaled:scale ) \n  ## Recombine the data \n  data = data[, setdiff(cns, nums), drop = FALSE] \n  data = cbind(data, as.data.frame(x)) \n  return(list(data = data, control = control)) \n} \npredictfun = function(data, target, args, control) { \n  ## Identify numerical features \n  cns = colnames(data) \n  nums = cns[sapply(data, is.numeric)] \n  ## Extract numerical features from the data set and call scale \n  x = as.matrix(data[, nums, drop = FALSE]) \n  x = scale(x, center = control$center, scale = control$scale) \n  ## Recombine the data \n  data = data[, setdiff(cns, nums), drop = FALSE] \n  data = cbind(data, as.data.frame(x)) \n  return(data) \n} \nlrn = makeLearner( regr.nnet , trace = FALSE, decay = 1e-02) \nlrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun, \n  par.vals = list(center = TRUE, scale = TRUE)) \nlrn \nrdesc = makeResampleDesc( CV , iters = 3) \n\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE) \nr \n\nlrn = makeLearner( regr.nnet , trace = FALSE, decay = 1e-02) \nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE) \nr \nlrn = makeLearner( regr.nnet , trace = FALSE) \nlrn = makePreprocWrapper(lrn, train = trainfun, predict = predictfun, \n  par.set = makeParamSet( \n    makeLogicalLearnerParam( center ), \n    makeLogicalLearnerParam( scale ) \n  ), \n  par.vals = list(center = TRUE, scale = TRUE)) \n\nlrn \n\ngetParamSet(lrn) \nrdesc = makeResampleDesc( Holdout ) \nps = makeParamSet( \n  makeDiscreteParam( decay , c(0, 0.05, 0.1)), \n  makeLogicalParam( center ), \n  makeLogicalParam( scale ) \n) \nctrl = makeTuneControlGrid() \nres = tuneParams(lrn, bh.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE) \n\nres \n\nas.data.frame(res$opt.path) \nmakePreprocWrapperScale = function(learner, center = TRUE, scale = TRUE) { \n  trainfun = function(data, target, args = list(center, scale)) { \n    cns = colnames(data) \n    nums = setdiff(cns[sapply(data, is.numeric)], target) \n    x = as.matrix(data[, nums, drop = FALSE]) \n    x = scale(x, center = args$center, scale = args$scale) \n    control = args \n    if (is.logical(control$center)   control$center) \n      control$center = attr(x,  scaled:center ) \n    if (is.logical(control$scale)   control$scale) \n      control$scale = attr(x,  scaled:scale ) \n    data = data[, setdiff(cns, nums), drop = FALSE] \n    data = cbind(data, as.data.frame(x)) \n    return(list(data = data, control = control)) \n  } \n  predictfun = function(data, target, args, control) { \n    cns = colnames(data) \n    nums = cns[sapply(data, is.numeric)] \n    x = as.matrix(data[, nums, drop = FALSE]) \n    x = scale(x, center = control$center, scale = control$scale) \n    data = data[, setdiff(cns, nums), drop = FALSE] \n    data = cbind(data, as.data.frame(x)) \n    return(data) \n  } \n  makePreprocWrapper( \n    learner, \n    train = trainfun, \n    predict = predictfun, \n    par.set = makeParamSet( \n      makeLogicalLearnerParam( center ), \n      makeLogicalLearnerParam( scale ) \n    ), \n    par.vals = list(center = center, scale = scale) \n  ) \n} \n\nlrn = makePreprocWrapperScale( classif.lda ) \ntrain(lrn, iris.task)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/performance/index.html", 
            "text": "Evaluating Learner Performance\n\n\nThe quality of the predictions of a model in \nmlr\n can be assessed with respect to a\nnumber of different performance measures.\nIn order to calculate the performance measures, call \nperformance\n on the object\nreturned by \npredict\n and specify the desired performance measures.\n\n\nAvailable performance measures\n\n\nmlr\n provides a large number of performance measures for all types of learning problems.\nTypical performance measures for \nclassification\n are the mean misclassification error (\nmmce\n),\naccuracy (\nacc\n) or measures based on \nROC analysis\n.\nFor \nregression\n the mean of squared errors (\nmse\n) or mean of absolute errors (\nmae\n)\nare usually considered.\nFor \nclustering\n tasks, measures such as the Dunn index (\ndunn\n) are provided,\nwhile for \nsurvival\n predictions, the Concordance Index (\ncindex\n) is\nsupported, and for \ncost-sensitive\n predictions the misclassification penalty\n(\nmcp\n) and others. It is also possible to access the time to train the\nlearner (\ntimetrain\n), the time to compute the prediction (\ntimepredict\n) and their\nsum (\ntimeboth\n) as performance measures.\n\n\nTo see which performance measures are implemented, have a look at the\n\ntable of performance measures\n and the \nmeasures\n documentation page.\n\n\nIf you want to implement an additional measure or include a measure with\nnon-standard misclassification costs, see the section on\n\ncreating custom measures\n.\n\n\nListing measures\n\n\nThe properties and requirements of the individual measures are shown in the \ntable of performance measures\n.\n\n\nIf you would like a list of available measures with certain properties or suitable for a\ncertain learning \nTask\n use the function \nlistMeasures\n.\n\n\n## Performance measures for classification with multiple classes\nlistMeasures(\nclassif\n, properties = \nclassif.multi\n)\n#\n  [1] \nfeatperc\n         \nmmce\n             \nlsr\n             \n#\n  [4] \nqsr\n              \ntimeboth\n         \nmulticlass.aunp\n \n#\n  [7] \ntimetrain\n        \nmulticlass.aunu\n  \nber\n             \n#\n [10] \ntimepredict\n      \nmulticlass.brier\n \nssr\n             \n#\n [13] \nacc\n              \nlogloss\n          \nwkappa\n          \n#\n [16] \nmulticlass.au1p\n  \nmulticlass.au1u\n  \nkappa\n\n## Performance measure suitable for the iris classification task\nlistMeasures(iris.task)\n#\n  [1] \nfeatperc\n         \nmmce\n             \nlsr\n             \n#\n  [4] \nqsr\n              \ntimeboth\n         \nmulticlass.aunp\n \n#\n  [7] \ntimetrain\n        \nmulticlass.aunu\n  \nber\n             \n#\n [10] \ntimepredict\n      \nmulticlass.brier\n \nssr\n             \n#\n [13] \nacc\n              \nlogloss\n          \nwkappa\n          \n#\n [16] \nmulticlass.au1p\n  \nmulticlass.au1u\n  \nkappa\n\n\n\n\n\nFor convenience there exists a default measure for each type of learning problem, which is\ncalculated if nothing else is specified. As defaults we chose the most commonly used measures for the\nrespective types, e.g., the mean squared error (\nmse\n) for regression and the\nmisclassification rate (\nmmce\n) for classification.\nThe help page of function \ngetDefaultMeasure\n lists all defaults for all types of learning problems.\nThe function itself returns the default measure for a given task type, \nTask\n or \nLearner\n.\n\n\n## Get default measure for iris.task\ngetDefaultMeasure(iris.task)\n#\n Name: Mean misclassification error\n#\n Performance measure: mmce\n#\n Properties: classif,classif.multi,req.pred,req.truth\n#\n Minimize: TRUE\n#\n Best: 0; Worst: 1\n#\n Aggregated by: test.mean\n#\n Arguments: \n#\n Note: Defined as: mean(response != truth)\n\n## Get the default measure for linear regression\ngetDefaultMeasure(makeLearner(\nregr.lm\n))\n#\n Name: Mean of squared errors\n#\n Performance measure: mse\n#\n Properties: regr,req.pred,req.truth\n#\n Minimize: TRUE\n#\n Best: 0; Worst: Inf\n#\n Aggregated by: test.mean\n#\n Arguments: \n#\n Note: Defined as: mean((response - truth)^2)\n\n\n\n\nCalculate performance measures\n\n\nIn the following example we fit a \ngradient boosting machine\n on a subset of the\n\nBostonHousing\n data set and calculate the default measure mean\nsquared error (\nmse\n) on the remaining observations.\n\n\nn = getTaskSize(bh.task)\nlrn = makeLearner(\nregr.gbm\n, n.trees = 1000)\nmod = train(lrn, task = bh.task, subset = seq(1, n, 2))\npred = predict(mod, task = bh.task, subset = seq(2, n, 2))\n\nperformance(pred)\n#\n      mse \n#\n 42.68414\n\n\n\n\nThe following code computes the median of squared errors (\nmedse\n) instead.\n\n\nperformance(pred, measures = medse)\n#\n    medse \n#\n 9.134965\n\n\n\n\nOf course, we can also calculate multiple performance measures at once by simply passing a\nlist of measures which can also include \nyour own measure\n.\n\n\nCalculate the mean squared error, median squared error and mean absolute error (\nmae\n).\n\n\nperformance(pred, measures = list(mse, medse, mae))\n#\n       mse     medse       mae \n#\n 42.684141  9.134965  4.536750\n\n\n\n\nFor the other types of learning problems and measures, calculating the performance basically\nworks in the same way.\n\n\nRequirements of performance measures\n\n\nNote that in order to calculate some performance measures it is required that you pass the\n\nTask\n or the \nfitted model\n in addition to the \nPrediction\n.\n\n\nFor example in order to assess the time needed for training (\ntimetrain\n), the fitted\nmodel has to be passed.\n\n\nperformance(pred, measures = timetrain, model = mod)\n#\n timetrain \n#\n     0.072\n\n\n\n\nFor many performance measures in cluster analysis the \nTask\n is required.\n\n\nlrn = makeLearner(\ncluster.kmeans\n, centers = 3)\nmod = train(lrn, mtcars.task)\npred = predict(mod, task = mtcars.task)\n\n## Calculate the Dunn index\nperformance(pred, measures = dunn, task = mtcars.task)\n#\n      dunn \n#\n 0.1462919\n\n\n\n\nMoreover, some measures require a certain type of prediction.\nFor example in binary classification in order to calculate the AUC (\nauc\n) -- the area\nunder the ROC (receiver operating characteristic) curve -- we have to make sure that posterior\nprobabilities are predicted.\nFor more information on ROC analysis, see the section on \nROC analysis\n.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, task = sonar.task)\npred = predict(mod, task = sonar.task)\n\nperformance(pred, measures = auc)\n#\n       auc \n#\n 0.9224018\n\n\n\n\nAlso bear in mind that many of the performance measures that are available for classification,\ne.g., the false positive rate (\nfpr\n), are only suitable for binary problems.\n\n\nAccess a performance measure\n\n\nPerformance measures in \nmlr\n are objects of class \nMeasure\n.\nIf you are interested in the properties or requirements of a single measure you can access it directly.\nSee the help page of \nMeasure\n for information on the individual slots.\n\n\n## Mean misclassification error\nstr(mmce)\n#\n List of 10\n#\n  $ id        : chr \nmmce\n\n#\n  $ minimize  : logi TRUE\n#\n  $ properties: chr [1:4] \nclassif\n \nclassif.multi\n \nreq.pred\n \nreq.truth\n\n#\n  $ fun       :function (task, model, pred, feats, extra.args)  \n#\n  $ extra.args: list()\n#\n  $ best      : num 0\n#\n  $ worst     : num 1\n#\n  $ name      : chr \nMean misclassification error\n\n#\n  $ note      : chr \nDefined as: mean(response != truth)\n\n#\n  $ aggr      :List of 4\n#\n   ..$ id        : chr \ntest.mean\n\n#\n   ..$ name      : chr \nTest mean\n\n#\n   ..$ fun       :function (task, perf.test, perf.train, measure, group, pred)  \n#\n   ..$ properties: chr \nreq.test\n\n#\n   ..- attr(*, \nclass\n)= chr \nAggregation\n\n#\n  - attr(*, \nclass\n)= chr \nMeasure\n\n\n\n\n\nBinary classification\n\n\nFor binary classification specialized techniques exist to analyze the performance.\n\n\nPlot performance versus threshold\n\n\nAs you may recall (see the previous section on \nmaking predictions\n)\nin binary classification we can adjust the threshold used to map probabilities to class labels.\nHelpful in this regard are the functions \ngenerateThreshVsPerfData\n and \nplotThreshVsPerf\n, which generate and plot, respectively, the learner performance versus the threshold.\n\n\nFor more performance plots and automatic threshold tuning see \nhere\n.\n\n\nIn the following example we consider the \nSonar\n data set and\nplot the false positive rate (\nfpr\n), the false negative rate (\nfnr\n)\nas well as the misclassification rate (\nmmce\n) for all possible threshold values.\n\n\nlrn = makeLearner(\nclassif.lda\n, predict.type = \nprob\n)\nn = getTaskSize(sonar.task)\nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))\npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))\n\n## Performance for the default threshold 0.5\nperformance(pred, measures = list(fpr, fnr, mmce))\n#\n       fpr       fnr      mmce \n#\n 0.2500000 0.3035714 0.2788462\n## Plot false negative and positive rates as well as the error rate versus the threshold\nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))\nplotThreshVsPerf(d)\n\n\n\n\n\n\nThere is an experimental \nggvis\n plotting function \nplotThreshVsPerfGGVIS\n which performs similarly\nto \nplotThreshVsPerf\n but instead of creating facetted subplots to visualize multiple learners and/or\nmultiple measures, one of them is mapped to an interactive sidebar which selects what to display.\n\n\nplotThreshVsPerfGGVIS(d)\n\n\n\n\nROC measures\n\n\nFor binary classification a large number of specialized measures exist, which can be nicely formatted into\none matrix, see for example the \nreceiver operating characteristic page on wikipedia\n.\n\n\nWe can generate a similiar table with the \ncalculateROCMeasures\n function.\n\n\nr = calculateROCMeasures(pred)\nr\n#\n     predicted\n#\n true M         R                            \n#\n    M 39        17        tpr: 0.7  fnr: 0.3 \n#\n    R 12        36        fpr: 0.25 tnr: 0.75\n#\n      ppv: 0.76 for: 0.32 lrp: 2.79 acc: 0.72\n#\n      fdr: 0.24 npv: 0.68 lrm: 0.4  dor: 6.88\n#\n \n#\n \n#\n Abbreviations:\n#\n tpr - True positive rate (Sensitivity, Recall)\n#\n fpr - False positive rate (Fall-out)\n#\n fnr - False negative rate (Miss rate)\n#\n tnr - True negative rate (Specificity)\n#\n ppv - Positive predictive value (Precision)\n#\n for - False omission rate\n#\n lrp - Positive likelihood ratio (LR+)\n#\n fdr - False discovery rate\n#\n npv - Negative predictive value\n#\n acc - Accuracy\n#\n lrm - Negative likelihood ratio (LR-)\n#\n dor - Diagnostic odds ratio\n\n\n\n\nThe top left \n2 \\times 2\n matrix is the \nconfusion matrix\n, which shows\nthe relative frequency of correctly and incorrectly classified observations. Below and to the right a large number of\nperformance measures that can be inferred from the confusion matrix are added. By default some\nadditional info about the measures is printed.\nYou can turn this off using the \nabbreviations\n argument of the \nprint\n method:\n\nprint(r, abbreviations = FALSE)\n.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## Performance measures for classification with multiple classes \nlistMeasures(\nclassif\n, properties = \nclassif.multi\n) \n## Performance measure suitable for the iris classification task \nlistMeasures(iris.task) \n## Get default measure for iris.task \ngetDefaultMeasure(iris.task) \n\n## Get the default measure for linear regression \ngetDefaultMeasure(makeLearner(\nregr.lm\n)) \nn = getTaskSize(bh.task) \nlrn = makeLearner(\nregr.gbm\n, n.trees = 1000) \nmod = train(lrn, task = bh.task, subset = seq(1, n, 2)) \npred = predict(mod, task = bh.task, subset = seq(2, n, 2)) \n\nperformance(pred) \nperformance(pred, measures = medse) \nperformance(pred, measures = list(mse, medse, mae)) \nperformance(pred, measures = timetrain, model = mod) \nlrn = makeLearner(\ncluster.kmeans\n, centers = 3) \nmod = train(lrn, mtcars.task) \npred = predict(mod, task = mtcars.task) \n\n## Calculate the Dunn index \nperformance(pred, measures = dunn, task = mtcars.task) \nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n) \nmod = train(lrn, task = sonar.task) \npred = predict(mod, task = sonar.task) \n\nperformance(pred, measures = auc) \n## Mean misclassification error \nstr(mmce) \nlrn = makeLearner(\nclassif.lda\n, predict.type = \nprob\n) \nn = getTaskSize(sonar.task) \nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2)) \npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2)) \n\n## Performance for the default threshold 0.5 \nperformance(pred, measures = list(fpr, fnr, mmce)) \n## Plot false negative and positive rates as well as the error rate versus the threshold \nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce)) \nplotThreshVsPerf(d) \n## plotThreshVsPerfGGVIS(d) \nr = calculateROCMeasures(pred) \nr", 
            "title": "Performance"
        }, 
        {
            "location": "/performance/index.html#evaluating-learner-performance", 
            "text": "The quality of the predictions of a model in  mlr  can be assessed with respect to a\nnumber of different performance measures.\nIn order to calculate the performance measures, call  performance  on the object\nreturned by  predict  and specify the desired performance measures.", 
            "title": "Evaluating Learner Performance"
        }, 
        {
            "location": "/performance/index.html#available-performance-measures", 
            "text": "mlr  provides a large number of performance measures for all types of learning problems.\nTypical performance measures for  classification  are the mean misclassification error ( mmce ),\naccuracy ( acc ) or measures based on  ROC analysis .\nFor  regression  the mean of squared errors ( mse ) or mean of absolute errors ( mae )\nare usually considered.\nFor  clustering  tasks, measures such as the Dunn index ( dunn ) are provided,\nwhile for  survival  predictions, the Concordance Index ( cindex ) is\nsupported, and for  cost-sensitive  predictions the misclassification penalty\n( mcp ) and others. It is also possible to access the time to train the\nlearner ( timetrain ), the time to compute the prediction ( timepredict ) and their\nsum ( timeboth ) as performance measures.  To see which performance measures are implemented, have a look at the table of performance measures  and the  measures  documentation page.  If you want to implement an additional measure or include a measure with\nnon-standard misclassification costs, see the section on creating custom measures .", 
            "title": "Available performance measures"
        }, 
        {
            "location": "/performance/index.html#listing-measures", 
            "text": "The properties and requirements of the individual measures are shown in the  table of performance measures .  If you would like a list of available measures with certain properties or suitable for a\ncertain learning  Task  use the function  listMeasures .  ## Performance measures for classification with multiple classes\nlistMeasures( classif , properties =  classif.multi )\n#   [1]  featperc           mmce               lsr              \n#   [4]  qsr                timeboth           multiclass.aunp  \n#   [7]  timetrain          multiclass.aunu    ber              \n#  [10]  timepredict        multiclass.brier   ssr              \n#  [13]  acc                logloss            wkappa           \n#  [16]  multiclass.au1p    multiclass.au1u    kappa \n## Performance measure suitable for the iris classification task\nlistMeasures(iris.task)\n#   [1]  featperc           mmce               lsr              \n#   [4]  qsr                timeboth           multiclass.aunp  \n#   [7]  timetrain          multiclass.aunu    ber              \n#  [10]  timepredict        multiclass.brier   ssr              \n#  [13]  acc                logloss            wkappa           \n#  [16]  multiclass.au1p    multiclass.au1u    kappa   For convenience there exists a default measure for each type of learning problem, which is\ncalculated if nothing else is specified. As defaults we chose the most commonly used measures for the\nrespective types, e.g., the mean squared error ( mse ) for regression and the\nmisclassification rate ( mmce ) for classification.\nThe help page of function  getDefaultMeasure  lists all defaults for all types of learning problems.\nThe function itself returns the default measure for a given task type,  Task  or  Learner .  ## Get default measure for iris.task\ngetDefaultMeasure(iris.task)\n#  Name: Mean misclassification error\n#  Performance measure: mmce\n#  Properties: classif,classif.multi,req.pred,req.truth\n#  Minimize: TRUE\n#  Best: 0; Worst: 1\n#  Aggregated by: test.mean\n#  Arguments: \n#  Note: Defined as: mean(response != truth)\n\n## Get the default measure for linear regression\ngetDefaultMeasure(makeLearner( regr.lm ))\n#  Name: Mean of squared errors\n#  Performance measure: mse\n#  Properties: regr,req.pred,req.truth\n#  Minimize: TRUE\n#  Best: 0; Worst: Inf\n#  Aggregated by: test.mean\n#  Arguments: \n#  Note: Defined as: mean((response - truth)^2)", 
            "title": "Listing measures"
        }, 
        {
            "location": "/performance/index.html#calculate-performance-measures", 
            "text": "In the following example we fit a  gradient boosting machine  on a subset of the BostonHousing  data set and calculate the default measure mean\nsquared error ( mse ) on the remaining observations.  n = getTaskSize(bh.task)\nlrn = makeLearner( regr.gbm , n.trees = 1000)\nmod = train(lrn, task = bh.task, subset = seq(1, n, 2))\npred = predict(mod, task = bh.task, subset = seq(2, n, 2))\n\nperformance(pred)\n#       mse \n#  42.68414  The following code computes the median of squared errors ( medse ) instead.  performance(pred, measures = medse)\n#     medse \n#  9.134965  Of course, we can also calculate multiple performance measures at once by simply passing a\nlist of measures which can also include  your own measure .  Calculate the mean squared error, median squared error and mean absolute error ( mae ).  performance(pred, measures = list(mse, medse, mae))\n#        mse     medse       mae \n#  42.684141  9.134965  4.536750  For the other types of learning problems and measures, calculating the performance basically\nworks in the same way.", 
            "title": "Calculate performance measures"
        }, 
        {
            "location": "/performance/index.html#requirements-of-performance-measures", 
            "text": "Note that in order to calculate some performance measures it is required that you pass the Task  or the  fitted model  in addition to the  Prediction .  For example in order to assess the time needed for training ( timetrain ), the fitted\nmodel has to be passed.  performance(pred, measures = timetrain, model = mod)\n#  timetrain \n#      0.072  For many performance measures in cluster analysis the  Task  is required.  lrn = makeLearner( cluster.kmeans , centers = 3)\nmod = train(lrn, mtcars.task)\npred = predict(mod, task = mtcars.task)\n\n## Calculate the Dunn index\nperformance(pred, measures = dunn, task = mtcars.task)\n#       dunn \n#  0.1462919  Moreover, some measures require a certain type of prediction.\nFor example in binary classification in order to calculate the AUC ( auc ) -- the area\nunder the ROC (receiver operating characteristic) curve -- we have to make sure that posterior\nprobabilities are predicted.\nFor more information on ROC analysis, see the section on  ROC analysis .  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, task = sonar.task)\npred = predict(mod, task = sonar.task)\n\nperformance(pred, measures = auc)\n#        auc \n#  0.9224018  Also bear in mind that many of the performance measures that are available for classification,\ne.g., the false positive rate ( fpr ), are only suitable for binary problems.", 
            "title": "Requirements of performance measures"
        }, 
        {
            "location": "/performance/index.html#access-a-performance-measure", 
            "text": "Performance measures in  mlr  are objects of class  Measure .\nIf you are interested in the properties or requirements of a single measure you can access it directly.\nSee the help page of  Measure  for information on the individual slots.  ## Mean misclassification error\nstr(mmce)\n#  List of 10\n#   $ id        : chr  mmce \n#   $ minimize  : logi TRUE\n#   $ properties: chr [1:4]  classif   classif.multi   req.pred   req.truth \n#   $ fun       :function (task, model, pred, feats, extra.args)  \n#   $ extra.args: list()\n#   $ best      : num 0\n#   $ worst     : num 1\n#   $ name      : chr  Mean misclassification error \n#   $ note      : chr  Defined as: mean(response != truth) \n#   $ aggr      :List of 4\n#    ..$ id        : chr  test.mean \n#    ..$ name      : chr  Test mean \n#    ..$ fun       :function (task, perf.test, perf.train, measure, group, pred)  \n#    ..$ properties: chr  req.test \n#    ..- attr(*,  class )= chr  Aggregation \n#   - attr(*,  class )= chr  Measure", 
            "title": "Access a performance measure"
        }, 
        {
            "location": "/performance/index.html#binary-classification", 
            "text": "For binary classification specialized techniques exist to analyze the performance.", 
            "title": "Binary classification"
        }, 
        {
            "location": "/performance/index.html#plot-performance-versus-threshold", 
            "text": "As you may recall (see the previous section on  making predictions )\nin binary classification we can adjust the threshold used to map probabilities to class labels.\nHelpful in this regard are the functions  generateThreshVsPerfData  and  plotThreshVsPerf , which generate and plot, respectively, the learner performance versus the threshold.  For more performance plots and automatic threshold tuning see  here .  In the following example we consider the  Sonar  data set and\nplot the false positive rate ( fpr ), the false negative rate ( fnr )\nas well as the misclassification rate ( mmce ) for all possible threshold values.  lrn = makeLearner( classif.lda , predict.type =  prob )\nn = getTaskSize(sonar.task)\nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))\npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))\n\n## Performance for the default threshold 0.5\nperformance(pred, measures = list(fpr, fnr, mmce))\n#        fpr       fnr      mmce \n#  0.2500000 0.3035714 0.2788462\n## Plot false negative and positive rates as well as the error rate versus the threshold\nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))\nplotThreshVsPerf(d)   There is an experimental  ggvis  plotting function  plotThreshVsPerfGGVIS  which performs similarly\nto  plotThreshVsPerf  but instead of creating facetted subplots to visualize multiple learners and/or\nmultiple measures, one of them is mapped to an interactive sidebar which selects what to display.  plotThreshVsPerfGGVIS(d)", 
            "title": "Plot performance versus threshold"
        }, 
        {
            "location": "/performance/index.html#roc-measures", 
            "text": "For binary classification a large number of specialized measures exist, which can be nicely formatted into\none matrix, see for example the  receiver operating characteristic page on wikipedia .  We can generate a similiar table with the  calculateROCMeasures  function.  r = calculateROCMeasures(pred)\nr\n#      predicted\n#  true M         R                            \n#     M 39        17        tpr: 0.7  fnr: 0.3 \n#     R 12        36        fpr: 0.25 tnr: 0.75\n#       ppv: 0.76 for: 0.32 lrp: 2.79 acc: 0.72\n#       fdr: 0.24 npv: 0.68 lrm: 0.4  dor: 6.88\n#  \n#  \n#  Abbreviations:\n#  tpr - True positive rate (Sensitivity, Recall)\n#  fpr - False positive rate (Fall-out)\n#  fnr - False negative rate (Miss rate)\n#  tnr - True negative rate (Specificity)\n#  ppv - Positive predictive value (Precision)\n#  for - False omission rate\n#  lrp - Positive likelihood ratio (LR+)\n#  fdr - False discovery rate\n#  npv - Negative predictive value\n#  acc - Accuracy\n#  lrm - Negative likelihood ratio (LR-)\n#  dor - Diagnostic odds ratio  The top left  2 \\times 2  matrix is the  confusion matrix , which shows\nthe relative frequency of correctly and incorrectly classified observations. Below and to the right a large number of\nperformance measures that can be inferred from the confusion matrix are added. By default some\nadditional info about the measures is printed.\nYou can turn this off using the  abbreviations  argument of the  print  method: print(r, abbreviations = FALSE) .", 
            "title": "ROC measures"
        }, 
        {
            "location": "/performance/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## Performance measures for classification with multiple classes \nlistMeasures( classif , properties =  classif.multi ) \n## Performance measure suitable for the iris classification task \nlistMeasures(iris.task) \n## Get default measure for iris.task \ngetDefaultMeasure(iris.task) \n\n## Get the default measure for linear regression \ngetDefaultMeasure(makeLearner( regr.lm )) \nn = getTaskSize(bh.task) \nlrn = makeLearner( regr.gbm , n.trees = 1000) \nmod = train(lrn, task = bh.task, subset = seq(1, n, 2)) \npred = predict(mod, task = bh.task, subset = seq(2, n, 2)) \n\nperformance(pred) \nperformance(pred, measures = medse) \nperformance(pred, measures = list(mse, medse, mae)) \nperformance(pred, measures = timetrain, model = mod) \nlrn = makeLearner( cluster.kmeans , centers = 3) \nmod = train(lrn, mtcars.task) \npred = predict(mod, task = mtcars.task) \n\n## Calculate the Dunn index \nperformance(pred, measures = dunn, task = mtcars.task) \nlrn = makeLearner( classif.rpart , predict.type =  prob ) \nmod = train(lrn, task = sonar.task) \npred = predict(mod, task = sonar.task) \n\nperformance(pred, measures = auc) \n## Mean misclassification error \nstr(mmce) \nlrn = makeLearner( classif.lda , predict.type =  prob ) \nn = getTaskSize(sonar.task) \nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2)) \npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2)) \n\n## Performance for the default threshold 0.5 \nperformance(pred, measures = list(fpr, fnr, mmce)) \n## Plot false negative and positive rates as well as the error rate versus the threshold \nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce)) \nplotThreshVsPerf(d) \n## plotThreshVsPerfGGVIS(d) \nr = calculateROCMeasures(pred) \nr", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/resample/index.html", 
            "text": "Resampling\n\n\nResampling strategies are usually used to assess the performance of a learning algorithm:\nThe entire data set is (repeatedly) split into training sets \nD^{*b}\n and test sets\n\nD \\setminus D^{*b}\n, \nb = 1,\\ldots,B\n.\nThe learner is trained on each training set, predictions are made on the corresponding test\nset (sometimes on the training set as well) and the performance measure\n\nS(D^{*b}, D \\setminus D^{*b})\n is calculated.\nThen the \nB\n individual performance values are aggregated, most often by calculating the mean.\nThere exist various different resampling strategies, for example cross-validation and bootstrap,\nto mention just two popular approaches.\n\n\n\n\nIf you want to read up on further details, the paper\n\nResampling Strategies for Model Assessment and Selection\n\nby Simon is probably not a bad choice.\nBernd has also published a paper\n\nResampling methods for meta-model validation with recommendations for evolutionary computation\n\nwhich contains detailed descriptions and lots of statistical background information on resampling methods.\n\n\nDefining the resampling strategy\n\n\nIn \nmlr\n the resampling strategy can be defined via function \nmakeResampleDesc\n.\nIt requires a string that specifies the resampling method and, depending on the selected\nstrategy, further information like the number of iterations.\nThe supported resampling strategies are:\n\n\n\n\nCross-validation (\n\"CV\"\n),\n\n\nLeave-one-out cross-validation (\n\"LOO\"\n),\n\n\nRepeated cross-validation (\n\"RepCV\"\n),\n\n\nOut-of-bag bootstrap and other variants like \nb632\n (\n\"Bootstrap\"\n),\n\n\nSubsampling, also called Monte-Carlo cross-validation (\n\"Subsample\"\n),\n\n\nHoldout (training/test) (\n\"Holdout\"\n).\n\n\n\n\nFor example if you want to use 3-fold cross-validation type:\n\n\n## 3-fold cross-validation\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nrdesc\n#\n Resample description: cross-validation with 3 iterations.\n#\n Predict: test\n#\n Stratification: FALSE\n\n\n\n\nFor holdout estimation use:\n\n\n## Holdout estimation\nrdesc = makeResampleDesc(\nHoldout\n)\nrdesc\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n\n\n\n\nIn order to save you some typing \nmlr\n contains some pre-defined resample descriptions for\nvery common strategies like holdout (\nhout\n) as well as cross-validation\nwith different numbers of folds (e.g., \ncv5\n or \ncv10\n).\n\n\nhout\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n\ncv3\n#\n Resample description: cross-validation with 3 iterations.\n#\n Predict: test\n#\n Stratification: FALSE\n\n\n\n\nPerforming the resampling\n\n\nFunction \nresample\n evaluates a \nLearner\n on a\ngiven machine learning \nTask\n using the selected \nresampling strategy\n.\n\n\nAs a first example, the performance of \nlinear regression\n on the\n\nBostonHousing\n data set is calculated using \n3-fold cross-validation\n.\n\n\nGenerally, for \nK\n-fold cross-validation\n the data set \nD\n is partitioned into \nK\n subsets\nof (approximately) equal size.\nIn the \nb\n-th of the \nK\n iterations, the \nb\n-th subset is used for testing, while the union\nof the remaining parts forms the training set.\n\n\nAs usual, you can either pass a \nLearner\n object to \nresample\n or, as done here,\nprovide the class name \n\"regr.lm\"\n of the learner.\nSince no performance measure is specified the default for regression learners\n(mean squared error, \nmse\n) is calculated.\n\n\n## Specify the resampling strategy (3-fold cross-validation)\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\n\n## Calculate the performance\nr = resample(\nregr.lm\n, bh.task, rdesc)\n#\n Resampling: cross-validation\n#\n Measures:             mse\n#\n [Resample] iter 1:    19.8982806\n#\n [Resample] iter 2:    31.7803405\n#\n [Resample] iter 3:    20.9698523\n#\n \n#\n Aggregated Result: mse.test.mean=24.2161578\n#\n \n\nr\n#\n Resample Result\n#\n Task: BostonHousing-example\n#\n Learner: regr.lm\n#\n Aggr perf: mse.test.mean=24.2161578\n#\n Runtime: 0.054549\n\n\n\n\nThe result \nr\n is an object of class \nResampleResult\n.\nIt contains performance results for the learner and some additional information\nlike the runtime, predicted values, and optionally the models fitted in single resampling\niterations.\n\n\n## Peak into r\nnames(r)\n#\n  [1] \nlearner.id\n     \ntask.id\n        \ntask.desc\n      \nmeasures.train\n\n#\n  [5] \nmeasures.test\n  \naggr\n           \npred\n           \nmodels\n        \n#\n  [9] \nerr.msgs\n       \nerr.dumps\n      \nextract\n        \nruntime\n\n\nr$aggr\n#\n mse.test.mean \n#\n      24.21616\n\nr$measures.test\n#\n   iter      mse\n#\n 1    1 19.89828\n#\n 2    2 31.78034\n#\n 3    3 20.96985\n\n\n\n\nr$measures.test\n gives the performance on each of the 3 test data sets.\n\nr$aggr\n shows the aggregated performance value.\nIts name \n\"mse.test.mean\"\n indicates the performance measure, \nmse\n,\nand the method, \ntest.mean\n, used to aggregate the 3 individual performances.\n\ntest.mean\n is the default aggregation scheme for most performance measures\nand, as the name implies, takes the mean over the performances on the test data sets.\n\n\nResampling in \nmlr\n works the same way for all types of learning problems and learners.\nBelow is a classification example where a \nclassification tree (rpart)\n is\nevaluated on the \nSonar\n data set by subsampling with 5 iterations.\n\n\nIn each subsampling iteration the data set \nD\n is randomly partitioned into a\ntraining and a test set according to a given percentage, e.g., 2/3\ntraining and 1/3 test set. If there is just one iteration, the strategy\nis commonly called \nholdout\n or \ntest sample estimation\n.\n\n\nYou can calculate several measures at once by passing a \nlist\n of\n\nMeasure\ns to \nresample\n.\nBelow, the error rate (\nmmce\n), false positive and false negative rates\n(\nfpr\n, \nfnr\n), and the time it takes to train the learner\n(\ntimetrain\n) are estimated by \nsubsampling\n with 5 iterations.\n\n\n## Subsampling with 5 iterations and default split ratio 2/3\nrdesc = makeResampleDesc(\nSubsample\n, iters = 5)\n\n## Subsampling with 5 iterations and 4/5 training data\nrdesc = makeResampleDesc(\nSubsample\n, iters = 5, split = 4/5)\n\n## Classification tree with information splitting criterion\nlrn = makeLearner(\nclassif.rpart\n, parms = list(split = \ninformation\n))\n\n## Calculate the performance measures\nr = resample(lrn, sonar.task, rdesc, measures = list(mmce, fpr, fnr, timetrain))\n#\n Resampling: subsampling\n#\n Measures:             mmce        fpr         fnr         timetrain\n#\n [Resample] iter 1:    0.2619048   0.2352941   0.2800000   0.0240000\n#\n [Resample] iter 2:    0.2857143   0.2857143   0.2857143   0.0190000\n#\n [Resample] iter 3:    0.2619048   0.2857143   0.2380952   0.0220000\n#\n [Resample] iter 4:    0.3333333   0.3500000   0.3181818   0.0190000\n#\n [Resample] iter 5:    0.3333333   0.1666667   0.4583333   0.0200000\n#\n \n#\n Aggregated Result: mmce.test.mean=0.2952381,fpr.test.mean=0.2646779,fnr.test.mean=0.3160649,timetrain.test.mean=0.0208000\n#\n \n\nr\n#\n Resample Result\n#\n Task: Sonar_example\n#\n Learner: classif.rpart\n#\n Aggr perf: mmce.test.mean=0.2952381,fpr.test.mean=0.2646779,fnr.test.mean=0.3160649,timetrain.test.mean=0.0208000\n#\n Runtime: 0.188301\n\n\n\n\nIf you want to add further measures afterwards, use \naddRRMeasure\n.\n\n\n## Add balanced error rate (ber) and time used to predict\naddRRMeasure(r, list(ber, timepredict))\n#\n Resample Result\n#\n Task: Sonar_example\n#\n Learner: classif.rpart\n#\n Aggr perf: mmce.test.mean=0.2952381,fpr.test.mean=0.2646779,fnr.test.mean=0.3160649,timetrain.test.mean=0.0208000,ber.test.mean=0.2903714,timepredict.test.mean=0.0062000\n#\n Runtime: 0.188301\n\n\n\n\nBy default, \nresample\n prints progress messages and intermediate results. You can turn this off by setting\n\nshow.info = FALSE\n, as done in the code chunk below. (If you are interested in suppressing\nthese messages permanently have a look at the tutorial page about \nconfiguring mlr\n.)\n\n\nIn the above example, the \nLearner\n was explicitly constructed. For convenience\nyou can also specify the learner as a string and pass any learner parameters via the \n...\n argument\nof \nresample\n.\n\n\nr = resample(\nclassif.rpart\n, parms = list(split = \ninformation\n), sonar.task, rdesc,\n  measures = list(mmce, fpr, fnr, timetrain), show.info = FALSE)\n\nr\n#\n Resample Result\n#\n Task: Sonar_example\n#\n Learner: classif.rpart\n#\n Aggr perf: mmce.test.mean=0.3047619,fpr.test.mean=0.2785319,fnr.test.mean=0.3093917,timetrain.test.mean=0.0230000\n#\n Runtime: 0.204483\n\n\n\n\nAccessing resample results\n\n\nApart from the learner performance you can extract further information from the\nresample results, for example predicted values or the models fitted in individual resample\niterations.\n\n\nPredictions\n\n\nPer default, the \nResampleResult\n contains the predictions made during the resampling.\nIf you do not want to keep them, e.g., in order to conserve memory,\nset \nkeep.pred = FALSE\n when calling \nresample\n.\n\n\nThe predictions are stored in slot \n$pred\n of the resampling result, which can also be accessed\nby function \ngetRRPredictions\n.\n\n\nr$pred\n#\n Resampled Prediction for:\n#\n Resample description: subsampling with 5 iterations and 0.80 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n#\n predict.type: response\n#\n threshold: \n#\n time (mean): 0.01\n#\n    id truth response iter  set\n#\n 1  36     R        M    1 test\n#\n 2 132     M        R    1 test\n#\n 3 145     M        R    1 test\n#\n 4 161     M        R    1 test\n#\n 5 108     M        M    1 test\n#\n 6 178     M        M    1 test\n#\n ... (#rows: 210, #cols: 5)\n\npred = getRRPredictions(r)\npred\n#\n Resampled Prediction for:\n#\n Resample description: subsampling with 5 iterations and 0.80 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n#\n predict.type: response\n#\n threshold: \n#\n time (mean): 0.01\n#\n    id truth response iter  set\n#\n 1  36     R        M    1 test\n#\n 2 132     M        R    1 test\n#\n 3 145     M        R    1 test\n#\n 4 161     M        R    1 test\n#\n 5 108     M        M    1 test\n#\n 6 178     M        M    1 test\n#\n ... (#rows: 210, #cols: 5)\n\n\n\n\npred\n is an object of class \nResamplePrediction\n.\nJust as a \nPrediction\n object (see the tutorial page on \nmaking predictions\n)\nit has an element \n$data\n which is a \ndata.frame\n that contains the\npredictions and in the case of a supervised learning problem the true values of the target\nvariable(s).\nYou can use \nas.data.frame\n to directly access the \n$data\n slot. Moreover, all getter\nfunctions for \nPrediction\n objects like \ngetPredictionResponse\n or\n\ngetPredictionProbabilities\n are applicable.\n\n\nhead(as.data.frame(pred))\n#\n    id truth response iter  set\n#\n 1  36     R        M    1 test\n#\n 2 132     M        R    1 test\n#\n 3 145     M        R    1 test\n#\n 4 161     M        R    1 test\n#\n 5 108     M        M    1 test\n#\n 6 178     M        M    1 test\n\nhead(getPredictionTruth(pred))\n#\n [1] R M M M M M\n#\n Levels: M R\n\nhead(getPredictionResponse(pred))\n#\n [1] M R R R M M\n#\n Levels: M R\n\n\n\n\nThe columns \niter\n and \nset\n in the \ndata.frame\n indicate the\nresampling iteration and the data set (\ntrain\n or \ntest\n) for which the prediction was made.\n\n\nBy default, predictions are made for the test sets only.\nIf predictions for the training set are required, set \npredict = \"train\"\n (for predictions\non the train set only) or \npredict = \"both\"\n (for predictions on both train and test sets)\nin \nmakeResampleDesc\n. In any case, this is necessary for some bootstrap methods\n(\nb632\n and \nb632+\n) and some examples are shown \nlater on\n.\n\n\nBelow, we use simple Holdout, i.e., split the data once into a training and test set, as\nresampling strategy and make predictions on both sets.\n\n\n## Make predictions on both training and test sets\nrdesc = makeResampleDesc(\nHoldout\n, predict = \nboth\n)\n\nr = resample(\nclassif.lda\n, iris.task, rdesc, show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: iris_example\n#\n Learner: classif.lda\n#\n Aggr perf: mmce.test.mean=0.0200000\n#\n Runtime: 0.0183854\n\nr$measures.train\n#\n   iter mmce\n#\n 1    1 0.02\n\n\n\n\n(Please note that nonetheless the misclassification rate \nr$aggr\n is estimated on the test data only.\nHow to calculate performance measures on the training sets is shown\n\nbelow\n.)\n\n\nA second function to extract predictions from resample results is \ngetRRPredictionList\n\nwhich returns a \nlist\n of predictions split by data set (train/test) and\nresampling iteration.\n\n\npredList = getRRPredictionList(r)\npredList\n#\n $train\n#\n $train$`1`\n#\n Prediction: 100 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n      id      truth   response\n#\n 123 123  virginica  virginica\n#\n 101 101  virginica  virginica\n#\n 51   51 versicolor versicolor\n#\n 45   45     setosa     setosa\n#\n 46   46     setosa     setosa\n#\n 3     3     setosa     setosa\n#\n ... (#rows: 100, #cols: 3)\n#\n \n#\n \n#\n $test\n#\n $test$`1`\n#\n Prediction: 50 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n      id      truth   response\n#\n 109 109  virginica  virginica\n#\n 80   80 versicolor versicolor\n#\n 40   40     setosa     setosa\n#\n 140 140  virginica  virginica\n#\n 125 125  virginica  virginica\n#\n 10   10     setosa     setosa\n#\n ... (#rows: 50, #cols: 3)\n\n\n\n\nLearner models\n\n\nIn each resampling iteration a \nLearner\n is fitted on the respective training set.\nBy default, the resulting \nWrappedModel\ns are not included in the\n\nResampleResult\n and slot \n$models\n is empty.\nIn order to keep them, set \nmodels = TRUE\n when calling \nresample\n, as in the following\nsurvival analysis example.\n\n\n## 3-fold cross-validation\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\n\nr = resample(\nsurv.coxph\n, lung.task, rdesc, show.info = FALSE, models = TRUE)\nr$models\n#\n [[1]]\n#\n Model for learner.id=surv.coxph; learner.class=surv.coxph\n#\n Trained on: task.id = lung-example; obs = 112; features = 8\n#\n Hyperparameters: \n#\n \n#\n [[2]]\n#\n Model for learner.id=surv.coxph; learner.class=surv.coxph\n#\n Trained on: task.id = lung-example; obs = 111; features = 8\n#\n Hyperparameters: \n#\n \n#\n [[3]]\n#\n Model for learner.id=surv.coxph; learner.class=surv.coxph\n#\n Trained on: task.id = lung-example; obs = 111; features = 8\n#\n Hyperparameters:\n\n\n\n\nThe extract option\n\n\nKeeping complete fitted models can be memory-intensive if these objects are large or\nthe number of resampling iterations is high.\nAlternatively, you can use the \nextract\n argument of \nresample\n to retain only the\ninformation you need.\nTo this end you need to pass a \nfunction\n to \nextract\n which is applied\nto each \nWrappedModel\n object fitted in each resampling iteration.\n\n\nBelow, we cluster the \nmtcars\n data using the \nk\n-means algorithm with \nk = 3\n\nand keep only the cluster centers.\n\n\n## 3-fold cross-validation\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\n\n## Extract the compute cluster centers\nr = resample(\ncluster.kmeans\n, mtcars.task, rdesc, show.info = FALSE,\n  centers = 3, extract = function(x) getLearnerModel(x)$centers)\nr$extract\n#\n [[1]]\n#\n        mpg      cyl     disp        hp     drat       wt     qsec\n#\n 1 16.23333 7.666667 308.9667 214.00000 3.400000 3.564167 16.37000\n#\n 2 26.00833 4.333333 113.5917  86.08333 4.040833 2.368583 18.88917\n#\n 3 13.33333 8.000000 444.0000 198.33333 3.003333 4.839667 17.61667\n#\n          vs        am     gear     carb\n#\n 1 0.1666667 0.3333333 3.666667 3.666667\n#\n 2 0.8333333 0.6666667 4.083333 1.916667\n#\n 3 0.0000000 0.0000000 3.000000 3.333333\n#\n \n#\n [[2]]\n#\n       mpg cyl     disp     hp  drat       wt     qsec    vs   am  gear\n#\n 1 15.5600   8 326.0400 207.00 3.198 3.830000 16.74600 0.000 0.10 3.200\n#\n 2 26.7125   4 102.8875  86.00 4.145 2.179125 19.05375 0.875 0.75 4.125\n#\n 3 19.1500   6 174.4000 128.25 3.550 3.136250 17.91000 0.500 0.50 4.000\n#\n    carb\n#\n 1 3.500\n#\n 2 1.625\n#\n 3 3.750\n#\n \n#\n [[3]]\n#\n        mpg cyl     disp       hp     drat       wt     qsec        vs\n#\n 1 25.25000   4 113.6000  82.5000 3.932500 2.622500 19.17000 1.0000000\n#\n 2 15.12000   8 369.8600 201.9000 3.211000 4.098900 17.05300 0.0000000\n#\n 3 19.74286   6 183.3143 122.2857 3.585714 3.117143 17.97714 0.5714286\n#\n          am     gear     carb\n#\n 1 0.7500000 4.000000 1.500000\n#\n 2 0.1000000 3.200000 3.200000\n#\n 3 0.4285714 3.857143 3.428571\n\n\n\n\nAs a second example, we extract the variable importances from fitted regression trees using\nfunction \ngetFeatureImportance\n.\n(For more detailed information on this topic see the\n\nfeature selection\n page.)\n\n\n## Extract the variable importance in a regression tree\nr = resample(\nregr.rpart\n, bh.task, rdesc, show.info = FALSE, extract = getFeatureImportance)\nr$extract\n#\n [[1]]\n#\n FeatureImportance:\n#\n Task: BostonHousing-example\n#\n \n#\n Learner: regr.rpart\n#\n Measure: NA\n#\n Contrast: NA\n#\n Aggregation: function (x)  x\n#\n Replace: NA\n#\n Number of Monte-Carlo iterations: NA\n#\n Local: FALSE\n#\n       crim       zn    indus     chas      nox       rm      age      dis\n#\n 1 3842.839 952.3849 4443.578 90.63669 3772.273 15853.01 3997.275 3355.651\n#\n        rad     tax  ptratio b    lstat\n#\n 1 987.4256 568.177 2860.129 0 11255.66\n#\n \n#\n [[2]]\n#\n FeatureImportance:\n#\n Task: BostonHousing-example\n#\n \n#\n Learner: regr.rpart\n#\n Measure: NA\n#\n Contrast: NA\n#\n Aggregation: function (x)  x\n#\n Replace: NA\n#\n Number of Monte-Carlo iterations: NA\n#\n Local: FALSE\n#\n       crim       zn    indus chas      nox       rm      age      dis\n#\n 1 3246.521 3411.444 5806.613    0 2349.776 10125.04 5692.587 2108.059\n#\n        rad     tax  ptratio        b    lstat\n#\n 1 312.6521 2159.42 1104.839 174.6412 15871.53\n#\n \n#\n [[3]]\n#\n FeatureImportance:\n#\n Task: BostonHousing-example\n#\n \n#\n Learner: regr.rpart\n#\n Measure: NA\n#\n Contrast: NA\n#\n Aggregation: function (x)  x\n#\n Replace: NA\n#\n Number of Monte-Carlo iterations: NA\n#\n Local: FALSE\n#\n       crim      zn    indus chas      nox       rm      age      dis\n#\n 1 3785.852 1649.28 4942.119    0 3989.326 18426.87 2604.239 350.8401\n#\n       rad      tax  ptratio        b    lstat\n#\n 1 800.798 2907.556 3871.556 491.6297 12505.88\n\n\n\n\nStratification and blocking\n\n\n\n\nStratification\n with respect to a categorical variable makes sure that all its values\n  are present in each training and test set in approximately the same proportion as in the original data set.\n  Stratification is possible with regard to categorical target variables (and thus for supervised\n  classification and survival analysis) or categorical explanatory variables.\n\n\nBlocking\n refers to the situation that subsets of observations belong together and must not\n  be separated during resampling.\n  Hence, for one train/test set pair the entire block is either in the training set or in\n  the test set.\n\n\n\n\nStratification with respect to the target variable(s)\n\n\nFor classification, it is usually desirable to have the same proportion of the classes in\nall of the partitions of the original data set.\nThis is particularly useful in the case of imbalanced classes and small data sets. Otherwise,\nit may happen that observations of less frequent classes are missing in some of the training\nsets which can decrease the performance of the learner, or lead to model crashes.\nIn order to conduct stratified resampling, set \nstratify = TRUE\n in \nmakeResampleDesc\n.\n\n\n## 3-fold cross-validation\nrdesc = makeResampleDesc(\nCV\n, iters = 3, stratify = TRUE)\n\nr = resample(\nclassif.lda\n, iris.task, rdesc, show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: iris_example\n#\n Learner: classif.lda\n#\n Aggr perf: mmce.test.mean=0.0132026\n#\n Runtime: 0.042295\n\n\n\n\nStratification is also available for survival tasks.\nHere the stratification balances the censoring rate.\n\n\nStratification with respect to explanatory variables\n\n\nSometimes it is required to also stratify on the input data, e.g., to ensure that all\nsubgroups are represented in all training and test sets.\nTo stratify on the input columns, specify \nfactor\n columns of your task data\nvia \nstratify.cols\n.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3, stratify.cols = \nchas\n)\n\nr = resample(\nregr.rpart\n, bh.task, rdesc, show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: BostonHousing-example\n#\n Learner: regr.rpart\n#\n Aggr perf: mse.test.mean=21.2385142\n#\n Runtime: 0.0552762\n\n\n\n\nBlocking\n\n\nIf some observations \"belong together\" and must not be separated when splitting the\ndata into training and test sets for resampling, you can supply this information via a\n\nblocking\n \nfactor\n when \ncreating the task\n.\n\n\n## 5 blocks containing 30 observations each\ntask = makeClassifTask(data = iris, target = \nSpecies\n, blocking = factor(rep(1:5, each = 30)))\ntask\n#\n Supervised task: iris\n#\n Type: classif\n#\n Target: Species\n#\n Observations: 150\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n           4           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: TRUE\n#\n Is spatial: FALSE\n#\n Classes: 3\n#\n     setosa versicolor  virginica \n#\n         50         50         50 \n#\n Positive class: NA\n\n\n\n\nResample descriptions and resample instances\n\n\nAs already mentioned, you can specify a resampling strategy using function \nmakeResampleDesc\n.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nrdesc\n#\n Resample description: cross-validation with 3 iterations.\n#\n Predict: test\n#\n Stratification: FALSE\n\nstr(rdesc)\n#\n List of 4\n#\n  $ id      : chr \ncross-validation\n\n#\n  $ iters   : int 3\n#\n  $ predict : chr \ntest\n\n#\n  $ stratify: logi FALSE\n#\n  - attr(*, \nclass\n)= chr [1:2] \nCVDesc\n \nResampleDesc\n\n\nstr(makeResampleDesc(\nSubsample\n, stratify.cols = \nchas\n))\n#\n List of 6\n#\n  $ split        : num 0.667\n#\n  $ id           : chr \nsubsampling\n\n#\n  $ iters        : int 30\n#\n  $ predict      : chr \ntest\n\n#\n  $ stratify     : logi FALSE\n#\n  $ stratify.cols: chr \nchas\n\n#\n  - attr(*, \nclass\n)= chr [1:2] \nSubsampleDesc\n \nResampleDesc\n\n\n\n\n\nThe result \nrdesc\n inherits from class \nResampleDesc\n (short for\nresample description) and, in principle, contains all necessary information about the\nresampling strategy including the number of iterations, the proportion of training and test\nsets, stratification variables, etc.\n\n\nGiven either the size of the data set at hand or the \nTask\n, function \nmakeResampleInstance\n\ndraws the training and test sets according to the \nResampleDesc\n.\n\n\n## Create a resample instance based an a task\nrin = makeResampleInstance(rdesc, iris.task)\nrin\n#\n Resample instance for 150 cases.\n#\n Resample description: cross-validation with 3 iterations.\n#\n Predict: test\n#\n Stratification: FALSE\n\nstr(rin)\n#\n List of 5\n#\n  $ desc      :List of 4\n#\n   ..$ id      : chr \ncross-validation\n\n#\n   ..$ iters   : int 3\n#\n   ..$ predict : chr \ntest\n\n#\n   ..$ stratify: logi FALSE\n#\n   ..- attr(*, \nclass\n)= chr [1:2] \nCVDesc\n \nResampleDesc\n\n#\n  $ size      : int 150\n#\n  $ train.inds:List of 3\n#\n   ..$ : int [1:100] 88 129 94 109 108 43 72 47 137 39 ...\n#\n   ..$ : int [1:100] 129 94 138 83 112 54 29 36 72 137 ...\n#\n   ..$ : int [1:100] 88 138 109 83 112 108 54 29 36 43 ...\n#\n  $ test.inds :List of 3\n#\n   ..$ : int [1:50] 2 5 6 13 14 17 20 21 24 25 ...\n#\n   ..$ : int [1:50] 3 4 7 8 11 12 22 30 34 35 ...\n#\n   ..$ : int [1:50] 1 9 10 15 16 18 19 23 27 28 ...\n#\n  $ group     : Factor w/ 0 levels: \n#\n  - attr(*, \nclass\n)= chr \nResampleInstance\n\n\n## Create a resample instance given the size of the data set\nrin = makeResampleInstance(rdesc, size = nrow(iris))\nstr(rin)\n#\n List of 5\n#\n  $ desc      :List of 4\n#\n   ..$ id      : chr \ncross-validation\n\n#\n   ..$ iters   : int 3\n#\n   ..$ predict : chr \ntest\n\n#\n   ..$ stratify: logi FALSE\n#\n   ..- attr(*, \nclass\n)= chr [1:2] \nCVDesc\n \nResampleDesc\n\n#\n  $ size      : int 150\n#\n  $ train.inds:List of 3\n#\n   ..$ : int [1:100] 149 58 120 44 148 29 66 46 124 137 ...\n#\n   ..$ : int [1:100] 51 58 64 148 56 46 124 8 14 137 ...\n#\n   ..$ : int [1:100] 149 51 120 44 64 56 29 66 8 14 ...\n#\n  $ test.inds :List of 3\n#\n   ..$ : int [1:50] 3 8 12 14 17 22 23 24 32 34 ...\n#\n   ..$ : int [1:50] 1 2 4 6 10 11 13 26 29 30 ...\n#\n   ..$ : int [1:50] 5 7 9 15 16 18 19 20 21 25 ...\n#\n  $ group     : Factor w/ 0 levels: \n#\n  - attr(*, \nclass\n)= chr \nResampleInstance\n\n\n## Access the indices of the training observations in iteration 3\nrin$train.inds[[3]]\n#\n   [1] 149  51 120  44  64  56  29  66   8  14  83  65  97 114  13   3 104\n#\n  [18]  88 130  81  89  23  63 131  92  31  41  78  72 139  67  10  57  12\n#\n  [35] 107  74  70 116  36  24  35  93 126 111  75  91  80  85  42  30  22\n#\n  [52]   1  69 113  87  26  17 150 119   4 138 129 147  38  99  60 142  50\n#\n  [69] 122  40 127  43  96  34 141 106  79 133 145 125 135 108  52 109  37\n#\n  [86]  61  84  59  39  82  32  53  94   6  45  86  95   2  68  11\n\n\n\n\nThe result \nrin\n inherits from class \nResampleInstance\n and contains\n\nlist\ns of index vectors for the train and test sets.\n\n\nIf a \nResampleDesc\n is passed to \nresample\n, it is instantiated internally.\nNaturally, it is also possible to pass a \nResampleInstance\n directly.\n\n\nWhile the separation between resample descriptions, resample instances, and the \nresample\n\nfunction itself seems overly complicated, it has several advantages:\n\n\n\n\nResample instances readily allow for paired experiments, that is comparing the performance\n  of several learners on exactly the same training and test sets.\n  This is particularly useful if you want to add another method to a comparison experiment\n  you already did.\n  Moreover, you can store the resample instance along with your data in order to be able to reproduce\n  your results later on.\n\n\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nrin = makeResampleInstance(rdesc, task = iris.task)\n\n## Calculate the performance of two learners based on the same resample instance\nr.lda = resample(\nclassif.lda\n, iris.task, rin, show.info = FALSE)\nr.rpart = resample(\nclassif.rpart\n, iris.task, rin, show.info = FALSE)\nr.lda$aggr\n#\n mmce.test.mean \n#\n           0.02\n\nr.rpart$aggr\n#\n mmce.test.mean \n#\n     0.05333333\n\n\n\n\n\n\nIn order to add further resampling methods you can simply derive from the \nResampleDesc\n\n  and \nResampleInstance\n classes, but you do neither have to touch \nresample\n\n  nor any further methods that use the resampling strategy.\n\n\n\n\nUsually, when calling \nmakeResampleInstance\n the train and test index sets are drawn randomly.\nMainly for \nholdout\n (\ntest sample\n) \nestimation\n you might want full control about the training\nand tests set and specify them manually.\nThis can be done using function \nmakeFixedHoldoutInstance\n.\n\n\nrin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)\nrin\n#\n Resample instance for 150 cases.\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n\n\n\n\nAggregating performance values\n\n\nIn each resampling iteration \nb = 1,\\ldots,B\n we get performance values\n\nS(D^{*b}, D \\setminus D^{*b})\n (for each measure we wish to calculate), which are then\naggregated to an overall performance.\n\n\nFor the great majority of common resampling strategies (like holdout, cross-validation, subsampling)\nperformance values are calculated on the test data sets only and for most measures aggregated\nby taking the mean (\ntest.mean\n).\n\n\nEach performance \nMeasure\n in \nmlr\n has a corresponding default aggregation\nmethod which is stored in slot \n$aggr\n.\nThe default aggregation for most measures is \ntest.mean\n.\nOne exception is the root mean square error (\nrmse\n).\n\n\n## Mean misclassification error\nmmce$aggr\n#\n Aggregation function: test.mean\n\nmmce$aggr$fun\n#\n function (task, perf.test, perf.train, measure, group, pred) \n#\n mean(perf.test)\n#\n \nbytecode: 0xa1eb838\n\n#\n \nenvironment: namespace:mlr\n\n\n## Root mean square error\nrmse$aggr\n#\n Aggregation function: test.rmse\n\nrmse$aggr$fun\n#\n function (task, perf.test, perf.train, measure, group, pred) \n#\n sqrt(mean(perf.test^2))\n#\n \nbytecode: 0xfe7c908\n\n#\n \nenvironment: namespace:mlr\n\n\n\n\n\nYou can change the aggregation method of a \nMeasure\n via function \nsetAggregation\n.\nAll available aggregation schemes are listed on the \naggregations\n documentation page.\n\n\nExample: One measure with different aggregations\n\n\nThe aggregation schemes \ntest.median\n, \ntest.min\n, and\n\ntest.max\n compute the median, minimum, and maximum of the performance values\non the test sets.\n\n\nmseTestMedian = setAggregation(mse, test.median)\nmseTestMin = setAggregation(mse, test.min)\nmseTestMax = setAggregation(mse, test.max)\n\nmseTestMedian\n#\n Name: Mean of squared errors\n#\n Performance measure: mse\n#\n Properties: regr,req.pred,req.truth\n#\n Minimize: TRUE\n#\n Best: 0; Worst: Inf\n#\n Aggregated by: test.median\n#\n Arguments: \n#\n Note: Defined as: mean((response - truth)^2)\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(\nregr.lm\n, bh.task, rdesc, measures = list(mse, mseTestMedian, mseTestMin, mseTestMax))\n#\n Resampling: cross-validation\n#\n Measures:             mse       mse       mse       mse\n#\n [Resample] iter 1:    28.164474328.164474328.164474328.1644743\n#\n [Resample] iter 2:    17.593981817.593981817.593981817.5939818\n#\n [Resample] iter 3:    24.957218724.957218724.957218724.9572187\n#\n \n#\n Aggregated Result: mse.test.mean=23.5718916,mse.test.median=24.9572187,mse.test.min=17.5939818,mse.test.max=28.1644743\n#\n \n\nr\n#\n Resample Result\n#\n Task: BostonHousing-example\n#\n Learner: regr.lm\n#\n Aggr perf: mse.test.mean=23.5718916,mse.test.median=24.9572187,mse.test.min=17.5939818,mse.test.max=28.1644743\n#\n Runtime: 0.044241\n\nr$aggr\n#\n   mse.test.mean mse.test.median    mse.test.min    mse.test.max \n#\n        23.57189        24.95722        17.59398        28.16447\n\n\n\n\nExample: Calculating the training error\n\n\nBelow we calculate the mean misclassification error (\nmmce\n) on the training\nand the test data sets. Note that we have to set \npredict = \"both\"\n when calling \nmakeResampleDesc\n\nin order to get predictions on both training and test sets.\n\n\nmmceTrainMean = setAggregation(mmce, train.mean)\nrdesc = makeResampleDesc(\nCV\n, iters = 3, predict = \nboth\n)\nr = resample(\nclassif.rpart\n, iris.task, rdesc, measures = list(mmce, mmceTrainMean))\n#\n Resampling: cross-validation\n#\n Measures:             mmce.train   mmce.test\n#\n [Resample] iter 1:    0.0200000    0.1000000\n#\n [Resample] iter 2:    0.0400000    0.0400000\n#\n [Resample] iter 3:    0.0400000    0.0400000\n#\n \n#\n Aggregated Result: mmce.test.mean=0.0600000,mmce.train.mean=0.0333333\n#\n \n\nr$measures.train\n#\n   iter mmce mmce\n#\n 1    1 0.02 0.02\n#\n 2    2 0.04 0.04\n#\n 3    3 0.04 0.04\n\nr$aggr\n#\n  mmce.test.mean mmce.train.mean \n#\n      0.06000000      0.03333333\n\n\n\n\nExample: Bootstrap\n\n\nIn \nout-of-bag bootstrap estimation\n \nB\n new data sets \nD^{*1}, \\ldots, D^{*B}\n are drawn\nfrom the data set \nD\n with replacement, each of the same size as \nD\n.\nIn the \nb\n-th iteration, \nD^{*b}\n forms the training set, while the remaining elements from\n\nD\n, i.e., \nD \\setminus D^{*b}\n, form the test set.\n\n\n\n\n\nThe \nb632\n and \nb632+\n variants calculate a convex combination of the training performance and\nthe out-of-bag bootstrap performance and thus require predictions on the training sets and an\nappropriate aggregation strategy.\n\n\n## Use bootstrap as resampling strategy and predict on both train and test sets\nrdesc = makeResampleDesc(\nBootstrap\n, predict = \nboth\n, iters = 10)\n\n## Set aggregation schemes for b632 and b632+ bootstrap\nmmceB632 = setAggregation(mmce, b632)\nmmceB632plus = setAggregation(mmce, b632plus)\n\nmmceB632\n#\n Name: Mean misclassification error\n#\n Performance measure: mmce\n#\n Properties: classif,classif.multi,req.pred,req.truth\n#\n Minimize: TRUE\n#\n Best: 0; Worst: 1\n#\n Aggregated by: b632\n#\n Arguments: \n#\n Note: Defined as: mean(response != truth)\n\nr = resample(\nclassif.rpart\n, iris.task, rdesc, measures = list(mmce, mmceB632, mmceB632plus),\n  show.info = FALSE)\nhead(r$measures.train)\n#\n   iter       mmce       mmce       mmce\n#\n 1    1 0.04000000 0.04000000 0.04000000\n#\n 2    2 0.04000000 0.04000000 0.04000000\n#\n 3    3 0.01333333 0.01333333 0.01333333\n#\n 4    4 0.02666667 0.02666667 0.02666667\n#\n 5    5 0.01333333 0.01333333 0.01333333\n#\n 6    6 0.02000000 0.02000000 0.02000000\n\n## Compare misclassification rates for out-of-bag, b632, and b632+ bootstrap\nr$aggr\n#\n mmce.test.mean      mmce.b632  mmce.b632plus \n#\n     0.05804883     0.04797219     0.04860054\n\n\n\n\nConvenience functions\n\n\nThe functionality described on this page allows for much control and flexibility.\nHowever, when quickly trying out some learners, it can get tedious to type all the\ncode for defining the resampling strategy, setting the aggregation scheme and so\non.\nAs mentioned above, \nmlr\n includes some pre-defined resample description objects for\nfrequently used strategies like, e.g., 5-fold cross-validation (\ncv5\n).\nMoreover, \nmlr\n provides special functions for the most common resampling methods,\nfor example \nholdout\n, \ncrossval\n, or \nbootstrapB632\n.\n\n\ncrossval(\nclassif.lda\n, iris.task, iters = 3, measures = list(mmce, ber))\n#\n Resampling: cross-validation\n#\n Measures:             mmce      ber\n#\n [Resample] iter 1:    0.0200000 0.0158730\n#\n [Resample] iter 2:    0.0400000 0.0415140\n#\n [Resample] iter 3:    0.0000000 0.0000000\n#\n \n#\n Aggregated Result: mmce.test.mean=0.0200000,ber.test.mean=0.0191290\n#\n \n#\n Resample Result\n#\n Task: iris_example\n#\n Learner: classif.lda\n#\n Aggr perf: mmce.test.mean=0.0200000,ber.test.mean=0.0191290\n#\n Runtime: 0.0376048\n\nbootstrapB632plus(\nregr.lm\n, bh.task, iters = 3, measures = list(mse, mae))\n#\n Resampling: OOB bootstrapping\n#\n Measures:             mse.train   mae.train   mse.test    mae.test\n#\n [Resample] iter 1:    18.9037446  3.0912153   29.2662169  3.7698624\n#\n [Resample] iter 2:    17.9389954  3.0343581   26.3888260  3.5992878\n#\n [Resample] iter 3:    20.9092738  3.2640991   23.7739540  3.6788560\n#\n \n#\n Aggregated Result: mse.b632plus=23.9312510,mae.b632plus=3.4912886\n#\n \n#\n Resample Result\n#\n Task: BostonHousing-example\n#\n Learner: regr.lm\n#\n Aggr perf: mse.b632plus=23.9312510,mae.b632plus=3.4912886\n#\n Runtime: 0.0672176\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n# Not strictly necessary, but otherwise we might get NAs later on \n## if 'rpart' is not installed. \nlibrary(\nrpart\n) \n## 3-fold cross-validation \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nrdesc \n## Holdout estimation \nrdesc = makeResampleDesc(\nHoldout\n) \nrdesc \nhout \n\ncv3 \n## Specify the resampling strategy (3-fold cross-validation) \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \n\n## Calculate the performance \nr = resample(\nregr.lm\n, bh.task, rdesc) \n\nr \n## Peak into r \nnames(r) \n\nr$aggr \n\nr$measures.test \n## Subsampling with 5 iterations and default split ratio 2/3 \nrdesc = makeResampleDesc(\nSubsample\n, iters = 5) \n\n## Subsampling with 5 iterations and 4/5 training data \nrdesc = makeResampleDesc(\nSubsample\n, iters = 5, split = 4/5) \n\n## Classification tree with information splitting criterion \nlrn = makeLearner(\nclassif.rpart\n, parms = list(split = \ninformation\n)) \n\n## Calculate the performance measures \nr = resample(lrn, sonar.task, rdesc, measures = list(mmce, fpr, fnr, timetrain)) \n\nr \n## Add balanced error rate (ber) and time used to predict \naddRRMeasure(r, list(ber, timepredict)) \nr = resample(\nclassif.rpart\n, parms = list(split = \ninformation\n), sonar.task, rdesc, \n  measures = list(mmce, fpr, fnr, timetrain), show.info = FALSE) \n\nr \nr$pred \n\npred = getRRPredictions(r) \npred \nhead(as.data.frame(pred)) \n\nhead(getPredictionTruth(pred)) \n\nhead(getPredictionResponse(pred)) \n## Make predictions on both training and test sets \nrdesc = makeResampleDesc(\nHoldout\n, predict = \nboth\n) \n\nr = resample(\nclassif.lda\n, iris.task, rdesc, show.info = FALSE) \nr \n\nr$measures.train \npredList = getRRPredictionList(r) \npredList \n## 3-fold cross-validation \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \n\nr = resample(\nsurv.coxph\n, lung.task, rdesc, show.info = FALSE, models = TRUE) \nr$models \n## 3-fold cross-validation \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \n\n## Extract the compute cluster centers \nr = resample(\ncluster.kmeans\n, mtcars.task, rdesc, show.info = FALSE, \n  centers = 3, extract = function(x) getLearnerModel(x)$centers) \nr$extract \n## Extract the variable importance in a regression tree \nr = resample(\nregr.rpart\n, bh.task, rdesc, show.info = FALSE, extract = getFeatureImportance) \nr$extract \n## 3-fold cross-validation \nrdesc = makeResampleDesc(\nCV\n, iters = 3, stratify = TRUE) \n\nr = resample(\nclassif.lda\n, iris.task, rdesc, show.info = FALSE) \nr \nrdesc = makeResampleDesc(\nCV\n, iters = 3, stratify.cols = \nchas\n) \n\nr = resample(\nregr.rpart\n, bh.task, rdesc, show.info = FALSE) \nr \n## 5 blocks containing 30 observations each \ntask = makeClassifTask(data = iris, target = \nSpecies\n, blocking = factor(rep(1:5, each = 30))) \ntask \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nrdesc \n\nstr(rdesc) \n\nstr(makeResampleDesc(\nSubsample\n, stratify.cols = \nchas\n)) \n## Create a resample instance based an a task \nrin = makeResampleInstance(rdesc, iris.task) \nrin \n\nstr(rin) \n\n## Create a resample instance given the size of the data set \nrin = makeResampleInstance(rdesc, size = nrow(iris)) \nstr(rin) \n\n## Access the indices of the training observations in iteration 3 \nrin$train.inds[[3]] \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nrin = makeResampleInstance(rdesc, task = iris.task) \n\n## Calculate the performance of two learners based on the same resample instance \nr.lda = resample(\nclassif.lda\n, iris.task, rin, show.info = FALSE) \nr.rpart = resample(\nclassif.rpart\n, iris.task, rin, show.info = FALSE) \nr.lda$aggr \n\nr.rpart$aggr \nrin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150) \nrin \n## Mean misclassification error \nmmce$aggr \n\nmmce$aggr$fun \n\n## Root mean square error \nrmse$aggr \n\nrmse$aggr$fun \nmseTestMedian = setAggregation(mse, test.median) \nmseTestMin = setAggregation(mse, test.min) \nmseTestMax = setAggregation(mse, test.max) \n\nmseTestMedian \n\nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nr = resample(\nregr.lm\n, bh.task, rdesc, measures = list(mse, mseTestMedian, mseTestMin, mseTestMax)) \n\nr \n\nr$aggr \nmmceTrainMean = setAggregation(mmce, train.mean) \nrdesc = makeResampleDesc(\nCV\n, iters = 3, predict = \nboth\n) \nr = resample(\nclassif.rpart\n, iris.task, rdesc, measures = list(mmce, mmceTrainMean)) \n\nr$measures.train \n\nr$aggr \n## Use bootstrap as resampling strategy and predict on both train and test sets \nrdesc = makeResampleDesc(\nBootstrap\n, predict = \nboth\n, iters = 10) \n\n## Set aggregation schemes for b632 and b632+ bootstrap \nmmceB632 = setAggregation(mmce, b632) \nmmceB632plus = setAggregation(mmce, b632plus) \n\nmmceB632 \n\nr = resample(\nclassif.rpart\n, iris.task, rdesc, measures = list(mmce, mmceB632, mmceB632plus), \n  show.info = FALSE) \nhead(r$measures.train) \n\n## Compare misclassification rates for out-of-bag, b632, and b632+ bootstrap \nr$aggr \ncrossval(\nclassif.lda\n, iris.task, iters = 3, measures = list(mmce, ber)) \n\nbootstrapB632plus(\nregr.lm\n, bh.task, iters = 3, measures = list(mse, mae))", 
            "title": "Resampling"
        }, 
        {
            "location": "/resample/index.html#resampling", 
            "text": "Resampling strategies are usually used to assess the performance of a learning algorithm:\nThe entire data set is (repeatedly) split into training sets  D^{*b}  and test sets D \\setminus D^{*b} ,  b = 1,\\ldots,B .\nThe learner is trained on each training set, predictions are made on the corresponding test\nset (sometimes on the training set as well) and the performance measure S(D^{*b}, D \\setminus D^{*b})  is calculated.\nThen the  B  individual performance values are aggregated, most often by calculating the mean.\nThere exist various different resampling strategies, for example cross-validation and bootstrap,\nto mention just two popular approaches.   If you want to read up on further details, the paper Resampling Strategies for Model Assessment and Selection \nby Simon is probably not a bad choice.\nBernd has also published a paper Resampling methods for meta-model validation with recommendations for evolutionary computation \nwhich contains detailed descriptions and lots of statistical background information on resampling methods.", 
            "title": "Resampling"
        }, 
        {
            "location": "/resample/index.html#defining-the-resampling-strategy", 
            "text": "In  mlr  the resampling strategy can be defined via function  makeResampleDesc .\nIt requires a string that specifies the resampling method and, depending on the selected\nstrategy, further information like the number of iterations.\nThe supported resampling strategies are:   Cross-validation ( \"CV\" ),  Leave-one-out cross-validation ( \"LOO\" ),  Repeated cross-validation ( \"RepCV\" ),  Out-of-bag bootstrap and other variants like  b632  ( \"Bootstrap\" ),  Subsampling, also called Monte-Carlo cross-validation ( \"Subsample\" ),  Holdout (training/test) ( \"Holdout\" ).   For example if you want to use 3-fold cross-validation type:  ## 3-fold cross-validation\nrdesc = makeResampleDesc( CV , iters = 3)\nrdesc\n#  Resample description: cross-validation with 3 iterations.\n#  Predict: test\n#  Stratification: FALSE  For holdout estimation use:  ## Holdout estimation\nrdesc = makeResampleDesc( Holdout )\nrdesc\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE  In order to save you some typing  mlr  contains some pre-defined resample descriptions for\nvery common strategies like holdout ( hout ) as well as cross-validation\nwith different numbers of folds (e.g.,  cv5  or  cv10 ).  hout\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE\n\ncv3\n#  Resample description: cross-validation with 3 iterations.\n#  Predict: test\n#  Stratification: FALSE", 
            "title": "Defining the resampling strategy"
        }, 
        {
            "location": "/resample/index.html#performing-the-resampling", 
            "text": "Function  resample  evaluates a  Learner  on a\ngiven machine learning  Task  using the selected  resampling strategy .  As a first example, the performance of  linear regression  on the BostonHousing  data set is calculated using  3-fold cross-validation .  Generally, for  K -fold cross-validation  the data set  D  is partitioned into  K  subsets\nof (approximately) equal size.\nIn the  b -th of the  K  iterations, the  b -th subset is used for testing, while the union\nof the remaining parts forms the training set.  As usual, you can either pass a  Learner  object to  resample  or, as done here,\nprovide the class name  \"regr.lm\"  of the learner.\nSince no performance measure is specified the default for regression learners\n(mean squared error,  mse ) is calculated.  ## Specify the resampling strategy (3-fold cross-validation)\nrdesc = makeResampleDesc( CV , iters = 3)\n\n## Calculate the performance\nr = resample( regr.lm , bh.task, rdesc)\n#  Resampling: cross-validation\n#  Measures:             mse\n#  [Resample] iter 1:    19.8982806\n#  [Resample] iter 2:    31.7803405\n#  [Resample] iter 3:    20.9698523\n#  \n#  Aggregated Result: mse.test.mean=24.2161578\n#  \n\nr\n#  Resample Result\n#  Task: BostonHousing-example\n#  Learner: regr.lm\n#  Aggr perf: mse.test.mean=24.2161578\n#  Runtime: 0.054549  The result  r  is an object of class  ResampleResult .\nIt contains performance results for the learner and some additional information\nlike the runtime, predicted values, and optionally the models fitted in single resampling\niterations.  ## Peak into r\nnames(r)\n#   [1]  learner.id       task.id          task.desc        measures.train \n#   [5]  measures.test    aggr             pred             models         \n#   [9]  err.msgs         err.dumps        extract          runtime \n\nr$aggr\n#  mse.test.mean \n#       24.21616\n\nr$measures.test\n#    iter      mse\n#  1    1 19.89828\n#  2    2 31.78034\n#  3    3 20.96985  r$measures.test  gives the performance on each of the 3 test data sets. r$aggr  shows the aggregated performance value.\nIts name  \"mse.test.mean\"  indicates the performance measure,  mse ,\nand the method,  test.mean , used to aggregate the 3 individual performances. test.mean  is the default aggregation scheme for most performance measures\nand, as the name implies, takes the mean over the performances on the test data sets.  Resampling in  mlr  works the same way for all types of learning problems and learners.\nBelow is a classification example where a  classification tree (rpart)  is\nevaluated on the  Sonar  data set by subsampling with 5 iterations.  In each subsampling iteration the data set  D  is randomly partitioned into a\ntraining and a test set according to a given percentage, e.g., 2/3\ntraining and 1/3 test set. If there is just one iteration, the strategy\nis commonly called  holdout  or  test sample estimation .  You can calculate several measures at once by passing a  list  of Measure s to  resample .\nBelow, the error rate ( mmce ), false positive and false negative rates\n( fpr ,  fnr ), and the time it takes to train the learner\n( timetrain ) are estimated by  subsampling  with 5 iterations.  ## Subsampling with 5 iterations and default split ratio 2/3\nrdesc = makeResampleDesc( Subsample , iters = 5)\n\n## Subsampling with 5 iterations and 4/5 training data\nrdesc = makeResampleDesc( Subsample , iters = 5, split = 4/5)\n\n## Classification tree with information splitting criterion\nlrn = makeLearner( classif.rpart , parms = list(split =  information ))\n\n## Calculate the performance measures\nr = resample(lrn, sonar.task, rdesc, measures = list(mmce, fpr, fnr, timetrain))\n#  Resampling: subsampling\n#  Measures:             mmce        fpr         fnr         timetrain\n#  [Resample] iter 1:    0.2619048   0.2352941   0.2800000   0.0240000\n#  [Resample] iter 2:    0.2857143   0.2857143   0.2857143   0.0190000\n#  [Resample] iter 3:    0.2619048   0.2857143   0.2380952   0.0220000\n#  [Resample] iter 4:    0.3333333   0.3500000   0.3181818   0.0190000\n#  [Resample] iter 5:    0.3333333   0.1666667   0.4583333   0.0200000\n#  \n#  Aggregated Result: mmce.test.mean=0.2952381,fpr.test.mean=0.2646779,fnr.test.mean=0.3160649,timetrain.test.mean=0.0208000\n#  \n\nr\n#  Resample Result\n#  Task: Sonar_example\n#  Learner: classif.rpart\n#  Aggr perf: mmce.test.mean=0.2952381,fpr.test.mean=0.2646779,fnr.test.mean=0.3160649,timetrain.test.mean=0.0208000\n#  Runtime: 0.188301  If you want to add further measures afterwards, use  addRRMeasure .  ## Add balanced error rate (ber) and time used to predict\naddRRMeasure(r, list(ber, timepredict))\n#  Resample Result\n#  Task: Sonar_example\n#  Learner: classif.rpart\n#  Aggr perf: mmce.test.mean=0.2952381,fpr.test.mean=0.2646779,fnr.test.mean=0.3160649,timetrain.test.mean=0.0208000,ber.test.mean=0.2903714,timepredict.test.mean=0.0062000\n#  Runtime: 0.188301  By default,  resample  prints progress messages and intermediate results. You can turn this off by setting show.info = FALSE , as done in the code chunk below. (If you are interested in suppressing\nthese messages permanently have a look at the tutorial page about  configuring mlr .)  In the above example, the  Learner  was explicitly constructed. For convenience\nyou can also specify the learner as a string and pass any learner parameters via the  ...  argument\nof  resample .  r = resample( classif.rpart , parms = list(split =  information ), sonar.task, rdesc,\n  measures = list(mmce, fpr, fnr, timetrain), show.info = FALSE)\n\nr\n#  Resample Result\n#  Task: Sonar_example\n#  Learner: classif.rpart\n#  Aggr perf: mmce.test.mean=0.3047619,fpr.test.mean=0.2785319,fnr.test.mean=0.3093917,timetrain.test.mean=0.0230000\n#  Runtime: 0.204483", 
            "title": "Performing the resampling"
        }, 
        {
            "location": "/resample/index.html#accessing-resample-results", 
            "text": "Apart from the learner performance you can extract further information from the\nresample results, for example predicted values or the models fitted in individual resample\niterations.", 
            "title": "Accessing resample results"
        }, 
        {
            "location": "/resample/index.html#predictions", 
            "text": "Per default, the  ResampleResult  contains the predictions made during the resampling.\nIf you do not want to keep them, e.g., in order to conserve memory,\nset  keep.pred = FALSE  when calling  resample .  The predictions are stored in slot  $pred  of the resampling result, which can also be accessed\nby function  getRRPredictions .  r$pred\n#  Resampled Prediction for:\n#  Resample description: subsampling with 5 iterations and 0.80 split rate.\n#  Predict: test\n#  Stratification: FALSE\n#  predict.type: response\n#  threshold: \n#  time (mean): 0.01\n#     id truth response iter  set\n#  1  36     R        M    1 test\n#  2 132     M        R    1 test\n#  3 145     M        R    1 test\n#  4 161     M        R    1 test\n#  5 108     M        M    1 test\n#  6 178     M        M    1 test\n#  ... (#rows: 210, #cols: 5)\n\npred = getRRPredictions(r)\npred\n#  Resampled Prediction for:\n#  Resample description: subsampling with 5 iterations and 0.80 split rate.\n#  Predict: test\n#  Stratification: FALSE\n#  predict.type: response\n#  threshold: \n#  time (mean): 0.01\n#     id truth response iter  set\n#  1  36     R        M    1 test\n#  2 132     M        R    1 test\n#  3 145     M        R    1 test\n#  4 161     M        R    1 test\n#  5 108     M        M    1 test\n#  6 178     M        M    1 test\n#  ... (#rows: 210, #cols: 5)  pred  is an object of class  ResamplePrediction .\nJust as a  Prediction  object (see the tutorial page on  making predictions )\nit has an element  $data  which is a  data.frame  that contains the\npredictions and in the case of a supervised learning problem the true values of the target\nvariable(s).\nYou can use  as.data.frame  to directly access the  $data  slot. Moreover, all getter\nfunctions for  Prediction  objects like  getPredictionResponse  or getPredictionProbabilities  are applicable.  head(as.data.frame(pred))\n#     id truth response iter  set\n#  1  36     R        M    1 test\n#  2 132     M        R    1 test\n#  3 145     M        R    1 test\n#  4 161     M        R    1 test\n#  5 108     M        M    1 test\n#  6 178     M        M    1 test\n\nhead(getPredictionTruth(pred))\n#  [1] R M M M M M\n#  Levels: M R\n\nhead(getPredictionResponse(pred))\n#  [1] M R R R M M\n#  Levels: M R  The columns  iter  and  set  in the  data.frame  indicate the\nresampling iteration and the data set ( train  or  test ) for which the prediction was made.  By default, predictions are made for the test sets only.\nIf predictions for the training set are required, set  predict = \"train\"  (for predictions\non the train set only) or  predict = \"both\"  (for predictions on both train and test sets)\nin  makeResampleDesc . In any case, this is necessary for some bootstrap methods\n( b632  and  b632+ ) and some examples are shown  later on .  Below, we use simple Holdout, i.e., split the data once into a training and test set, as\nresampling strategy and make predictions on both sets.  ## Make predictions on both training and test sets\nrdesc = makeResampleDesc( Holdout , predict =  both )\n\nr = resample( classif.lda , iris.task, rdesc, show.info = FALSE)\nr\n#  Resample Result\n#  Task: iris_example\n#  Learner: classif.lda\n#  Aggr perf: mmce.test.mean=0.0200000\n#  Runtime: 0.0183854\n\nr$measures.train\n#    iter mmce\n#  1    1 0.02  (Please note that nonetheless the misclassification rate  r$aggr  is estimated on the test data only.\nHow to calculate performance measures on the training sets is shown below .)  A second function to extract predictions from resample results is  getRRPredictionList \nwhich returns a  list  of predictions split by data set (train/test) and\nresampling iteration.  predList = getRRPredictionList(r)\npredList\n#  $train\n#  $train$`1`\n#  Prediction: 100 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#       id      truth   response\n#  123 123  virginica  virginica\n#  101 101  virginica  virginica\n#  51   51 versicolor versicolor\n#  45   45     setosa     setosa\n#  46   46     setosa     setosa\n#  3     3     setosa     setosa\n#  ... (#rows: 100, #cols: 3)\n#  \n#  \n#  $test\n#  $test$`1`\n#  Prediction: 50 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#       id      truth   response\n#  109 109  virginica  virginica\n#  80   80 versicolor versicolor\n#  40   40     setosa     setosa\n#  140 140  virginica  virginica\n#  125 125  virginica  virginica\n#  10   10     setosa     setosa\n#  ... (#rows: 50, #cols: 3)", 
            "title": "Predictions"
        }, 
        {
            "location": "/resample/index.html#learner-models", 
            "text": "In each resampling iteration a  Learner  is fitted on the respective training set.\nBy default, the resulting  WrappedModel s are not included in the ResampleResult  and slot  $models  is empty.\nIn order to keep them, set  models = TRUE  when calling  resample , as in the following\nsurvival analysis example.  ## 3-fold cross-validation\nrdesc = makeResampleDesc( CV , iters = 3)\n\nr = resample( surv.coxph , lung.task, rdesc, show.info = FALSE, models = TRUE)\nr$models\n#  [[1]]\n#  Model for learner.id=surv.coxph; learner.class=surv.coxph\n#  Trained on: task.id = lung-example; obs = 112; features = 8\n#  Hyperparameters: \n#  \n#  [[2]]\n#  Model for learner.id=surv.coxph; learner.class=surv.coxph\n#  Trained on: task.id = lung-example; obs = 111; features = 8\n#  Hyperparameters: \n#  \n#  [[3]]\n#  Model for learner.id=surv.coxph; learner.class=surv.coxph\n#  Trained on: task.id = lung-example; obs = 111; features = 8\n#  Hyperparameters:", 
            "title": "Learner models"
        }, 
        {
            "location": "/resample/index.html#the-extract-option", 
            "text": "Keeping complete fitted models can be memory-intensive if these objects are large or\nthe number of resampling iterations is high.\nAlternatively, you can use the  extract  argument of  resample  to retain only the\ninformation you need.\nTo this end you need to pass a  function  to  extract  which is applied\nto each  WrappedModel  object fitted in each resampling iteration.  Below, we cluster the  mtcars  data using the  k -means algorithm with  k = 3 \nand keep only the cluster centers.  ## 3-fold cross-validation\nrdesc = makeResampleDesc( CV , iters = 3)\n\n## Extract the compute cluster centers\nr = resample( cluster.kmeans , mtcars.task, rdesc, show.info = FALSE,\n  centers = 3, extract = function(x) getLearnerModel(x)$centers)\nr$extract\n#  [[1]]\n#         mpg      cyl     disp        hp     drat       wt     qsec\n#  1 16.23333 7.666667 308.9667 214.00000 3.400000 3.564167 16.37000\n#  2 26.00833 4.333333 113.5917  86.08333 4.040833 2.368583 18.88917\n#  3 13.33333 8.000000 444.0000 198.33333 3.003333 4.839667 17.61667\n#           vs        am     gear     carb\n#  1 0.1666667 0.3333333 3.666667 3.666667\n#  2 0.8333333 0.6666667 4.083333 1.916667\n#  3 0.0000000 0.0000000 3.000000 3.333333\n#  \n#  [[2]]\n#        mpg cyl     disp     hp  drat       wt     qsec    vs   am  gear\n#  1 15.5600   8 326.0400 207.00 3.198 3.830000 16.74600 0.000 0.10 3.200\n#  2 26.7125   4 102.8875  86.00 4.145 2.179125 19.05375 0.875 0.75 4.125\n#  3 19.1500   6 174.4000 128.25 3.550 3.136250 17.91000 0.500 0.50 4.000\n#     carb\n#  1 3.500\n#  2 1.625\n#  3 3.750\n#  \n#  [[3]]\n#         mpg cyl     disp       hp     drat       wt     qsec        vs\n#  1 25.25000   4 113.6000  82.5000 3.932500 2.622500 19.17000 1.0000000\n#  2 15.12000   8 369.8600 201.9000 3.211000 4.098900 17.05300 0.0000000\n#  3 19.74286   6 183.3143 122.2857 3.585714 3.117143 17.97714 0.5714286\n#           am     gear     carb\n#  1 0.7500000 4.000000 1.500000\n#  2 0.1000000 3.200000 3.200000\n#  3 0.4285714 3.857143 3.428571  As a second example, we extract the variable importances from fitted regression trees using\nfunction  getFeatureImportance .\n(For more detailed information on this topic see the feature selection  page.)  ## Extract the variable importance in a regression tree\nr = resample( regr.rpart , bh.task, rdesc, show.info = FALSE, extract = getFeatureImportance)\nr$extract\n#  [[1]]\n#  FeatureImportance:\n#  Task: BostonHousing-example\n#  \n#  Learner: regr.rpart\n#  Measure: NA\n#  Contrast: NA\n#  Aggregation: function (x)  x\n#  Replace: NA\n#  Number of Monte-Carlo iterations: NA\n#  Local: FALSE\n#        crim       zn    indus     chas      nox       rm      age      dis\n#  1 3842.839 952.3849 4443.578 90.63669 3772.273 15853.01 3997.275 3355.651\n#         rad     tax  ptratio b    lstat\n#  1 987.4256 568.177 2860.129 0 11255.66\n#  \n#  [[2]]\n#  FeatureImportance:\n#  Task: BostonHousing-example\n#  \n#  Learner: regr.rpart\n#  Measure: NA\n#  Contrast: NA\n#  Aggregation: function (x)  x\n#  Replace: NA\n#  Number of Monte-Carlo iterations: NA\n#  Local: FALSE\n#        crim       zn    indus chas      nox       rm      age      dis\n#  1 3246.521 3411.444 5806.613    0 2349.776 10125.04 5692.587 2108.059\n#         rad     tax  ptratio        b    lstat\n#  1 312.6521 2159.42 1104.839 174.6412 15871.53\n#  \n#  [[3]]\n#  FeatureImportance:\n#  Task: BostonHousing-example\n#  \n#  Learner: regr.rpart\n#  Measure: NA\n#  Contrast: NA\n#  Aggregation: function (x)  x\n#  Replace: NA\n#  Number of Monte-Carlo iterations: NA\n#  Local: FALSE\n#        crim      zn    indus chas      nox       rm      age      dis\n#  1 3785.852 1649.28 4942.119    0 3989.326 18426.87 2604.239 350.8401\n#        rad      tax  ptratio        b    lstat\n#  1 800.798 2907.556 3871.556 491.6297 12505.88", 
            "title": "The extract option"
        }, 
        {
            "location": "/resample/index.html#stratification-and-blocking", 
            "text": "Stratification  with respect to a categorical variable makes sure that all its values\n  are present in each training and test set in approximately the same proportion as in the original data set.\n  Stratification is possible with regard to categorical target variables (and thus for supervised\n  classification and survival analysis) or categorical explanatory variables.  Blocking  refers to the situation that subsets of observations belong together and must not\n  be separated during resampling.\n  Hence, for one train/test set pair the entire block is either in the training set or in\n  the test set.", 
            "title": "Stratification and blocking"
        }, 
        {
            "location": "/resample/index.html#stratification-with-respect-to-the-target-variables", 
            "text": "For classification, it is usually desirable to have the same proportion of the classes in\nall of the partitions of the original data set.\nThis is particularly useful in the case of imbalanced classes and small data sets. Otherwise,\nit may happen that observations of less frequent classes are missing in some of the training\nsets which can decrease the performance of the learner, or lead to model crashes.\nIn order to conduct stratified resampling, set  stratify = TRUE  in  makeResampleDesc .  ## 3-fold cross-validation\nrdesc = makeResampleDesc( CV , iters = 3, stratify = TRUE)\n\nr = resample( classif.lda , iris.task, rdesc, show.info = FALSE)\nr\n#  Resample Result\n#  Task: iris_example\n#  Learner: classif.lda\n#  Aggr perf: mmce.test.mean=0.0132026\n#  Runtime: 0.042295  Stratification is also available for survival tasks.\nHere the stratification balances the censoring rate.", 
            "title": "Stratification with respect to the target variable(s)"
        }, 
        {
            "location": "/resample/index.html#stratification-with-respect-to-explanatory-variables", 
            "text": "Sometimes it is required to also stratify on the input data, e.g., to ensure that all\nsubgroups are represented in all training and test sets.\nTo stratify on the input columns, specify  factor  columns of your task data\nvia  stratify.cols .  rdesc = makeResampleDesc( CV , iters = 3, stratify.cols =  chas )\n\nr = resample( regr.rpart , bh.task, rdesc, show.info = FALSE)\nr\n#  Resample Result\n#  Task: BostonHousing-example\n#  Learner: regr.rpart\n#  Aggr perf: mse.test.mean=21.2385142\n#  Runtime: 0.0552762", 
            "title": "Stratification with respect to explanatory variables"
        }, 
        {
            "location": "/resample/index.html#blocking", 
            "text": "If some observations \"belong together\" and must not be separated when splitting the\ndata into training and test sets for resampling, you can supply this information via a blocking   factor  when  creating the task .  ## 5 blocks containing 30 observations each\ntask = makeClassifTask(data = iris, target =  Species , blocking = factor(rep(1:5, each = 30)))\ntask\n#  Supervised task: iris\n#  Type: classif\n#  Target: Species\n#  Observations: 150\n#  Features:\n#     numerics     factors     ordered functionals \n#            4           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: TRUE\n#  Is spatial: FALSE\n#  Classes: 3\n#      setosa versicolor  virginica \n#          50         50         50 \n#  Positive class: NA", 
            "title": "Blocking"
        }, 
        {
            "location": "/resample/index.html#resample-descriptions-and-resample-instances", 
            "text": "As already mentioned, you can specify a resampling strategy using function  makeResampleDesc .  rdesc = makeResampleDesc( CV , iters = 3)\nrdesc\n#  Resample description: cross-validation with 3 iterations.\n#  Predict: test\n#  Stratification: FALSE\n\nstr(rdesc)\n#  List of 4\n#   $ id      : chr  cross-validation \n#   $ iters   : int 3\n#   $ predict : chr  test \n#   $ stratify: logi FALSE\n#   - attr(*,  class )= chr [1:2]  CVDesc   ResampleDesc \n\nstr(makeResampleDesc( Subsample , stratify.cols =  chas ))\n#  List of 6\n#   $ split        : num 0.667\n#   $ id           : chr  subsampling \n#   $ iters        : int 30\n#   $ predict      : chr  test \n#   $ stratify     : logi FALSE\n#   $ stratify.cols: chr  chas \n#   - attr(*,  class )= chr [1:2]  SubsampleDesc   ResampleDesc   The result  rdesc  inherits from class  ResampleDesc  (short for\nresample description) and, in principle, contains all necessary information about the\nresampling strategy including the number of iterations, the proportion of training and test\nsets, stratification variables, etc.  Given either the size of the data set at hand or the  Task , function  makeResampleInstance \ndraws the training and test sets according to the  ResampleDesc .  ## Create a resample instance based an a task\nrin = makeResampleInstance(rdesc, iris.task)\nrin\n#  Resample instance for 150 cases.\n#  Resample description: cross-validation with 3 iterations.\n#  Predict: test\n#  Stratification: FALSE\n\nstr(rin)\n#  List of 5\n#   $ desc      :List of 4\n#    ..$ id      : chr  cross-validation \n#    ..$ iters   : int 3\n#    ..$ predict : chr  test \n#    ..$ stratify: logi FALSE\n#    ..- attr(*,  class )= chr [1:2]  CVDesc   ResampleDesc \n#   $ size      : int 150\n#   $ train.inds:List of 3\n#    ..$ : int [1:100] 88 129 94 109 108 43 72 47 137 39 ...\n#    ..$ : int [1:100] 129 94 138 83 112 54 29 36 72 137 ...\n#    ..$ : int [1:100] 88 138 109 83 112 108 54 29 36 43 ...\n#   $ test.inds :List of 3\n#    ..$ : int [1:50] 2 5 6 13 14 17 20 21 24 25 ...\n#    ..$ : int [1:50] 3 4 7 8 11 12 22 30 34 35 ...\n#    ..$ : int [1:50] 1 9 10 15 16 18 19 23 27 28 ...\n#   $ group     : Factor w/ 0 levels: \n#   - attr(*,  class )= chr  ResampleInstance \n\n## Create a resample instance given the size of the data set\nrin = makeResampleInstance(rdesc, size = nrow(iris))\nstr(rin)\n#  List of 5\n#   $ desc      :List of 4\n#    ..$ id      : chr  cross-validation \n#    ..$ iters   : int 3\n#    ..$ predict : chr  test \n#    ..$ stratify: logi FALSE\n#    ..- attr(*,  class )= chr [1:2]  CVDesc   ResampleDesc \n#   $ size      : int 150\n#   $ train.inds:List of 3\n#    ..$ : int [1:100] 149 58 120 44 148 29 66 46 124 137 ...\n#    ..$ : int [1:100] 51 58 64 148 56 46 124 8 14 137 ...\n#    ..$ : int [1:100] 149 51 120 44 64 56 29 66 8 14 ...\n#   $ test.inds :List of 3\n#    ..$ : int [1:50] 3 8 12 14 17 22 23 24 32 34 ...\n#    ..$ : int [1:50] 1 2 4 6 10 11 13 26 29 30 ...\n#    ..$ : int [1:50] 5 7 9 15 16 18 19 20 21 25 ...\n#   $ group     : Factor w/ 0 levels: \n#   - attr(*,  class )= chr  ResampleInstance \n\n## Access the indices of the training observations in iteration 3\nrin$train.inds[[3]]\n#    [1] 149  51 120  44  64  56  29  66   8  14  83  65  97 114  13   3 104\n#   [18]  88 130  81  89  23  63 131  92  31  41  78  72 139  67  10  57  12\n#   [35] 107  74  70 116  36  24  35  93 126 111  75  91  80  85  42  30  22\n#   [52]   1  69 113  87  26  17 150 119   4 138 129 147  38  99  60 142  50\n#   [69] 122  40 127  43  96  34 141 106  79 133 145 125 135 108  52 109  37\n#   [86]  61  84  59  39  82  32  53  94   6  45  86  95   2  68  11  The result  rin  inherits from class  ResampleInstance  and contains list s of index vectors for the train and test sets.  If a  ResampleDesc  is passed to  resample , it is instantiated internally.\nNaturally, it is also possible to pass a  ResampleInstance  directly.  While the separation between resample descriptions, resample instances, and the  resample \nfunction itself seems overly complicated, it has several advantages:   Resample instances readily allow for paired experiments, that is comparing the performance\n  of several learners on exactly the same training and test sets.\n  This is particularly useful if you want to add another method to a comparison experiment\n  you already did.\n  Moreover, you can store the resample instance along with your data in order to be able to reproduce\n  your results later on.   rdesc = makeResampleDesc( CV , iters = 3)\nrin = makeResampleInstance(rdesc, task = iris.task)\n\n## Calculate the performance of two learners based on the same resample instance\nr.lda = resample( classif.lda , iris.task, rin, show.info = FALSE)\nr.rpart = resample( classif.rpart , iris.task, rin, show.info = FALSE)\nr.lda$aggr\n#  mmce.test.mean \n#            0.02\n\nr.rpart$aggr\n#  mmce.test.mean \n#      0.05333333   In order to add further resampling methods you can simply derive from the  ResampleDesc \n  and  ResampleInstance  classes, but you do neither have to touch  resample \n  nor any further methods that use the resampling strategy.   Usually, when calling  makeResampleInstance  the train and test index sets are drawn randomly.\nMainly for  holdout  ( test sample )  estimation  you might want full control about the training\nand tests set and specify them manually.\nThis can be done using function  makeFixedHoldoutInstance .  rin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)\nrin\n#  Resample instance for 150 cases.\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE", 
            "title": "Resample descriptions and resample instances"
        }, 
        {
            "location": "/resample/index.html#aggregating-performance-values", 
            "text": "In each resampling iteration  b = 1,\\ldots,B  we get performance values S(D^{*b}, D \\setminus D^{*b})  (for each measure we wish to calculate), which are then\naggregated to an overall performance.  For the great majority of common resampling strategies (like holdout, cross-validation, subsampling)\nperformance values are calculated on the test data sets only and for most measures aggregated\nby taking the mean ( test.mean ).  Each performance  Measure  in  mlr  has a corresponding default aggregation\nmethod which is stored in slot  $aggr .\nThe default aggregation for most measures is  test.mean .\nOne exception is the root mean square error ( rmse ).  ## Mean misclassification error\nmmce$aggr\n#  Aggregation function: test.mean\n\nmmce$aggr$fun\n#  function (task, perf.test, perf.train, measure, group, pred) \n#  mean(perf.test)\n#   bytecode: 0xa1eb838 \n#   environment: namespace:mlr \n\n## Root mean square error\nrmse$aggr\n#  Aggregation function: test.rmse\n\nrmse$aggr$fun\n#  function (task, perf.test, perf.train, measure, group, pred) \n#  sqrt(mean(perf.test^2))\n#   bytecode: 0xfe7c908 \n#   environment: namespace:mlr   You can change the aggregation method of a  Measure  via function  setAggregation .\nAll available aggregation schemes are listed on the  aggregations  documentation page.", 
            "title": "Aggregating performance values"
        }, 
        {
            "location": "/resample/index.html#example-one-measure-with-different-aggregations", 
            "text": "The aggregation schemes  test.median ,  test.min , and test.max  compute the median, minimum, and maximum of the performance values\non the test sets.  mseTestMedian = setAggregation(mse, test.median)\nmseTestMin = setAggregation(mse, test.min)\nmseTestMax = setAggregation(mse, test.max)\n\nmseTestMedian\n#  Name: Mean of squared errors\n#  Performance measure: mse\n#  Properties: regr,req.pred,req.truth\n#  Minimize: TRUE\n#  Best: 0; Worst: Inf\n#  Aggregated by: test.median\n#  Arguments: \n#  Note: Defined as: mean((response - truth)^2)\n\nrdesc = makeResampleDesc( CV , iters = 3)\nr = resample( regr.lm , bh.task, rdesc, measures = list(mse, mseTestMedian, mseTestMin, mseTestMax))\n#  Resampling: cross-validation\n#  Measures:             mse       mse       mse       mse\n#  [Resample] iter 1:    28.164474328.164474328.164474328.1644743\n#  [Resample] iter 2:    17.593981817.593981817.593981817.5939818\n#  [Resample] iter 3:    24.957218724.957218724.957218724.9572187\n#  \n#  Aggregated Result: mse.test.mean=23.5718916,mse.test.median=24.9572187,mse.test.min=17.5939818,mse.test.max=28.1644743\n#  \n\nr\n#  Resample Result\n#  Task: BostonHousing-example\n#  Learner: regr.lm\n#  Aggr perf: mse.test.mean=23.5718916,mse.test.median=24.9572187,mse.test.min=17.5939818,mse.test.max=28.1644743\n#  Runtime: 0.044241\n\nr$aggr\n#    mse.test.mean mse.test.median    mse.test.min    mse.test.max \n#         23.57189        24.95722        17.59398        28.16447", 
            "title": "Example: One measure with different aggregations"
        }, 
        {
            "location": "/resample/index.html#example-calculating-the-training-error", 
            "text": "Below we calculate the mean misclassification error ( mmce ) on the training\nand the test data sets. Note that we have to set  predict = \"both\"  when calling  makeResampleDesc \nin order to get predictions on both training and test sets.  mmceTrainMean = setAggregation(mmce, train.mean)\nrdesc = makeResampleDesc( CV , iters = 3, predict =  both )\nr = resample( classif.rpart , iris.task, rdesc, measures = list(mmce, mmceTrainMean))\n#  Resampling: cross-validation\n#  Measures:             mmce.train   mmce.test\n#  [Resample] iter 1:    0.0200000    0.1000000\n#  [Resample] iter 2:    0.0400000    0.0400000\n#  [Resample] iter 3:    0.0400000    0.0400000\n#  \n#  Aggregated Result: mmce.test.mean=0.0600000,mmce.train.mean=0.0333333\n#  \n\nr$measures.train\n#    iter mmce mmce\n#  1    1 0.02 0.02\n#  2    2 0.04 0.04\n#  3    3 0.04 0.04\n\nr$aggr\n#   mmce.test.mean mmce.train.mean \n#       0.06000000      0.03333333", 
            "title": "Example: Calculating the training error"
        }, 
        {
            "location": "/resample/index.html#example-bootstrap", 
            "text": "In  out-of-bag bootstrap estimation   B  new data sets  D^{*1}, \\ldots, D^{*B}  are drawn\nfrom the data set  D  with replacement, each of the same size as  D .\nIn the  b -th iteration,  D^{*b}  forms the training set, while the remaining elements from D , i.e.,  D \\setminus D^{*b} , form the test set.   The  b632  and  b632+  variants calculate a convex combination of the training performance and\nthe out-of-bag bootstrap performance and thus require predictions on the training sets and an\nappropriate aggregation strategy.  ## Use bootstrap as resampling strategy and predict on both train and test sets\nrdesc = makeResampleDesc( Bootstrap , predict =  both , iters = 10)\n\n## Set aggregation schemes for b632 and b632+ bootstrap\nmmceB632 = setAggregation(mmce, b632)\nmmceB632plus = setAggregation(mmce, b632plus)\n\nmmceB632\n#  Name: Mean misclassification error\n#  Performance measure: mmce\n#  Properties: classif,classif.multi,req.pred,req.truth\n#  Minimize: TRUE\n#  Best: 0; Worst: 1\n#  Aggregated by: b632\n#  Arguments: \n#  Note: Defined as: mean(response != truth)\n\nr = resample( classif.rpart , iris.task, rdesc, measures = list(mmce, mmceB632, mmceB632plus),\n  show.info = FALSE)\nhead(r$measures.train)\n#    iter       mmce       mmce       mmce\n#  1    1 0.04000000 0.04000000 0.04000000\n#  2    2 0.04000000 0.04000000 0.04000000\n#  3    3 0.01333333 0.01333333 0.01333333\n#  4    4 0.02666667 0.02666667 0.02666667\n#  5    5 0.01333333 0.01333333 0.01333333\n#  6    6 0.02000000 0.02000000 0.02000000\n\n## Compare misclassification rates for out-of-bag, b632, and b632+ bootstrap\nr$aggr\n#  mmce.test.mean      mmce.b632  mmce.b632plus \n#      0.05804883     0.04797219     0.04860054", 
            "title": "Example: Bootstrap"
        }, 
        {
            "location": "/resample/index.html#convenience-functions", 
            "text": "The functionality described on this page allows for much control and flexibility.\nHowever, when quickly trying out some learners, it can get tedious to type all the\ncode for defining the resampling strategy, setting the aggregation scheme and so\non.\nAs mentioned above,  mlr  includes some pre-defined resample description objects for\nfrequently used strategies like, e.g., 5-fold cross-validation ( cv5 ).\nMoreover,  mlr  provides special functions for the most common resampling methods,\nfor example  holdout ,  crossval , or  bootstrapB632 .  crossval( classif.lda , iris.task, iters = 3, measures = list(mmce, ber))\n#  Resampling: cross-validation\n#  Measures:             mmce      ber\n#  [Resample] iter 1:    0.0200000 0.0158730\n#  [Resample] iter 2:    0.0400000 0.0415140\n#  [Resample] iter 3:    0.0000000 0.0000000\n#  \n#  Aggregated Result: mmce.test.mean=0.0200000,ber.test.mean=0.0191290\n#  \n#  Resample Result\n#  Task: iris_example\n#  Learner: classif.lda\n#  Aggr perf: mmce.test.mean=0.0200000,ber.test.mean=0.0191290\n#  Runtime: 0.0376048\n\nbootstrapB632plus( regr.lm , bh.task, iters = 3, measures = list(mse, mae))\n#  Resampling: OOB bootstrapping\n#  Measures:             mse.train   mae.train   mse.test    mae.test\n#  [Resample] iter 1:    18.9037446  3.0912153   29.2662169  3.7698624\n#  [Resample] iter 2:    17.9389954  3.0343581   26.3888260  3.5992878\n#  [Resample] iter 3:    20.9092738  3.2640991   23.7739540  3.6788560\n#  \n#  Aggregated Result: mse.b632plus=23.9312510,mae.b632plus=3.4912886\n#  \n#  Resample Result\n#  Task: BostonHousing-example\n#  Learner: regr.lm\n#  Aggr perf: mse.b632plus=23.9312510,mae.b632plus=3.4912886\n#  Runtime: 0.0672176", 
            "title": "Convenience functions"
        }, 
        {
            "location": "/resample/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  # Not strictly necessary, but otherwise we might get NAs later on \n## if 'rpart' is not installed. \nlibrary( rpart ) \n## 3-fold cross-validation \nrdesc = makeResampleDesc( CV , iters = 3) \nrdesc \n## Holdout estimation \nrdesc = makeResampleDesc( Holdout ) \nrdesc \nhout \n\ncv3 \n## Specify the resampling strategy (3-fold cross-validation) \nrdesc = makeResampleDesc( CV , iters = 3) \n\n## Calculate the performance \nr = resample( regr.lm , bh.task, rdesc) \n\nr \n## Peak into r \nnames(r) \n\nr$aggr \n\nr$measures.test \n## Subsampling with 5 iterations and default split ratio 2/3 \nrdesc = makeResampleDesc( Subsample , iters = 5) \n\n## Subsampling with 5 iterations and 4/5 training data \nrdesc = makeResampleDesc( Subsample , iters = 5, split = 4/5) \n\n## Classification tree with information splitting criterion \nlrn = makeLearner( classif.rpart , parms = list(split =  information )) \n\n## Calculate the performance measures \nr = resample(lrn, sonar.task, rdesc, measures = list(mmce, fpr, fnr, timetrain)) \n\nr \n## Add balanced error rate (ber) and time used to predict \naddRRMeasure(r, list(ber, timepredict)) \nr = resample( classif.rpart , parms = list(split =  information ), sonar.task, rdesc, \n  measures = list(mmce, fpr, fnr, timetrain), show.info = FALSE) \n\nr \nr$pred \n\npred = getRRPredictions(r) \npred \nhead(as.data.frame(pred)) \n\nhead(getPredictionTruth(pred)) \n\nhead(getPredictionResponse(pred)) \n## Make predictions on both training and test sets \nrdesc = makeResampleDesc( Holdout , predict =  both ) \n\nr = resample( classif.lda , iris.task, rdesc, show.info = FALSE) \nr \n\nr$measures.train \npredList = getRRPredictionList(r) \npredList \n## 3-fold cross-validation \nrdesc = makeResampleDesc( CV , iters = 3) \n\nr = resample( surv.coxph , lung.task, rdesc, show.info = FALSE, models = TRUE) \nr$models \n## 3-fold cross-validation \nrdesc = makeResampleDesc( CV , iters = 3) \n\n## Extract the compute cluster centers \nr = resample( cluster.kmeans , mtcars.task, rdesc, show.info = FALSE, \n  centers = 3, extract = function(x) getLearnerModel(x)$centers) \nr$extract \n## Extract the variable importance in a regression tree \nr = resample( regr.rpart , bh.task, rdesc, show.info = FALSE, extract = getFeatureImportance) \nr$extract \n## 3-fold cross-validation \nrdesc = makeResampleDesc( CV , iters = 3, stratify = TRUE) \n\nr = resample( classif.lda , iris.task, rdesc, show.info = FALSE) \nr \nrdesc = makeResampleDesc( CV , iters = 3, stratify.cols =  chas ) \n\nr = resample( regr.rpart , bh.task, rdesc, show.info = FALSE) \nr \n## 5 blocks containing 30 observations each \ntask = makeClassifTask(data = iris, target =  Species , blocking = factor(rep(1:5, each = 30))) \ntask \nrdesc = makeResampleDesc( CV , iters = 3) \nrdesc \n\nstr(rdesc) \n\nstr(makeResampleDesc( Subsample , stratify.cols =  chas )) \n## Create a resample instance based an a task \nrin = makeResampleInstance(rdesc, iris.task) \nrin \n\nstr(rin) \n\n## Create a resample instance given the size of the data set \nrin = makeResampleInstance(rdesc, size = nrow(iris)) \nstr(rin) \n\n## Access the indices of the training observations in iteration 3 \nrin$train.inds[[3]] \nrdesc = makeResampleDesc( CV , iters = 3) \nrin = makeResampleInstance(rdesc, task = iris.task) \n\n## Calculate the performance of two learners based on the same resample instance \nr.lda = resample( classif.lda , iris.task, rin, show.info = FALSE) \nr.rpart = resample( classif.rpart , iris.task, rin, show.info = FALSE) \nr.lda$aggr \n\nr.rpart$aggr \nrin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150) \nrin \n## Mean misclassification error \nmmce$aggr \n\nmmce$aggr$fun \n\n## Root mean square error \nrmse$aggr \n\nrmse$aggr$fun \nmseTestMedian = setAggregation(mse, test.median) \nmseTestMin = setAggregation(mse, test.min) \nmseTestMax = setAggregation(mse, test.max) \n\nmseTestMedian \n\nrdesc = makeResampleDesc( CV , iters = 3) \nr = resample( regr.lm , bh.task, rdesc, measures = list(mse, mseTestMedian, mseTestMin, mseTestMax)) \n\nr \n\nr$aggr \nmmceTrainMean = setAggregation(mmce, train.mean) \nrdesc = makeResampleDesc( CV , iters = 3, predict =  both ) \nr = resample( classif.rpart , iris.task, rdesc, measures = list(mmce, mmceTrainMean)) \n\nr$measures.train \n\nr$aggr \n## Use bootstrap as resampling strategy and predict on both train and test sets \nrdesc = makeResampleDesc( Bootstrap , predict =  both , iters = 10) \n\n## Set aggregation schemes for b632 and b632+ bootstrap \nmmceB632 = setAggregation(mmce, b632) \nmmceB632plus = setAggregation(mmce, b632plus) \n\nmmceB632 \n\nr = resample( classif.rpart , iris.task, rdesc, measures = list(mmce, mmceB632, mmceB632plus), \n  show.info = FALSE) \nhead(r$measures.train) \n\n## Compare misclassification rates for out-of-bag, b632, and b632+ bootstrap \nr$aggr \ncrossval( classif.lda , iris.task, iters = 3, measures = list(mmce, ber)) \n\nbootstrapB632plus( regr.lm , bh.task, iters = 3, measures = list(mse, mae))", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/tune/index.html", 
            "text": "Tuning Hyperparameters\n\n\nMany machine learning algorithms have hyperparameters that need to be set.\nIf selected by the user they can be specified as explained on the tutorial page on\n\nLearners\n -- simply pass them to \nmakeLearner\n.\nOften suitable parameter values are not obvious and it is preferable to tune the hyperparameters,\nthat is automatically identify values that lead to the best performance.\n\n\nIn order to tune a machine learning algorithm, you have to specify:\n\n\n\n\nthe search space\n\n\nthe optimization algorithm (aka tuning method)\n\n\nan evaluation method, i.e., a resampling strategy and a performance measure\n\n\n\n\nAn example of the search space could be searching values of the \nC\n parameter for\n\nSVM\n:\n\n\n## ex: create a search space for the C hyperparameter from 0.01 to 0.1\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = 0.01, upper = 0.1)\n)\n\n\n\n\nAn example of the optimization algorithm could be performing random search on\nthe space:\n\n\n## ex: random search with 100 iterations\nctrl = makeTuneControlRandom(maxit = 100L)\n\n\n\n\nAn example of an evaluation method could be 3-fold CV using accuracy as the\nperformance measure:\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3L)\nmeasure = acc\n\n\n\n\nThe evaluation method is already covered in detail in\n\nevaluation of learning methods\n and \nresampling\n.\n\n\nIn this tutorial, we show how to specify the search space and optimization algorithm, how to do the\ntuning and how to access the tuning result, and how to visualize the hyperparameter tuning effects through\nseveral examples.\n\n\nThroughout this section we consider classification examples. For the other types of learning\nproblems, you can follow the same process analogously.\n\n\nWe use the \niris classification task\n for illustration and tune the\nhyperparameters of an SVM (function \nksvm\n from the \nkernlab\n package)\nwith a radial basis kernel. The following examples tune the cost parameter \nC\n and\nthe RBF kernel parameter \nsigma\n of the \nksvm\n function.\n\n\nSpecifying the search space\n\n\nWe first must define a space to search when tuning our learner. For example, maybe\nwe want to tune several specific values of a hyperparameter or perhaps we want to\ndefine a space from \n10^{-10}\n to \n10^{10}\n and let the optimization algorithm decide\nwhich points to choose.\n\n\nIn order to define a search space, we create a \nParamSet\n object, which describes the\nparameter space we wish to search. This is done via the function \nmakeParamSet\n.\n\n\nFor example, we could define a search space with just the values 0.5, 1.0, 1.5, 2.0\nfor both \nC\n and \ngamma\n. Notice how we name each parameter as it's defined in\nthe \nkernlab\n package:\n\n\ndiscrete_ps = makeParamSet(\n  makeDiscreteParam(\nC\n, values = c(0.5, 1.0, 1.5, 2.0)),\n  makeDiscreteParam(\nsigma\n, values = c(0.5, 1.0, 1.5, 2.0))\n)\nprint(discrete_ps)\n#\n           Type len Def      Constr Req Tunable Trafo\n#\n C     discrete   -   - 0.5,1,1.5,2   -    TRUE     -\n#\n sigma discrete   -   - 0.5,1,1.5,2   -    TRUE     -\n\n\n\n\nWe could also define a continuous search space (using \nmakeNumericParam\n\ninstead of \nmakeDiscreteParam\n) from \n10^{-10}\n to \n10^{10}\n for\nboth parameters through the use of the \ntrafo\n\nargument (trafo is short for transformation). Transformations work like this: All optimizers basically see the parameters on their\noriginal scale (from \n-10\n to \n10\n in this case) and produce values on this scale during the search.\nRight before they are passed to the learning algorithm, the transformation function is applied.\n\n\nNotice this time we use \nmakeNumericParam\n:\n\n\nnum_ps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -10, upper = 10, trafo = function(x) 2^x),\n  makeNumericParam(\nsigma\n, lower = -10, upper = 10, trafo = function(x) 2^x)\n)\n\n\n\n\nMany other parameters can be created, check out the examples in \nmakeParamSet\n.\n\n\nIn order to standardize your workflow across several packages, whenever parameters in the underlying \nR\n functions should be\npassed in a \nlist\n structure, \nmlr\n tries to give you direct access to\neach parameter and get rid of the list structure!\n\n\nThis is the case with the \nkpar\n argument of \nksvm\n which is a list of kernel parameters like \nsigma\n.\nThis allows us to interface with learners from different packages in the same way when defining parameters to tune!\n\n\nSpecifying the optimization algorithm\n\n\nNow that we have specified the search space, we need to choose an optimization algorithm for our\nparameters to pass to the \nksvm\n learner. Optimization algorithms are considered\n\nTuneControl\n objects in \nmlr\n.\n\n\nA grid search is one of the standard -- albeit slow -- ways to choose an\nappropriate set of parameters from a given search space.\n\n\nIn the case of \ndiscrete_ps\n above, since we have manually specified the values,\ngrid search will simply be the cross product. We create the grid search object using\nthe defaults, noting that we will have \n4 \\times 4 = 16\n combinations in the case of \ndiscrete_ps\n:\n\n\nctrl = makeTuneControlGrid()\n\n\n\n\nIn the case of \nnum_ps\n above, since we have only specified the upper and lower bounds\nfor the search space, grid search will create a grid using equally-sized steps. By default,\ngrid search will span the space in 10 equal-sized steps. The number of steps can be changed\nwith the \nresolution\n argument. Here we change to 15 equal-sized steps in the space defined within the\n\nParamSet\n object. For \nnum_ps\n, this means 15 steps in the form of\n\n10 ^ seq(-10, 10, length.out = 15)\n:\n\n\nctrl = makeTuneControlGrid(resolution = 15L)\n\n\n\n\nMany other types of optimization algorithms are available. Check out \nTuneControl\n\nfor some examples.\n\n\nSince grid search is normally too slow in practice, we'll also examine random search.\nIn the case of \ndiscrete_ps\n, random search will randomly choose from the specified values. The\n\nmaxit\n argument controls the amount of iterations.\n\n\nctrl = makeTuneControlRandom(maxit = 10L)\n\n\n\n\nIn the case of \nnum_ps\n, random search will randomly choose points within the space according to\nthe specified bounds. Perhaps in this case we would want to increase the amount of iterations to\nensure we adequately cover the space:\n\n\nctrl = makeTuneControlRandom(maxit = 200L)\n\n\n\n\nPerforming the tuning\n\n\nNow that we have specified a search space and the optimization algorithm, it's time to\nperform the tuning. We will need to define a resampling strategy and make note of our performance measure.\n\n\nWe will use 3-fold cross-validation to assess the quality of a specific parameter setting.\nFor this we need to create a resampling description just like in the \nresampling\n\npart of the tutorial.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3L)\n\n\n\n\nFinally, by combining all the previous pieces, we can tune the SVM parameters by calling\n\ntuneParams\n. We will use \ndiscrete_ps\n with grid search:\n\n\ndiscrete_ps = makeParamSet(\n  makeDiscreteParam(\nC\n, values = c(0.5, 1.0, 1.5, 2.0)),\n  makeDiscreteParam(\nsigma\n, values = c(0.5, 1.0, 1.5, 2.0))\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc(\nCV\n, iters = 3L)\nres = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc,\n  par.set = discrete_ps, control = ctrl)\n#\n [Tune] Started tuning learner classif.ksvm for parameter set:\n#\n           Type len Def      Constr Req Tunable Trafo\n#\n C     discrete   -   - 0.5,1,1.5,2   -    TRUE     -\n#\n sigma discrete   -   - 0.5,1,1.5,2   -    TRUE     -\n#\n With control class: TuneControlGrid\n#\n Imputation value: 1\n#\n [Tune-x] 1: C=0.5; sigma=0.5\n#\n [Tune-y] 1: mmce.test.mean=0.0400000; time: 0.0 min\n#\n [Tune-x] 2: C=1; sigma=0.5\n#\n [Tune-y] 2: mmce.test.mean=0.0400000; time: 0.0 min\n#\n [Tune-x] 3: C=1.5; sigma=0.5\n#\n [Tune-y] 3: mmce.test.mean=0.0466667; time: 0.0 min\n#\n [Tune-x] 4: C=2; sigma=0.5\n#\n [Tune-y] 4: mmce.test.mean=0.0466667; time: 0.0 min\n#\n [Tune-x] 5: C=0.5; sigma=1\n#\n [Tune-y] 5: mmce.test.mean=0.0400000; time: 0.0 min\n#\n [Tune-x] 6: C=1; sigma=1\n#\n [Tune-y] 6: mmce.test.mean=0.0466667; time: 0.0 min\n#\n [Tune-x] 7: C=1.5; sigma=1\n#\n [Tune-y] 7: mmce.test.mean=0.0466667; time: 0.0 min\n#\n [Tune-x] 8: C=2; sigma=1\n#\n [Tune-y] 8: mmce.test.mean=0.0466667; time: 0.0 min\n#\n [Tune-x] 9: C=0.5; sigma=1.5\n#\n [Tune-y] 9: mmce.test.mean=0.0333333; time: 0.0 min\n#\n [Tune-x] 10: C=1; sigma=1.5\n#\n [Tune-y] 10: mmce.test.mean=0.0400000; time: 0.0 min\n#\n [Tune-x] 11: C=1.5; sigma=1.5\n#\n [Tune-y] 11: mmce.test.mean=0.0400000; time: 0.0 min\n#\n [Tune-x] 12: C=2; sigma=1.5\n#\n [Tune-y] 12: mmce.test.mean=0.0466667; time: 0.0 min\n#\n [Tune-x] 13: C=0.5; sigma=2\n#\n [Tune-y] 13: mmce.test.mean=0.0400000; time: 0.0 min\n#\n [Tune-x] 14: C=1; sigma=2\n#\n [Tune-y] 14: mmce.test.mean=0.0333333; time: 0.0 min\n#\n [Tune-x] 15: C=1.5; sigma=2\n#\n [Tune-y] 15: mmce.test.mean=0.0400000; time: 0.0 min\n#\n [Tune-x] 16: C=2; sigma=2\n#\n [Tune-y] 16: mmce.test.mean=0.0400000; time: 0.0 min\n#\n [Tune] Result: C=0.5; sigma=1.5 : mmce.test.mean=0.0333333\n\nres\n#\n Tune result:\n#\n Op. pars: C=0.5; sigma=1.5\n#\n mmce.test.mean=0.0333333\n\n\n\n\ntuneParams\n simply performs the cross-validation for every element of the\ncross-product and selects the parameter setting with the best mean performance.\nAs no performance measure was specified, by default the error rate (\nmmce\n) is\nused.\n\n\nNote that each \nmeasure\n \"knows\" if it is minimized or maximized during tuning.\n\n\n## error rate\nmmce$minimize\n#\n [1] TRUE\n\n## accuracy\nacc$minimize\n#\n [1] FALSE\n\n\n\n\nOf course, you can pass other measures and also a \nlist\n of measures to \ntuneParams\n.\nIn the latter case the first measure is optimized during tuning, the others are simply evaluated.\nIf you are interested in optimizing several measures simultaneously have a look at\n\nAdvanced Tuning\n.\n\n\nIn the example below we calculate the accuracy (\nacc\n) instead of the error\nrate. We use function \nsetAggregation\n, as described on the \nresampling\n page,\nto additionally obtain the standard deviation of the accuracy. We also use random search with 100 iterations on\nthe \nnum_set\n we defined above and set \nshow.info\n to \nFALSE\n to hide the output for all 100 iterations:\n\n\nnum_ps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -10, upper = 10, trafo = function(x) 10^x),\n  makeNumericParam(\nsigma\n, lower = -10, upper = 10, trafo = function(x) 10^x)\n)\nctrl = makeTuneControlRandom(maxit = 100L)\nres = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc, par.set = num_ps,\n  control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE)\nres\n#\n Tune result:\n#\n Op. pars: C=95.2; sigma=0.0067\n#\n acc.test.mean=0.9866667,acc.test.sd=0.0230940\n\n\n\n\nAccessing the tuning result\n\n\nThe result object \nTuneResult\n allows you to access the best found settings \n$x\n and their\nestimated performance \n$y\n.\n\n\nres$x\n#\n $C\n#\n [1] 95.22422\n#\n \n#\n $sigma\n#\n [1] 0.006695534\n\nres$y\n#\n acc.test.mean   acc.test.sd \n#\n    0.98666667    0.02309401\n\n\n\n\nWe can generate a \nLearner\n with optimal hyperparameter settings\nas follows:\n\n\nlrn = setHyperPars(makeLearner(\nclassif.ksvm\n), par.vals = res$x)\nlrn\n#\n Learner classif.ksvm from package kernlab\n#\n Type: classif\n#\n Name: Support Vector Machines; Short name: ksvm\n#\n Class: classif.ksvm\n#\n Properties: twoclass,multiclass,numerics,factors,prob,class.weights\n#\n Predict-Type: response\n#\n Hyperparameters: fit=FALSE,C=95.2,sigma=0.0067\n\n\n\n\nThen you can proceed as usual. Here we refit and predict the learner on the complete \niris\n data\nset:\n\n\nm = train(lrn, iris.task)\npredict(m, task = iris.task)\n#\n Prediction: 150 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n   id  truth response\n#\n 1  1 setosa   setosa\n#\n 2  2 setosa   setosa\n#\n 3  3 setosa   setosa\n#\n 4  4 setosa   setosa\n#\n 5  5 setosa   setosa\n#\n 6  6 setosa   setosa\n#\n ... (#rows: 150, #cols: 3)\n\n\n\n\nBut what if you wanted to inspect the other points on the search path, not just the optimal?\n\n\nInvestigating hyperparameter tuning effects\n\n\nWe can inspect all points evaluated during the search by using\n\ngenerateHyperParsEffectData\n:\n\n\ngenerateHyperParsEffectData(res)\n#\n HyperParsEffectData:\n#\n Hyperparameters: C,sigma\n#\n Measures: acc.test.mean,acc.test.sd\n#\n Optimizer: TuneControlRandom\n#\n Nested CV Used: FALSE\n#\n Snapshot of data:\n#\n            C      sigma acc.test.mean acc.test.sd iteration exec.time\n#\n 1 -9.9783231  1.0531818     0.2733333  0.02309401         1     0.056\n#\n 2 -0.5292817  3.2214785     0.2733333  0.02309401         2     0.055\n#\n 3 -0.3544567  4.1644832     0.2733333  0.02309401         3     0.057\n#\n 4  0.6341910  7.8640461     0.2866667  0.03055050         4     0.058\n#\n 5  5.7640748 -3.3159251     0.9533333  0.03055050         5     0.057\n#\n 6 -6.5880397  0.4600323     0.2733333  0.02309401         6     0.057\n\n\n\n\nNote that the result of \ngenerateHyperParsEffectData\n\ncontains the parameter values \non the original scale\n.\nIn order to get the \ntransformed\n parameter values instead, use the \ntrafo\n argument:\n\n\ngenerateHyperParsEffectData(res, trafo = TRUE)\n#\n HyperParsEffectData:\n#\n Hyperparameters: C,sigma\n#\n Measures: acc.test.mean,acc.test.sd\n#\n Optimizer: TuneControlRandom\n#\n Nested CV Used: FALSE\n#\n Snapshot of data:\n#\n              C        sigma acc.test.mean acc.test.sd iteration exec.time\n#\n 1 1.051180e-10 1.130269e+01     0.2733333  0.02309401         1     0.056\n#\n 2 2.956095e-01 1.665246e+03     0.2733333  0.02309401         2     0.055\n#\n 3 4.421232e-01 1.460438e+04     0.2733333  0.02309401         3     0.057\n#\n 4 4.307159e+00 7.312168e+07     0.2866667  0.03055050         4     0.058\n#\n 5 5.808644e+05 4.831421e-04     0.9533333  0.03055050         5     0.057\n#\n 6 2.582024e-07 2.884246e+00     0.2733333  0.02309401         6     0.057\n\n\n\n\nNote that we can also generate performance on the train data along with the validation/test\ndata, as discussed on the \nresampling\n tutorial\npage:\n\n\nrdesc2 = makeResampleDesc(\nHoldout\n, predict = \nboth\n)\nres2 = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc2, par.set = num_ps,\n  control = ctrl, measures = list(acc, setAggregation(acc, train.mean)), show.info = FALSE)\ngenerateHyperParsEffectData(res2)\n#\n HyperParsEffectData:\n#\n Hyperparameters: C,sigma\n#\n Measures: acc.test.mean,acc.train.mean\n#\n Optimizer: TuneControlRandom\n#\n Nested CV Used: FALSE\n#\n Snapshot of data:\n#\n           C      sigma acc.test.mean acc.train.mean iteration exec.time\n#\n 1  9.457202 -4.0536025          0.98           0.97         1     0.039\n#\n 2  9.900523  1.8815923          0.40           1.00         2     0.029\n#\n 3  2.363975  5.3202458          0.26           1.00         3     0.030\n#\n 4 -1.530251  4.7579424          0.26           0.37         4     0.030\n#\n 5 -7.837476  2.4352698          0.26           0.37         5     0.029\n#\n 6  8.782931 -0.4143757          0.92           1.00         6     0.028\n\n\n\n\nWe can also easily visualize the points evaluated by using \nplotHyperParsEffect\n. In the\nexample below, we plot the performance over iterations, using the \nres\n from the previous section\nbut instead with 2 performance measures:\n\n\nres = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc, par.set = num_ps,\n  control = ctrl, measures = list(acc, mmce), show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplotHyperParsEffect(data, x = \niteration\n, y = \nacc.test.mean\n,\n  plot.type = \nline\n)\n\n\n\n\n\n\nNote that by default, we only plot the current global optima. This can be changed with\nthe \nglobal.only\n argument.\n\n\nFor an in-depth exploration of generating hyperparameter tuning effects and plotting the data,\ncheck out \nHyperparameter Tuning Effects\n.\n\n\nFurther comments\n\n\n\n\n\n\nTuning works for all other tasks like regression, survival analysis and so on in a completely\n  similar fashion.\n\n\n\n\n\n\nIn longer running tuning experiments it is very annoying if the computation stops due to\n  numerical or other errors. Have a look at \non.learner.error\n in \nconfigureMlr\n as well as\n  the examples given in section \nConfigure mlr\n of this tutorial.\n  You might also want to inform yourself about \nimpute.val\n in \nTuneControl\n.\n\n\n\n\n\n\nAs we continually optimize over the same data during tuning, the estimated\nperformance value might be optimistically biased.\nA clean approach to ensure unbiased performance estimation is \nnested resampling\n,\nwhere we embed the whole model selection process into an outer resampling loop.\n\n\n\n\n\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## ex: create a search space for the C hyperparameter from 0.01 to 0.1 \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = 0.01, upper = 0.1) \n) \n## ex: random search with 100 iterations \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc(\nCV\n, iters = 3L) \nmeasure = acc \ndiscrete_ps = makeParamSet( \n  makeDiscreteParam(\nC\n, values = c(0.5, 1.0, 1.5, 2.0)), \n  makeDiscreteParam(\nsigma\n, values = c(0.5, 1.0, 1.5, 2.0)) \n) \nprint(discrete_ps) \nnum_ps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -10, upper = 10, trafo = function(x) 2^x), \n  makeNumericParam(\nsigma\n, lower = -10, upper = 10, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlGrid() \nctrl = makeTuneControlGrid(resolution = 15L) \nctrl = makeTuneControlRandom(maxit = 10L) \nctrl = makeTuneControlRandom(maxit = 200L) \nrdesc = makeResampleDesc(\nCV\n, iters = 3L) \ndiscrete_ps = makeParamSet( \n  makeDiscreteParam(\nC\n, values = c(0.5, 1.0, 1.5, 2.0)), \n  makeDiscreteParam(\nsigma\n, values = c(0.5, 1.0, 1.5, 2.0)) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc(\nCV\n, iters = 3L) \nres = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc, \n  par.set = discrete_ps, control = ctrl) \n\nres \n## error rate \nmmce$minimize \n\n## accuracy \nacc$minimize \nnum_ps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -10, upper = 10, trafo = function(x) 10^x), \n  makeNumericParam(\nsigma\n, lower = -10, upper = 10, trafo = function(x) 10^x) \n) \nctrl = makeTuneControlRandom(maxit = 100L) \nres = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc, par.set = num_ps, \n  control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE) \nres \nres$x \n\nres$y \nlrn = setHyperPars(makeLearner(\nclassif.ksvm\n), par.vals = res$x) \nlrn \nm = train(lrn, iris.task) \npredict(m, task = iris.task) \ngenerateHyperParsEffectData(res) \ngenerateHyperParsEffectData(res, trafo = TRUE) \nrdesc2 = makeResampleDesc(\nHoldout\n, predict = \nboth\n) \nres2 = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc2, par.set = num_ps, \n  control = ctrl, measures = list(acc, setAggregation(acc, train.mean)), show.info = FALSE) \ngenerateHyperParsEffectData(res2) \nres = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc, par.set = num_ps, \n  control = ctrl, measures = list(acc, mmce), show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplotHyperParsEffect(data, x = \niteration\n, y = \nacc.test.mean\n, \n  plot.type = \nline\n)", 
            "title": "Tuning"
        }, 
        {
            "location": "/tune/index.html#tuning-hyperparameters", 
            "text": "Many machine learning algorithms have hyperparameters that need to be set.\nIf selected by the user they can be specified as explained on the tutorial page on Learners  -- simply pass them to  makeLearner .\nOften suitable parameter values are not obvious and it is preferable to tune the hyperparameters,\nthat is automatically identify values that lead to the best performance.  In order to tune a machine learning algorithm, you have to specify:   the search space  the optimization algorithm (aka tuning method)  an evaluation method, i.e., a resampling strategy and a performance measure   An example of the search space could be searching values of the  C  parameter for SVM :  ## ex: create a search space for the C hyperparameter from 0.01 to 0.1\nps = makeParamSet(\n  makeNumericParam( C , lower = 0.01, upper = 0.1)\n)  An example of the optimization algorithm could be performing random search on\nthe space:  ## ex: random search with 100 iterations\nctrl = makeTuneControlRandom(maxit = 100L)  An example of an evaluation method could be 3-fold CV using accuracy as the\nperformance measure:  rdesc = makeResampleDesc( CV , iters = 3L)\nmeasure = acc  The evaluation method is already covered in detail in evaluation of learning methods  and  resampling .  In this tutorial, we show how to specify the search space and optimization algorithm, how to do the\ntuning and how to access the tuning result, and how to visualize the hyperparameter tuning effects through\nseveral examples.  Throughout this section we consider classification examples. For the other types of learning\nproblems, you can follow the same process analogously.  We use the  iris classification task  for illustration and tune the\nhyperparameters of an SVM (function  ksvm  from the  kernlab  package)\nwith a radial basis kernel. The following examples tune the cost parameter  C  and\nthe RBF kernel parameter  sigma  of the  ksvm  function.", 
            "title": "Tuning Hyperparameters"
        }, 
        {
            "location": "/tune/index.html#specifying-the-search-space", 
            "text": "We first must define a space to search when tuning our learner. For example, maybe\nwe want to tune several specific values of a hyperparameter or perhaps we want to\ndefine a space from  10^{-10}  to  10^{10}  and let the optimization algorithm decide\nwhich points to choose.  In order to define a search space, we create a  ParamSet  object, which describes the\nparameter space we wish to search. This is done via the function  makeParamSet .  For example, we could define a search space with just the values 0.5, 1.0, 1.5, 2.0\nfor both  C  and  gamma . Notice how we name each parameter as it's defined in\nthe  kernlab  package:  discrete_ps = makeParamSet(\n  makeDiscreteParam( C , values = c(0.5, 1.0, 1.5, 2.0)),\n  makeDiscreteParam( sigma , values = c(0.5, 1.0, 1.5, 2.0))\n)\nprint(discrete_ps)\n#            Type len Def      Constr Req Tunable Trafo\n#  C     discrete   -   - 0.5,1,1.5,2   -    TRUE     -\n#  sigma discrete   -   - 0.5,1,1.5,2   -    TRUE     -  We could also define a continuous search space (using  makeNumericParam \ninstead of  makeDiscreteParam ) from  10^{-10}  to  10^{10}  for\nboth parameters through the use of the  trafo \nargument (trafo is short for transformation). Transformations work like this: All optimizers basically see the parameters on their\noriginal scale (from  -10  to  10  in this case) and produce values on this scale during the search.\nRight before they are passed to the learning algorithm, the transformation function is applied.  Notice this time we use  makeNumericParam :  num_ps = makeParamSet(\n  makeNumericParam( C , lower = -10, upper = 10, trafo = function(x) 2^x),\n  makeNumericParam( sigma , lower = -10, upper = 10, trafo = function(x) 2^x)\n)  Many other parameters can be created, check out the examples in  makeParamSet .  In order to standardize your workflow across several packages, whenever parameters in the underlying  R  functions should be\npassed in a  list  structure,  mlr  tries to give you direct access to\neach parameter and get rid of the list structure!  This is the case with the  kpar  argument of  ksvm  which is a list of kernel parameters like  sigma .\nThis allows us to interface with learners from different packages in the same way when defining parameters to tune!", 
            "title": "Specifying the search space"
        }, 
        {
            "location": "/tune/index.html#specifying-the-optimization-algorithm", 
            "text": "Now that we have specified the search space, we need to choose an optimization algorithm for our\nparameters to pass to the  ksvm  learner. Optimization algorithms are considered TuneControl  objects in  mlr .  A grid search is one of the standard -- albeit slow -- ways to choose an\nappropriate set of parameters from a given search space.  In the case of  discrete_ps  above, since we have manually specified the values,\ngrid search will simply be the cross product. We create the grid search object using\nthe defaults, noting that we will have  4 \\times 4 = 16  combinations in the case of  discrete_ps :  ctrl = makeTuneControlGrid()  In the case of  num_ps  above, since we have only specified the upper and lower bounds\nfor the search space, grid search will create a grid using equally-sized steps. By default,\ngrid search will span the space in 10 equal-sized steps. The number of steps can be changed\nwith the  resolution  argument. Here we change to 15 equal-sized steps in the space defined within the ParamSet  object. For  num_ps , this means 15 steps in the form of 10 ^ seq(-10, 10, length.out = 15) :  ctrl = makeTuneControlGrid(resolution = 15L)  Many other types of optimization algorithms are available. Check out  TuneControl \nfor some examples.  Since grid search is normally too slow in practice, we'll also examine random search.\nIn the case of  discrete_ps , random search will randomly choose from the specified values. The maxit  argument controls the amount of iterations.  ctrl = makeTuneControlRandom(maxit = 10L)  In the case of  num_ps , random search will randomly choose points within the space according to\nthe specified bounds. Perhaps in this case we would want to increase the amount of iterations to\nensure we adequately cover the space:  ctrl = makeTuneControlRandom(maxit = 200L)", 
            "title": "Specifying the optimization algorithm"
        }, 
        {
            "location": "/tune/index.html#performing-the-tuning", 
            "text": "Now that we have specified a search space and the optimization algorithm, it's time to\nperform the tuning. We will need to define a resampling strategy and make note of our performance measure.  We will use 3-fold cross-validation to assess the quality of a specific parameter setting.\nFor this we need to create a resampling description just like in the  resampling \npart of the tutorial.  rdesc = makeResampleDesc( CV , iters = 3L)  Finally, by combining all the previous pieces, we can tune the SVM parameters by calling tuneParams . We will use  discrete_ps  with grid search:  discrete_ps = makeParamSet(\n  makeDiscreteParam( C , values = c(0.5, 1.0, 1.5, 2.0)),\n  makeDiscreteParam( sigma , values = c(0.5, 1.0, 1.5, 2.0))\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc( CV , iters = 3L)\nres = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc,\n  par.set = discrete_ps, control = ctrl)\n#  [Tune] Started tuning learner classif.ksvm for parameter set:\n#            Type len Def      Constr Req Tunable Trafo\n#  C     discrete   -   - 0.5,1,1.5,2   -    TRUE     -\n#  sigma discrete   -   - 0.5,1,1.5,2   -    TRUE     -\n#  With control class: TuneControlGrid\n#  Imputation value: 1\n#  [Tune-x] 1: C=0.5; sigma=0.5\n#  [Tune-y] 1: mmce.test.mean=0.0400000; time: 0.0 min\n#  [Tune-x] 2: C=1; sigma=0.5\n#  [Tune-y] 2: mmce.test.mean=0.0400000; time: 0.0 min\n#  [Tune-x] 3: C=1.5; sigma=0.5\n#  [Tune-y] 3: mmce.test.mean=0.0466667; time: 0.0 min\n#  [Tune-x] 4: C=2; sigma=0.5\n#  [Tune-y] 4: mmce.test.mean=0.0466667; time: 0.0 min\n#  [Tune-x] 5: C=0.5; sigma=1\n#  [Tune-y] 5: mmce.test.mean=0.0400000; time: 0.0 min\n#  [Tune-x] 6: C=1; sigma=1\n#  [Tune-y] 6: mmce.test.mean=0.0466667; time: 0.0 min\n#  [Tune-x] 7: C=1.5; sigma=1\n#  [Tune-y] 7: mmce.test.mean=0.0466667; time: 0.0 min\n#  [Tune-x] 8: C=2; sigma=1\n#  [Tune-y] 8: mmce.test.mean=0.0466667; time: 0.0 min\n#  [Tune-x] 9: C=0.5; sigma=1.5\n#  [Tune-y] 9: mmce.test.mean=0.0333333; time: 0.0 min\n#  [Tune-x] 10: C=1; sigma=1.5\n#  [Tune-y] 10: mmce.test.mean=0.0400000; time: 0.0 min\n#  [Tune-x] 11: C=1.5; sigma=1.5\n#  [Tune-y] 11: mmce.test.mean=0.0400000; time: 0.0 min\n#  [Tune-x] 12: C=2; sigma=1.5\n#  [Tune-y] 12: mmce.test.mean=0.0466667; time: 0.0 min\n#  [Tune-x] 13: C=0.5; sigma=2\n#  [Tune-y] 13: mmce.test.mean=0.0400000; time: 0.0 min\n#  [Tune-x] 14: C=1; sigma=2\n#  [Tune-y] 14: mmce.test.mean=0.0333333; time: 0.0 min\n#  [Tune-x] 15: C=1.5; sigma=2\n#  [Tune-y] 15: mmce.test.mean=0.0400000; time: 0.0 min\n#  [Tune-x] 16: C=2; sigma=2\n#  [Tune-y] 16: mmce.test.mean=0.0400000; time: 0.0 min\n#  [Tune] Result: C=0.5; sigma=1.5 : mmce.test.mean=0.0333333\n\nres\n#  Tune result:\n#  Op. pars: C=0.5; sigma=1.5\n#  mmce.test.mean=0.0333333  tuneParams  simply performs the cross-validation for every element of the\ncross-product and selects the parameter setting with the best mean performance.\nAs no performance measure was specified, by default the error rate ( mmce ) is\nused.  Note that each  measure  \"knows\" if it is minimized or maximized during tuning.  ## error rate\nmmce$minimize\n#  [1] TRUE\n\n## accuracy\nacc$minimize\n#  [1] FALSE  Of course, you can pass other measures and also a  list  of measures to  tuneParams .\nIn the latter case the first measure is optimized during tuning, the others are simply evaluated.\nIf you are interested in optimizing several measures simultaneously have a look at Advanced Tuning .  In the example below we calculate the accuracy ( acc ) instead of the error\nrate. We use function  setAggregation , as described on the  resampling  page,\nto additionally obtain the standard deviation of the accuracy. We also use random search with 100 iterations on\nthe  num_set  we defined above and set  show.info  to  FALSE  to hide the output for all 100 iterations:  num_ps = makeParamSet(\n  makeNumericParam( C , lower = -10, upper = 10, trafo = function(x) 10^x),\n  makeNumericParam( sigma , lower = -10, upper = 10, trafo = function(x) 10^x)\n)\nctrl = makeTuneControlRandom(maxit = 100L)\nres = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc, par.set = num_ps,\n  control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE)\nres\n#  Tune result:\n#  Op. pars: C=95.2; sigma=0.0067\n#  acc.test.mean=0.9866667,acc.test.sd=0.0230940", 
            "title": "Performing the tuning"
        }, 
        {
            "location": "/tune/index.html#accessing-the-tuning-result", 
            "text": "The result object  TuneResult  allows you to access the best found settings  $x  and their\nestimated performance  $y .  res$x\n#  $C\n#  [1] 95.22422\n#  \n#  $sigma\n#  [1] 0.006695534\n\nres$y\n#  acc.test.mean   acc.test.sd \n#     0.98666667    0.02309401  We can generate a  Learner  with optimal hyperparameter settings\nas follows:  lrn = setHyperPars(makeLearner( classif.ksvm ), par.vals = res$x)\nlrn\n#  Learner classif.ksvm from package kernlab\n#  Type: classif\n#  Name: Support Vector Machines; Short name: ksvm\n#  Class: classif.ksvm\n#  Properties: twoclass,multiclass,numerics,factors,prob,class.weights\n#  Predict-Type: response\n#  Hyperparameters: fit=FALSE,C=95.2,sigma=0.0067  Then you can proceed as usual. Here we refit and predict the learner on the complete  iris  data\nset:  m = train(lrn, iris.task)\npredict(m, task = iris.task)\n#  Prediction: 150 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#    id  truth response\n#  1  1 setosa   setosa\n#  2  2 setosa   setosa\n#  3  3 setosa   setosa\n#  4  4 setosa   setosa\n#  5  5 setosa   setosa\n#  6  6 setosa   setosa\n#  ... (#rows: 150, #cols: 3)  But what if you wanted to inspect the other points on the search path, not just the optimal?", 
            "title": "Accessing the tuning result"
        }, 
        {
            "location": "/tune/index.html#investigating-hyperparameter-tuning-effects", 
            "text": "We can inspect all points evaluated during the search by using generateHyperParsEffectData :  generateHyperParsEffectData(res)\n#  HyperParsEffectData:\n#  Hyperparameters: C,sigma\n#  Measures: acc.test.mean,acc.test.sd\n#  Optimizer: TuneControlRandom\n#  Nested CV Used: FALSE\n#  Snapshot of data:\n#             C      sigma acc.test.mean acc.test.sd iteration exec.time\n#  1 -9.9783231  1.0531818     0.2733333  0.02309401         1     0.056\n#  2 -0.5292817  3.2214785     0.2733333  0.02309401         2     0.055\n#  3 -0.3544567  4.1644832     0.2733333  0.02309401         3     0.057\n#  4  0.6341910  7.8640461     0.2866667  0.03055050         4     0.058\n#  5  5.7640748 -3.3159251     0.9533333  0.03055050         5     0.057\n#  6 -6.5880397  0.4600323     0.2733333  0.02309401         6     0.057  Note that the result of  generateHyperParsEffectData \ncontains the parameter values  on the original scale .\nIn order to get the  transformed  parameter values instead, use the  trafo  argument:  generateHyperParsEffectData(res, trafo = TRUE)\n#  HyperParsEffectData:\n#  Hyperparameters: C,sigma\n#  Measures: acc.test.mean,acc.test.sd\n#  Optimizer: TuneControlRandom\n#  Nested CV Used: FALSE\n#  Snapshot of data:\n#               C        sigma acc.test.mean acc.test.sd iteration exec.time\n#  1 1.051180e-10 1.130269e+01     0.2733333  0.02309401         1     0.056\n#  2 2.956095e-01 1.665246e+03     0.2733333  0.02309401         2     0.055\n#  3 4.421232e-01 1.460438e+04     0.2733333  0.02309401         3     0.057\n#  4 4.307159e+00 7.312168e+07     0.2866667  0.03055050         4     0.058\n#  5 5.808644e+05 4.831421e-04     0.9533333  0.03055050         5     0.057\n#  6 2.582024e-07 2.884246e+00     0.2733333  0.02309401         6     0.057  Note that we can also generate performance on the train data along with the validation/test\ndata, as discussed on the  resampling  tutorial\npage:  rdesc2 = makeResampleDesc( Holdout , predict =  both )\nres2 = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc2, par.set = num_ps,\n  control = ctrl, measures = list(acc, setAggregation(acc, train.mean)), show.info = FALSE)\ngenerateHyperParsEffectData(res2)\n#  HyperParsEffectData:\n#  Hyperparameters: C,sigma\n#  Measures: acc.test.mean,acc.train.mean\n#  Optimizer: TuneControlRandom\n#  Nested CV Used: FALSE\n#  Snapshot of data:\n#            C      sigma acc.test.mean acc.train.mean iteration exec.time\n#  1  9.457202 -4.0536025          0.98           0.97         1     0.039\n#  2  9.900523  1.8815923          0.40           1.00         2     0.029\n#  3  2.363975  5.3202458          0.26           1.00         3     0.030\n#  4 -1.530251  4.7579424          0.26           0.37         4     0.030\n#  5 -7.837476  2.4352698          0.26           0.37         5     0.029\n#  6  8.782931 -0.4143757          0.92           1.00         6     0.028  We can also easily visualize the points evaluated by using  plotHyperParsEffect . In the\nexample below, we plot the performance over iterations, using the  res  from the previous section\nbut instead with 2 performance measures:  res = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc, par.set = num_ps,\n  control = ctrl, measures = list(acc, mmce), show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplotHyperParsEffect(data, x =  iteration , y =  acc.test.mean ,\n  plot.type =  line )   Note that by default, we only plot the current global optima. This can be changed with\nthe  global.only  argument.  For an in-depth exploration of generating hyperparameter tuning effects and plotting the data,\ncheck out  Hyperparameter Tuning Effects .", 
            "title": "Investigating hyperparameter tuning effects"
        }, 
        {
            "location": "/tune/index.html#further-comments", 
            "text": "Tuning works for all other tasks like regression, survival analysis and so on in a completely\n  similar fashion.    In longer running tuning experiments it is very annoying if the computation stops due to\n  numerical or other errors. Have a look at  on.learner.error  in  configureMlr  as well as\n  the examples given in section  Configure mlr  of this tutorial.\n  You might also want to inform yourself about  impute.val  in  TuneControl .    As we continually optimize over the same data during tuning, the estimated\nperformance value might be optimistically biased.\nA clean approach to ensure unbiased performance estimation is  nested resampling ,\nwhere we embed the whole model selection process into an outer resampling loop.", 
            "title": "Further comments"
        }, 
        {
            "location": "/tune/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## ex: create a search space for the C hyperparameter from 0.01 to 0.1 \nps = makeParamSet( \n  makeNumericParam( C , lower = 0.01, upper = 0.1) \n) \n## ex: random search with 100 iterations \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc( CV , iters = 3L) \nmeasure = acc \ndiscrete_ps = makeParamSet( \n  makeDiscreteParam( C , values = c(0.5, 1.0, 1.5, 2.0)), \n  makeDiscreteParam( sigma , values = c(0.5, 1.0, 1.5, 2.0)) \n) \nprint(discrete_ps) \nnum_ps = makeParamSet( \n  makeNumericParam( C , lower = -10, upper = 10, trafo = function(x) 2^x), \n  makeNumericParam( sigma , lower = -10, upper = 10, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlGrid() \nctrl = makeTuneControlGrid(resolution = 15L) \nctrl = makeTuneControlRandom(maxit = 10L) \nctrl = makeTuneControlRandom(maxit = 200L) \nrdesc = makeResampleDesc( CV , iters = 3L) \ndiscrete_ps = makeParamSet( \n  makeDiscreteParam( C , values = c(0.5, 1.0, 1.5, 2.0)), \n  makeDiscreteParam( sigma , values = c(0.5, 1.0, 1.5, 2.0)) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc( CV , iters = 3L) \nres = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc, \n  par.set = discrete_ps, control = ctrl) \n\nres \n## error rate \nmmce$minimize \n\n## accuracy \nacc$minimize \nnum_ps = makeParamSet( \n  makeNumericParam( C , lower = -10, upper = 10, trafo = function(x) 10^x), \n  makeNumericParam( sigma , lower = -10, upper = 10, trafo = function(x) 10^x) \n) \nctrl = makeTuneControlRandom(maxit = 100L) \nres = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc, par.set = num_ps, \n  control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE) \nres \nres$x \n\nres$y \nlrn = setHyperPars(makeLearner( classif.ksvm ), par.vals = res$x) \nlrn \nm = train(lrn, iris.task) \npredict(m, task = iris.task) \ngenerateHyperParsEffectData(res) \ngenerateHyperParsEffectData(res, trafo = TRUE) \nrdesc2 = makeResampleDesc( Holdout , predict =  both ) \nres2 = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc2, par.set = num_ps, \n  control = ctrl, measures = list(acc, setAggregation(acc, train.mean)), show.info = FALSE) \ngenerateHyperParsEffectData(res2) \nres = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc, par.set = num_ps, \n  control = ctrl, measures = list(acc, mmce), show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplotHyperParsEffect(data, x =  iteration , y =  acc.test.mean , \n  plot.type =  line )", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/benchmark_experiments/index.html", 
            "text": "Benchmark Experiments\n\n\nIn a benchmark experiment different learning methods are applied to one or several data sets\nwith the aim to compare and rank the algorithms with respect to one or more\nperformance measures.\n\n\nIn \nmlr\n a benchmark experiment can be conducted by calling function \nbenchmark\n on\na \nlist\n of \nLearner\ns and a \nlist\n of \nTask\ns.\n\nbenchmark\n basically executes \nresample\n for each combination of \nLearner\n\nand \nTask\n.\nYou can specify an individual resampling strategy for each \nTask\n and select one or\nmultiple performance measures to be calculated.\n\n\nConducting benchmark experiments\n\n\nWe start with a small example. Two learners, \nlinear discriminant analysis (lda)\n and a\n\nclassification tree (rpart)\n, are applied to one classification problem (\nsonar.task\n).\nAs resampling strategy we choose \n\"Holdout\"\n.\nThe performance is thus calculated on a single randomly sampled test data set.\n\n\nIn the example below we create a resample description (\nResampleDesc\n),\nwhich is automatically instantiated by \nbenchmark\n.\nThe instantiation is done only once per \nTask\n, i.e., the same training and test sets\nare used for all learners.\nIt is also possible to directly pass a \nResampleInstance\n.\n\n\nIf you would like to use a \nfixed test data set\n instead of a randomly selected one, you can\ncreate a suitable \nResampleInstance\n through function\n\nmakeFixedHoldoutInstance\n.\n\n\n## Two learners to be compared\nlrns = list(makeLearner(\nclassif.lda\n), makeLearner(\nclassif.rpart\n))\n\n## Choose the resampling strategy\nrdesc = makeResampleDesc(\nHoldout\n)\n\n## Conduct the benchmark experiment\nbmr = benchmark(lrns, sonar.task, rdesc)\n#\n Task: Sonar_example, Learner: classif.lda\n#\n Resampling: holdout\n#\n Measures:             mmce\n#\n [Resample] iter 1:    0.3000000\n#\n \n#\n Aggregated Result: mmce.test.mean=0.3000000\n#\n \n#\n Task: Sonar_example, Learner: classif.rpart\n#\n Resampling: holdout\n#\n Measures:             mmce\n#\n [Resample] iter 1:    0.2857143\n#\n \n#\n Aggregated Result: mmce.test.mean=0.2857143\n#\n \n\nbmr\n#\n         task.id    learner.id mmce.test.mean\n#\n 1 Sonar_example   classif.lda      0.3000000\n#\n 2 Sonar_example classif.rpart      0.2857143\n\n\n\n\nFor convenience, if you don't want to pass any additional arguments to \nmakeLearner\n, you\ndon't need to generate the \nLearner\ns explicitly, but it's sufficient to\nprovide the learner name.\nIn the above example we could also have written:\n\n\n## Vector of strings\nlrns = c(\nclassif.lda\n, \nclassif.rpart\n)\n\n## A mixed list of Learner objects and strings works, too\nlrns = list(makeLearner(\nclassif.lda\n, predict.type = \nprob\n), \nclassif.rpart\n)\n\nbmr = benchmark(lrns, sonar.task, rdesc)\n#\n Task: Sonar_example, Learner: classif.lda\n#\n Resampling: holdout\n#\n Measures:             mmce\n#\n [Resample] iter 1:    0.2571429\n#\n \n#\n Aggregated Result: mmce.test.mean=0.2571429\n#\n \n#\n Task: Sonar_example, Learner: classif.rpart\n#\n Resampling: holdout\n#\n Measures:             mmce\n#\n [Resample] iter 1:    0.2714286\n#\n \n#\n Aggregated Result: mmce.test.mean=0.2714286\n#\n \n\nbmr\n#\n         task.id    learner.id mmce.test.mean\n#\n 1 Sonar_example   classif.lda      0.2571429\n#\n 2 Sonar_example classif.rpart      0.2714286\n\n\n\n\nIn the printed summary table every row corresponds to one pair of \nTask\n and \nLearner\n.\nThe entries show the mean misclassification error (\nmmce\n), the default performance\nmeasure for classification, on the test data set.\n\n\nThe result \nbmr\n is an object of class \nBenchmarkResult\n. Basically, it contains a \nlist\n\nof lists of \nResampleResult\n objects, first ordered by \nTask\n and then by \nLearner\n.\n\n\nMaking experiments reproducible\n\n\nTypically, we would want our experiment results to be reproducible. \nmlr\n obeys\nthe \nset.seed\n function, so make sure to use \nset.seed\n at the beginning of your\nscript if you would like your results to be reproducible.\n\n\nNote that if you are using parallel computing, you may need to adjust how you\ncall \nset.seed\n depending on your usecase. One possibility is to use\n\nset.seed(123, \"L'Ecuyer\")\n in order to ensure the results are reproducible for\neach child process. See the examples in \nmclapply\n for more\ninformation on reproducibility and parallel computing.\n\n\nAccessing benchmark results\n\n\nmlr\n provides several accessor functions, named \ngetBMR\nWhatToExtract\n, that permit\nto retrieve information for further analyses. This includes for example the performances\nor predictions of the learning algorithms under consideration.\n\n\nLearner performances\n\n\nLet's have a look at the benchmark result above.\n\ngetBMRPerformances\n returns individual performances in resampling runs, while\n\ngetBMRAggrPerformances\n gives the aggregated values.\n\n\ngetBMRPerformances(bmr)\n#\n $Sonar_example\n#\n $Sonar_example$classif.lda\n#\n   iter      mmce\n#\n 1    1 0.2571429\n#\n \n#\n $Sonar_example$classif.rpart\n#\n   iter      mmce\n#\n 1    1 0.2714286\n\ngetBMRAggrPerformances(bmr)\n#\n $Sonar_example\n#\n $Sonar_example$classif.lda\n#\n mmce.test.mean \n#\n      0.2571429 \n#\n \n#\n $Sonar_example$classif.rpart\n#\n mmce.test.mean \n#\n      0.2714286\n\n\n\n\nSince we used holdout as resampling strategy, individual and aggregated performance values\ncoincide.\n\n\nBy default, nearly all \"getter\" functions return a nested \nlist\n, with the first\nlevel indicating the task and the second level indicating the learner.\nIf only a single learner or, as in our case a single task is considered, setting\n\ndrop = TRUE\n simplifies the result to a flat \nlist\n.\n\n\ngetBMRPerformances(bmr, drop = TRUE)\n#\n $classif.lda\n#\n   iter      mmce\n#\n 1    1 0.2571429\n#\n \n#\n $classif.rpart\n#\n   iter      mmce\n#\n 1    1 0.2714286\n\n\n\n\nOften it is more convenient to work with \ndata.frame\ns. You can easily\nconvert the result structure by setting \nas.df = TRUE\n.\n\n\ngetBMRPerformances(bmr, as.df = TRUE)\n#\n         task.id    learner.id iter      mmce\n#\n 1 Sonar_example   classif.lda    1 0.2571429\n#\n 2 Sonar_example classif.rpart    1 0.2714286\n\ngetBMRAggrPerformances(bmr, as.df = TRUE)\n#\n         task.id    learner.id mmce.test.mean\n#\n 1 Sonar_example   classif.lda      0.2571429\n#\n 2 Sonar_example classif.rpart      0.2714286\n\n\n\n\nPredictions\n\n\nPer default, the \nBenchmarkResult\n contains the learner predictions.\nIf you do not want to keep them, e.g., to conserve memory, set \nkeep.pred = FALSE\n when\ncalling \nbenchmark\n.\n\n\nYou can access the predictions using function \ngetBMRPredictions\n.\nPer default, you get a nested \nlist\n of \nResamplePrediction\n objects.\nAs above, you can use the \ndrop\n or \nas.df\n options to simplify the result.\n\n\ngetBMRPredictions(bmr)\n#\n $Sonar_example\n#\n $Sonar_example$classif.lda\n#\n Resampled Prediction for:\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n#\n predict.type: prob\n#\n threshold: M=0.50,R=0.50\n#\n time (mean): 0.01\n#\n    id truth     prob.M      prob.R response iter  set\n#\n 1 127     M 0.98673000 0.013270001        M    1 test\n#\n 2 159     M 0.99659179 0.003408211        M    1 test\n#\n 3  81     R 0.55436799 0.445632009        M    1 test\n#\n 4 207     M 0.98660766 0.013392337        M    1 test\n#\n 5  74     R 0.94120073 0.058799272        M    1 test\n#\n 6 154     M 0.03862365 0.961376347        R    1 test\n#\n ... (#rows: 70, #cols: 7)\n#\n \n#\n $Sonar_example$classif.rpart\n#\n Resampled Prediction for:\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n#\n predict.type: response\n#\n threshold: \n#\n time (mean): 0.01\n#\n    id truth response iter  set\n#\n 1 127     M        R    1 test\n#\n 2 159     M        R    1 test\n#\n 3  81     R        R    1 test\n#\n 4 207     M        M    1 test\n#\n 5  74     R        R    1 test\n#\n 6 154     M        M    1 test\n#\n ... (#rows: 70, #cols: 5)\n\nhead(getBMRPredictions(bmr, as.df = TRUE))\n#\n         task.id  learner.id  id truth     prob.M      prob.R response iter\n#\n 1 Sonar_example classif.lda 127     M 0.98673000 0.013270001        M    1\n#\n 2 Sonar_example classif.lda 159     M 0.99659179 0.003408211        M    1\n#\n 3 Sonar_example classif.lda  81     R 0.55436799 0.445632009        M    1\n#\n 4 Sonar_example classif.lda 207     M 0.98660766 0.013392337        M    1\n#\n 5 Sonar_example classif.lda  74     R 0.94120073 0.058799272        M    1\n#\n 6 Sonar_example classif.lda 154     M 0.03862365 0.961376347        R    1\n#\n    set\n#\n 1 test\n#\n 2 test\n#\n 3 test\n#\n 4 test\n#\n 5 test\n#\n 6 test\n\n\n\n\nIt is also easily possible to access results for certain learners or tasks via their\nIDs. For this purpose many \"getter\" functions have a \nlearner.ids\n and a \ntask.ids\n argument.\n\n\nhead(getBMRPredictions(bmr, learner.ids = \nclassif.rpart\n, as.df = TRUE))\n#\n         task.id    learner.id  id truth response iter  set\n#\n 1 Sonar_example classif.rpart 127     M        R    1 test\n#\n 2 Sonar_example classif.rpart 159     M        R    1 test\n#\n 3 Sonar_example classif.rpart  81     R        R    1 test\n#\n 4 Sonar_example classif.rpart 207     M        M    1 test\n#\n 5 Sonar_example classif.rpart  74     R        R    1 test\n#\n 6 Sonar_example classif.rpart 154     M        M    1 test\n\n\n\n\nIf you don't like the default IDs, you can set the IDs of learners and tasks via the \nid\n option of\n\nmakeLearner\n and \nmake*Task\n.\nMoreover, you can conveniently change the ID of a \nLearner\n via function \nsetLearnerId\n.\n\n\nIDs\n\n\nThe IDs of all \nLearner\ns, \nTask\ns and \nMeasure\ns in a benchmark\nexperiment can be retrieved as follows:\n\n\ngetBMRTaskIds(bmr)\n#\n [1] \nSonar_example\n\n\ngetBMRLearnerIds(bmr)\n#\n [1] \nclassif.lda\n   \nclassif.rpart\n\n\ngetBMRMeasureIds(bmr)\n#\n [1] \nmmce\n\n\n\n\n\nFitted models\n\n\nPer default the \nBenchmarkResult\n also contains the fitted models for all learners on all tasks.\nIf you do not want to keep them set \nmodels = FALSE\n when calling \nbenchmark\n.\nThe fitted models can be retrieved by function \ngetBMRModels\n.\nIt returns a (possibly nested) \nlist\n of \nWrappedModel\n objects.\n\n\ngetBMRModels(bmr)\n#\n $Sonar_example\n#\n $Sonar_example$classif.lda\n#\n $Sonar_example$classif.lda[[1]]\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = Sonar_example; obs = 138; features = 60\n#\n Hyperparameters: \n#\n \n#\n \n#\n $Sonar_example$classif.rpart\n#\n $Sonar_example$classif.rpart[[1]]\n#\n Model for learner.id=classif.rpart; learner.class=classif.rpart\n#\n Trained on: task.id = Sonar_example; obs = 138; features = 60\n#\n Hyperparameters: xval=0\n\ngetBMRModels(bmr, drop = TRUE)\n#\n $classif.lda\n#\n $classif.lda[[1]]\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = Sonar_example; obs = 138; features = 60\n#\n Hyperparameters: \n#\n \n#\n \n#\n $classif.rpart\n#\n $classif.rpart[[1]]\n#\n Model for learner.id=classif.rpart; learner.class=classif.rpart\n#\n Trained on: task.id = Sonar_example; obs = 138; features = 60\n#\n Hyperparameters: xval=0\n\ngetBMRModels(bmr, learner.ids = \nclassif.lda\n)\n#\n $Sonar_example\n#\n $Sonar_example$classif.lda\n#\n $Sonar_example$classif.lda[[1]]\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = Sonar_example; obs = 138; features = 60\n#\n Hyperparameters:\n\n\n\n\nLearners and measures\n\n\nMoreover, you can extract the employed \nLearner\ns and \nMeasure\ns.\n\n\ngetBMRLearners(bmr)\n#\n $classif.lda\n#\n Learner classif.lda from package MASS\n#\n Type: classif\n#\n Name: Linear Discriminant Analysis; Short name: lda\n#\n Class: classif.lda\n#\n Properties: twoclass,multiclass,numerics,factors,prob\n#\n Predict-Type: prob\n#\n Hyperparameters: \n#\n \n#\n \n#\n $classif.rpart\n#\n Learner classif.rpart from package rpart\n#\n Type: classif\n#\n Name: Decision Tree; Short name: rpart\n#\n Class: classif.rpart\n#\n Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp\n#\n Predict-Type: response\n#\n Hyperparameters: xval=0\n\ngetBMRMeasures(bmr)\n#\n [[1]]\n#\n Name: Mean misclassification error\n#\n Performance measure: mmce\n#\n Properties: classif,classif.multi,req.pred,req.truth\n#\n Minimize: TRUE\n#\n Best: 0; Worst: 1\n#\n Aggregated by: test.mean\n#\n Arguments: \n#\n Note: Defined as: mean(response != truth)\n\n\n\n\nMerging benchmark results\n\n\nSometimes after completing a benchmark experiment it turns out that you want to extend it\nby another \nLearner\n or another \nTask\n.\nIn this case you can perform an additional benchmark experiment and then use function\n\nmergeBenchmarkResults\n to combine the results to a single \nBenchmarkResult\n object that\ncan be accessed and analyzed as usual.\n\n\nFor example in the benchmark experiment above we applied \nlda\n and \nrpart\n\nto the \nsonar.task\n. We now perform a second experiment using a \nrandom forest\n\nand \nquadratic discriminant analysis (qda)\n and merge the results.\n\n\n## First benchmark result\nbmr\n#\n         task.id    learner.id mmce.test.mean\n#\n 1 Sonar_example   classif.lda      0.2571429\n#\n 2 Sonar_example classif.rpart      0.2714286\n\n## Benchmark experiment for the additional learners\nlrns2 = list(makeLearner(\nclassif.randomForest\n), makeLearner(\nclassif.qda\n))\nbmr2 = benchmark(lrns2, sonar.task, rdesc, show.info = FALSE)\nbmr2\n#\n         task.id           learner.id mmce.test.mean\n#\n 1 Sonar_example classif.randomForest      0.1428571\n#\n 2 Sonar_example          classif.qda      0.2714286\n\n## Merge the results\nmergeBenchmarkResults(list(bmr, bmr2))\n#\n         task.id           learner.id mmce.test.mean\n#\n 1 Sonar_example          classif.lda      0.2571429\n#\n 2 Sonar_example        classif.rpart      0.2714286\n#\n 3 Sonar_example classif.randomForest      0.1428571\n#\n 4 Sonar_example          classif.qda      0.2714286\n\n\n\n\nNote that in the above examples in each case a \nresample description\n\nwas passed to the \nbenchmark\n function.\nFor this reason \nlda\n and \nrpart\n were most likely evaluated on\na different training/test set pair than \nrandom forest\n and\n\nqda\n.\n\n\nDiffering training/test set pairs across learners pose an additional source of variation in\nthe results, which can make it harder to detect actual performance differences between learners.\nTherefore, if you suspect that you will have to extend your benchmark experiment by another\n\nLearner\n later on it's probably easiest to work with\n\nResampleInstance\ns from the start. These can be stored and used for\nany additional experiments.\n\n\nAlternatively, if you used a resample description in the first benchmark experiment you could\nalso extract the \nResampleInstance\ns from the \nBenchmarkResult\n \nbmr\n\nand pass these to all further \nbenchmark\n calls.\n\n\nrin = getBMRPredictions(bmr)[[1]][[1]]$instance\nrin\n#\n Resample instance for 208 cases.\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n\n## Benchmark experiment for the additional random forest\nbmr3 = benchmark(lrns2, sonar.task, rin, show.info = FALSE)\nbmr3\n#\n         task.id           learner.id mmce.test.mean\n#\n 1 Sonar_example classif.randomForest      0.2000000\n#\n 2 Sonar_example          classif.qda      0.5142857\n\n## Merge the results\nmergeBenchmarkResults(list(bmr, bmr3))\n#\n         task.id           learner.id mmce.test.mean\n#\n 1 Sonar_example          classif.lda      0.2571429\n#\n 2 Sonar_example        classif.rpart      0.2714286\n#\n 3 Sonar_example classif.randomForest      0.2000000\n#\n 4 Sonar_example          classif.qda      0.5142857\n\n\n\n\nBenchmark analysis and visualization\n\n\nmlr\n offers several ways to analyze the results of a benchmark experiment.\nThis includes visualization, ranking of learning algorithms and hypothesis tests to\nassess performance differences between learners.\n\n\nIn order to demonstrate the functionality we conduct a slightly larger benchmark experiment\nwith three learning algorithms that are applied to five classification tasks.\n\n\nExample: Comparing lda, rpart and random Forest\n\n\nWe consider \nlinear discriminant analysis (lda)\n,\n\nclassification trees (rpart)\n,\nand \nrandom forests (randomForest)\n.\nSince the default learner IDs are a little long, we choose shorter names in the\n\nR\n code below.\n\n\nWe use five classification tasks. Three are already provided by \nmlr\n, two more data sets\nare taken from package \nmlbench\n and converted to \nTask\ns by function \nconvertMLBenchObjToTask\n.\n\n\nFor all tasks 10-fold cross-validation is chosen as resampling strategy.\nThis is achieved by passing a single \nresample description\n to \nbenchmark\n,\nwhich is then instantiated automatically once for each \nTask\n. This way, the same instance\nis used for all learners applied to a single task.\n\n\nIt is also possible to choose a different resampling strategy for each \nTask\n by passing a\n\nlist\n of the same length as the number of tasks that can contain both\n\nresample descriptions\n and \nresample instances\n.\n\n\nWe use the mean misclassification error \nmmce\n as primary performance measure,\nbut also calculate the balanced error rate (\nber\n) and the training time\n(\ntimetrain\n).\n\n\n## Create a list of learners\nlrns = list(\n  makeLearner(\nclassif.lda\n, id = \nlda\n),\n  makeLearner(\nclassif.rpart\n, id = \nrpart\n),\n  makeLearner(\nclassif.randomForest\n, id = \nrandomForest\n)\n)\n\n## Get additional Tasks from package mlbench\nring.task = convertMLBenchObjToTask(\nmlbench.ringnorm\n, n = 600)\nwave.task = convertMLBenchObjToTask(\nmlbench.waveform\n, n = 600)\n\ntasks = list(iris.task, sonar.task, pid.task, ring.task, wave.task)\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nmeas = list(mmce, ber, timetrain)\nbmr = benchmark(lrns, tasks, rdesc, meas, show.info = FALSE)\nbmr\n#\n                        task.id   learner.id mmce.test.mean ber.test.mean\n#\n 1                 iris_example          lda     0.02000000    0.02222222\n#\n 2                 iris_example        rpart     0.08000000    0.07555556\n#\n 3                 iris_example randomForest     0.05333333    0.05250000\n#\n 4             mlbench.ringnorm          lda     0.35000000    0.34605671\n#\n 5             mlbench.ringnorm        rpart     0.17333333    0.17313632\n#\n 6             mlbench.ringnorm randomForest     0.05833333    0.05806121\n#\n 7             mlbench.waveform          lda     0.19000000    0.18257244\n#\n 8             mlbench.waveform        rpart     0.28833333    0.28765247\n#\n 9             mlbench.waveform randomForest     0.16500000    0.16306057\n#\n 10 PimaIndiansDiabetes_example          lda     0.22778537    0.27148893\n#\n 11 PimaIndiansDiabetes_example        rpart     0.25133288    0.28967870\n#\n 12 PimaIndiansDiabetes_example randomForest     0.23685919    0.27543146\n#\n 13               Sonar_example          lda     0.24619048    0.23986694\n#\n 14               Sonar_example        rpart     0.30785714    0.31153361\n#\n 15               Sonar_example randomForest     0.17785714    0.17442696\n#\n    timetrain.test.mean\n#\n 1               0.0039\n#\n 2               0.0072\n#\n 3               0.0286\n#\n 4               0.0083\n#\n 5               0.0146\n#\n 6               0.3865\n#\n 7               0.0102\n#\n 8               0.0145\n#\n 9               0.4340\n#\n 10              0.0050\n#\n 11              0.0110\n#\n 12              0.2933\n#\n 13              0.0172\n#\n 14              0.0166\n#\n 15              0.2458\n\n\n\n\nFrom the aggregated performance values we can see that for the iris- and PimaIndiansDiabetes-example\n\nlinear discriminant analysis\n performs well while for all other tasks the\n\nrandom forest\n seems superior.\nTraining takes longer for the \nrandom forest\n than for the other\nlearners.\n\n\nIn order to draw any conclusions from the average performances at least their variability\nhas to be taken into account or, preferably, the distribution of performance values across\nresampling iterations.\n\n\nThe individual performances on the 10 folds for every task, learner, and measure are\nretrieved below.\n\n\nperf = getBMRPerformances(bmr, as.df = TRUE)\nhead(perf)\n#\n        task.id learner.id iter      mmce       ber timetrain\n#\n 1 iris_example        lda    1 0.0000000 0.0000000     0.004\n#\n 2 iris_example        lda    2 0.1333333 0.1666667     0.003\n#\n 3 iris_example        lda    3 0.0000000 0.0000000     0.004\n#\n 4 iris_example        lda    4 0.0000000 0.0000000     0.004\n#\n 5 iris_example        lda    5 0.0000000 0.0000000     0.004\n#\n 6 iris_example        lda    6 0.0000000 0.0000000     0.004\n\n\n\n\nA closer look at the result reveals that the \nrandom forest\n\noutperforms the \nclassification tree\n in every instance, while\n\nlinear discriminant analysis\n performs better than \nrpart\n most\nof the time. Additionally \nlda\n sometimes even beats the \nrandom forest\n.\nWith increasing size of such \nbenchmark\n experiments, those tables become almost unreadable\nand hard to comprehend.\n\n\nmlr\n features some plotting functions to visualize results of benchmark experiments that\nyou might find useful.\nMoreover, \nmlr\n offers statistical hypothesis tests to assess performance differences\nbetween learners.\n\n\nIntegrated plots\n\n\nPlots are generated using \nggplot2\n. Further customization, such as renaming plot elements\nor changing colors, is easily possible.\n\n\nVisualizing performances\n\n\nplotBMRBoxplots\n creates box or violin plots which show the\ndistribution of performance values across resampling iterations for one performance measure\nand for all learners and tasks (and thus visualize the output of \ngetBMRPerformances\n).\n\n\nBelow are both variants, box and violin plots. The first plot shows the \nmmce\n\nand the second plot the balanced error rate (\nber\n).\nMoreover, in the second plot we color the boxes according to the \nlearner.id\ns.\n\n\nplotBMRBoxplots(bmr, measure = mmce)\n\n\n\n\n\n\nplotBMRBoxplots(bmr, measure = ber, style = \nviolin\n, pretty.names = FALSE) +\n  aes(color = learner.id) +\n  theme(strip.text.x = element_text(size = 8))\n\n\n\n\n\n\nNote that by default the measure \nname\ns and the learner \nshort.name\ns are used as axis\nlabels.\n\n\nmmce$name\n#\n [1] \nMean misclassification error\n\n\nmmce$id\n#\n [1] \nmmce\n\n\ngetBMRLearnerIds(bmr)\n#\n [1] \nlda\n          \nrpart\n        \nrandomForest\n\n\ngetBMRLearnerShortNames(bmr)\n#\n [1] \nlda\n   \nrpart\n \nrf\n\n\n\n\n\nIf you prefer the \nid\ns like, e.g., mmce and ber set \npretty.names = FALSE\n (as done for\nthe second plot).\nOf course you can also use the \nggplot2\n functionality like the \nylab\n\nfunction to choose completely different labels.\n\n\nOne question which comes up quite often is how to change the panel headers (which default\nto the \nTask\n IDs) and the learner names on the x-axis.\nFor example looking at the above plots we would like to remove the \"example\" suffixes and the\n\"mlbench\" prefixes from the panel headers.\nMoreover, we want uppercase learner labels.\nCurrently, the probably simplest solution is to change the factor levels of the plotted\ndata as shown below.\n\n\nplt = plotBMRBoxplots(bmr, measure = mmce)\nhead(plt$data)\n#\n        task.id learner.id iter      mmce       ber timetrain\n#\n 1 iris_example        lda    1 0.0000000 0.0000000     0.004\n#\n 2 iris_example        lda    2 0.1333333 0.1666667     0.003\n#\n 3 iris_example        lda    3 0.0000000 0.0000000     0.004\n#\n 4 iris_example        lda    4 0.0000000 0.0000000     0.004\n#\n 5 iris_example        lda    5 0.0000000 0.0000000     0.004\n#\n 6 iris_example        lda    6 0.0000000 0.0000000     0.004\n\nlevels(plt$data$task.id) = c(\nIris\n, \nRingnorm\n, \nWaveform\n, \nDiabetes\n, \nSonar\n)\nlevels(plt$data$learner.id) = c(\nLDA\n, \nCART\n, \nRF\n)\n\nplt + ylab(\nError rate\n)\n\n\n\n\n\n\nVisualizing aggregated performances\n\n\nThe aggregated performance values (resulting from \ngetBMRAggrPerformances\n) can be visualized\nby function \nplotBMRSummary\n. This plot draws one line for each task on which the aggregated\nvalues of one performance measure for all learners are displayed.\nBy default, the first measure in the \nlist\n of \nMeasure\ns passed\nto \nbenchmark\n is used, in our example \nmmce\n.\nMoreover, a small vertical jitter is added to prevent overplotting.\n\n\nplotBMRSummary(bmr)\n\n\n\n\n\n\nCalculating and visualizing ranks\n\n\nAdditional to the absolute performance, relative performance, i.e., ranking the learners\nis usually of interest and might provide valuable additional insight.\n\n\nFunction \nconvertBMRToRankMatrix\n calculates ranks based on aggregated learner performances\nof one measure. We choose the mean misclassification error (\nmmce\n).\nThe rank structure can be visualized by \nplotBMRRanksAsBarChart\n.\n\n\nm = convertBMRToRankMatrix(bmr, mmce)\nm\n#\n              iris_example mlbench.ringnorm mlbench.waveform\n#\n lda                     1                3                2\n#\n rpart                   3                2                3\n#\n randomForest            2                1                1\n#\n              PimaIndiansDiabetes_example Sonar_example\n#\n lda                                    1             2\n#\n rpart                                  3             3\n#\n randomForest                           2             1\n\n\n\n\nMethods with best performance, i.e., with lowest \nmmce\n, are assigned the lowest\nrank.\n\nLinear discriminant analysis\n is best for the iris and PimaIndiansDiabetes-examples\nwhile the \nrandom forest\n shows best results on the remaining\ntasks.\n\n\nplotBMRRanksAsBarChart\n with option \npos = \"tile\"\n shows a corresponding heat map. The\nranks are displayed on the x-axis and the learners are color-coded.\n\n\nplotBMRRanksAsBarChart(bmr, pos = \ntile\n)\n\n\n\n\n\n\nA similar plot can also be obtained via \nplotBMRSummary\n. With option \ntrafo = \"rank\"\n the\nranks are displayed instead of the aggregated performances.\n\n\nplotBMRSummary(bmr, trafo = \nrank\n, jitter = 0)\n\n\n\n\n\n\nAlternatively, you can draw stacked bar charts (the default) or bar charts with juxtaposed\nbars (\npos = \"dodge\"\n) that are better suited to compare the frequencies of learners within\nand across ranks.\n\n\nplotBMRRanksAsBarChart(bmr)\nplotBMRRanksAsBarChart(bmr, pos = \ndodge\n)\n\n\n\n\n\n\nComparing learners using hypothesis tests\n\n\nMany researchers feel the need to display an algorithm's superiority by employing some sort\nof hypothesis testing. As non-parametric tests seem better suited for such benchmark results\nthe tests provided in \nmlr\n are the \nOverall Friedman test\n and the\n\nFriedman-Nemenyi post hoc test\n.\n\n\nWhile the ad hoc \nFriedman test\n based on \nfriedman.test\n\nfrom the \nstats\n package is testing the hypothesis whether there is a significant difference\nbetween the employed learners, the post hoc \nFriedman-Nemenyi test\n tests\nfor significant differences between all pairs of learners. \nNon parametric\n tests often do\nhave less power then their \nparametric\n counterparts but less assumptions about underlying\ndistributions have to be made. This often means many \ndata sets\n are needed in order to be\nable to show significant differences at reasonable significance levels.\n\n\nIn our example, we want to compare the three learners on the selected data sets.\nFirst we might we want to test the hypothesis whether there is a difference between the learners.\n\n\nfriedmanTestBMR(bmr)\n#\n \n#\n  Friedman rank sum test\n#\n \n#\n data:  mmce.test.mean and learner.id and task.id\n#\n Friedman chi-squared = 5.2, df = 2, p-value = 0.07427\n\n\n\n\nIn order to keep the computation time for this tutorial small, the \nLearner\ns\nare only evaluated on five tasks. This also means that we operate on a relatively low significance\nlevel \n\\alpha = 0.1\n.\nAs we can reject the null hypothesis of the Friedman test at a reasonable significance level\nwe might now want to test where these differences lie exactly.\n\n\nfriedmanPostHocTestBMR(bmr, p.value = 0.1)\n#\n \n#\n  Pairwise comparisons using Nemenyi multiple comparison test \n#\n              with q approximation for unreplicated blocked data \n#\n \n#\n data:  mmce.test.mean and learner.id and task.id \n#\n \n#\n              lda   rpart\n#\n rpart        0.254 -    \n#\n randomForest 0.802 0.069\n#\n \n#\n P value adjustment method: none\n\n\n\n\nAt this level of significance, we can reject the null hypothesis that there exists no\nperformance difference between the decision tree (\nrpart\n) and the\n\nrandom Forest\n.\n\n\nCritical differences diagram\n\n\nIn order to visualize differently performing learners, a\n\ncritical differences diagram\n can be plotted, using either the\nNemenyi test (\ntest = \"nemenyi\"\n) or the Bonferroni-Dunn test (\ntest = \"bd\"\n).\n\n\nThe mean rank of learners is displayed on the x-axis.\n\n\n\n\nChoosing \ntest = \"nemenyi\"\n compares all pairs of \nLearner\ns to each other, thus\n  the output are groups of not significantly different learners. The diagram connects all groups\n  of learners where the mean ranks do not differ by more than the critical differences. Learners\n  that are not connected by a bar are significantly different, and the learner(s) with the\n  lower mean rank can be considered \"better\" at the chosen significance level.\n\n\nChoosing \ntest = \"bd\"\n performs a \npairwise comparison with a baseline\n. An interval which\n  extends by the given \ncritical difference\n in both directions is drawn around the\n  \nLearner\n chosen as baseline, though only comparisons with the baseline are\n  possible. All learners within the interval are not significantly different, while the\n  baseline can be considered better or worse than a given learner which is outside of the\n  interval.\n\n\n\n\nThe critical difference \n\\mathit{CD}\n is calculated by\n\n\\mathit{CD} = q_\\alpha \\cdot \\sqrt{\\frac{k(k+1)}{6N}},\n\nwhere \nN\n denotes the number of tasks, \nk\n is the number of learners, and\n\nq_\\alpha\n comes from the studentized range statistic divided by \n\\sqrt{2}\n.\nFor details see \nDemsar (2006)\n.\n\n\nFunction \ngenerateCritDifferencesData\n does all necessary calculations while function\n\nplotCritDifferences\n draws the plot. See the tutorial page about \nvisualization\n\nfor details on data generation and plotting functions.\n\n\n## Nemenyi test\ng = generateCritDifferencesData(bmr, p.value = 0.1, test = \nnemenyi\n)\nplotCritDifferences(g) + coord_cartesian(xlim = c(-1,5), ylim = c(0,2))\n\n\n\n\n\n\n## Bonferroni-Dunn test\ng = generateCritDifferencesData(bmr, p.value = 0.1, test = \nbd\n, baseline = \nrandomForest\n)\nplotCritDifferences(g) + coord_cartesian(xlim = c(-1,5), ylim = c(0,2))\n\n\n\n\n\n\nCustom plots\n\n\nYou can easily generate your own visualizations by customizing the \nggplot\n\nobjects returned by the plots above, retrieve the data from the \nggplot\n\nobjects and use them as basis for your own plots, or rely on the \ndata.frame\ns\nreturned by \ngetBMRPerformances\n or \ngetBMRAggrPerformances\n. Here are some examples.\n\n\nInstead of boxplots (as in \nplotBMRBoxplots\n) we could create density plots\nto show the performance values resulting from individual resampling iterations.\n\n\nperf = getBMRPerformances(bmr, as.df = TRUE)\n\n## Density plots for two tasks\nqplot(mmce, colour = learner.id, facets = . ~ task.id,\n  data = perf[perf$task.id %in% c(\niris_example\n, \nSonar_example\n),], geom = \ndensity\n) +\n  theme(strip.text.x = element_text(size = 8))\n\n\n\n\n\n\nIn order to plot multiple performance measures in parallel, \nperf\n is reshaped to long format.\nBelow we generate grouped boxplots showing the error rate (\nmmce\n) and the\ntraining time \ntimetrain\n.\n\n\n## Compare mmce and timetrain\ndf = reshape2::melt(perf, id.vars = c(\ntask.id\n, \nlearner.id\n, \niter\n))\ndf = df[df$variable != \nber\n,]\nhead(df)\n#\n        task.id learner.id iter variable     value\n#\n 1 iris_example        lda    1     mmce 0.0000000\n#\n 2 iris_example        lda    2     mmce 0.1333333\n#\n 3 iris_example        lda    3     mmce 0.0000000\n#\n 4 iris_example        lda    4     mmce 0.0000000\n#\n 5 iris_example        lda    5     mmce 0.0000000\n#\n 6 iris_example        lda    6     mmce 0.0000000\n\nqplot(variable, value, data = df, colour = learner.id, geom = \nboxplot\n,\n  xlab = \nmeasure\n, ylab = \nperformance\n) +\n  facet_wrap(~ task.id, nrow = 2)\n\n\n\n\n\n\nIt might also be useful to assess if learner performances in single resampling iterations,\ni.e., in one fold, are related.\nThis might help to gain further insight, for example by having a closer look at train and\ntest sets from iterations where one learner performs exceptionally well while another one\nis fairly bad.\nMoreover, this might be useful for the construction of ensembles of learning algorithms.\nBelow, function \nggpairs\n from package \nGGally\n is used to generate a scatterplot\nmatrix of mean misclassification errors (\nmmce\n) on the \nSonar\n\ndata set.\n\n\nperf = getBMRPerformances(bmr, task.id = \nSonar_example\n, as.df = TRUE)\ndf = reshape2::melt(perf, id.vars = c(\ntask.id\n, \nlearner.id\n, \niter\n))\ndf = df[df$variable == \nmmce\n,]\ndf = reshape2::dcast(df, task.id + iter ~ variable + learner.id)\nhead(df)\n#\n         task.id iter  mmce_lda mmce_rpart mmce_randomForest\n#\n 1 Sonar_example    1 0.2857143  0.2857143        0.14285714\n#\n 2 Sonar_example    2 0.2380952  0.2380952        0.23809524\n#\n 3 Sonar_example    3 0.3333333  0.2857143        0.28571429\n#\n 4 Sonar_example    4 0.2380952  0.3333333        0.04761905\n#\n 5 Sonar_example    5 0.1428571  0.2857143        0.19047619\n#\n 6 Sonar_example    6 0.4000000  0.4500000        0.25000000\n\nGGally::ggpairs(df, 3:5)\n\n\n\n\n\n\nFurther comments\n\n\n\n\nNote that for supervised classification \nmlr\n offers some more plots that operate\n  on \nBenchmarkResult\n objects and allow you to compare the performance of learning\n  algorithms.\n  See for example the tutorial page on \nROC curves\n and functions\n  \ngenerateThreshVsPerfData\n, \nplotROCCurves\n, and \nplotViperCharts\n as well as the\n  page about \nclassifier calibration\n and function \ngenerateCalibrationData\n.\n\n\nIn the examples shown in this section we applied \"raw\" learning algorithms, but often things\n  are more complicated.\n  At the very least, many learners have hyperparameters that need to be tuned to get sensible\n  results.\n  Reliable performance estimates can be obtained by \nnested resampling\n,\n  i.e., by doing the tuning in an\n  inner resampling loop while estimating the performance in an outer loop.\n  Moreover, you might want to combine learners with pre-processing steps like imputation, scaling,\n  outlier removal, dimensionality reduction or feature selection and so on.\n  All this can be easily done using \nmlr\n's wrapper functionality.\n  The general principle is explained in the section about \nwrapped learners\n in the\n  Advanced part of this tutorial. There are also several sections devoted to common pre-processing\n  steps.\n\n\nBenchmark experiments can very quickly become computationally demanding. \nmlr\n offers\n  some possibilities for \nparallelization\n.\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## Two learners to be compared \nlrns = list(makeLearner(\nclassif.lda\n), makeLearner(\nclassif.rpart\n)) \n\n## Choose the resampling strategy \nrdesc = makeResampleDesc(\nHoldout\n) \n\n## Conduct the benchmark experiment \nbmr = benchmark(lrns, sonar.task, rdesc) \n\nbmr \n## Vector of strings \nlrns = c(\nclassif.lda\n, \nclassif.rpart\n) \n\n## A mixed list of Learner objects and strings works, too \nlrns = list(makeLearner(\nclassif.lda\n, predict.type = \nprob\n), \nclassif.rpart\n) \n\nbmr = benchmark(lrns, sonar.task, rdesc) \n\nbmr \ngetBMRPerformances(bmr) \n\ngetBMRAggrPerformances(bmr) \ngetBMRPerformances(bmr, drop = TRUE) \ngetBMRPerformances(bmr, as.df = TRUE) \n\ngetBMRAggrPerformances(bmr, as.df = TRUE) \ngetBMRPredictions(bmr) \n\nhead(getBMRPredictions(bmr, as.df = TRUE)) \nhead(getBMRPredictions(bmr, learner.ids = \nclassif.rpart\n, as.df = TRUE)) \ngetBMRTaskIds(bmr) \n\ngetBMRLearnerIds(bmr) \n\ngetBMRMeasureIds(bmr) \ngetBMRModels(bmr) \n\ngetBMRModels(bmr, drop = TRUE) \n\ngetBMRModels(bmr, learner.ids = \nclassif.lda\n) \ngetBMRLearners(bmr) \n\ngetBMRMeasures(bmr) \n## First benchmark result \nbmr \n\n## Benchmark experiment for the additional learners \nlrns2 = list(makeLearner(\nclassif.randomForest\n), makeLearner(\nclassif.qda\n)) \nbmr2 = benchmark(lrns2, sonar.task, rdesc, show.info = FALSE) \nbmr2 \n\n## Merge the results \nmergeBenchmarkResults(list(bmr, bmr2)) \nrin = getBMRPredictions(bmr)[[1]][[1]]$instance \nrin \n\n## Benchmark experiment for the additional random forest \nbmr3 = benchmark(lrns2, sonar.task, rin, show.info = FALSE) \nbmr3 \n\n## Merge the results \nmergeBenchmarkResults(list(bmr, bmr3)) \nset.seed(4444) \n## Create a list of learners \nlrns = list( \n  makeLearner(\nclassif.lda\n, id = \nlda\n), \n  makeLearner(\nclassif.rpart\n, id = \nrpart\n), \n  makeLearner(\nclassif.randomForest\n, id = \nrandomForest\n) \n) \n\n## Get additional Tasks from package mlbench \nring.task = convertMLBenchObjToTask(\nmlbench.ringnorm\n, n = 600) \nwave.task = convertMLBenchObjToTask(\nmlbench.waveform\n, n = 600) \n\ntasks = list(iris.task, sonar.task, pid.task, ring.task, wave.task) \nrdesc = makeResampleDesc(\nCV\n, iters = 10) \nmeas = list(mmce, ber, timetrain) \nbmr = benchmark(lrns, tasks, rdesc, meas, show.info = FALSE) \nbmr \nperf = getBMRPerformances(bmr, as.df = TRUE) \nhead(perf) \nplotBMRBoxplots(bmr, measure = mmce) \nplotBMRBoxplots(bmr, measure = ber, style = \nviolin\n, pretty.names = FALSE) + \n  aes(color = learner.id) + \n  theme(strip.text.x = element_text(size = 8)) \nmmce$name \n\nmmce$id \n\ngetBMRLearnerIds(bmr) \n\ngetBMRLearnerShortNames(bmr) \nplt = plotBMRBoxplots(bmr, measure = mmce) \nhead(plt$data) \n\nlevels(plt$data$task.id) = c(\nIris\n, \nRingnorm\n, \nWaveform\n, \nDiabetes\n, \nSonar\n) \nlevels(plt$data$learner.id) = c(\nLDA\n, \nCART\n, \nRF\n) \n\nplt + ylab(\nError rate\n) \nplotBMRSummary(bmr) \nm = convertBMRToRankMatrix(bmr, mmce) \nm \nplotBMRRanksAsBarChart(bmr, pos = \ntile\n) \nplotBMRSummary(bmr, trafo = \nrank\n, jitter = 0) \nplotBMRRanksAsBarChart(bmr) \nplotBMRRanksAsBarChart(bmr, pos = \ndodge\n) \nfriedmanTestBMR(bmr) \nfriedmanPostHocTestBMR(bmr, p.value = 0.1) \n## Nemenyi test \ng = generateCritDifferencesData(bmr, p.value = 0.1, test = \nnemenyi\n) \nplotCritDifferences(g) + coord_cartesian(xlim = c(-1,5), ylim = c(0,2)) \n## Bonferroni-Dunn test \ng = generateCritDifferencesData(bmr, p.value = 0.1, test = \nbd\n, baseline = \nrandomForest\n) \nplotCritDifferences(g) + coord_cartesian(xlim = c(-1,5), ylim = c(0,2)) \nperf = getBMRPerformances(bmr, as.df = TRUE) \n\n## Density plots for two tasks \nqplot(mmce, colour = learner.id, facets = . ~ task.id, \n  data = perf[perf$task.id %in% c(\niris_example\n, \nSonar_example\n),], geom = \ndensity\n) + \n  theme(strip.text.x = element_text(size = 8)) \n## Compare mmce and timetrain \ndf = reshape2::melt(perf, id.vars = c(\ntask.id\n, \nlearner.id\n, \niter\n)) \ndf = df[df$variable != \nber\n,] \nhead(df) \n\nqplot(variable, value, data = df, colour = learner.id, geom = \nboxplot\n, \n  xlab = \nmeasure\n, ylab = \nperformance\n) + \n  facet_wrap(~ task.id, nrow = 2) \nperf = getBMRPerformances(bmr, task.id = \nSonar_example\n, as.df = TRUE) \ndf = reshape2::melt(perf, id.vars = c(\ntask.id\n, \nlearner.id\n, \niter\n)) \ndf = df[df$variable == \nmmce\n,] \ndf = reshape2::dcast(df, task.id + iter ~ variable + learner.id) \nhead(df) \n\nGGally::ggpairs(df, 3:5)", 
            "title": "Benchmark Experiments"
        }, 
        {
            "location": "/benchmark_experiments/index.html#benchmark-experiments", 
            "text": "In a benchmark experiment different learning methods are applied to one or several data sets\nwith the aim to compare and rank the algorithms with respect to one or more\nperformance measures.  In  mlr  a benchmark experiment can be conducted by calling function  benchmark  on\na  list  of  Learner s and a  list  of  Task s. benchmark  basically executes  resample  for each combination of  Learner \nand  Task .\nYou can specify an individual resampling strategy for each  Task  and select one or\nmultiple performance measures to be calculated.", 
            "title": "Benchmark Experiments"
        }, 
        {
            "location": "/benchmark_experiments/index.html#conducting-benchmark-experiments", 
            "text": "We start with a small example. Two learners,  linear discriminant analysis (lda)  and a classification tree (rpart) , are applied to one classification problem ( sonar.task ).\nAs resampling strategy we choose  \"Holdout\" .\nThe performance is thus calculated on a single randomly sampled test data set.  In the example below we create a resample description ( ResampleDesc ),\nwhich is automatically instantiated by  benchmark .\nThe instantiation is done only once per  Task , i.e., the same training and test sets\nare used for all learners.\nIt is also possible to directly pass a  ResampleInstance .  If you would like to use a  fixed test data set  instead of a randomly selected one, you can\ncreate a suitable  ResampleInstance  through function makeFixedHoldoutInstance .  ## Two learners to be compared\nlrns = list(makeLearner( classif.lda ), makeLearner( classif.rpart ))\n\n## Choose the resampling strategy\nrdesc = makeResampleDesc( Holdout )\n\n## Conduct the benchmark experiment\nbmr = benchmark(lrns, sonar.task, rdesc)\n#  Task: Sonar_example, Learner: classif.lda\n#  Resampling: holdout\n#  Measures:             mmce\n#  [Resample] iter 1:    0.3000000\n#  \n#  Aggregated Result: mmce.test.mean=0.3000000\n#  \n#  Task: Sonar_example, Learner: classif.rpart\n#  Resampling: holdout\n#  Measures:             mmce\n#  [Resample] iter 1:    0.2857143\n#  \n#  Aggregated Result: mmce.test.mean=0.2857143\n#  \n\nbmr\n#          task.id    learner.id mmce.test.mean\n#  1 Sonar_example   classif.lda      0.3000000\n#  2 Sonar_example classif.rpart      0.2857143  For convenience, if you don't want to pass any additional arguments to  makeLearner , you\ndon't need to generate the  Learner s explicitly, but it's sufficient to\nprovide the learner name.\nIn the above example we could also have written:  ## Vector of strings\nlrns = c( classif.lda ,  classif.rpart )\n\n## A mixed list of Learner objects and strings works, too\nlrns = list(makeLearner( classif.lda , predict.type =  prob ),  classif.rpart )\n\nbmr = benchmark(lrns, sonar.task, rdesc)\n#  Task: Sonar_example, Learner: classif.lda\n#  Resampling: holdout\n#  Measures:             mmce\n#  [Resample] iter 1:    0.2571429\n#  \n#  Aggregated Result: mmce.test.mean=0.2571429\n#  \n#  Task: Sonar_example, Learner: classif.rpart\n#  Resampling: holdout\n#  Measures:             mmce\n#  [Resample] iter 1:    0.2714286\n#  \n#  Aggregated Result: mmce.test.mean=0.2714286\n#  \n\nbmr\n#          task.id    learner.id mmce.test.mean\n#  1 Sonar_example   classif.lda      0.2571429\n#  2 Sonar_example classif.rpart      0.2714286  In the printed summary table every row corresponds to one pair of  Task  and  Learner .\nThe entries show the mean misclassification error ( mmce ), the default performance\nmeasure for classification, on the test data set.  The result  bmr  is an object of class  BenchmarkResult . Basically, it contains a  list \nof lists of  ResampleResult  objects, first ordered by  Task  and then by  Learner .", 
            "title": "Conducting benchmark experiments"
        }, 
        {
            "location": "/benchmark_experiments/index.html#making-experiments-reproducible", 
            "text": "Typically, we would want our experiment results to be reproducible.  mlr  obeys\nthe  set.seed  function, so make sure to use  set.seed  at the beginning of your\nscript if you would like your results to be reproducible.  Note that if you are using parallel computing, you may need to adjust how you\ncall  set.seed  depending on your usecase. One possibility is to use set.seed(123, \"L'Ecuyer\")  in order to ensure the results are reproducible for\neach child process. See the examples in  mclapply  for more\ninformation on reproducibility and parallel computing.", 
            "title": "Making experiments reproducible"
        }, 
        {
            "location": "/benchmark_experiments/index.html#accessing-benchmark-results", 
            "text": "mlr  provides several accessor functions, named  getBMR WhatToExtract , that permit\nto retrieve information for further analyses. This includes for example the performances\nor predictions of the learning algorithms under consideration.", 
            "title": "Accessing benchmark results"
        }, 
        {
            "location": "/benchmark_experiments/index.html#learner-performances", 
            "text": "Let's have a look at the benchmark result above. getBMRPerformances  returns individual performances in resampling runs, while getBMRAggrPerformances  gives the aggregated values.  getBMRPerformances(bmr)\n#  $Sonar_example\n#  $Sonar_example$classif.lda\n#    iter      mmce\n#  1    1 0.2571429\n#  \n#  $Sonar_example$classif.rpart\n#    iter      mmce\n#  1    1 0.2714286\n\ngetBMRAggrPerformances(bmr)\n#  $Sonar_example\n#  $Sonar_example$classif.lda\n#  mmce.test.mean \n#       0.2571429 \n#  \n#  $Sonar_example$classif.rpart\n#  mmce.test.mean \n#       0.2714286  Since we used holdout as resampling strategy, individual and aggregated performance values\ncoincide.  By default, nearly all \"getter\" functions return a nested  list , with the first\nlevel indicating the task and the second level indicating the learner.\nIf only a single learner or, as in our case a single task is considered, setting drop = TRUE  simplifies the result to a flat  list .  getBMRPerformances(bmr, drop = TRUE)\n#  $classif.lda\n#    iter      mmce\n#  1    1 0.2571429\n#  \n#  $classif.rpart\n#    iter      mmce\n#  1    1 0.2714286  Often it is more convenient to work with  data.frame s. You can easily\nconvert the result structure by setting  as.df = TRUE .  getBMRPerformances(bmr, as.df = TRUE)\n#          task.id    learner.id iter      mmce\n#  1 Sonar_example   classif.lda    1 0.2571429\n#  2 Sonar_example classif.rpart    1 0.2714286\n\ngetBMRAggrPerformances(bmr, as.df = TRUE)\n#          task.id    learner.id mmce.test.mean\n#  1 Sonar_example   classif.lda      0.2571429\n#  2 Sonar_example classif.rpart      0.2714286", 
            "title": "Learner performances"
        }, 
        {
            "location": "/benchmark_experiments/index.html#predictions", 
            "text": "Per default, the  BenchmarkResult  contains the learner predictions.\nIf you do not want to keep them, e.g., to conserve memory, set  keep.pred = FALSE  when\ncalling  benchmark .  You can access the predictions using function  getBMRPredictions .\nPer default, you get a nested  list  of  ResamplePrediction  objects.\nAs above, you can use the  drop  or  as.df  options to simplify the result.  getBMRPredictions(bmr)\n#  $Sonar_example\n#  $Sonar_example$classif.lda\n#  Resampled Prediction for:\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE\n#  predict.type: prob\n#  threshold: M=0.50,R=0.50\n#  time (mean): 0.01\n#     id truth     prob.M      prob.R response iter  set\n#  1 127     M 0.98673000 0.013270001        M    1 test\n#  2 159     M 0.99659179 0.003408211        M    1 test\n#  3  81     R 0.55436799 0.445632009        M    1 test\n#  4 207     M 0.98660766 0.013392337        M    1 test\n#  5  74     R 0.94120073 0.058799272        M    1 test\n#  6 154     M 0.03862365 0.961376347        R    1 test\n#  ... (#rows: 70, #cols: 7)\n#  \n#  $Sonar_example$classif.rpart\n#  Resampled Prediction for:\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE\n#  predict.type: response\n#  threshold: \n#  time (mean): 0.01\n#     id truth response iter  set\n#  1 127     M        R    1 test\n#  2 159     M        R    1 test\n#  3  81     R        R    1 test\n#  4 207     M        M    1 test\n#  5  74     R        R    1 test\n#  6 154     M        M    1 test\n#  ... (#rows: 70, #cols: 5)\n\nhead(getBMRPredictions(bmr, as.df = TRUE))\n#          task.id  learner.id  id truth     prob.M      prob.R response iter\n#  1 Sonar_example classif.lda 127     M 0.98673000 0.013270001        M    1\n#  2 Sonar_example classif.lda 159     M 0.99659179 0.003408211        M    1\n#  3 Sonar_example classif.lda  81     R 0.55436799 0.445632009        M    1\n#  4 Sonar_example classif.lda 207     M 0.98660766 0.013392337        M    1\n#  5 Sonar_example classif.lda  74     R 0.94120073 0.058799272        M    1\n#  6 Sonar_example classif.lda 154     M 0.03862365 0.961376347        R    1\n#     set\n#  1 test\n#  2 test\n#  3 test\n#  4 test\n#  5 test\n#  6 test  It is also easily possible to access results for certain learners or tasks via their\nIDs. For this purpose many \"getter\" functions have a  learner.ids  and a  task.ids  argument.  head(getBMRPredictions(bmr, learner.ids =  classif.rpart , as.df = TRUE))\n#          task.id    learner.id  id truth response iter  set\n#  1 Sonar_example classif.rpart 127     M        R    1 test\n#  2 Sonar_example classif.rpart 159     M        R    1 test\n#  3 Sonar_example classif.rpart  81     R        R    1 test\n#  4 Sonar_example classif.rpart 207     M        M    1 test\n#  5 Sonar_example classif.rpart  74     R        R    1 test\n#  6 Sonar_example classif.rpart 154     M        M    1 test  If you don't like the default IDs, you can set the IDs of learners and tasks via the  id  option of makeLearner  and  make*Task .\nMoreover, you can conveniently change the ID of a  Learner  via function  setLearnerId .", 
            "title": "Predictions"
        }, 
        {
            "location": "/benchmark_experiments/index.html#ids", 
            "text": "The IDs of all  Learner s,  Task s and  Measure s in a benchmark\nexperiment can be retrieved as follows:  getBMRTaskIds(bmr)\n#  [1]  Sonar_example \n\ngetBMRLearnerIds(bmr)\n#  [1]  classif.lda     classif.rpart \n\ngetBMRMeasureIds(bmr)\n#  [1]  mmce", 
            "title": "IDs"
        }, 
        {
            "location": "/benchmark_experiments/index.html#fitted-models", 
            "text": "Per default the  BenchmarkResult  also contains the fitted models for all learners on all tasks.\nIf you do not want to keep them set  models = FALSE  when calling  benchmark .\nThe fitted models can be retrieved by function  getBMRModels .\nIt returns a (possibly nested)  list  of  WrappedModel  objects.  getBMRModels(bmr)\n#  $Sonar_example\n#  $Sonar_example$classif.lda\n#  $Sonar_example$classif.lda[[1]]\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = Sonar_example; obs = 138; features = 60\n#  Hyperparameters: \n#  \n#  \n#  $Sonar_example$classif.rpart\n#  $Sonar_example$classif.rpart[[1]]\n#  Model for learner.id=classif.rpart; learner.class=classif.rpart\n#  Trained on: task.id = Sonar_example; obs = 138; features = 60\n#  Hyperparameters: xval=0\n\ngetBMRModels(bmr, drop = TRUE)\n#  $classif.lda\n#  $classif.lda[[1]]\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = Sonar_example; obs = 138; features = 60\n#  Hyperparameters: \n#  \n#  \n#  $classif.rpart\n#  $classif.rpart[[1]]\n#  Model for learner.id=classif.rpart; learner.class=classif.rpart\n#  Trained on: task.id = Sonar_example; obs = 138; features = 60\n#  Hyperparameters: xval=0\n\ngetBMRModels(bmr, learner.ids =  classif.lda )\n#  $Sonar_example\n#  $Sonar_example$classif.lda\n#  $Sonar_example$classif.lda[[1]]\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = Sonar_example; obs = 138; features = 60\n#  Hyperparameters:", 
            "title": "Fitted models"
        }, 
        {
            "location": "/benchmark_experiments/index.html#learners-and-measures", 
            "text": "Moreover, you can extract the employed  Learner s and  Measure s.  getBMRLearners(bmr)\n#  $classif.lda\n#  Learner classif.lda from package MASS\n#  Type: classif\n#  Name: Linear Discriminant Analysis; Short name: lda\n#  Class: classif.lda\n#  Properties: twoclass,multiclass,numerics,factors,prob\n#  Predict-Type: prob\n#  Hyperparameters: \n#  \n#  \n#  $classif.rpart\n#  Learner classif.rpart from package rpart\n#  Type: classif\n#  Name: Decision Tree; Short name: rpart\n#  Class: classif.rpart\n#  Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp\n#  Predict-Type: response\n#  Hyperparameters: xval=0\n\ngetBMRMeasures(bmr)\n#  [[1]]\n#  Name: Mean misclassification error\n#  Performance measure: mmce\n#  Properties: classif,classif.multi,req.pred,req.truth\n#  Minimize: TRUE\n#  Best: 0; Worst: 1\n#  Aggregated by: test.mean\n#  Arguments: \n#  Note: Defined as: mean(response != truth)", 
            "title": "Learners and measures"
        }, 
        {
            "location": "/benchmark_experiments/index.html#merging-benchmark-results", 
            "text": "Sometimes after completing a benchmark experiment it turns out that you want to extend it\nby another  Learner  or another  Task .\nIn this case you can perform an additional benchmark experiment and then use function mergeBenchmarkResults  to combine the results to a single  BenchmarkResult  object that\ncan be accessed and analyzed as usual.  For example in the benchmark experiment above we applied  lda  and  rpart \nto the  sonar.task . We now perform a second experiment using a  random forest \nand  quadratic discriminant analysis (qda)  and merge the results.  ## First benchmark result\nbmr\n#          task.id    learner.id mmce.test.mean\n#  1 Sonar_example   classif.lda      0.2571429\n#  2 Sonar_example classif.rpart      0.2714286\n\n## Benchmark experiment for the additional learners\nlrns2 = list(makeLearner( classif.randomForest ), makeLearner( classif.qda ))\nbmr2 = benchmark(lrns2, sonar.task, rdesc, show.info = FALSE)\nbmr2\n#          task.id           learner.id mmce.test.mean\n#  1 Sonar_example classif.randomForest      0.1428571\n#  2 Sonar_example          classif.qda      0.2714286\n\n## Merge the results\nmergeBenchmarkResults(list(bmr, bmr2))\n#          task.id           learner.id mmce.test.mean\n#  1 Sonar_example          classif.lda      0.2571429\n#  2 Sonar_example        classif.rpart      0.2714286\n#  3 Sonar_example classif.randomForest      0.1428571\n#  4 Sonar_example          classif.qda      0.2714286  Note that in the above examples in each case a  resample description \nwas passed to the  benchmark  function.\nFor this reason  lda  and  rpart  were most likely evaluated on\na different training/test set pair than  random forest  and qda .  Differing training/test set pairs across learners pose an additional source of variation in\nthe results, which can make it harder to detect actual performance differences between learners.\nTherefore, if you suspect that you will have to extend your benchmark experiment by another Learner  later on it's probably easiest to work with ResampleInstance s from the start. These can be stored and used for\nany additional experiments.  Alternatively, if you used a resample description in the first benchmark experiment you could\nalso extract the  ResampleInstance s from the  BenchmarkResult   bmr \nand pass these to all further  benchmark  calls.  rin = getBMRPredictions(bmr)[[1]][[1]]$instance\nrin\n#  Resample instance for 208 cases.\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE\n\n## Benchmark experiment for the additional random forest\nbmr3 = benchmark(lrns2, sonar.task, rin, show.info = FALSE)\nbmr3\n#          task.id           learner.id mmce.test.mean\n#  1 Sonar_example classif.randomForest      0.2000000\n#  2 Sonar_example          classif.qda      0.5142857\n\n## Merge the results\nmergeBenchmarkResults(list(bmr, bmr3))\n#          task.id           learner.id mmce.test.mean\n#  1 Sonar_example          classif.lda      0.2571429\n#  2 Sonar_example        classif.rpart      0.2714286\n#  3 Sonar_example classif.randomForest      0.2000000\n#  4 Sonar_example          classif.qda      0.5142857", 
            "title": "Merging benchmark results"
        }, 
        {
            "location": "/benchmark_experiments/index.html#benchmark-analysis-and-visualization", 
            "text": "mlr  offers several ways to analyze the results of a benchmark experiment.\nThis includes visualization, ranking of learning algorithms and hypothesis tests to\nassess performance differences between learners.  In order to demonstrate the functionality we conduct a slightly larger benchmark experiment\nwith three learning algorithms that are applied to five classification tasks.", 
            "title": "Benchmark analysis and visualization"
        }, 
        {
            "location": "/benchmark_experiments/index.html#example-comparing-lda-rpart-and-random-forest", 
            "text": "We consider  linear discriminant analysis (lda) , classification trees (rpart) ,\nand  random forests (randomForest) .\nSince the default learner IDs are a little long, we choose shorter names in the R  code below.  We use five classification tasks. Three are already provided by  mlr , two more data sets\nare taken from package  mlbench  and converted to  Task s by function  convertMLBenchObjToTask .  For all tasks 10-fold cross-validation is chosen as resampling strategy.\nThis is achieved by passing a single  resample description  to  benchmark ,\nwhich is then instantiated automatically once for each  Task . This way, the same instance\nis used for all learners applied to a single task.  It is also possible to choose a different resampling strategy for each  Task  by passing a list  of the same length as the number of tasks that can contain both resample descriptions  and  resample instances .  We use the mean misclassification error  mmce  as primary performance measure,\nbut also calculate the balanced error rate ( ber ) and the training time\n( timetrain ).  ## Create a list of learners\nlrns = list(\n  makeLearner( classif.lda , id =  lda ),\n  makeLearner( classif.rpart , id =  rpart ),\n  makeLearner( classif.randomForest , id =  randomForest )\n)\n\n## Get additional Tasks from package mlbench\nring.task = convertMLBenchObjToTask( mlbench.ringnorm , n = 600)\nwave.task = convertMLBenchObjToTask( mlbench.waveform , n = 600)\n\ntasks = list(iris.task, sonar.task, pid.task, ring.task, wave.task)\nrdesc = makeResampleDesc( CV , iters = 10)\nmeas = list(mmce, ber, timetrain)\nbmr = benchmark(lrns, tasks, rdesc, meas, show.info = FALSE)\nbmr\n#                         task.id   learner.id mmce.test.mean ber.test.mean\n#  1                 iris_example          lda     0.02000000    0.02222222\n#  2                 iris_example        rpart     0.08000000    0.07555556\n#  3                 iris_example randomForest     0.05333333    0.05250000\n#  4             mlbench.ringnorm          lda     0.35000000    0.34605671\n#  5             mlbench.ringnorm        rpart     0.17333333    0.17313632\n#  6             mlbench.ringnorm randomForest     0.05833333    0.05806121\n#  7             mlbench.waveform          lda     0.19000000    0.18257244\n#  8             mlbench.waveform        rpart     0.28833333    0.28765247\n#  9             mlbench.waveform randomForest     0.16500000    0.16306057\n#  10 PimaIndiansDiabetes_example          lda     0.22778537    0.27148893\n#  11 PimaIndiansDiabetes_example        rpart     0.25133288    0.28967870\n#  12 PimaIndiansDiabetes_example randomForest     0.23685919    0.27543146\n#  13               Sonar_example          lda     0.24619048    0.23986694\n#  14               Sonar_example        rpart     0.30785714    0.31153361\n#  15               Sonar_example randomForest     0.17785714    0.17442696\n#     timetrain.test.mean\n#  1               0.0039\n#  2               0.0072\n#  3               0.0286\n#  4               0.0083\n#  5               0.0146\n#  6               0.3865\n#  7               0.0102\n#  8               0.0145\n#  9               0.4340\n#  10              0.0050\n#  11              0.0110\n#  12              0.2933\n#  13              0.0172\n#  14              0.0166\n#  15              0.2458  From the aggregated performance values we can see that for the iris- and PimaIndiansDiabetes-example linear discriminant analysis  performs well while for all other tasks the random forest  seems superior.\nTraining takes longer for the  random forest  than for the other\nlearners.  In order to draw any conclusions from the average performances at least their variability\nhas to be taken into account or, preferably, the distribution of performance values across\nresampling iterations.  The individual performances on the 10 folds for every task, learner, and measure are\nretrieved below.  perf = getBMRPerformances(bmr, as.df = TRUE)\nhead(perf)\n#         task.id learner.id iter      mmce       ber timetrain\n#  1 iris_example        lda    1 0.0000000 0.0000000     0.004\n#  2 iris_example        lda    2 0.1333333 0.1666667     0.003\n#  3 iris_example        lda    3 0.0000000 0.0000000     0.004\n#  4 iris_example        lda    4 0.0000000 0.0000000     0.004\n#  5 iris_example        lda    5 0.0000000 0.0000000     0.004\n#  6 iris_example        lda    6 0.0000000 0.0000000     0.004  A closer look at the result reveals that the  random forest \noutperforms the  classification tree  in every instance, while linear discriminant analysis  performs better than  rpart  most\nof the time. Additionally  lda  sometimes even beats the  random forest .\nWith increasing size of such  benchmark  experiments, those tables become almost unreadable\nand hard to comprehend.  mlr  features some plotting functions to visualize results of benchmark experiments that\nyou might find useful.\nMoreover,  mlr  offers statistical hypothesis tests to assess performance differences\nbetween learners.", 
            "title": "Example: Comparing lda, rpart and random Forest"
        }, 
        {
            "location": "/benchmark_experiments/index.html#integrated-plots", 
            "text": "Plots are generated using  ggplot2 . Further customization, such as renaming plot elements\nor changing colors, is easily possible.", 
            "title": "Integrated plots"
        }, 
        {
            "location": "/benchmark_experiments/index.html#visualizing-performances", 
            "text": "plotBMRBoxplots  creates box or violin plots which show the\ndistribution of performance values across resampling iterations for one performance measure\nand for all learners and tasks (and thus visualize the output of  getBMRPerformances ).  Below are both variants, box and violin plots. The first plot shows the  mmce \nand the second plot the balanced error rate ( ber ).\nMoreover, in the second plot we color the boxes according to the  learner.id s.  plotBMRBoxplots(bmr, measure = mmce)   plotBMRBoxplots(bmr, measure = ber, style =  violin , pretty.names = FALSE) +\n  aes(color = learner.id) +\n  theme(strip.text.x = element_text(size = 8))   Note that by default the measure  name s and the learner  short.name s are used as axis\nlabels.  mmce$name\n#  [1]  Mean misclassification error \n\nmmce$id\n#  [1]  mmce \n\ngetBMRLearnerIds(bmr)\n#  [1]  lda            rpart          randomForest \n\ngetBMRLearnerShortNames(bmr)\n#  [1]  lda     rpart   rf   If you prefer the  id s like, e.g., mmce and ber set  pretty.names = FALSE  (as done for\nthe second plot).\nOf course you can also use the  ggplot2  functionality like the  ylab \nfunction to choose completely different labels.  One question which comes up quite often is how to change the panel headers (which default\nto the  Task  IDs) and the learner names on the x-axis.\nFor example looking at the above plots we would like to remove the \"example\" suffixes and the\n\"mlbench\" prefixes from the panel headers.\nMoreover, we want uppercase learner labels.\nCurrently, the probably simplest solution is to change the factor levels of the plotted\ndata as shown below.  plt = plotBMRBoxplots(bmr, measure = mmce)\nhead(plt$data)\n#         task.id learner.id iter      mmce       ber timetrain\n#  1 iris_example        lda    1 0.0000000 0.0000000     0.004\n#  2 iris_example        lda    2 0.1333333 0.1666667     0.003\n#  3 iris_example        lda    3 0.0000000 0.0000000     0.004\n#  4 iris_example        lda    4 0.0000000 0.0000000     0.004\n#  5 iris_example        lda    5 0.0000000 0.0000000     0.004\n#  6 iris_example        lda    6 0.0000000 0.0000000     0.004\n\nlevels(plt$data$task.id) = c( Iris ,  Ringnorm ,  Waveform ,  Diabetes ,  Sonar )\nlevels(plt$data$learner.id) = c( LDA ,  CART ,  RF )\n\nplt + ylab( Error rate )", 
            "title": "Visualizing performances"
        }, 
        {
            "location": "/benchmark_experiments/index.html#visualizing-aggregated-performances", 
            "text": "The aggregated performance values (resulting from  getBMRAggrPerformances ) can be visualized\nby function  plotBMRSummary . This plot draws one line for each task on which the aggregated\nvalues of one performance measure for all learners are displayed.\nBy default, the first measure in the  list  of  Measure s passed\nto  benchmark  is used, in our example  mmce .\nMoreover, a small vertical jitter is added to prevent overplotting.  plotBMRSummary(bmr)", 
            "title": "Visualizing aggregated performances"
        }, 
        {
            "location": "/benchmark_experiments/index.html#calculating-and-visualizing-ranks", 
            "text": "Additional to the absolute performance, relative performance, i.e., ranking the learners\nis usually of interest and might provide valuable additional insight.  Function  convertBMRToRankMatrix  calculates ranks based on aggregated learner performances\nof one measure. We choose the mean misclassification error ( mmce ).\nThe rank structure can be visualized by  plotBMRRanksAsBarChart .  m = convertBMRToRankMatrix(bmr, mmce)\nm\n#               iris_example mlbench.ringnorm mlbench.waveform\n#  lda                     1                3                2\n#  rpart                   3                2                3\n#  randomForest            2                1                1\n#               PimaIndiansDiabetes_example Sonar_example\n#  lda                                    1             2\n#  rpart                                  3             3\n#  randomForest                           2             1  Methods with best performance, i.e., with lowest  mmce , are assigned the lowest\nrank. Linear discriminant analysis  is best for the iris and PimaIndiansDiabetes-examples\nwhile the  random forest  shows best results on the remaining\ntasks.  plotBMRRanksAsBarChart  with option  pos = \"tile\"  shows a corresponding heat map. The\nranks are displayed on the x-axis and the learners are color-coded.  plotBMRRanksAsBarChart(bmr, pos =  tile )   A similar plot can also be obtained via  plotBMRSummary . With option  trafo = \"rank\"  the\nranks are displayed instead of the aggregated performances.  plotBMRSummary(bmr, trafo =  rank , jitter = 0)   Alternatively, you can draw stacked bar charts (the default) or bar charts with juxtaposed\nbars ( pos = \"dodge\" ) that are better suited to compare the frequencies of learners within\nand across ranks.  plotBMRRanksAsBarChart(bmr)\nplotBMRRanksAsBarChart(bmr, pos =  dodge )", 
            "title": "Calculating and visualizing ranks"
        }, 
        {
            "location": "/benchmark_experiments/index.html#comparing-learners-using-hypothesis-tests", 
            "text": "Many researchers feel the need to display an algorithm's superiority by employing some sort\nof hypothesis testing. As non-parametric tests seem better suited for such benchmark results\nthe tests provided in  mlr  are the  Overall Friedman test  and the Friedman-Nemenyi post hoc test .  While the ad hoc  Friedman test  based on  friedman.test \nfrom the  stats  package is testing the hypothesis whether there is a significant difference\nbetween the employed learners, the post hoc  Friedman-Nemenyi test  tests\nfor significant differences between all pairs of learners.  Non parametric  tests often do\nhave less power then their  parametric  counterparts but less assumptions about underlying\ndistributions have to be made. This often means many  data sets  are needed in order to be\nable to show significant differences at reasonable significance levels.  In our example, we want to compare the three learners on the selected data sets.\nFirst we might we want to test the hypothesis whether there is a difference between the learners.  friedmanTestBMR(bmr)\n#  \n#   Friedman rank sum test\n#  \n#  data:  mmce.test.mean and learner.id and task.id\n#  Friedman chi-squared = 5.2, df = 2, p-value = 0.07427  In order to keep the computation time for this tutorial small, the  Learner s\nare only evaluated on five tasks. This also means that we operate on a relatively low significance\nlevel  \\alpha = 0.1 .\nAs we can reject the null hypothesis of the Friedman test at a reasonable significance level\nwe might now want to test where these differences lie exactly.  friedmanPostHocTestBMR(bmr, p.value = 0.1)\n#  \n#   Pairwise comparisons using Nemenyi multiple comparison test \n#               with q approximation for unreplicated blocked data \n#  \n#  data:  mmce.test.mean and learner.id and task.id \n#  \n#               lda   rpart\n#  rpart        0.254 -    \n#  randomForest 0.802 0.069\n#  \n#  P value adjustment method: none  At this level of significance, we can reject the null hypothesis that there exists no\nperformance difference between the decision tree ( rpart ) and the random Forest .", 
            "title": "Comparing learners using hypothesis tests"
        }, 
        {
            "location": "/benchmark_experiments/index.html#critical-differences-diagram", 
            "text": "In order to visualize differently performing learners, a critical differences diagram  can be plotted, using either the\nNemenyi test ( test = \"nemenyi\" ) or the Bonferroni-Dunn test ( test = \"bd\" ).  The mean rank of learners is displayed on the x-axis.   Choosing  test = \"nemenyi\"  compares all pairs of  Learner s to each other, thus\n  the output are groups of not significantly different learners. The diagram connects all groups\n  of learners where the mean ranks do not differ by more than the critical differences. Learners\n  that are not connected by a bar are significantly different, and the learner(s) with the\n  lower mean rank can be considered \"better\" at the chosen significance level.  Choosing  test = \"bd\"  performs a  pairwise comparison with a baseline . An interval which\n  extends by the given  critical difference  in both directions is drawn around the\n   Learner  chosen as baseline, though only comparisons with the baseline are\n  possible. All learners within the interval are not significantly different, while the\n  baseline can be considered better or worse than a given learner which is outside of the\n  interval.   The critical difference  \\mathit{CD}  is calculated by \\mathit{CD} = q_\\alpha \\cdot \\sqrt{\\frac{k(k+1)}{6N}}, \nwhere  N  denotes the number of tasks,  k  is the number of learners, and q_\\alpha  comes from the studentized range statistic divided by  \\sqrt{2} .\nFor details see  Demsar (2006) .  Function  generateCritDifferencesData  does all necessary calculations while function plotCritDifferences  draws the plot. See the tutorial page about  visualization \nfor details on data generation and plotting functions.  ## Nemenyi test\ng = generateCritDifferencesData(bmr, p.value = 0.1, test =  nemenyi )\nplotCritDifferences(g) + coord_cartesian(xlim = c(-1,5), ylim = c(0,2))   ## Bonferroni-Dunn test\ng = generateCritDifferencesData(bmr, p.value = 0.1, test =  bd , baseline =  randomForest )\nplotCritDifferences(g) + coord_cartesian(xlim = c(-1,5), ylim = c(0,2))", 
            "title": "Critical differences diagram"
        }, 
        {
            "location": "/benchmark_experiments/index.html#custom-plots", 
            "text": "You can easily generate your own visualizations by customizing the  ggplot \nobjects returned by the plots above, retrieve the data from the  ggplot \nobjects and use them as basis for your own plots, or rely on the  data.frame s\nreturned by  getBMRPerformances  or  getBMRAggrPerformances . Here are some examples.  Instead of boxplots (as in  plotBMRBoxplots ) we could create density plots\nto show the performance values resulting from individual resampling iterations.  perf = getBMRPerformances(bmr, as.df = TRUE)\n\n## Density plots for two tasks\nqplot(mmce, colour = learner.id, facets = . ~ task.id,\n  data = perf[perf$task.id %in% c( iris_example ,  Sonar_example ),], geom =  density ) +\n  theme(strip.text.x = element_text(size = 8))   In order to plot multiple performance measures in parallel,  perf  is reshaped to long format.\nBelow we generate grouped boxplots showing the error rate ( mmce ) and the\ntraining time  timetrain .  ## Compare mmce and timetrain\ndf = reshape2::melt(perf, id.vars = c( task.id ,  learner.id ,  iter ))\ndf = df[df$variable !=  ber ,]\nhead(df)\n#         task.id learner.id iter variable     value\n#  1 iris_example        lda    1     mmce 0.0000000\n#  2 iris_example        lda    2     mmce 0.1333333\n#  3 iris_example        lda    3     mmce 0.0000000\n#  4 iris_example        lda    4     mmce 0.0000000\n#  5 iris_example        lda    5     mmce 0.0000000\n#  6 iris_example        lda    6     mmce 0.0000000\n\nqplot(variable, value, data = df, colour = learner.id, geom =  boxplot ,\n  xlab =  measure , ylab =  performance ) +\n  facet_wrap(~ task.id, nrow = 2)   It might also be useful to assess if learner performances in single resampling iterations,\ni.e., in one fold, are related.\nThis might help to gain further insight, for example by having a closer look at train and\ntest sets from iterations where one learner performs exceptionally well while another one\nis fairly bad.\nMoreover, this might be useful for the construction of ensembles of learning algorithms.\nBelow, function  ggpairs  from package  GGally  is used to generate a scatterplot\nmatrix of mean misclassification errors ( mmce ) on the  Sonar \ndata set.  perf = getBMRPerformances(bmr, task.id =  Sonar_example , as.df = TRUE)\ndf = reshape2::melt(perf, id.vars = c( task.id ,  learner.id ,  iter ))\ndf = df[df$variable ==  mmce ,]\ndf = reshape2::dcast(df, task.id + iter ~ variable + learner.id)\nhead(df)\n#          task.id iter  mmce_lda mmce_rpart mmce_randomForest\n#  1 Sonar_example    1 0.2857143  0.2857143        0.14285714\n#  2 Sonar_example    2 0.2380952  0.2380952        0.23809524\n#  3 Sonar_example    3 0.3333333  0.2857143        0.28571429\n#  4 Sonar_example    4 0.2380952  0.3333333        0.04761905\n#  5 Sonar_example    5 0.1428571  0.2857143        0.19047619\n#  6 Sonar_example    6 0.4000000  0.4500000        0.25000000\n\nGGally::ggpairs(df, 3:5)", 
            "title": "Custom plots"
        }, 
        {
            "location": "/benchmark_experiments/index.html#further-comments", 
            "text": "Note that for supervised classification  mlr  offers some more plots that operate\n  on  BenchmarkResult  objects and allow you to compare the performance of learning\n  algorithms.\n  See for example the tutorial page on  ROC curves  and functions\n   generateThreshVsPerfData ,  plotROCCurves , and  plotViperCharts  as well as the\n  page about  classifier calibration  and function  generateCalibrationData .  In the examples shown in this section we applied \"raw\" learning algorithms, but often things\n  are more complicated.\n  At the very least, many learners have hyperparameters that need to be tuned to get sensible\n  results.\n  Reliable performance estimates can be obtained by  nested resampling ,\n  i.e., by doing the tuning in an\n  inner resampling loop while estimating the performance in an outer loop.\n  Moreover, you might want to combine learners with pre-processing steps like imputation, scaling,\n  outlier removal, dimensionality reduction or feature selection and so on.\n  All this can be easily done using  mlr 's wrapper functionality.\n  The general principle is explained in the section about  wrapped learners  in the\n  Advanced part of this tutorial. There are also several sections devoted to common pre-processing\n  steps.  Benchmark experiments can very quickly become computationally demanding.  mlr  offers\n  some possibilities for  parallelization .", 
            "title": "Further comments"
        }, 
        {
            "location": "/benchmark_experiments/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## Two learners to be compared \nlrns = list(makeLearner( classif.lda ), makeLearner( classif.rpart )) \n\n## Choose the resampling strategy \nrdesc = makeResampleDesc( Holdout ) \n\n## Conduct the benchmark experiment \nbmr = benchmark(lrns, sonar.task, rdesc) \n\nbmr \n## Vector of strings \nlrns = c( classif.lda ,  classif.rpart ) \n\n## A mixed list of Learner objects and strings works, too \nlrns = list(makeLearner( classif.lda , predict.type =  prob ),  classif.rpart ) \n\nbmr = benchmark(lrns, sonar.task, rdesc) \n\nbmr \ngetBMRPerformances(bmr) \n\ngetBMRAggrPerformances(bmr) \ngetBMRPerformances(bmr, drop = TRUE) \ngetBMRPerformances(bmr, as.df = TRUE) \n\ngetBMRAggrPerformances(bmr, as.df = TRUE) \ngetBMRPredictions(bmr) \n\nhead(getBMRPredictions(bmr, as.df = TRUE)) \nhead(getBMRPredictions(bmr, learner.ids =  classif.rpart , as.df = TRUE)) \ngetBMRTaskIds(bmr) \n\ngetBMRLearnerIds(bmr) \n\ngetBMRMeasureIds(bmr) \ngetBMRModels(bmr) \n\ngetBMRModels(bmr, drop = TRUE) \n\ngetBMRModels(bmr, learner.ids =  classif.lda ) \ngetBMRLearners(bmr) \n\ngetBMRMeasures(bmr) \n## First benchmark result \nbmr \n\n## Benchmark experiment for the additional learners \nlrns2 = list(makeLearner( classif.randomForest ), makeLearner( classif.qda )) \nbmr2 = benchmark(lrns2, sonar.task, rdesc, show.info = FALSE) \nbmr2 \n\n## Merge the results \nmergeBenchmarkResults(list(bmr, bmr2)) \nrin = getBMRPredictions(bmr)[[1]][[1]]$instance \nrin \n\n## Benchmark experiment for the additional random forest \nbmr3 = benchmark(lrns2, sonar.task, rin, show.info = FALSE) \nbmr3 \n\n## Merge the results \nmergeBenchmarkResults(list(bmr, bmr3)) \nset.seed(4444) \n## Create a list of learners \nlrns = list( \n  makeLearner( classif.lda , id =  lda ), \n  makeLearner( classif.rpart , id =  rpart ), \n  makeLearner( classif.randomForest , id =  randomForest ) \n) \n\n## Get additional Tasks from package mlbench \nring.task = convertMLBenchObjToTask( mlbench.ringnorm , n = 600) \nwave.task = convertMLBenchObjToTask( mlbench.waveform , n = 600) \n\ntasks = list(iris.task, sonar.task, pid.task, ring.task, wave.task) \nrdesc = makeResampleDesc( CV , iters = 10) \nmeas = list(mmce, ber, timetrain) \nbmr = benchmark(lrns, tasks, rdesc, meas, show.info = FALSE) \nbmr \nperf = getBMRPerformances(bmr, as.df = TRUE) \nhead(perf) \nplotBMRBoxplots(bmr, measure = mmce) \nplotBMRBoxplots(bmr, measure = ber, style =  violin , pretty.names = FALSE) + \n  aes(color = learner.id) + \n  theme(strip.text.x = element_text(size = 8)) \nmmce$name \n\nmmce$id \n\ngetBMRLearnerIds(bmr) \n\ngetBMRLearnerShortNames(bmr) \nplt = plotBMRBoxplots(bmr, measure = mmce) \nhead(plt$data) \n\nlevels(plt$data$task.id) = c( Iris ,  Ringnorm ,  Waveform ,  Diabetes ,  Sonar ) \nlevels(plt$data$learner.id) = c( LDA ,  CART ,  RF ) \n\nplt + ylab( Error rate ) \nplotBMRSummary(bmr) \nm = convertBMRToRankMatrix(bmr, mmce) \nm \nplotBMRRanksAsBarChart(bmr, pos =  tile ) \nplotBMRSummary(bmr, trafo =  rank , jitter = 0) \nplotBMRRanksAsBarChart(bmr) \nplotBMRRanksAsBarChart(bmr, pos =  dodge ) \nfriedmanTestBMR(bmr) \nfriedmanPostHocTestBMR(bmr, p.value = 0.1) \n## Nemenyi test \ng = generateCritDifferencesData(bmr, p.value = 0.1, test =  nemenyi ) \nplotCritDifferences(g) + coord_cartesian(xlim = c(-1,5), ylim = c(0,2)) \n## Bonferroni-Dunn test \ng = generateCritDifferencesData(bmr, p.value = 0.1, test =  bd , baseline =  randomForest ) \nplotCritDifferences(g) + coord_cartesian(xlim = c(-1,5), ylim = c(0,2)) \nperf = getBMRPerformances(bmr, as.df = TRUE) \n\n## Density plots for two tasks \nqplot(mmce, colour = learner.id, facets = . ~ task.id, \n  data = perf[perf$task.id %in% c( iris_example ,  Sonar_example ),], geom =  density ) + \n  theme(strip.text.x = element_text(size = 8)) \n## Compare mmce and timetrain \ndf = reshape2::melt(perf, id.vars = c( task.id ,  learner.id ,  iter )) \ndf = df[df$variable !=  ber ,] \nhead(df) \n\nqplot(variable, value, data = df, colour = learner.id, geom =  boxplot , \n  xlab =  measure , ylab =  performance ) + \n  facet_wrap(~ task.id, nrow = 2) \nperf = getBMRPerformances(bmr, task.id =  Sonar_example , as.df = TRUE) \ndf = reshape2::melt(perf, id.vars = c( task.id ,  learner.id ,  iter )) \ndf = df[df$variable ==  mmce ,] \ndf = reshape2::dcast(df, task.id + iter ~ variable + learner.id) \nhead(df) \n\nGGally::ggpairs(df, 3:5)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/parallelization/index.html", 
            "text": "Parallelization\n\n\nR\n by default does not make use of parallelization.\nWith the integration of \nparallelMap\n into \nmlr\n, it becomes easy to activate the parallel\ncomputing capabilities already supported by \nmlr\n.\n\nparallelMap\n works with all major parallelization backends: local multicore execution using\n\nparallel\n, socket and MPI clusters using \nsnow\n, makeshift SSH-clusters using \nBatchJobs\n\nand high performance computing clusters (managed by a scheduler like SLURM, Torque/PBS, SGE or LSF)\nalso using \nBatchJobs\n.\n\n\nAll you have to do is select a backend by calling one of the\n\nparallelStart*\n functions.\nThe first loop \nmlr\n encounters which is marked as parallel executable will be automatically parallelized.\nIt is good practice to call \nparallelStop\n at the end of your script.\n\n\nlibrary(\nparallelMap\n)\nparallelStartSocket(2)\n#\n Starting parallelization in mode=socket with cpus=2.\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(\nclassif.lda\n, iris.task, rdesc)\n#\n Exporting objects to slaves for mode socket: .mlr.slave.options\n#\n Resampling: cross-validation\n#\n Measures:             mmce\n#\n Mapping in parallel: mode = socket; cpus = 2; elements = 3.\n#\n \n#\n Aggregated Result: mmce.test.mean=0.0200000\n#\n \n\nparallelStop()\n#\n Stopped parallelization. All cleaned up.\n\n\n\n\nOn Linux or Mac OS X, you may want to use\n\nparallelStartMulticore\n instead.\n\n\nParallelization levels\n\n\nWe offer different parallelization levels for fine grained control over the parallelization.\nE.g., if you do not want to parallelize the \nbenchmark\n function because it has only very\nfew iterations but want to parallelize the \nresampling\n of each learner instead,\nyou can specifically pass the \nlevel\n \n\"mlr.resample\"\n to the \nparallelStart*\n\nfunction.\nCurrently the following levels are supported:\n\n\nparallelGetRegisteredLevels()\n#\n mlr: mlr.benchmark, mlr.resample, mlr.selectFeatures, mlr.tuneParams, mlr.ensemble\n\n\n\n\nFor further details please see the \nparallelization\n documentation page.\n\n\nCustom learners and parallelization\n\n\nIf you have \nimplemented a custom learner yourself\n, locally, you currently\nneed to export this to the slave.\nSo if you see an error after calling, e.g., a parallelized version of \nresample\n like this:\n\n\nno applicable method for 'trainLearner' applied to an object of class \nmy_new_learner\n\n\n\n\n\nsimply add the following line somewhere after calling \nparallelStart\n.\n\n\nparallelExport(\ntrainLearner.\nmy_new_learner\n, \npredictLearner.\nmy_new_learner\n)\n\n\n\n\nThe end\n\n\nFor further details, consult the\n\nparallelMap tutorial\n and \nhelp\n.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nlibrary(\nparallelMap\n) \nparallelStartSocket(2) \n\nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nr = resample(\nclassif.lda\n, iris.task, rdesc) \n\nparallelStop() \nparallelGetRegisteredLevels()", 
            "title": "Parallelization"
        }, 
        {
            "location": "/parallelization/index.html#parallelization", 
            "text": "R  by default does not make use of parallelization.\nWith the integration of  parallelMap  into  mlr , it becomes easy to activate the parallel\ncomputing capabilities already supported by  mlr . parallelMap  works with all major parallelization backends: local multicore execution using parallel , socket and MPI clusters using  snow , makeshift SSH-clusters using  BatchJobs \nand high performance computing clusters (managed by a scheduler like SLURM, Torque/PBS, SGE or LSF)\nalso using  BatchJobs .  All you have to do is select a backend by calling one of the parallelStart*  functions.\nThe first loop  mlr  encounters which is marked as parallel executable will be automatically parallelized.\nIt is good practice to call  parallelStop  at the end of your script.  library( parallelMap )\nparallelStartSocket(2)\n#  Starting parallelization in mode=socket with cpus=2.\n\nrdesc = makeResampleDesc( CV , iters = 3)\nr = resample( classif.lda , iris.task, rdesc)\n#  Exporting objects to slaves for mode socket: .mlr.slave.options\n#  Resampling: cross-validation\n#  Measures:             mmce\n#  Mapping in parallel: mode = socket; cpus = 2; elements = 3.\n#  \n#  Aggregated Result: mmce.test.mean=0.0200000\n#  \n\nparallelStop()\n#  Stopped parallelization. All cleaned up.  On Linux or Mac OS X, you may want to use parallelStartMulticore  instead.", 
            "title": "Parallelization"
        }, 
        {
            "location": "/parallelization/index.html#parallelization-levels", 
            "text": "We offer different parallelization levels for fine grained control over the parallelization.\nE.g., if you do not want to parallelize the  benchmark  function because it has only very\nfew iterations but want to parallelize the  resampling  of each learner instead,\nyou can specifically pass the  level   \"mlr.resample\"  to the  parallelStart* \nfunction.\nCurrently the following levels are supported:  parallelGetRegisteredLevels()\n#  mlr: mlr.benchmark, mlr.resample, mlr.selectFeatures, mlr.tuneParams, mlr.ensemble  For further details please see the  parallelization  documentation page.", 
            "title": "Parallelization levels"
        }, 
        {
            "location": "/parallelization/index.html#custom-learners-and-parallelization", 
            "text": "If you have  implemented a custom learner yourself , locally, you currently\nneed to export this to the slave.\nSo if you see an error after calling, e.g., a parallelized version of  resample  like this:  no applicable method for 'trainLearner' applied to an object of class  my_new_learner   simply add the following line somewhere after calling  parallelStart .  parallelExport( trainLearner. my_new_learner ,  predictLearner. my_new_learner )", 
            "title": "Custom learners and parallelization"
        }, 
        {
            "location": "/parallelization/index.html#the-end", 
            "text": "For further details, consult the parallelMap tutorial  and  help .", 
            "title": "The end"
        }, 
        {
            "location": "/parallelization/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  library( parallelMap ) \nparallelStartSocket(2) \n\nrdesc = makeResampleDesc( CV , iters = 3) \nr = resample( classif.lda , iris.task, rdesc) \n\nparallelStop() \nparallelGetRegisteredLevels()", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/visualization/index.html", 
            "text": "Visualization\n\n\nGeneration and plotting functions\n\n\nmlr\n's visualization capabilities rely on \ngeneration functions\n which generate data for\nplots, and \nplotting functions\n which plot this output using either \nggplot2\n or \nggvis\n\n(the latter being currently experimental).\n\n\nThis separation allows users to easily make custom visualizations by\ntaking advantage of the generation functions. The only data transformation that is handled inside\nplotting functions is reshaping. The reshaped data is also accessible by calling the plotting\nfunctions and then extracting the data from the \nggplot\n object.\n\n\nThe functions are named accordingly.\n\n\n\n\nNames of generation functions start with \ngenerate\n and are followed by a title-case description of\n  their \nFunctionPurpose\n, followed by \nData\n, i.e., \ngenerateFunctionPurposeData\n.\n  These functions output objects of class \nFunctionPurposeData\n.\n\n\nPlotting functions are prefixed by \nplot\n followed by their purpose, i.e., \nplotFunctionPurpose\n.\n\n\nggvis\n plotting functions have an additional suffix \nGGVIS\n, i.e., \nplotFunctionPurposeGGVIS\n.\n\n\n\n\nSome examples\n\n\nIn the example below we create a plot of classifier performance as function of the decision\nthreshold for the binary classification problem \nsonar.task\n.\nThe generation function \ngenerateThreshVsPerfData\n creates an object of class\n\nThreshVsPerfData\n which contains the data for the plot in slot\n\n$data\n.\n\n\nlrn = makeLearner(\nclassif.lda\n, predict.type = \nprob\n)\nn = getTaskSize(sonar.task)\nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))\npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))\nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))\n\nclass(d)\n#\n [1] \nThreshVsPerfData\n\n\nhead(d$data)\n#\n         fpr       fnr      mmce  threshold\n#\n 1 1.0000000 0.0000000 0.4615385 0.00000000\n#\n 2 0.3541667 0.1964286 0.2692308 0.01010101\n#\n 3 0.3333333 0.2321429 0.2788462 0.02020202\n#\n 4 0.3333333 0.2321429 0.2788462 0.03030303\n#\n 5 0.3333333 0.2321429 0.2788462 0.04040404\n#\n 6 0.3125000 0.2321429 0.2692308 0.05050505\n\n\n\n\nFor plotting we can use the built-in \nmlr\n function \nplotThreshVsPerf\n.\n\n\nplotThreshVsPerf(d)\n\n\n\n\n\n\nNote that by default the \nMeasure\n \nname\ns are used to annotate the panels.\n\n\nfpr$name\n#\n [1] \nFalse positive rate\n\n\nfpr$id\n#\n [1] \nfpr\n\n\n\n\n\nThis does not only apply to \nplotThreshVsPerf\n, but to other plot functions that\nshow performance measures as well, for example \nplotLearningCurve\n.\nYou can use the \nid\ns instead of the names by setting \npretty.names = FALSE\n.\n\n\nCustomizing plots\n\n\nAs mentioned above it is easily possible to customize the built-in plots or making your\nown visualizations from scratch based on the generated data.\n\n\nWhat will probably come up most often is changing labels and annotations.\nGenerally, this can be done by manipulating the \nggplot\n object,\nin this example the object returned by \nplotThreshVsPerf\n, using the usual \nggplot2\n\nfunctions like \nylab\n or \nlabeller\n.\nMoreover, you can change the underlying data, either \nd$data\n (resulting from\n\ngenerateThreshVsPerfData\n) or the possibly reshaped data contained in the\n\nggplot\n object (resulting from \nplotThreshVsPerf\n), most often by\nrenaming columns or factor levels.\n\n\nBelow are two examples of how to alter the axis and panel labels of the above plot.\n\n\nImagine you want to change the order of the panels and also are not satisfied with the\npanel names, for example you find that \"Mean misclassification error\" is too long and you\nprefer \"Error rate\" instead. Moreover, you want the error rate to be displayed first.\n\n\nplt = plotThreshVsPerf(d, pretty.names = FALSE)\n\n## Reshaped version of the underlying data d\nhead(plt$data)\n#\n    threshold measure performance\n#\n 1 0.00000000     fpr   1.0000000\n#\n 2 0.01010101     fpr   0.3541667\n#\n 3 0.02020202     fpr   0.3333333\n#\n 4 0.03030303     fpr   0.3333333\n#\n 5 0.04040404     fpr   0.3333333\n#\n 6 0.05050505     fpr   0.3125000\n\nlevels(plt$data$measure)\n#\n [1] \nfpr\n  \nfnr\n  \nmmce\n\n\n## Rename and reorder factor levels\nplt$data$measure = factor(plt$data$measure, levels = c(\nmmce\n, \nfpr\n, \nfnr\n),\n  labels = c(\nError rate\n, \nFalse positive rate\n, \nFalse negative rate\n))\nplt = plt + xlab(\nCutoff\n) + ylab(\nPerformance\n)\nplt\n\n\n\n\n\n\nUsing the \nlabeller\n function requires calling\n\nfacet_wrap\n (or \nfacet_grid\n), which can be\nuseful if you want to change how the panels are positioned (number of rows and columns)\nor influence the axis limits.\n\n\nplt = plotThreshVsPerf(d, pretty.names = FALSE)\n\nmeasure_names = c(\n  fpr = \nFalse positive rate\n,\n  fnr = \nFalse negative rate\n,\n  mmce = \nError rate\n\n)\n## Manipulate the measure names via the labeller function and\n## arrange the panels in two columns and choose common axis limits for all panels\nplt = plt + facet_wrap( ~ measure, labeller = labeller(measure = measure_names), ncol = 2)\nplt = plt + xlab(\nDecision threshold\n) + ylab(\nPerformance\n)\nplt\n\n\n\n\n\n\nInstead of using the built-in function \nplotThreshVsPerf\n we could also manually create\nthe plot based on the output of \ngenerateThreshVsPerfData\n: in this case to plot only one\nmeasure.\n\n\nggplot(d$data, aes(threshold, fpr)) + geom_line()\n\n\n\n\n\n\nThe decoupling of generation and plotting functions is especially practical if you\nprefer traditional \ngraphics\n or \nlattice\n. Here is a \nlattice\n plot which gives a\nresult similar to that of \nplotThreshVsPerf\n.\n\n\nlattice::xyplot(fpr + fnr + mmce ~ threshold, data = d$data, type = \nl\n, ylab = \nperformance\n,\n  outer = TRUE, scales = list(relation = \nfree\n),\n  strip = strip.custom(factor.levels = sapply(d$measures, function(x) x$name),\n    par.strip.text = list(cex = 0.8)))\n\n\n\n\n\n\nLet's conclude with a brief look on a second example.\nHere we use \nplotPartialDependence\n but extract the data from the \nggplot\n\nobject \nplt\nand use it to create a traditional \ngraphics::plot\n, additional to the \nggplot2\n\nplot.\n\n\nsonar = getTaskData(sonar.task)\npd = generatePartialDependenceData(mod, sonar, \nV11\n)\nplt = plotPartialDependence(pd)\nhead(plt$data)\n#\n   Class Probability Feature     Value\n#\n 1     M   0.2737158     V11 0.0289000\n#\n 2     M   0.3689970     V11 0.1072667\n#\n 3     M   0.4765742     V11 0.1856333\n#\n 4     M   0.5741233     V11 0.2640000\n#\n 5     M   0.6557857     V11 0.3423667\n#\n 6     M   0.7387962     V11 0.4207333\n\nplt\n\n\n\n\n\n\nplot(Probability ~ Value, data = plt$data, type = \nb\n, xlab = plt$data$Feature[1])\n\n\n\n\n\n\nAvailable generation and plotting functions\n\n\nBelow the currently available generation and plotting functions are listed and tutorial\npages that provide in depth descriptions of the listed functions are referenced.\n\n\nNote that some plots, e.g., \nplotTuneMultiCritResult\n are not mentioned here since they lack\na generation function.\nBoth \nplotThreshVsPerf\n and \nplotROCCurves\n operate on the result of \ngenerateThreshVsPerfData\n.\nFunctions \nplotPartialDependence\n and \nplotPartialDependenceGGVIS\n can be applied to the\nresults of both \ngeneratePartialDependenceData\n and \ngenerateFunctionalANOVAData\n.\n\n\nThe \nggvis\n functions are experimental and are subject to change, though they should work.\nMost generate interactive \nshiny\n applications, that automatically start and run locally.\n\n\n\n\n\n\n\n\ngeneration function\n\n\nggplot2 plotting function\n\n\nggvis plotting function\n\n\ntutorial pages\n\n\n\n\n\n\n\n\n\n\ngenerateThreshVsPerfData\n\n\nplotThresVsPerf\n\n\nplotThreshVsPerfGGVIS\n\n\nPerformance\n\n\n\n\n\n\n\n\nplotROCCurves\n\n\n--\n\n\nROC Analysis\n\n\n\n\n\n\ngenerateCritDifferencesData\n\n\nplotCritDifferences\n\n\n--\n\n\nBenchmark Experiments\n\n\n\n\n\n\ngenerateHyperParsEffectData\n\n\nplotHyperParsEffect\n\n\n\n\nTuning\n, \nHyperparameter Tuning Effects\n\n\n\n\n\n\ngenerateFilterValuesData\n\n\nplotFilterValues\n\n\nplotFilterValuesGGVIS\n\n\nFeature Selection\n\n\n\n\n\n\ngenerateLearningCurveData\n\n\nplotLearningCurve\n\n\nplotLearningCurveGGVIS\n\n\nLearning Curves\n\n\n\n\n\n\ngeneratePartialDependenceData\n\n\nplotPartialDependence\n\n\nplotPartialDependenceGGVIS\n\n\nPartial Dependence Plots\n\n\n\n\n\n\ngenerateFunctionalANOVAData\n\n\n\n\n\n\n\n\n\n\n\n\ngenerateCalibrationData\n\n\nplotCalibration\n\n\n--\n\n\nClassifier Calibration Plots\n\n\n\n\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nlrn = makeLearner(\nclassif.lda\n, predict.type = \nprob\n) \nn = getTaskSize(sonar.task) \nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2)) \npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2)) \nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce)) \n\nclass(d) \n\nhead(d$data) \nplotThreshVsPerf(d) \nfpr$name \n\nfpr$id \nplt = plotThreshVsPerf(d, pretty.names = FALSE) \n\n## Reshaped version of the underlying data d \nhead(plt$data) \n\nlevels(plt$data$measure) \n\n## Rename and reorder factor levels \nplt$data$measure = factor(plt$data$measure, levels = c(\nmmce\n, \nfpr\n, \nfnr\n), \n  labels = c(\nError rate\n, \nFalse positive rate\n, \nFalse negative rate\n)) \nplt = plt + xlab(\nCutoff\n) + ylab(\nPerformance\n) \nplt \nplt = plotThreshVsPerf(d, pretty.names = FALSE) \n\nmeasure_names = c( \n  fpr = \nFalse positive rate\n, \n  fnr = \nFalse negative rate\n, \n  mmce = \nError rate\n \n) \n## Manipulate the measure names via the labeller function and \n## arrange the panels in two columns and choose common axis limits for all panels \nplt = plt + facet_wrap( ~ measure, labeller = labeller(measure = measure_names), ncol = 2) \nplt = plt + xlab(\nDecision threshold\n) + ylab(\nPerformance\n) \nplt \nggplot(d$data, aes(threshold, fpr)) + geom_line() \nlattice::xyplot(fpr + fnr + mmce ~ threshold, data = d$data, type = \nl\n, ylab = \nperformance\n, \n  outer = TRUE, scales = list(relation = \nfree\n), \n  strip = strip.custom(factor.levels = sapply(d$measures, function(x) x$name), \n    par.strip.text = list(cex = 0.8))) \nsonar = getTaskData(sonar.task) \npd = generatePartialDependenceData(mod, sonar, \nV11\n) \nplt = plotPartialDependence(pd) \nhead(plt$data) \n\nplt \nplot(Probability ~ Value, data = plt$data, type = \nb\n, xlab = plt$data$Feature[1])", 
            "title": "Visualization"
        }, 
        {
            "location": "/visualization/index.html#visualization", 
            "text": "", 
            "title": "Visualization"
        }, 
        {
            "location": "/visualization/index.html#generation-and-plotting-functions", 
            "text": "mlr 's visualization capabilities rely on  generation functions  which generate data for\nplots, and  plotting functions  which plot this output using either  ggplot2  or  ggvis \n(the latter being currently experimental).  This separation allows users to easily make custom visualizations by\ntaking advantage of the generation functions. The only data transformation that is handled inside\nplotting functions is reshaping. The reshaped data is also accessible by calling the plotting\nfunctions and then extracting the data from the  ggplot  object.  The functions are named accordingly.   Names of generation functions start with  generate  and are followed by a title-case description of\n  their  FunctionPurpose , followed by  Data , i.e.,  generateFunctionPurposeData .\n  These functions output objects of class  FunctionPurposeData .  Plotting functions are prefixed by  plot  followed by their purpose, i.e.,  plotFunctionPurpose .  ggvis  plotting functions have an additional suffix  GGVIS , i.e.,  plotFunctionPurposeGGVIS .", 
            "title": "Generation and plotting functions"
        }, 
        {
            "location": "/visualization/index.html#some-examples", 
            "text": "In the example below we create a plot of classifier performance as function of the decision\nthreshold for the binary classification problem  sonar.task .\nThe generation function  generateThreshVsPerfData  creates an object of class ThreshVsPerfData  which contains the data for the plot in slot $data .  lrn = makeLearner( classif.lda , predict.type =  prob )\nn = getTaskSize(sonar.task)\nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))\npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))\nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))\n\nclass(d)\n#  [1]  ThreshVsPerfData \n\nhead(d$data)\n#          fpr       fnr      mmce  threshold\n#  1 1.0000000 0.0000000 0.4615385 0.00000000\n#  2 0.3541667 0.1964286 0.2692308 0.01010101\n#  3 0.3333333 0.2321429 0.2788462 0.02020202\n#  4 0.3333333 0.2321429 0.2788462 0.03030303\n#  5 0.3333333 0.2321429 0.2788462 0.04040404\n#  6 0.3125000 0.2321429 0.2692308 0.05050505  For plotting we can use the built-in  mlr  function  plotThreshVsPerf .  plotThreshVsPerf(d)   Note that by default the  Measure   name s are used to annotate the panels.  fpr$name\n#  [1]  False positive rate \n\nfpr$id\n#  [1]  fpr   This does not only apply to  plotThreshVsPerf , but to other plot functions that\nshow performance measures as well, for example  plotLearningCurve .\nYou can use the  id s instead of the names by setting  pretty.names = FALSE .", 
            "title": "Some examples"
        }, 
        {
            "location": "/visualization/index.html#customizing-plots", 
            "text": "As mentioned above it is easily possible to customize the built-in plots or making your\nown visualizations from scratch based on the generated data.  What will probably come up most often is changing labels and annotations.\nGenerally, this can be done by manipulating the  ggplot  object,\nin this example the object returned by  plotThreshVsPerf , using the usual  ggplot2 \nfunctions like  ylab  or  labeller .\nMoreover, you can change the underlying data, either  d$data  (resulting from generateThreshVsPerfData ) or the possibly reshaped data contained in the ggplot  object (resulting from  plotThreshVsPerf ), most often by\nrenaming columns or factor levels.  Below are two examples of how to alter the axis and panel labels of the above plot.  Imagine you want to change the order of the panels and also are not satisfied with the\npanel names, for example you find that \"Mean misclassification error\" is too long and you\nprefer \"Error rate\" instead. Moreover, you want the error rate to be displayed first.  plt = plotThreshVsPerf(d, pretty.names = FALSE)\n\n## Reshaped version of the underlying data d\nhead(plt$data)\n#     threshold measure performance\n#  1 0.00000000     fpr   1.0000000\n#  2 0.01010101     fpr   0.3541667\n#  3 0.02020202     fpr   0.3333333\n#  4 0.03030303     fpr   0.3333333\n#  5 0.04040404     fpr   0.3333333\n#  6 0.05050505     fpr   0.3125000\n\nlevels(plt$data$measure)\n#  [1]  fpr    fnr    mmce \n\n## Rename and reorder factor levels\nplt$data$measure = factor(plt$data$measure, levels = c( mmce ,  fpr ,  fnr ),\n  labels = c( Error rate ,  False positive rate ,  False negative rate ))\nplt = plt + xlab( Cutoff ) + ylab( Performance )\nplt   Using the  labeller  function requires calling facet_wrap  (or  facet_grid ), which can be\nuseful if you want to change how the panels are positioned (number of rows and columns)\nor influence the axis limits.  plt = plotThreshVsPerf(d, pretty.names = FALSE)\n\nmeasure_names = c(\n  fpr =  False positive rate ,\n  fnr =  False negative rate ,\n  mmce =  Error rate \n)\n## Manipulate the measure names via the labeller function and\n## arrange the panels in two columns and choose common axis limits for all panels\nplt = plt + facet_wrap( ~ measure, labeller = labeller(measure = measure_names), ncol = 2)\nplt = plt + xlab( Decision threshold ) + ylab( Performance )\nplt   Instead of using the built-in function  plotThreshVsPerf  we could also manually create\nthe plot based on the output of  generateThreshVsPerfData : in this case to plot only one\nmeasure.  ggplot(d$data, aes(threshold, fpr)) + geom_line()   The decoupling of generation and plotting functions is especially practical if you\nprefer traditional  graphics  or  lattice . Here is a  lattice  plot which gives a\nresult similar to that of  plotThreshVsPerf .  lattice::xyplot(fpr + fnr + mmce ~ threshold, data = d$data, type =  l , ylab =  performance ,\n  outer = TRUE, scales = list(relation =  free ),\n  strip = strip.custom(factor.levels = sapply(d$measures, function(x) x$name),\n    par.strip.text = list(cex = 0.8)))   Let's conclude with a brief look on a second example.\nHere we use  plotPartialDependence  but extract the data from the  ggplot \nobject  plt and use it to create a traditional  graphics::plot , additional to the  ggplot2 \nplot.  sonar = getTaskData(sonar.task)\npd = generatePartialDependenceData(mod, sonar,  V11 )\nplt = plotPartialDependence(pd)\nhead(plt$data)\n#    Class Probability Feature     Value\n#  1     M   0.2737158     V11 0.0289000\n#  2     M   0.3689970     V11 0.1072667\n#  3     M   0.4765742     V11 0.1856333\n#  4     M   0.5741233     V11 0.2640000\n#  5     M   0.6557857     V11 0.3423667\n#  6     M   0.7387962     V11 0.4207333\n\nplt   plot(Probability ~ Value, data = plt$data, type =  b , xlab = plt$data$Feature[1])", 
            "title": "Customizing plots"
        }, 
        {
            "location": "/visualization/index.html#available-generation-and-plotting-functions", 
            "text": "Below the currently available generation and plotting functions are listed and tutorial\npages that provide in depth descriptions of the listed functions are referenced.  Note that some plots, e.g.,  plotTuneMultiCritResult  are not mentioned here since they lack\na generation function.\nBoth  plotThreshVsPerf  and  plotROCCurves  operate on the result of  generateThreshVsPerfData .\nFunctions  plotPartialDependence  and  plotPartialDependenceGGVIS  can be applied to the\nresults of both  generatePartialDependenceData  and  generateFunctionalANOVAData .  The  ggvis  functions are experimental and are subject to change, though they should work.\nMost generate interactive  shiny  applications, that automatically start and run locally.     generation function  ggplot2 plotting function  ggvis plotting function  tutorial pages      generateThreshVsPerfData  plotThresVsPerf  plotThreshVsPerfGGVIS  Performance     plotROCCurves  --  ROC Analysis    generateCritDifferencesData  plotCritDifferences  --  Benchmark Experiments    generateHyperParsEffectData  plotHyperParsEffect   Tuning ,  Hyperparameter Tuning Effects    generateFilterValuesData  plotFilterValues  plotFilterValuesGGVIS  Feature Selection    generateLearningCurveData  plotLearningCurve  plotLearningCurveGGVIS  Learning Curves    generatePartialDependenceData  plotPartialDependence  plotPartialDependenceGGVIS  Partial Dependence Plots    generateFunctionalANOVAData       generateCalibrationData  plotCalibration  --  Classifier Calibration Plots", 
            "title": "Available generation and plotting functions"
        }, 
        {
            "location": "/visualization/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  lrn = makeLearner( classif.lda , predict.type =  prob ) \nn = getTaskSize(sonar.task) \nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2)) \npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2)) \nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce)) \n\nclass(d) \n\nhead(d$data) \nplotThreshVsPerf(d) \nfpr$name \n\nfpr$id \nplt = plotThreshVsPerf(d, pretty.names = FALSE) \n\n## Reshaped version of the underlying data d \nhead(plt$data) \n\nlevels(plt$data$measure) \n\n## Rename and reorder factor levels \nplt$data$measure = factor(plt$data$measure, levels = c( mmce ,  fpr ,  fnr ), \n  labels = c( Error rate ,  False positive rate ,  False negative rate )) \nplt = plt + xlab( Cutoff ) + ylab( Performance ) \nplt \nplt = plotThreshVsPerf(d, pretty.names = FALSE) \n\nmeasure_names = c( \n  fpr =  False positive rate , \n  fnr =  False negative rate , \n  mmce =  Error rate  \n) \n## Manipulate the measure names via the labeller function and \n## arrange the panels in two columns and choose common axis limits for all panels \nplt = plt + facet_wrap( ~ measure, labeller = labeller(measure = measure_names), ncol = 2) \nplt = plt + xlab( Decision threshold ) + ylab( Performance ) \nplt \nggplot(d$data, aes(threshold, fpr)) + geom_line() \nlattice::xyplot(fpr + fnr + mmce ~ threshold, data = d$data, type =  l , ylab =  performance , \n  outer = TRUE, scales = list(relation =  free ), \n  strip = strip.custom(factor.levels = sapply(d$measures, function(x) x$name), \n    par.strip.text = list(cex = 0.8))) \nsonar = getTaskData(sonar.task) \npd = generatePartialDependenceData(mod, sonar,  V11 ) \nplt = plotPartialDependence(pd) \nhead(plt$data) \n\nplt \nplot(Probability ~ Value, data = plt$data, type =  b , xlab = plt$data$Feature[1])", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/configureMlr/index.html", 
            "text": "Configuring mlr\n\n\nmlr\n is designed to make usage errors due to typos or invalid parameter values\nas unlikely as possible.\nOccasionally, you might want to break those barriers and get full access, for example to\nreduce the amount of output on the console or to turn off checks.\nFor all available options simply refer to the documentation of \nconfigureMlr\n.\nIn the following we show some common use cases.\n\n\nGenerally, function \nconfigureMlr\n permits to set options globally for your current \nR\n\nsession.\n\n\nIt is also possible to set options locally.\n\n\n\n\nAll options referring to the behavior of learners (these are all options except \nshow.info\n)\n  can be set for an individual learner via the \nconfig\n argument of \nmakeLearner\n.\n  The local precedes the global configuration.\n\n\nSome functions like \nresample\n, \nbenchmark\n, \nselectFeatures\n, \ntuneParams\n,\n  and \ntuneParamsMultiCrit\n have a \nshow.info\n flag that controls if progress messages\n  are shown. The default value of \nshow.info\n can be set by \nconfigureMlr\n.\n\n\n\n\nExample: Reducing the output on the console\n\n\nYou are bothered by all the output on the console like in this example?\n\n\nrdesc = makeResampleDesc(\nHoldout\n)\nr = resample(\nclassif.multinom\n, iris.task, rdesc)\n#\n Resampling: holdout\n#\n Measures:             mmce\n#\n # weights:  18 (10 variable)\n#\n initial  value 109.861229 \n#\n iter  10 value 12.256619\n#\n iter  20 value 3.638740\n#\n iter  30 value 3.228628\n#\n iter  40 value 2.951100\n#\n iter  50 value 2.806521\n#\n iter  60 value 2.739076\n#\n iter  70 value 2.522206\n#\n iter  80 value 2.485225\n#\n iter  90 value 2.381397\n#\n iter 100 value 2.360602\n#\n final  value 2.360602 \n#\n stopped after 100 iterations\n#\n [Resample] iter 1:    0.0200000\n#\n \n#\n Aggregated Result: mmce.test.mean=0.0200000\n#\n \n\n\n\n\nYou can suppress the output for this \nLearner\n and this \nresample\n call as follows:\n\n\nlrn = makeLearner(\nclassif.multinom\n, config = list(show.learner.output = FALSE))\nr = resample(lrn, iris.task, rdesc, show.info = FALSE)\n\n\n\n\n(Note that \nmultinom\n has a \ntrace\n switch that can alternatively be used to turn off\nthe progress messages.)\n\n\nTo globally suppress the output for all subsequent learners and calls to \nresample\n,\n\nbenchmark\n etc. do the following:\n\n\nconfigureMlr(show.learner.output = FALSE, show.info = FALSE)\nr = resample(\nclassif.multinom\n, iris.task, rdesc)\n\n\n\n\nAccessing and resetting the configuration\n\n\nFunction \ngetMlrOptions\n returns a \nlist\n with the current configuration.\n\n\ngetMlrOptions()\n#\n $show.info\n#\n [1] FALSE\n#\n \n#\n $on.learner.error\n#\n [1] \nwarn\n\n#\n \n#\n $on.learner.warning\n#\n [1] \nwarn\n\n#\n \n#\n $on.par.without.desc\n#\n [1] \nwarn\n\n#\n \n#\n $on.par.out.of.bounds\n#\n [1] \nstop\n\n#\n \n#\n $on.measure.not.applicable\n#\n [1] \nstop\n\n#\n \n#\n $show.learner.output\n#\n [1] FALSE\n#\n \n#\n $on.error.dump\n#\n [1] FALSE\n\n\n\n\nTo restore the default configuration call \nconfigureMlr\n with an empty argument list.\n\n\nconfigureMlr()\n\n\n\n\ngetMlrOptions()\n#\n $show.info\n#\n [1] TRUE\n#\n \n#\n $on.learner.error\n#\n [1] \nstop\n\n#\n \n#\n $on.learner.warning\n#\n [1] \nwarn\n\n#\n \n#\n $on.par.without.desc\n#\n [1] \nstop\n\n#\n \n#\n $on.par.out.of.bounds\n#\n [1] \nstop\n\n#\n \n#\n $on.measure.not.applicable\n#\n [1] \nstop\n\n#\n \n#\n $show.learner.output\n#\n [1] TRUE\n#\n \n#\n $on.error.dump\n#\n [1] FALSE\n\n\n\n\nExample: Turning off parameter checking\n\n\nIt might happen that you want to set a parameter of a \nLearner\n, but the\nparameter is not registered in the learner's \nparameter set\n\nyet.\nIn this case you might want to \ncontact us\n\nor \nopen an issue\n as well!\nBut until the problem is fixed you can turn off \nmlr\n's parameter checking.\nThe parameter setting will then be passed to the underlying function without further ado.\n\n\n## Support Vector Machine with linear kernel and new parameter 'newParam'\nlrn = makeLearner(\nclassif.ksvm\n, kernel = \nvanilladot\n, newParam = 3)\n#\n Error in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter newParam without available description object!\n#\n Did you mean one of these hyperparameters instead: degree scaled kernel\n#\n You can switch off this check by using configureMlr!\n\n## Turn off parameter checking completely\nconfigureMlr(on.par.without.desc = \nquiet\n)\nlrn = makeLearner(\nclassif.ksvm\n, kernel = \nvanilladot\n, newParam = 3)\ntrain(lrn, iris.task)\n#\n  Setting default kernel parameters\n#\n Model for learner.id=classif.ksvm; learner.class=classif.ksvm\n#\n Trained on: task.id = iris_example; obs = 150; features = 4\n#\n Hyperparameters: fit=FALSE,kernel=vanilladot,newParam=3\n\n## Option \nquiet\n also masks typos\nlrn = makeLearner(\nclassif.ksvm\n, kernl = \nvanilladot\n)\ntrain(lrn, iris.task)\n#\n Model for learner.id=classif.ksvm; learner.class=classif.ksvm\n#\n Trained on: task.id = iris_example; obs = 150; features = 4\n#\n Hyperparameters: fit=FALSE,kernl=vanilladot\n\n## Alternatively turn off parameter checking, but still see warnings\nconfigureMlr(on.par.without.desc = \nwarn\n)\nlrn = makeLearner(\nclassif.ksvm\n, kernl = \nvanilladot\n, newParam = 3)\n#\n Warning in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter kernl without available description object!\n#\n Did you mean one of these hyperparameters instead: kernel nu degree\n#\n You can switch off this check by using configureMlr!\n#\n Warning in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter newParam without available description object!\n#\n Did you mean one of these hyperparameters instead: degree scaled kernel\n#\n You can switch off this check by using configureMlr!\n\ntrain(lrn, iris.task)\n#\n Model for learner.id=classif.ksvm; learner.class=classif.ksvm\n#\n Trained on: task.id = iris_example; obs = 150; features = 4\n#\n Hyperparameters: fit=FALSE,kernl=vanilladot,newParam=3\n\n\n\n\nExample: Handling errors in a learning method\n\n\nIf a learning method throws an error the default behavior of \nmlr\n is to\ngenerate an exception as well.\nHowever, in some situations, for example if you conduct a larger \nbenchmark study\n\nwith multiple data sets and learners, you usually don't want the whole experiment stopped due\nto one error.\nYou can prevent this using the \non.learner.error\n option of \nconfigureMlr\n.\n\n\n## This call gives an error caused by the low number of observations in class \nvirginica\n\ntrain(\nclassif.qda\n, task = iris.task, subset = 1:104)\n#\n Error in qda.default(x, grouping, ...): some group is too small for 'qda'\n\n## Get a warning instead of an error\nconfigureMlr(on.learner.error = \nwarn\n)\nmod = train(\nclassif.qda\n, task = iris.task, subset = 1:104)\n#\n Warning in train(\nclassif.qda\n, task = iris.task, subset = 1:104): Could not train learner classif.qda: Error in qda.default(x, grouping, ...) : \n#\n   some group is too small for 'qda'\n\nmod\n#\n Model for learner.id=classif.qda; learner.class=classif.qda\n#\n Trained on: task.id = iris_example; obs = 104; features = 4\n#\n Hyperparameters: \n#\n Training failed: Error in qda.default(x, grouping, ...) : \n#\n   some group is too small for 'qda'\n#\n \n#\n Training failed: Error in qda.default(x, grouping, ...) : \n#\n   some group is too small for 'qda'\n\n## mod is an object of class FailureModel\nisFailureModel(mod)\n#\n [1] TRUE\n\n## Retrieve the error message\ngetFailureModelMsg(mod)\n#\n [1] \nError in qda.default(x, grouping, ...) : \\n  some group is too small for 'qda'\\n\n\n\n## predict and performance return NA's\npred = predict(mod, iris.task)\npred\n#\n Prediction: 150 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: NA\n#\n   id  truth response\n#\n 1  1 setosa     \nNA\n\n#\n 2  2 setosa     \nNA\n\n#\n 3  3 setosa     \nNA\n\n#\n 4  4 setosa     \nNA\n\n#\n 5  5 setosa     \nNA\n\n#\n 6  6 setosa     \nNA\n\n#\n ... (#rows: 150, #cols: 3)\n\nperformance(pred)\n#\n mmce \n#\n   NA\n\n\n\n\nIf \non.learner.error = \"warn\"\n a warning is issued instead of an exception and an object\nof class \nFailureModel\n is created.\nYou can extract the error message using function \ngetFailureModelMsg\n.\nAll further steps like prediction and performance calculation work and return \nNA's\n.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nrdesc = makeResampleDesc(\nHoldout\n) \nr = resample(\nclassif.multinom\n, iris.task, rdesc) \nlrn = makeLearner(\nclassif.multinom\n, config = list(show.learner.output = FALSE)) \nr = resample(lrn, iris.task, rdesc, show.info = FALSE) \nconfigureMlr(show.learner.output = FALSE, show.info = FALSE) \nr = resample(\nclassif.multinom\n, iris.task, rdesc) \ngetMlrOptions() \nconfigureMlr() \ngetMlrOptions() \n## Support Vector Machine with linear kernel and new parameter 'newParam' \nlrn = makeLearner(\nclassif.ksvm\n, kernel = \nvanilladot\n, newParam = 3) \n\n## Turn off parameter checking completely \nconfigureMlr(on.par.without.desc = \nquiet\n) \nlrn = makeLearner(\nclassif.ksvm\n, kernel = \nvanilladot\n, newParam = 3) \ntrain(lrn, iris.task) \n\n## Option \nquiet\n also masks typos \nlrn = makeLearner(\nclassif.ksvm\n, kernl = \nvanilladot\n) \ntrain(lrn, iris.task) \n\n## Alternatively turn off parameter checking, but still see warnings \nconfigureMlr(on.par.without.desc = \nwarn\n) \nlrn = makeLearner(\nclassif.ksvm\n, kernl = \nvanilladot\n, newParam = 3) \n\ntrain(lrn, iris.task) \n## This call gives an error caused by the low number of observations in class \nvirginica\n \ntrain(\nclassif.qda\n, task = iris.task, subset = 1:104) \n\n## Get a warning instead of an error \nconfigureMlr(on.learner.error = \nwarn\n) \nmod = train(\nclassif.qda\n, task = iris.task, subset = 1:104) \n\nmod \n\n## mod is an object of class FailureModel \nisFailureModel(mod) \n\n## Retrieve the error message \ngetFailureModelMsg(mod) \n\n## predict and performance return NA's \npred = predict(mod, iris.task) \npred \n\nperformance(pred)", 
            "title": "Configuration"
        }, 
        {
            "location": "/configureMlr/index.html#configuring-mlr", 
            "text": "mlr  is designed to make usage errors due to typos or invalid parameter values\nas unlikely as possible.\nOccasionally, you might want to break those barriers and get full access, for example to\nreduce the amount of output on the console or to turn off checks.\nFor all available options simply refer to the documentation of  configureMlr .\nIn the following we show some common use cases.  Generally, function  configureMlr  permits to set options globally for your current  R \nsession.  It is also possible to set options locally.   All options referring to the behavior of learners (these are all options except  show.info )\n  can be set for an individual learner via the  config  argument of  makeLearner .\n  The local precedes the global configuration.  Some functions like  resample ,  benchmark ,  selectFeatures ,  tuneParams ,\n  and  tuneParamsMultiCrit  have a  show.info  flag that controls if progress messages\n  are shown. The default value of  show.info  can be set by  configureMlr .", 
            "title": "Configuring mlr"
        }, 
        {
            "location": "/configureMlr/index.html#example-reducing-the-output-on-the-console", 
            "text": "You are bothered by all the output on the console like in this example?  rdesc = makeResampleDesc( Holdout )\nr = resample( classif.multinom , iris.task, rdesc)\n#  Resampling: holdout\n#  Measures:             mmce\n#  # weights:  18 (10 variable)\n#  initial  value 109.861229 \n#  iter  10 value 12.256619\n#  iter  20 value 3.638740\n#  iter  30 value 3.228628\n#  iter  40 value 2.951100\n#  iter  50 value 2.806521\n#  iter  60 value 2.739076\n#  iter  70 value 2.522206\n#  iter  80 value 2.485225\n#  iter  90 value 2.381397\n#  iter 100 value 2.360602\n#  final  value 2.360602 \n#  stopped after 100 iterations\n#  [Resample] iter 1:    0.0200000\n#  \n#  Aggregated Result: mmce.test.mean=0.0200000\n#    You can suppress the output for this  Learner  and this  resample  call as follows:  lrn = makeLearner( classif.multinom , config = list(show.learner.output = FALSE))\nr = resample(lrn, iris.task, rdesc, show.info = FALSE)  (Note that  multinom  has a  trace  switch that can alternatively be used to turn off\nthe progress messages.)  To globally suppress the output for all subsequent learners and calls to  resample , benchmark  etc. do the following:  configureMlr(show.learner.output = FALSE, show.info = FALSE)\nr = resample( classif.multinom , iris.task, rdesc)", 
            "title": "Example: Reducing the output on the console"
        }, 
        {
            "location": "/configureMlr/index.html#accessing-and-resetting-the-configuration", 
            "text": "Function  getMlrOptions  returns a  list  with the current configuration.  getMlrOptions()\n#  $show.info\n#  [1] FALSE\n#  \n#  $on.learner.error\n#  [1]  warn \n#  \n#  $on.learner.warning\n#  [1]  warn \n#  \n#  $on.par.without.desc\n#  [1]  warn \n#  \n#  $on.par.out.of.bounds\n#  [1]  stop \n#  \n#  $on.measure.not.applicable\n#  [1]  stop \n#  \n#  $show.learner.output\n#  [1] FALSE\n#  \n#  $on.error.dump\n#  [1] FALSE  To restore the default configuration call  configureMlr  with an empty argument list.  configureMlr()  getMlrOptions()\n#  $show.info\n#  [1] TRUE\n#  \n#  $on.learner.error\n#  [1]  stop \n#  \n#  $on.learner.warning\n#  [1]  warn \n#  \n#  $on.par.without.desc\n#  [1]  stop \n#  \n#  $on.par.out.of.bounds\n#  [1]  stop \n#  \n#  $on.measure.not.applicable\n#  [1]  stop \n#  \n#  $show.learner.output\n#  [1] TRUE\n#  \n#  $on.error.dump\n#  [1] FALSE", 
            "title": "Accessing and resetting the configuration"
        }, 
        {
            "location": "/configureMlr/index.html#example-turning-off-parameter-checking", 
            "text": "It might happen that you want to set a parameter of a  Learner , but the\nparameter is not registered in the learner's  parameter set \nyet.\nIn this case you might want to  contact us \nor  open an issue  as well!\nBut until the problem is fixed you can turn off  mlr 's parameter checking.\nThe parameter setting will then be passed to the underlying function without further ado.  ## Support Vector Machine with linear kernel and new parameter 'newParam'\nlrn = makeLearner( classif.ksvm , kernel =  vanilladot , newParam = 3)\n#  Error in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter newParam without available description object!\n#  Did you mean one of these hyperparameters instead: degree scaled kernel\n#  You can switch off this check by using configureMlr!\n\n## Turn off parameter checking completely\nconfigureMlr(on.par.without.desc =  quiet )\nlrn = makeLearner( classif.ksvm , kernel =  vanilladot , newParam = 3)\ntrain(lrn, iris.task)\n#   Setting default kernel parameters\n#  Model for learner.id=classif.ksvm; learner.class=classif.ksvm\n#  Trained on: task.id = iris_example; obs = 150; features = 4\n#  Hyperparameters: fit=FALSE,kernel=vanilladot,newParam=3\n\n## Option  quiet  also masks typos\nlrn = makeLearner( classif.ksvm , kernl =  vanilladot )\ntrain(lrn, iris.task)\n#  Model for learner.id=classif.ksvm; learner.class=classif.ksvm\n#  Trained on: task.id = iris_example; obs = 150; features = 4\n#  Hyperparameters: fit=FALSE,kernl=vanilladot\n\n## Alternatively turn off parameter checking, but still see warnings\nconfigureMlr(on.par.without.desc =  warn )\nlrn = makeLearner( classif.ksvm , kernl =  vanilladot , newParam = 3)\n#  Warning in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter kernl without available description object!\n#  Did you mean one of these hyperparameters instead: kernel nu degree\n#  You can switch off this check by using configureMlr!\n#  Warning in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter newParam without available description object!\n#  Did you mean one of these hyperparameters instead: degree scaled kernel\n#  You can switch off this check by using configureMlr!\n\ntrain(lrn, iris.task)\n#  Model for learner.id=classif.ksvm; learner.class=classif.ksvm\n#  Trained on: task.id = iris_example; obs = 150; features = 4\n#  Hyperparameters: fit=FALSE,kernl=vanilladot,newParam=3", 
            "title": "Example: Turning off parameter checking"
        }, 
        {
            "location": "/configureMlr/index.html#example-handling-errors-in-a-learning-method", 
            "text": "If a learning method throws an error the default behavior of  mlr  is to\ngenerate an exception as well.\nHowever, in some situations, for example if you conduct a larger  benchmark study \nwith multiple data sets and learners, you usually don't want the whole experiment stopped due\nto one error.\nYou can prevent this using the  on.learner.error  option of  configureMlr .  ## This call gives an error caused by the low number of observations in class  virginica \ntrain( classif.qda , task = iris.task, subset = 1:104)\n#  Error in qda.default(x, grouping, ...): some group is too small for 'qda'\n\n## Get a warning instead of an error\nconfigureMlr(on.learner.error =  warn )\nmod = train( classif.qda , task = iris.task, subset = 1:104)\n#  Warning in train( classif.qda , task = iris.task, subset = 1:104): Could not train learner classif.qda: Error in qda.default(x, grouping, ...) : \n#    some group is too small for 'qda'\n\nmod\n#  Model for learner.id=classif.qda; learner.class=classif.qda\n#  Trained on: task.id = iris_example; obs = 104; features = 4\n#  Hyperparameters: \n#  Training failed: Error in qda.default(x, grouping, ...) : \n#    some group is too small for 'qda'\n#  \n#  Training failed: Error in qda.default(x, grouping, ...) : \n#    some group is too small for 'qda'\n\n## mod is an object of class FailureModel\nisFailureModel(mod)\n#  [1] TRUE\n\n## Retrieve the error message\ngetFailureModelMsg(mod)\n#  [1]  Error in qda.default(x, grouping, ...) : \\n  some group is too small for 'qda'\\n \n\n## predict and performance return NA's\npred = predict(mod, iris.task)\npred\n#  Prediction: 150 observations\n#  predict.type: response\n#  threshold: \n#  time: NA\n#    id  truth response\n#  1  1 setosa      NA \n#  2  2 setosa      NA \n#  3  3 setosa      NA \n#  4  4 setosa      NA \n#  5  5 setosa      NA \n#  6  6 setosa      NA \n#  ... (#rows: 150, #cols: 3)\n\nperformance(pred)\n#  mmce \n#    NA  If  on.learner.error = \"warn\"  a warning is issued instead of an exception and an object\nof class  FailureModel  is created.\nYou can extract the error message using function  getFailureModelMsg .\nAll further steps like prediction and performance calculation work and return  NA's .", 
            "title": "Example: Handling errors in a learning method"
        }, 
        {
            "location": "/configureMlr/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  rdesc = makeResampleDesc( Holdout ) \nr = resample( classif.multinom , iris.task, rdesc) \nlrn = makeLearner( classif.multinom , config = list(show.learner.output = FALSE)) \nr = resample(lrn, iris.task, rdesc, show.info = FALSE) \nconfigureMlr(show.learner.output = FALSE, show.info = FALSE) \nr = resample( classif.multinom , iris.task, rdesc) \ngetMlrOptions() \nconfigureMlr() \ngetMlrOptions() \n## Support Vector Machine with linear kernel and new parameter 'newParam' \nlrn = makeLearner( classif.ksvm , kernel =  vanilladot , newParam = 3) \n\n## Turn off parameter checking completely \nconfigureMlr(on.par.without.desc =  quiet ) \nlrn = makeLearner( classif.ksvm , kernel =  vanilladot , newParam = 3) \ntrain(lrn, iris.task) \n\n## Option  quiet  also masks typos \nlrn = makeLearner( classif.ksvm , kernl =  vanilladot ) \ntrain(lrn, iris.task) \n\n## Alternatively turn off parameter checking, but still see warnings \nconfigureMlr(on.par.without.desc =  warn ) \nlrn = makeLearner( classif.ksvm , kernl =  vanilladot , newParam = 3) \n\ntrain(lrn, iris.task) \n## This call gives an error caused by the low number of observations in class  virginica  \ntrain( classif.qda , task = iris.task, subset = 1:104) \n\n## Get a warning instead of an error \nconfigureMlr(on.learner.error =  warn ) \nmod = train( classif.qda , task = iris.task, subset = 1:104) \n\nmod \n\n## mod is an object of class FailureModel \nisFailureModel(mod) \n\n## Retrieve the error message \ngetFailureModelMsg(mod) \n\n## predict and performance return NA's \npred = predict(mod, iris.task) \npred \n\nperformance(pred)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/wrapper/index.html", 
            "text": "Wrapper\n\n\nWrappers can be employed to extend integrated \nlearners\n with new functionality.\nThe broad scope of operations and methods which are implemented as wrappers underline the flexibility of the wrapping approach:\n\n\n\n\nData preprocessing\n\n\nImputation\n\n\nBagging\n\n\nTuning\n\n\nFeature selection\n\n\nCost-sensitive classification\n\n\nOver- and undersampling\n for imbalanced classification problems\n\n\nMulticlass extension\n for binary-class learners\n\n\nMultilabel classification\n\n\n\n\nAll these operations and methods have a few things in common:\nFirst, they all wrap around \nmlr\n \nlearners\n and they return a new learner.\nTherefore learners can be wrapped multiple times.\nSecond, they are implemented using a \ntrain\n (pre-model hook) and \npredict\n (post-model hook) method.\n\n\nExample: Bagging wrapper\n\n\nIn this section we exemplary describe the bagging wrapper to create a random forest which supports weights.\nTo achieve that we combine several decision trees from the \nrpart\n package to create our own custom random forest.\n\n\nFirst, we create a weighted toy task.\n\n\ndata(iris)\ntask = makeClassifTask(data = iris, target = \nSpecies\n, weights = as.integer(iris$Species))\n\n\n\n\nNext, we use \nmakeBaggingWrapper\n to create the base learners and the bagged learner.\nWe choose to set equivalents of \nntree\n (100 base learners) and \nmtry\n (proportion of randomly selected features).\n\n\nbase.lrn = makeLearner(\nclassif.rpart\n)\nwrapped.lrn = makeBaggingWrapper(base.lrn, bw.iters = 100, bw.feats = 0.5)\nprint(wrapped.lrn)\n#\n Learner classif.rpart.bagged from package rpart\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: BaggingWrapper\n#\n Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp\n#\n Predict-Type: response\n#\n Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5\n\n\n\n\nAs we can see in the output, the wrapped learner inherited all properties from the base learner, especially the \"weights\" attribute is still present.\nWe can use this newly constructed learner like all base learners, i.e. we can use it in \ntrain\n, \nbenchmark\n, \nresample\n, etc.\n\n\nbenchmark(tasks = task, learners = list(base.lrn, wrapped.lrn))\n#\n Task: iris, Learner: classif.rpart\n#\n Resampling: cross-validation\n#\n Measures:             mmce\n#\n [Resample] iter 1:    0.2000000\n#\n [Resample] iter 2:    0.0666667\n#\n [Resample] iter 3:    0.0000000\n#\n [Resample] iter 4:    0.0666667\n#\n [Resample] iter 5:    0.0000000\n#\n [Resample] iter 6:    0.0666667\n#\n [Resample] iter 7:    0.0000000\n#\n [Resample] iter 8:    0.1333333\n#\n [Resample] iter 9:    0.0666667\n#\n [Resample] iter 10:   0.0666667\n#\n \n#\n Aggregated Result: mmce.test.mean=0.0666667\n#\n \n#\n Task: iris, Learner: classif.rpart.bagged\n#\n Resampling: cross-validation\n#\n Measures:             mmce\n#\n [Resample] iter 1:    0.2000000\n#\n [Resample] iter 2:    0.0666667\n#\n [Resample] iter 3:    0.0000000\n#\n [Resample] iter 4:    0.0666667\n#\n [Resample] iter 5:    0.0000000\n#\n [Resample] iter 6:    0.0666667\n#\n [Resample] iter 7:    0.0000000\n#\n [Resample] iter 8:    0.1333333\n#\n [Resample] iter 9:    0.0000000\n#\n [Resample] iter 10:   0.0666667\n#\n \n#\n Aggregated Result: mmce.test.mean=0.0600000\n#\n \n#\n   task.id           learner.id mmce.test.mean\n#\n 1    iris        classif.rpart     0.06666667\n#\n 2    iris classif.rpart.bagged     0.06000000\n\n\n\n\nThat far we are quite happy with our new learner.\nBut we hope for a better performance by tuning some hyperparameters of both the decision trees and bagging wrapper.\nLet's have a look at the available hyperparameters of the fused learner:\n\n\ngetParamSet(wrapped.lrn)\n#\n                    Type len   Def   Constr Req Tunable Trafo\n#\n bw.iters        integer   -    10 1 to Inf   -    TRUE     -\n#\n bw.replace      logical   -  TRUE        -   -    TRUE     -\n#\n bw.size         numeric   -     -   0 to 1   -    TRUE     -\n#\n bw.feats        numeric   - 0.667   0 to 1   -    TRUE     -\n#\n minsplit        integer   -    20 1 to Inf   -    TRUE     -\n#\n minbucket       integer   -     - 1 to Inf   -    TRUE     -\n#\n cp              numeric   -  0.01   0 to 1   -    TRUE     -\n#\n maxcompete      integer   -     4 0 to Inf   -    TRUE     -\n#\n maxsurrogate    integer   -     5 0 to Inf   -    TRUE     -\n#\n usesurrogate   discrete   -     2    0,1,2   -    TRUE     -\n#\n surrogatestyle discrete   -     0      0,1   -    TRUE     -\n#\n maxdepth        integer   -    30  1 to 30   -    TRUE     -\n#\n xval            integer   -    10 0 to Inf   -   FALSE     -\n#\n parms           untyped   -     -        -   -    TRUE     -\n\n\n\n\nWe choose to tune the parameters \nminsplit\n and \nbw.feats\n for the \nmmce\n using a \nrandom search\n in a 3-fold CV:\n\n\nctrl = makeTuneControlRandom(maxit = 10)\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\npar.set = makeParamSet(\n  makeIntegerParam(\nminsplit\n, lower = 1, upper = 10),\n  makeNumericParam(\nbw.feats\n, lower = 0.25, upper = 1)\n)\ntuned.lrn = makeTuneWrapper(wrapped.lrn, rdesc, mmce, par.set, ctrl)\nprint(tuned.lrn)\n#\n Learner classif.rpart.bagged.tuned from package rpart\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: TuneWrapper\n#\n Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass,featimp\n#\n Predict-Type: response\n#\n Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5\n\n\n\n\nCalling the train method of the newly constructed learner performs the following steps:\n\n\n\n\nThe tuning wrapper sets parameters for the underlying model in slot \n$next.learner\n and calls its train method.\n\n\nNext learner is the bagging wrapper. The passed down argument \nbw.feats\n is used in the bagging wrapper training function, the argument \nminsplit\n gets passed down to \n$next.learner\n.\n   The base wrapper function calls the base learner \nbw.iters\n times and stores the resulting models.\n\n\nThe bagged models are evaluated using the mean \nmmce\n (default aggregation for this performance measure) and new parameters are selected using the tuning method.\n\n\nThis is repeated until the tuner terminates. Output is a tuned bagged learner.\n\n\n\n\nlrn = train(tuned.lrn, task = task)\n#\n [Tune] Started tuning learner classif.rpart.bagged for parameter set:\n#\n             Type len Def    Constr Req Tunable Trafo\n#\n minsplit integer   -   -   1 to 10   -    TRUE     -\n#\n bw.feats numeric   -   - 0.25 to 1   -    TRUE     -\n#\n With control class: TuneControlRandom\n#\n Imputation value: 1\n#\n [Tune-x] 1: minsplit=5; bw.feats=0.935\n#\n [Tune-y] 1: mmce.test.mean=0.0466667; time: 0.1 min\n#\n [Tune-x] 2: minsplit=9; bw.feats=0.675\n#\n [Tune-y] 2: mmce.test.mean=0.0466667; time: 0.1 min\n#\n [Tune-x] 3: minsplit=2; bw.feats=0.847\n#\n [Tune-y] 3: mmce.test.mean=0.0466667; time: 0.1 min\n#\n [Tune-x] 4: minsplit=4; bw.feats=0.761\n#\n [Tune-y] 4: mmce.test.mean=0.0466667; time: 0.1 min\n#\n [Tune-x] 5: minsplit=6; bw.feats=0.338\n#\n [Tune-y] 5: mmce.test.mean=0.0866667; time: 0.1 min\n#\n [Tune-x] 6: minsplit=1; bw.feats=0.637\n#\n [Tune-y] 6: mmce.test.mean=0.0466667; time: 0.1 min\n#\n [Tune-x] 7: minsplit=1; bw.feats=0.998\n#\n [Tune-y] 7: mmce.test.mean=0.0466667; time: 0.1 min\n#\n [Tune-x] 8: minsplit=4; bw.feats=0.698\n#\n [Tune-y] 8: mmce.test.mean=0.0466667; time: 0.1 min\n#\n [Tune-x] 9: minsplit=3; bw.feats=0.836\n#\n [Tune-y] 9: mmce.test.mean=0.0466667; time: 0.1 min\n#\n [Tune-x] 10: minsplit=10; bw.feats=0.529\n#\n [Tune-y] 10: mmce.test.mean=0.0533333; time: 0.1 min\n#\n [Tune] Result: minsplit=1; bw.feats=0.998 : mmce.test.mean=0.0466667\nprint(lrn)\n#\n Model for learner.id=classif.rpart.bagged.tuned; learner.class=TuneWrapper\n#\n Trained on: task.id = iris; obs = 150; features = 4\n#\n Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\ndata(iris) \ntask = makeClassifTask(data = iris, target = \nSpecies\n, weights = as.integer(iris$Species)) \nbase.lrn = makeLearner(\nclassif.rpart\n) \nwrapped.lrn = makeBaggingWrapper(base.lrn, bw.iters = 100, bw.feats = 0.5) \nprint(wrapped.lrn) \nbenchmark(tasks = task, learners = list(base.lrn, wrapped.lrn)) \ngetParamSet(wrapped.lrn) \nctrl = makeTuneControlRandom(maxit = 10) \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \npar.set = makeParamSet( \n  makeIntegerParam(\nminsplit\n, lower = 1, upper = 10), \n  makeNumericParam(\nbw.feats\n, lower = 0.25, upper = 1) \n) \ntuned.lrn = makeTuneWrapper(wrapped.lrn, rdesc, mmce, par.set, ctrl) \nprint(tuned.lrn) \nlrn = train(tuned.lrn, task = task) \nprint(lrn)", 
            "title": "Wrapped Learners"
        }, 
        {
            "location": "/wrapper/index.html#wrapper", 
            "text": "Wrappers can be employed to extend integrated  learners  with new functionality.\nThe broad scope of operations and methods which are implemented as wrappers underline the flexibility of the wrapping approach:   Data preprocessing  Imputation  Bagging  Tuning  Feature selection  Cost-sensitive classification  Over- and undersampling  for imbalanced classification problems  Multiclass extension  for binary-class learners  Multilabel classification   All these operations and methods have a few things in common:\nFirst, they all wrap around  mlr   learners  and they return a new learner.\nTherefore learners can be wrapped multiple times.\nSecond, they are implemented using a  train  (pre-model hook) and  predict  (post-model hook) method.", 
            "title": "Wrapper"
        }, 
        {
            "location": "/wrapper/index.html#example-bagging-wrapper", 
            "text": "In this section we exemplary describe the bagging wrapper to create a random forest which supports weights.\nTo achieve that we combine several decision trees from the  rpart  package to create our own custom random forest.  First, we create a weighted toy task.  data(iris)\ntask = makeClassifTask(data = iris, target =  Species , weights = as.integer(iris$Species))  Next, we use  makeBaggingWrapper  to create the base learners and the bagged learner.\nWe choose to set equivalents of  ntree  (100 base learners) and  mtry  (proportion of randomly selected features).  base.lrn = makeLearner( classif.rpart )\nwrapped.lrn = makeBaggingWrapper(base.lrn, bw.iters = 100, bw.feats = 0.5)\nprint(wrapped.lrn)\n#  Learner classif.rpart.bagged from package rpart\n#  Type: classif\n#  Name: ; Short name: \n#  Class: BaggingWrapper\n#  Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp\n#  Predict-Type: response\n#  Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5  As we can see in the output, the wrapped learner inherited all properties from the base learner, especially the \"weights\" attribute is still present.\nWe can use this newly constructed learner like all base learners, i.e. we can use it in  train ,  benchmark ,  resample , etc.  benchmark(tasks = task, learners = list(base.lrn, wrapped.lrn))\n#  Task: iris, Learner: classif.rpart\n#  Resampling: cross-validation\n#  Measures:             mmce\n#  [Resample] iter 1:    0.2000000\n#  [Resample] iter 2:    0.0666667\n#  [Resample] iter 3:    0.0000000\n#  [Resample] iter 4:    0.0666667\n#  [Resample] iter 5:    0.0000000\n#  [Resample] iter 6:    0.0666667\n#  [Resample] iter 7:    0.0000000\n#  [Resample] iter 8:    0.1333333\n#  [Resample] iter 9:    0.0666667\n#  [Resample] iter 10:   0.0666667\n#  \n#  Aggregated Result: mmce.test.mean=0.0666667\n#  \n#  Task: iris, Learner: classif.rpart.bagged\n#  Resampling: cross-validation\n#  Measures:             mmce\n#  [Resample] iter 1:    0.2000000\n#  [Resample] iter 2:    0.0666667\n#  [Resample] iter 3:    0.0000000\n#  [Resample] iter 4:    0.0666667\n#  [Resample] iter 5:    0.0000000\n#  [Resample] iter 6:    0.0666667\n#  [Resample] iter 7:    0.0000000\n#  [Resample] iter 8:    0.1333333\n#  [Resample] iter 9:    0.0000000\n#  [Resample] iter 10:   0.0666667\n#  \n#  Aggregated Result: mmce.test.mean=0.0600000\n#  \n#    task.id           learner.id mmce.test.mean\n#  1    iris        classif.rpart     0.06666667\n#  2    iris classif.rpart.bagged     0.06000000  That far we are quite happy with our new learner.\nBut we hope for a better performance by tuning some hyperparameters of both the decision trees and bagging wrapper.\nLet's have a look at the available hyperparameters of the fused learner:  getParamSet(wrapped.lrn)\n#                     Type len   Def   Constr Req Tunable Trafo\n#  bw.iters        integer   -    10 1 to Inf   -    TRUE     -\n#  bw.replace      logical   -  TRUE        -   -    TRUE     -\n#  bw.size         numeric   -     -   0 to 1   -    TRUE     -\n#  bw.feats        numeric   - 0.667   0 to 1   -    TRUE     -\n#  minsplit        integer   -    20 1 to Inf   -    TRUE     -\n#  minbucket       integer   -     - 1 to Inf   -    TRUE     -\n#  cp              numeric   -  0.01   0 to 1   -    TRUE     -\n#  maxcompete      integer   -     4 0 to Inf   -    TRUE     -\n#  maxsurrogate    integer   -     5 0 to Inf   -    TRUE     -\n#  usesurrogate   discrete   -     2    0,1,2   -    TRUE     -\n#  surrogatestyle discrete   -     0      0,1   -    TRUE     -\n#  maxdepth        integer   -    30  1 to 30   -    TRUE     -\n#  xval            integer   -    10 0 to Inf   -   FALSE     -\n#  parms           untyped   -     -        -   -    TRUE     -  We choose to tune the parameters  minsplit  and  bw.feats  for the  mmce  using a  random search  in a 3-fold CV:  ctrl = makeTuneControlRandom(maxit = 10)\nrdesc = makeResampleDesc( CV , iters = 3)\npar.set = makeParamSet(\n  makeIntegerParam( minsplit , lower = 1, upper = 10),\n  makeNumericParam( bw.feats , lower = 0.25, upper = 1)\n)\ntuned.lrn = makeTuneWrapper(wrapped.lrn, rdesc, mmce, par.set, ctrl)\nprint(tuned.lrn)\n#  Learner classif.rpart.bagged.tuned from package rpart\n#  Type: classif\n#  Name: ; Short name: \n#  Class: TuneWrapper\n#  Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass,featimp\n#  Predict-Type: response\n#  Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5  Calling the train method of the newly constructed learner performs the following steps:   The tuning wrapper sets parameters for the underlying model in slot  $next.learner  and calls its train method.  Next learner is the bagging wrapper. The passed down argument  bw.feats  is used in the bagging wrapper training function, the argument  minsplit  gets passed down to  $next.learner .\n   The base wrapper function calls the base learner  bw.iters  times and stores the resulting models.  The bagged models are evaluated using the mean  mmce  (default aggregation for this performance measure) and new parameters are selected using the tuning method.  This is repeated until the tuner terminates. Output is a tuned bagged learner.   lrn = train(tuned.lrn, task = task)\n#  [Tune] Started tuning learner classif.rpart.bagged for parameter set:\n#              Type len Def    Constr Req Tunable Trafo\n#  minsplit integer   -   -   1 to 10   -    TRUE     -\n#  bw.feats numeric   -   - 0.25 to 1   -    TRUE     -\n#  With control class: TuneControlRandom\n#  Imputation value: 1\n#  [Tune-x] 1: minsplit=5; bw.feats=0.935\n#  [Tune-y] 1: mmce.test.mean=0.0466667; time: 0.1 min\n#  [Tune-x] 2: minsplit=9; bw.feats=0.675\n#  [Tune-y] 2: mmce.test.mean=0.0466667; time: 0.1 min\n#  [Tune-x] 3: minsplit=2; bw.feats=0.847\n#  [Tune-y] 3: mmce.test.mean=0.0466667; time: 0.1 min\n#  [Tune-x] 4: minsplit=4; bw.feats=0.761\n#  [Tune-y] 4: mmce.test.mean=0.0466667; time: 0.1 min\n#  [Tune-x] 5: minsplit=6; bw.feats=0.338\n#  [Tune-y] 5: mmce.test.mean=0.0866667; time: 0.1 min\n#  [Tune-x] 6: minsplit=1; bw.feats=0.637\n#  [Tune-y] 6: mmce.test.mean=0.0466667; time: 0.1 min\n#  [Tune-x] 7: minsplit=1; bw.feats=0.998\n#  [Tune-y] 7: mmce.test.mean=0.0466667; time: 0.1 min\n#  [Tune-x] 8: minsplit=4; bw.feats=0.698\n#  [Tune-y] 8: mmce.test.mean=0.0466667; time: 0.1 min\n#  [Tune-x] 9: minsplit=3; bw.feats=0.836\n#  [Tune-y] 9: mmce.test.mean=0.0466667; time: 0.1 min\n#  [Tune-x] 10: minsplit=10; bw.feats=0.529\n#  [Tune-y] 10: mmce.test.mean=0.0533333; time: 0.1 min\n#  [Tune] Result: minsplit=1; bw.feats=0.998 : mmce.test.mean=0.0466667\nprint(lrn)\n#  Model for learner.id=classif.rpart.bagged.tuned; learner.class=TuneWrapper\n#  Trained on: task.id = iris; obs = 150; features = 4\n#  Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5", 
            "title": "Example: Bagging wrapper"
        }, 
        {
            "location": "/wrapper/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  data(iris) \ntask = makeClassifTask(data = iris, target =  Species , weights = as.integer(iris$Species)) \nbase.lrn = makeLearner( classif.rpart ) \nwrapped.lrn = makeBaggingWrapper(base.lrn, bw.iters = 100, bw.feats = 0.5) \nprint(wrapped.lrn) \nbenchmark(tasks = task, learners = list(base.lrn, wrapped.lrn)) \ngetParamSet(wrapped.lrn) \nctrl = makeTuneControlRandom(maxit = 10) \nrdesc = makeResampleDesc( CV , iters = 3) \npar.set = makeParamSet( \n  makeIntegerParam( minsplit , lower = 1, upper = 10), \n  makeNumericParam( bw.feats , lower = 0.25, upper = 1) \n) \ntuned.lrn = makeTuneWrapper(wrapped.lrn, rdesc, mmce, par.set, ctrl) \nprint(tuned.lrn) \nlrn = train(tuned.lrn, task = task) \nprint(lrn)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/impute/index.html", 
            "text": "Imputation of Missing Values\n\n\nmlr\n provides several imputation methods which are listed on the help page \nimputations\n.\nThese include standard techniques as imputation by a constant value (like a fixed constant, the mean, median or mode)\nand random numbers (either from the empirical distribution of the feature under consideration or a certain\ndistribution family).\nMoreover, missing values in one feature can be replaced based on the other features by predictions from\nany supervised \nLearner\n integrated into \nmlr\n.\n\n\nIf your favourite option is not implemented in \nmlr\n yet, you can easily\n\ncreate your own imputation method\n.\n\n\nAlso note that some of the learning algorithms included in \nmlr\n can deal with missing values\nin a sensible way, i.e., other than simply deleting observations with missing values.\nThose \nLearner\ns have the property \n\"missings\"\n and thus can be identified\nusing \nlistLearners\n.\n\n\n## Regression learners that can deal with missing values\nlistLearners(\nregr\n, properties = \nmissings\n)[c(\nclass\n, \npackage\n)]\n#\n              class      package\n#\n 1 regr.bartMachine  bartMachine\n#\n 2  regr.blackboost mboost,party\n#\n 3     regr.cforest        party\n#\n 4       regr.ctree        party\n#\n 5      regr.cubist       Cubist\n#\n 6 regr.featureless          mlr\n#\n ... (#rows: 14, #cols: 2)\n\n\n\n\nSee also the list of \nintegrated learners\n in the Appendix.\n\n\nImputation and reimputation\n\n\nImputation can be done by function \nimpute\n.\nYou can specify an imputation method for each feature individually or for classes of features like numerics or factors.\nMoreover, you can generate dummy variables that indicate which values are missing, also either for classes of features\nor for individual features.\nThese allow to identify the patterns and reasons for missing data and permit to treat imputed and observed values\ndifferently in a subsequent analysis.\n\n\nLet's have a look at the \nairquality\n data set.\n\n\ndata(airquality)\nsummary(airquality)\n#\n      Ozone           Solar.R           Wind             Temp      \n#\n  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n#\n  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n#\n  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n#\n  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n#\n  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n#\n  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n#\n  NA's   :37       NA's   :7                                       \n#\n      Month            Day      \n#\n  Min.   :5.000   Min.   : 1.0  \n#\n  1st Qu.:6.000   1st Qu.: 8.0  \n#\n  Median :7.000   Median :16.0  \n#\n  Mean   :6.993   Mean   :15.8  \n#\n  3rd Qu.:8.000   3rd Qu.:23.0  \n#\n  Max.   :9.000   Max.   :31.0  \n#\n \n\n\n\n\nThere are 37 \nNA's\n in variable \nOzone\n (ozone pollution) and 7 \nNA's\n in variable \nSolar.R\n (solar radiation).\nFor demonstration purposes we insert artificial \nNA's\n in column \nWind\n (wind speed) and coerce it into a\n\nfactor\n.\n\n\nairq = airquality\nind = sample(nrow(airq), 10)\nairq$Wind[ind] = NA\nairq$Wind = cut(airq$Wind, c(0,8,16,24))\nsummary(airq)\n#\n      Ozone           Solar.R           Wind         Temp      \n#\n  Min.   :  1.00   Min.   :  7.0   (0,8]  :51   Min.   :56.00  \n#\n  1st Qu.: 18.00   1st Qu.:115.8   (8,16] :86   1st Qu.:72.00  \n#\n  Median : 31.50   Median :205.0   (16,24]: 6   Median :79.00  \n#\n  Mean   : 42.13   Mean   :185.9   NA's   :10   Mean   :77.88  \n#\n  3rd Qu.: 63.25   3rd Qu.:258.8                3rd Qu.:85.00  \n#\n  Max.   :168.00   Max.   :334.0                Max.   :97.00  \n#\n  NA's   :37       NA's   :7                                   \n#\n      Month            Day      \n#\n  Min.   :5.000   Min.   : 1.0  \n#\n  1st Qu.:6.000   1st Qu.: 8.0  \n#\n  Median :7.000   Median :16.0  \n#\n  Mean   :6.993   Mean   :15.8  \n#\n  3rd Qu.:8.000   3rd Qu.:23.0  \n#\n  Max.   :9.000   Max.   :31.0  \n#\n \n\n\n\n\nIf you want to impute \nNA's\n in all integer features (these include \nOzone\n and \nSolar.R\n) by the mean,\nin all factor features (\nWind\n) by the mode and additionally generate dummy variables for all integer features,\nyou can do this as follows:\n\n\nimp = impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()),\n  dummy.classes = \ninteger\n)\n\n\n\n\nimpute\n returns a \nlist\n where slot \n$data\n contains the imputed data set.\nPer default, the dummy variables are factors with levels \n\"TRUE\"\n and \n\"FALSE\"\n.\nIt is also possible to create numeric zero-one indicator variables.\n\n\nhead(imp$data, 10)\n#\n       Ozone  Solar.R    Wind Temp Month Day Ozone.dummy Solar.R.dummy\n#\n 1  41.00000 190.0000   (0,8]   67     5   1       FALSE         FALSE\n#\n 2  36.00000 118.0000   (0,8]   72     5   2       FALSE         FALSE\n#\n 3  12.00000 149.0000  (8,16]   74     5   3       FALSE         FALSE\n#\n 4  18.00000 313.0000  (8,16]   62     5   4       FALSE         FALSE\n#\n 5  42.12931 185.9315  (8,16]   56     5   5        TRUE          TRUE\n#\n 6  28.00000 185.9315  (8,16]   66     5   6       FALSE          TRUE\n#\n 7  23.00000 299.0000  (8,16]   65     5   7       FALSE         FALSE\n#\n 8  19.00000  99.0000  (8,16]   59     5   8       FALSE         FALSE\n#\n 9   8.00000  19.0000 (16,24]   61     5   9       FALSE         FALSE\n#\n 10 42.12931 194.0000  (8,16]   69     5  10        TRUE         FALSE\n\n\n\n\nSlot \n$desc\n is an \nImputationDesc\n object that stores all relevant information about the\nimputation.\nFor the current example this includes the means and the mode computed on the non-missing data.\n\n\nimp$desc\n#\n Imputation description\n#\n Target: \n#\n Features: 6; Imputed: 6\n#\n impute.new.levels: TRUE\n#\n recode.factor.levels: TRUE\n#\n dummy.type: factor\n\n\n\n\nThe imputation description shows the name of the target variable (not present), the number of features\nand the number of imputed features.\nNote that the latter number refers to the features for which an imputation method was specified\n(five integers plus one factor) and not to the features actually containing \nNA's\n.\n\ndummy.type\n indicates that the dummy variables are factors.\nFor details on \nimpute.new.levels\n and \nrecode.factor.levels\n see the help page of function \nimpute\n.\n\n\nLet's have a look at another example involving a target variable.\nA possible learning task associated with the \nairquality\n data is to predict the ozone\npollution based on the meteorological features.\nSince we do not want to use columns \nDay\n and \nMonth\n we remove them.\n\n\nairq = subset(airq, select = 1:4)\n\n\n\n\nThe first 100 observations are used as training data set.\n\n\nairq.train = airq[1:100,]\nairq.test = airq[-c(1:100),]\n\n\n\n\nIn case of a supervised learning problem you need to pass the name of the target variable to \nimpute\n.\nThis prevents imputation and creation of a dummy variable for the target variable itself and makes sure\nthat the target variable is not used to impute the features.\n\n\nIn contrast to the example above we specify imputation methods for individual features instead of classes of features.\n\n\nMissing values in \nSolar.R\n are imputed by random numbers drawn from the empirical distribution of\nthe non-missing observations.\n\n\nFunction \nimputeLearner\n allows to use all supervised learning algorithms integrated into \nmlr\n\nfor imputation.\nThe type of the \nLearner\n (\nregr\n, \nclassif\n) must correspond to the class of the feature to\nbe imputed.\nThe missing values in \nWind\n are replaced by the predictions of a classification tree (\nrpart\n).\nPer default, all available columns in \nairq.train\n except the target variable (\nOzone\n) and the variable to\nbe imputed (\nWind\n) are used as features in the classification tree, here \nSolar.R\n and \nTemp\n.\nYou can also select manually which columns to use.\nNote that \nrpart\n can deal with missing feature values, therefore the \nNA's\n in column \nSolar.R\n\ndo not pose a problem.\n\n\nimp = impute(airq.train, target = \nOzone\n, cols = list(Solar.R = imputeHist(),\n  Wind = imputeLearner(\nclassif.rpart\n)), dummy.cols = c(\nSolar.R\n, \nWind\n))\nsummary(imp$data)\n#\n      Ozone           Solar.R            Wind         Temp      \n#\n  Min.   :  1.00   Min.   :  7.00   (0,8]  :34   Min.   :56.00  \n#\n  1st Qu.: 16.00   1st Qu.: 98.75   (8,16] :61   1st Qu.:69.00  \n#\n  Median : 34.00   Median :221.50   (16,24]: 5   Median :79.50  \n#\n  Mean   : 41.59   Mean   :191.54                Mean   :76.87  \n#\n  3rd Qu.: 63.00   3rd Qu.:274.25                3rd Qu.:84.00  \n#\n  Max.   :135.00   Max.   :334.00                Max.   :93.00  \n#\n  NA's   :31                                                    \n#\n  Solar.R.dummy Wind.dummy\n#\n  FALSE:93      FALSE:92  \n#\n  TRUE : 7      TRUE : 8  \n#\n                          \n#\n                          \n#\n                          \n#\n                          \n#\n \n\nimp$desc\n#\n Imputation description\n#\n Target: Ozone\n#\n Features: 3; Imputed: 2\n#\n impute.new.levels: TRUE\n#\n recode.factor.levels: TRUE\n#\n dummy.type: factor\n\n\n\n\nThe \nImputationDesc\n object can be used by function \nreimpute\n to impute the test data set the same way\nas the training data.\n\n\nairq.test.imp = reimpute(airq.test, imp$desc)\nhead(airq.test.imp)\n#\n   Ozone Solar.R   Wind Temp Solar.R.dummy Wind.dummy\n#\n 1   110     207  (0,8]   90         FALSE      FALSE\n#\n 2    NA     222 (8,16]   92         FALSE      FALSE\n#\n 3    NA     137 (8,16]   86         FALSE      FALSE\n#\n 4    44     192 (8,16]   86         FALSE      FALSE\n#\n 5    28     273 (8,16]   82         FALSE      FALSE\n#\n 6    65     157 (8,16]   80         FALSE      FALSE\n\n\n\n\nEspecially when evaluating a machine learning method by some resampling technique you might want that\n\nimpute\n/\nreimpute\n are called automatically each time before training/prediction.\nThis can be achieved by creating an imputation wrapper.\n\n\nFusing a learner with imputation\n\n\nYou can couple a \nLearner\n with imputation by function \nmakeImputeWrapper\n which basically\nhas the same formal arguments as \nimpute\n.\nLike in the example above we impute \nSolar.R\n by random numbers from its empirical distribution,\n\nWind\n by the predictions of a classification tree and generate dummy variables for both features.\n\n\nlrn = makeImputeWrapper(\nregr.lm\n, cols = list(Solar.R = imputeHist(),\n  Wind = imputeLearner(\nclassif.rpart\n)), dummy.cols = c(\nSolar.R\n, \nWind\n))\nlrn\n#\n Learner regr.lm.imputed from package stats\n#\n Type: regr\n#\n Name: ; Short name: \n#\n Class: ImputeWrapper\n#\n Properties: numerics,factors,se,weights,missings\n#\n Predict-Type: response\n#\n Hyperparameters:\n\n\n\n\nBefore training the resulting \nLearner\n, \nimpute\n is applied to the training set.\nBefore prediction \nreimpute\n is called on the test set and the \nImputationDesc\n object\nfrom the training stage.\n\n\nWe again aim to predict the ozone pollution from the meteorological variables.\nIn order to create the \nTask\n we need to delete observations with missing values in the target variable.\n\n\nairq = subset(airq, subset = !is.na(airq$Ozone))\ntask = makeRegrTask(data = airq, target = \nOzone\n)\n\n\n\n\nIn the following the 3-fold cross-validated \nmean squared error\n is calculated.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(lrn, task, resampling = rdesc, show.info = FALSE, models = TRUE)\nr$aggr\n#\n mse.test.mean \n#\n      524.3392\n\n\n\n\nlapply(r$models, getLearnerModel, more.unwrap = TRUE)\n#\n [[1]]\n#\n \n#\n Call:\n#\n stats::lm(formula = f, data = d)\n#\n \n#\n Coefficients:\n#\n       (Intercept)            Solar.R         Wind(8,16]  \n#\n         -117.0954             0.0853           -27.6763  \n#\n       Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#\n           -9.0988             2.0505           -27.4152  \n#\n    Wind.dummyTRUE  \n#\n            2.2535  \n#\n \n#\n \n#\n [[2]]\n#\n \n#\n Call:\n#\n stats::lm(formula = f, data = d)\n#\n \n#\n Coefficients:\n#\n       (Intercept)            Solar.R         Wind(8,16]  \n#\n         -94.84542            0.03936          -16.26255  \n#\n       Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#\n          -7.00707            1.79513          -11.08578  \n#\n    Wind.dummyTRUE  \n#\n          -0.68340  \n#\n \n#\n \n#\n [[3]]\n#\n \n#\n Call:\n#\n stats::lm(formula = f, data = d)\n#\n \n#\n Coefficients:\n#\n       (Intercept)            Solar.R         Wind(8,16]  \n#\n         -57.30438            0.07426          -30.70737  \n#\n       Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#\n         -18.25055            1.35898           -2.16654  \n#\n    Wind.dummyTRUE  \n#\n          -5.56400\n\n\n\n\nA second possibility to fuse a learner with imputation is provided by \nmakePreprocWrapperCaret\n,\nwhich is an interface to \ncaret\n's \npreProcess\n function.\n\npreProcess\n only works for numeric features and offers imputation by\nk-nearest neighbors, bagged trees, and by the median.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## Regression learners that can deal with missing values \nlistLearners(\nregr\n, properties = \nmissings\n)[c(\nclass\n, \npackage\n)] \ndata(airquality) \nsummary(airquality) \nairq = airquality \nind = sample(nrow(airq), 10) \nairq$Wind[ind] = NA \nairq$Wind = cut(airq$Wind, c(0,8,16,24)) \nsummary(airq) \nimp = impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()), \n  dummy.classes = \ninteger\n) \nhead(imp$data, 10) \nimp$desc \nairq = subset(airq, select = 1:4) \nairq.train = airq[1:100,] \nairq.test = airq[-c(1:100),] \nimp = impute(airq.train, target = \nOzone\n, cols = list(Solar.R = imputeHist(), \n  Wind = imputeLearner(\nclassif.rpart\n)), dummy.cols = c(\nSolar.R\n, \nWind\n)) \nsummary(imp$data) \n\nimp$desc \nairq.test.imp = reimpute(airq.test, imp$desc) \nhead(airq.test.imp) \nlrn = makeImputeWrapper(\nregr.lm\n, cols = list(Solar.R = imputeHist(), \n  Wind = imputeLearner(\nclassif.rpart\n)), dummy.cols = c(\nSolar.R\n, \nWind\n)) \nlrn \nairq = subset(airq, subset = !is.na(airq$Ozone)) \ntask = makeRegrTask(data = airq, target = \nOzone\n) \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nr = resample(lrn, task, resampling = rdesc, show.info = FALSE, models = TRUE) \nr$aggr \nlapply(r$models, getLearnerModel, more.unwrap = TRUE)", 
            "title": "Imputation"
        }, 
        {
            "location": "/impute/index.html#imputation-of-missing-values", 
            "text": "mlr  provides several imputation methods which are listed on the help page  imputations .\nThese include standard techniques as imputation by a constant value (like a fixed constant, the mean, median or mode)\nand random numbers (either from the empirical distribution of the feature under consideration or a certain\ndistribution family).\nMoreover, missing values in one feature can be replaced based on the other features by predictions from\nany supervised  Learner  integrated into  mlr .  If your favourite option is not implemented in  mlr  yet, you can easily create your own imputation method .  Also note that some of the learning algorithms included in  mlr  can deal with missing values\nin a sensible way, i.e., other than simply deleting observations with missing values.\nThose  Learner s have the property  \"missings\"  and thus can be identified\nusing  listLearners .  ## Regression learners that can deal with missing values\nlistLearners( regr , properties =  missings )[c( class ,  package )]\n#               class      package\n#  1 regr.bartMachine  bartMachine\n#  2  regr.blackboost mboost,party\n#  3     regr.cforest        party\n#  4       regr.ctree        party\n#  5      regr.cubist       Cubist\n#  6 regr.featureless          mlr\n#  ... (#rows: 14, #cols: 2)  See also the list of  integrated learners  in the Appendix.", 
            "title": "Imputation of Missing Values"
        }, 
        {
            "location": "/impute/index.html#imputation-and-reimputation", 
            "text": "Imputation can be done by function  impute .\nYou can specify an imputation method for each feature individually or for classes of features like numerics or factors.\nMoreover, you can generate dummy variables that indicate which values are missing, also either for classes of features\nor for individual features.\nThese allow to identify the patterns and reasons for missing data and permit to treat imputed and observed values\ndifferently in a subsequent analysis.  Let's have a look at the  airquality  data set.  data(airquality)\nsummary(airquality)\n#       Ozone           Solar.R           Wind             Temp      \n#   Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n#   1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n#   Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n#   Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n#   3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n#   Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n#   NA's   :37       NA's   :7                                       \n#       Month            Day      \n#   Min.   :5.000   Min.   : 1.0  \n#   1st Qu.:6.000   1st Qu.: 8.0  \n#   Median :7.000   Median :16.0  \n#   Mean   :6.993   Mean   :15.8  \n#   3rd Qu.:8.000   3rd Qu.:23.0  \n#   Max.   :9.000   Max.   :31.0  \n#    There are 37  NA's  in variable  Ozone  (ozone pollution) and 7  NA's  in variable  Solar.R  (solar radiation).\nFor demonstration purposes we insert artificial  NA's  in column  Wind  (wind speed) and coerce it into a factor .  airq = airquality\nind = sample(nrow(airq), 10)\nairq$Wind[ind] = NA\nairq$Wind = cut(airq$Wind, c(0,8,16,24))\nsummary(airq)\n#       Ozone           Solar.R           Wind         Temp      \n#   Min.   :  1.00   Min.   :  7.0   (0,8]  :51   Min.   :56.00  \n#   1st Qu.: 18.00   1st Qu.:115.8   (8,16] :86   1st Qu.:72.00  \n#   Median : 31.50   Median :205.0   (16,24]: 6   Median :79.00  \n#   Mean   : 42.13   Mean   :185.9   NA's   :10   Mean   :77.88  \n#   3rd Qu.: 63.25   3rd Qu.:258.8                3rd Qu.:85.00  \n#   Max.   :168.00   Max.   :334.0                Max.   :97.00  \n#   NA's   :37       NA's   :7                                   \n#       Month            Day      \n#   Min.   :5.000   Min.   : 1.0  \n#   1st Qu.:6.000   1st Qu.: 8.0  \n#   Median :7.000   Median :16.0  \n#   Mean   :6.993   Mean   :15.8  \n#   3rd Qu.:8.000   3rd Qu.:23.0  \n#   Max.   :9.000   Max.   :31.0  \n#    If you want to impute  NA's  in all integer features (these include  Ozone  and  Solar.R ) by the mean,\nin all factor features ( Wind ) by the mode and additionally generate dummy variables for all integer features,\nyou can do this as follows:  imp = impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()),\n  dummy.classes =  integer )  impute  returns a  list  where slot  $data  contains the imputed data set.\nPer default, the dummy variables are factors with levels  \"TRUE\"  and  \"FALSE\" .\nIt is also possible to create numeric zero-one indicator variables.  head(imp$data, 10)\n#        Ozone  Solar.R    Wind Temp Month Day Ozone.dummy Solar.R.dummy\n#  1  41.00000 190.0000   (0,8]   67     5   1       FALSE         FALSE\n#  2  36.00000 118.0000   (0,8]   72     5   2       FALSE         FALSE\n#  3  12.00000 149.0000  (8,16]   74     5   3       FALSE         FALSE\n#  4  18.00000 313.0000  (8,16]   62     5   4       FALSE         FALSE\n#  5  42.12931 185.9315  (8,16]   56     5   5        TRUE          TRUE\n#  6  28.00000 185.9315  (8,16]   66     5   6       FALSE          TRUE\n#  7  23.00000 299.0000  (8,16]   65     5   7       FALSE         FALSE\n#  8  19.00000  99.0000  (8,16]   59     5   8       FALSE         FALSE\n#  9   8.00000  19.0000 (16,24]   61     5   9       FALSE         FALSE\n#  10 42.12931 194.0000  (8,16]   69     5  10        TRUE         FALSE  Slot  $desc  is an  ImputationDesc  object that stores all relevant information about the\nimputation.\nFor the current example this includes the means and the mode computed on the non-missing data.  imp$desc\n#  Imputation description\n#  Target: \n#  Features: 6; Imputed: 6\n#  impute.new.levels: TRUE\n#  recode.factor.levels: TRUE\n#  dummy.type: factor  The imputation description shows the name of the target variable (not present), the number of features\nand the number of imputed features.\nNote that the latter number refers to the features for which an imputation method was specified\n(five integers plus one factor) and not to the features actually containing  NA's . dummy.type  indicates that the dummy variables are factors.\nFor details on  impute.new.levels  and  recode.factor.levels  see the help page of function  impute .  Let's have a look at another example involving a target variable.\nA possible learning task associated with the  airquality  data is to predict the ozone\npollution based on the meteorological features.\nSince we do not want to use columns  Day  and  Month  we remove them.  airq = subset(airq, select = 1:4)  The first 100 observations are used as training data set.  airq.train = airq[1:100,]\nairq.test = airq[-c(1:100),]  In case of a supervised learning problem you need to pass the name of the target variable to  impute .\nThis prevents imputation and creation of a dummy variable for the target variable itself and makes sure\nthat the target variable is not used to impute the features.  In contrast to the example above we specify imputation methods for individual features instead of classes of features.  Missing values in  Solar.R  are imputed by random numbers drawn from the empirical distribution of\nthe non-missing observations.  Function  imputeLearner  allows to use all supervised learning algorithms integrated into  mlr \nfor imputation.\nThe type of the  Learner  ( regr ,  classif ) must correspond to the class of the feature to\nbe imputed.\nThe missing values in  Wind  are replaced by the predictions of a classification tree ( rpart ).\nPer default, all available columns in  airq.train  except the target variable ( Ozone ) and the variable to\nbe imputed ( Wind ) are used as features in the classification tree, here  Solar.R  and  Temp .\nYou can also select manually which columns to use.\nNote that  rpart  can deal with missing feature values, therefore the  NA's  in column  Solar.R \ndo not pose a problem.  imp = impute(airq.train, target =  Ozone , cols = list(Solar.R = imputeHist(),\n  Wind = imputeLearner( classif.rpart )), dummy.cols = c( Solar.R ,  Wind ))\nsummary(imp$data)\n#       Ozone           Solar.R            Wind         Temp      \n#   Min.   :  1.00   Min.   :  7.00   (0,8]  :34   Min.   :56.00  \n#   1st Qu.: 16.00   1st Qu.: 98.75   (8,16] :61   1st Qu.:69.00  \n#   Median : 34.00   Median :221.50   (16,24]: 5   Median :79.50  \n#   Mean   : 41.59   Mean   :191.54                Mean   :76.87  \n#   3rd Qu.: 63.00   3rd Qu.:274.25                3rd Qu.:84.00  \n#   Max.   :135.00   Max.   :334.00                Max.   :93.00  \n#   NA's   :31                                                    \n#   Solar.R.dummy Wind.dummy\n#   FALSE:93      FALSE:92  \n#   TRUE : 7      TRUE : 8  \n#                           \n#                           \n#                           \n#                           \n#  \n\nimp$desc\n#  Imputation description\n#  Target: Ozone\n#  Features: 3; Imputed: 2\n#  impute.new.levels: TRUE\n#  recode.factor.levels: TRUE\n#  dummy.type: factor  The  ImputationDesc  object can be used by function  reimpute  to impute the test data set the same way\nas the training data.  airq.test.imp = reimpute(airq.test, imp$desc)\nhead(airq.test.imp)\n#    Ozone Solar.R   Wind Temp Solar.R.dummy Wind.dummy\n#  1   110     207  (0,8]   90         FALSE      FALSE\n#  2    NA     222 (8,16]   92         FALSE      FALSE\n#  3    NA     137 (8,16]   86         FALSE      FALSE\n#  4    44     192 (8,16]   86         FALSE      FALSE\n#  5    28     273 (8,16]   82         FALSE      FALSE\n#  6    65     157 (8,16]   80         FALSE      FALSE  Especially when evaluating a machine learning method by some resampling technique you might want that impute / reimpute  are called automatically each time before training/prediction.\nThis can be achieved by creating an imputation wrapper.", 
            "title": "Imputation and reimputation"
        }, 
        {
            "location": "/impute/index.html#fusing-a-learner-with-imputation", 
            "text": "You can couple a  Learner  with imputation by function  makeImputeWrapper  which basically\nhas the same formal arguments as  impute .\nLike in the example above we impute  Solar.R  by random numbers from its empirical distribution, Wind  by the predictions of a classification tree and generate dummy variables for both features.  lrn = makeImputeWrapper( regr.lm , cols = list(Solar.R = imputeHist(),\n  Wind = imputeLearner( classif.rpart )), dummy.cols = c( Solar.R ,  Wind ))\nlrn\n#  Learner regr.lm.imputed from package stats\n#  Type: regr\n#  Name: ; Short name: \n#  Class: ImputeWrapper\n#  Properties: numerics,factors,se,weights,missings\n#  Predict-Type: response\n#  Hyperparameters:  Before training the resulting  Learner ,  impute  is applied to the training set.\nBefore prediction  reimpute  is called on the test set and the  ImputationDesc  object\nfrom the training stage.  We again aim to predict the ozone pollution from the meteorological variables.\nIn order to create the  Task  we need to delete observations with missing values in the target variable.  airq = subset(airq, subset = !is.na(airq$Ozone))\ntask = makeRegrTask(data = airq, target =  Ozone )  In the following the 3-fold cross-validated  mean squared error  is calculated.  rdesc = makeResampleDesc( CV , iters = 3)\nr = resample(lrn, task, resampling = rdesc, show.info = FALSE, models = TRUE)\nr$aggr\n#  mse.test.mean \n#       524.3392  lapply(r$models, getLearnerModel, more.unwrap = TRUE)\n#  [[1]]\n#  \n#  Call:\n#  stats::lm(formula = f, data = d)\n#  \n#  Coefficients:\n#        (Intercept)            Solar.R         Wind(8,16]  \n#          -117.0954             0.0853           -27.6763  \n#        Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#            -9.0988             2.0505           -27.4152  \n#     Wind.dummyTRUE  \n#             2.2535  \n#  \n#  \n#  [[2]]\n#  \n#  Call:\n#  stats::lm(formula = f, data = d)\n#  \n#  Coefficients:\n#        (Intercept)            Solar.R         Wind(8,16]  \n#          -94.84542            0.03936          -16.26255  \n#        Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#           -7.00707            1.79513          -11.08578  \n#     Wind.dummyTRUE  \n#           -0.68340  \n#  \n#  \n#  [[3]]\n#  \n#  Call:\n#  stats::lm(formula = f, data = d)\n#  \n#  Coefficients:\n#        (Intercept)            Solar.R         Wind(8,16]  \n#          -57.30438            0.07426          -30.70737  \n#        Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#          -18.25055            1.35898           -2.16654  \n#     Wind.dummyTRUE  \n#           -5.56400  A second possibility to fuse a learner with imputation is provided by  makePreprocWrapperCaret ,\nwhich is an interface to  caret 's  preProcess  function. preProcess  only works for numeric features and offers imputation by\nk-nearest neighbors, bagged trees, and by the median.", 
            "title": "Fusing a learner with imputation"
        }, 
        {
            "location": "/impute/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## Regression learners that can deal with missing values \nlistLearners( regr , properties =  missings )[c( class ,  package )] \ndata(airquality) \nsummary(airquality) \nairq = airquality \nind = sample(nrow(airq), 10) \nairq$Wind[ind] = NA \nairq$Wind = cut(airq$Wind, c(0,8,16,24)) \nsummary(airq) \nimp = impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()), \n  dummy.classes =  integer ) \nhead(imp$data, 10) \nimp$desc \nairq = subset(airq, select = 1:4) \nairq.train = airq[1:100,] \nairq.test = airq[-c(1:100),] \nimp = impute(airq.train, target =  Ozone , cols = list(Solar.R = imputeHist(), \n  Wind = imputeLearner( classif.rpart )), dummy.cols = c( Solar.R ,  Wind )) \nsummary(imp$data) \n\nimp$desc \nairq.test.imp = reimpute(airq.test, imp$desc) \nhead(airq.test.imp) \nlrn = makeImputeWrapper( regr.lm , cols = list(Solar.R = imputeHist(), \n  Wind = imputeLearner( classif.rpart )), dummy.cols = c( Solar.R ,  Wind )) \nlrn \nairq = subset(airq, subset = !is.na(airq$Ozone)) \ntask = makeRegrTask(data = airq, target =  Ozone ) \nrdesc = makeResampleDesc( CV , iters = 3) \nr = resample(lrn, task, resampling = rdesc, show.info = FALSE, models = TRUE) \nr$aggr \nlapply(r$models, getLearnerModel, more.unwrap = TRUE)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/bagging/index.html", 
            "text": "Generic Bagging\n\n\nOne reason why random forests perform so well is that they are using bagging as\na technique to gain more stability. But why do you want to limit yourself to the\nclassifiers already implemented in well known random forests when it is\nreally easy to build your own with \nmlr\n?\n\n\nJust bag an \nmlr\n learner already \nmakeBaggingWrapper\n.\n\n\nAs in a random forest, we need a \nLearner\n which is trained on a subset of the\ndata during each iteration of the bagging process.\nThe subsets are chosen according to the parameters given to \nmakeBaggingWrapper\n:\n\n\n\n\nbw.iters\n On how many subsets (samples) do we want to train our \nLearner\n?\n\n\nbw.replace\n Sample with replacement (also known as \nbootstrapping\n)?\n\n\nbw.size\n Percentage size of the samples. If \nbw.replace = TRUE\n, \nbw.size = 1\n is the default. This does not mean that one sample will contain all the observations as observations will occur multiple times in each sample.\n\n\nbw.feats\n Percentage size of randomly selected features for each iteration.\n\n\n\n\nOf course we also need a \nLearner\n which we have to pass to\n\nmakeBaggingWrapper\n.\n\n\nlrn = makeLearner(\nclassif.rpart\n)\nbag.lrn = makeBaggingWrapper(lrn, bw.iters = 50, bw.replace = TRUE, bw.size = 0.8, bw.feats = 3/4)\n\n\n\n\nNow we can compare the performance with and without bagging.\nFirst let's try it without bagging:\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nr = resample(learner = lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)\nr$aggr\n#\n mmce.test.mean \n#\n      0.2735714\n\n\n\n\nAnd now with bagging:\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nresult = resample(learner = bag.lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)\nresult$aggr\n#\n mmce.test.mean \n#\n      0.2069048\n\n\n\n\nTraining more learners takes more time, but can outperform pure learners\non noisy data with many features.\n\n\nChanging the type of prediction\n\n\nIn case of a \nclassification\n problem the predicted class labels are determined by majority\nvoting over the predictions of the individual models.\nAdditionally, posterior probabilities can be estimated as the relative proportions\nof the predicted class labels.\nFor this purpose you have to change the predict type of the \nbagging learner\n as follows.\n\n\nbag.lrn = setPredictType(bag.lrn, predict.type = \nprob\n)\n\n\n\n\nNote that it is not relevant if the \nbase learner\n itself can predict probabilities and that\nfor this reason the predict type of the \nbase learner\n always has to be \n\"response\"\n.\n\n\nFor \nregression\n the mean value across predictions is computed.\nMoreover, the standard deviation across predictions is estimated if the predict type\nof the bagging learner is changed to \n\"se\"\n.\nBelow, we give a small example for regression.\n\n\nn = getTaskSize(bh.task)\ntrain.inds = seq(1, n, 3)\ntest.inds  = setdiff(1:n, train.inds)\nlrn = makeLearner(\nregr.rpart\n)\nbag.lrn = makeBaggingWrapper(lrn)\nbag.lrn = setPredictType(bag.lrn, predict.type = \nse\n)\nmod = train(learner = bag.lrn, task = bh.task, subset = train.inds)\n\n\n\n\nWith function \ngetLearnerModel\n, you can access the models fitted in the\nindividual iterations.\n\n\nhead(getLearnerModel(mod), 2)\n#\n [[1]]\n#\n Model for learner.id=regr.rpart; learner.class=regr.rpart\n#\n Trained on: task.id = BostonHousing-example; obs = 169; features = 13\n#\n Hyperparameters: xval=0\n#\n \n#\n [[2]]\n#\n Model for learner.id=regr.rpart; learner.class=regr.rpart\n#\n Trained on: task.id = BostonHousing-example; obs = 169; features = 13\n#\n Hyperparameters: xval=0\n\n\n\n\nPredict the response and calculate the standard deviation:\n\n\npred = predict(mod, task = bh.task, subset = test.inds)\nhead(as.data.frame(pred))\n#\n   id truth response       se\n#\n 2  2  21.6 21.74909 1.397291\n#\n 3  3  34.7 33.50682 4.496894\n#\n 5  5  36.2 32.37504 1.982321\n#\n 6  6  28.7 24.00791 1.262131\n#\n 8  8  27.1 15.57187 3.079712\n#\n 9  9  16.5 15.04702 3.833359\n\n\n\n\nIn the column labelled \nse\n the standard deviation for each prediction is given.\n\n\nLet's visualise this a bit using \nggplot2\n.\nHere we plot the percentage of lower status of the population (\nlstat\n) against the prediction.\n\n\nlibrary(\nggplot2\n)\nlibrary(\nreshape2\n)\ndata = cbind(as.data.frame(pred), getTaskData(bh.task, subset = test.inds))\ng = ggplot(data, aes(x = lstat, y = response, ymin = response-se, ymax = response+se, col = age))\ng + geom_point() + geom_linerange(alpha=0.5)\n\n\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nlrn = makeLearner(\nclassif.rpart\n) \nbag.lrn = makeBaggingWrapper(lrn, bw.iters = 50, bw.replace = TRUE, bw.size = 0.8, bw.feats = 3/4) \nrdesc = makeResampleDesc(\nCV\n, iters = 10) \nr = resample(learner = lrn, task = sonar.task, resampling = rdesc, show.info = FALSE) \nr$aggr \nrdesc = makeResampleDesc(\nCV\n, iters = 10) \nresult = resample(learner = bag.lrn, task = sonar.task, resampling = rdesc, show.info = FALSE) \nresult$aggr \n## bag.lrn = setPredictType(bag.lrn, predict.type = \nprob\n) \nn = getTaskSize(bh.task) \ntrain.inds = seq(1, n, 3) \ntest.inds  = setdiff(1:n, train.inds) \nlrn = makeLearner(\nregr.rpart\n) \nbag.lrn = makeBaggingWrapper(lrn) \nbag.lrn = setPredictType(bag.lrn, predict.type = \nse\n) \nmod = train(learner = bag.lrn, task = bh.task, subset = train.inds) \nhead(getLearnerModel(mod), 2) \npred = predict(mod, task = bh.task, subset = test.inds) \nhead(as.data.frame(pred)) \nlibrary(\nggplot2\n) \nlibrary(\nreshape2\n) \ndata = cbind(as.data.frame(pred), getTaskData(bh.task, subset = test.inds)) \ng = ggplot(data, aes(x = lstat, y = response, ymin = response-se, ymax = response+se, col = age)) \ng + geom_point() + geom_linerange(alpha=0.5)", 
            "title": "Bagging"
        }, 
        {
            "location": "/bagging/index.html#generic-bagging", 
            "text": "One reason why random forests perform so well is that they are using bagging as\na technique to gain more stability. But why do you want to limit yourself to the\nclassifiers already implemented in well known random forests when it is\nreally easy to build your own with  mlr ?  Just bag an  mlr  learner already  makeBaggingWrapper .  As in a random forest, we need a  Learner  which is trained on a subset of the\ndata during each iteration of the bagging process.\nThe subsets are chosen according to the parameters given to  makeBaggingWrapper :   bw.iters  On how many subsets (samples) do we want to train our  Learner ?  bw.replace  Sample with replacement (also known as  bootstrapping )?  bw.size  Percentage size of the samples. If  bw.replace = TRUE ,  bw.size = 1  is the default. This does not mean that one sample will contain all the observations as observations will occur multiple times in each sample.  bw.feats  Percentage size of randomly selected features for each iteration.   Of course we also need a  Learner  which we have to pass to makeBaggingWrapper .  lrn = makeLearner( classif.rpart )\nbag.lrn = makeBaggingWrapper(lrn, bw.iters = 50, bw.replace = TRUE, bw.size = 0.8, bw.feats = 3/4)  Now we can compare the performance with and without bagging.\nFirst let's try it without bagging:  rdesc = makeResampleDesc( CV , iters = 10)\nr = resample(learner = lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)\nr$aggr\n#  mmce.test.mean \n#       0.2735714  And now with bagging:  rdesc = makeResampleDesc( CV , iters = 10)\nresult = resample(learner = bag.lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)\nresult$aggr\n#  mmce.test.mean \n#       0.2069048  Training more learners takes more time, but can outperform pure learners\non noisy data with many features.", 
            "title": "Generic Bagging"
        }, 
        {
            "location": "/bagging/index.html#changing-the-type-of-prediction", 
            "text": "In case of a  classification  problem the predicted class labels are determined by majority\nvoting over the predictions of the individual models.\nAdditionally, posterior probabilities can be estimated as the relative proportions\nof the predicted class labels.\nFor this purpose you have to change the predict type of the  bagging learner  as follows.  bag.lrn = setPredictType(bag.lrn, predict.type =  prob )  Note that it is not relevant if the  base learner  itself can predict probabilities and that\nfor this reason the predict type of the  base learner  always has to be  \"response\" .  For  regression  the mean value across predictions is computed.\nMoreover, the standard deviation across predictions is estimated if the predict type\nof the bagging learner is changed to  \"se\" .\nBelow, we give a small example for regression.  n = getTaskSize(bh.task)\ntrain.inds = seq(1, n, 3)\ntest.inds  = setdiff(1:n, train.inds)\nlrn = makeLearner( regr.rpart )\nbag.lrn = makeBaggingWrapper(lrn)\nbag.lrn = setPredictType(bag.lrn, predict.type =  se )\nmod = train(learner = bag.lrn, task = bh.task, subset = train.inds)  With function  getLearnerModel , you can access the models fitted in the\nindividual iterations.  head(getLearnerModel(mod), 2)\n#  [[1]]\n#  Model for learner.id=regr.rpart; learner.class=regr.rpart\n#  Trained on: task.id = BostonHousing-example; obs = 169; features = 13\n#  Hyperparameters: xval=0\n#  \n#  [[2]]\n#  Model for learner.id=regr.rpart; learner.class=regr.rpart\n#  Trained on: task.id = BostonHousing-example; obs = 169; features = 13\n#  Hyperparameters: xval=0  Predict the response and calculate the standard deviation:  pred = predict(mod, task = bh.task, subset = test.inds)\nhead(as.data.frame(pred))\n#    id truth response       se\n#  2  2  21.6 21.74909 1.397291\n#  3  3  34.7 33.50682 4.496894\n#  5  5  36.2 32.37504 1.982321\n#  6  6  28.7 24.00791 1.262131\n#  8  8  27.1 15.57187 3.079712\n#  9  9  16.5 15.04702 3.833359  In the column labelled  se  the standard deviation for each prediction is given.  Let's visualise this a bit using  ggplot2 .\nHere we plot the percentage of lower status of the population ( lstat ) against the prediction.  library( ggplot2 )\nlibrary( reshape2 )\ndata = cbind(as.data.frame(pred), getTaskData(bh.task, subset = test.inds))\ng = ggplot(data, aes(x = lstat, y = response, ymin = response-se, ymax = response+se, col = age))\ng + geom_point() + geom_linerange(alpha=0.5)", 
            "title": "Changing the type of prediction"
        }, 
        {
            "location": "/bagging/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  lrn = makeLearner( classif.rpart ) \nbag.lrn = makeBaggingWrapper(lrn, bw.iters = 50, bw.replace = TRUE, bw.size = 0.8, bw.feats = 3/4) \nrdesc = makeResampleDesc( CV , iters = 10) \nr = resample(learner = lrn, task = sonar.task, resampling = rdesc, show.info = FALSE) \nr$aggr \nrdesc = makeResampleDesc( CV , iters = 10) \nresult = resample(learner = bag.lrn, task = sonar.task, resampling = rdesc, show.info = FALSE) \nresult$aggr \n## bag.lrn = setPredictType(bag.lrn, predict.type =  prob ) \nn = getTaskSize(bh.task) \ntrain.inds = seq(1, n, 3) \ntest.inds  = setdiff(1:n, train.inds) \nlrn = makeLearner( regr.rpart ) \nbag.lrn = makeBaggingWrapper(lrn) \nbag.lrn = setPredictType(bag.lrn, predict.type =  se ) \nmod = train(learner = bag.lrn, task = bh.task, subset = train.inds) \nhead(getLearnerModel(mod), 2) \npred = predict(mod, task = bh.task, subset = test.inds) \nhead(as.data.frame(pred)) \nlibrary( ggplot2 ) \nlibrary( reshape2 ) \ndata = cbind(as.data.frame(pred), getTaskData(bh.task, subset = test.inds)) \ng = ggplot(data, aes(x = lstat, y = response, ymin = response-se, ymax = response+se, col = age)) \ng + geom_point() + geom_linerange(alpha=0.5)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/advanced_tune/index.html", 
            "text": "Advanced Tuning\n\n\nIterated F-Racing for mixed spaces and dependencies\n\n\nThe package supports a larger number of tuning algorithms, which can all be looked up and\nselected via \nTuneControl\n. One of the cooler algorithms is iterated F-racing from the\n\nirace\n package (technical description \nhere\n). This not only works for arbitrary parameter types (numeric, integer,\ndiscrete, logical), but also for so-called dependent / hierarchical parameters:\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeDiscreteParam(\nkernel\n, values = c(\nvanilladot\n, \npolydot\n, \nrbfdot\n)),\n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x,\n    requires = quote(kernel == \nrbfdot\n)),\n  makeIntegerParam(\ndegree\n, lower = 2L, upper = 5L,\n    requires = quote(kernel == \npolydot\n))\n)\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nrdesc = makeResampleDesc(\nHoldout\n)\nres = tuneParams(\nclassif.ksvm\n, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\nprint(head(as.data.frame(res$opt.path)))\n#\n            C     kernel     sigma degree mmce.test.mean dob eol\n#\n 1 -2.8894525    polydot        NA      4           0.06   1  NA\n#\n 2 -2.6793542 vanilladot        NA     NA           0.04   1  NA\n#\n 3 -0.7855061     rbfdot 11.049783     NA           0.68   1  NA\n#\n 4  1.7678978    polydot        NA      5           0.14   1  NA\n#\n 5  9.7729840 vanilladot        NA     NA           0.04   1  NA\n#\n 6 -2.7930352     rbfdot -7.198476     NA           0.36   1  NA\n#\n   error.message exec.time\n#\n 1          \nNA\n     0.026\n#\n 2          \nNA\n     0.025\n#\n 3          \nNA\n     0.025\n#\n 4          \nNA\n     0.024\n#\n 5          \nNA\n     0.023\n#\n 6          \nNA\n     0.032\n\n\n\n\nSee how we made the kernel parameters like \nsigma\n and \ndegree\n dependent on the \nkernel\n\nselection parameters? This approach allows you to tune parameters of multiple kernels at once,\nefficiently concentrating on the ones which work best for your given data set.\n\n\nTuning across whole model spaces with ModelMultiplexer\n\n\nWe can now take the following example even one step further. If we use the\n\nModelMultiplexer\n we can tune over different model classes at once,\njust as we did with the SVM kernels above.\n\n\nbase.learners = list(\n  makeLearner(\nclassif.ksvm\n),\n  makeLearner(\nclassif.randomForest\n)\n)\nlrn = makeModelMultiplexer(base.learners)\n\n\n\n\nFunction \nmakeModelMultiplexerParamSet\n offers a simple way to construct a parameter set for tuning:\nThe parameter names are prefixed automatically and the \nrequires\n element is set, too,\nto make all parameters subordinate to \nselected.learner\n.\n\n\nps = makeModelMultiplexerParamSet(lrn,\n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeIntegerParam(\nntree\n, lower = 1L, upper = 500L)\n)\nprint(ps)\n#\n                                Type len Def\n#\n selected.learner           discrete   -   -\n#\n classif.ksvm.sigma          numeric   -   -\n#\n classif.randomForest.ntree  integer   -   -\n#\n                                                       Constr Req Tunable\n#\n selected.learner           classif.ksvm,classif.randomForest   -    TRUE\n#\n classif.ksvm.sigma                                 -12 to 12   Y    TRUE\n#\n classif.randomForest.ntree                          1 to 500   Y    TRUE\n#\n                            Trafo\n#\n selected.learner               -\n#\n classif.ksvm.sigma             Y\n#\n classif.randomForest.ntree     -\n\nrdesc = makeResampleDesc(\nCV\n, iters = 2L)\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nres = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\nprint(head(as.data.frame(res$opt.path)))\n#\n       selected.learner classif.ksvm.sigma classif.randomForest.ntree\n#\n 1 classif.randomForest                 NA                        273\n#\n 2         classif.ksvm           10.53605                         NA\n#\n 3         classif.ksvm          -11.79057                         NA\n#\n 4         classif.ksvm           10.42478                         NA\n#\n 5 classif.randomForest                 NA                        394\n#\n 6         classif.ksvm           11.02356                         NA\n#\n   mmce.test.mean dob eol error.message exec.time\n#\n 1     0.04666667   1  NA          \nNA\n     0.084\n#\n 2     0.68000000   1  NA          \nNA\n     0.062\n#\n 3     0.52666667   1  NA          \nNA\n     0.060\n#\n 4     0.68000000   1  NA          \nNA\n     0.056\n#\n 5     0.04666667   1  NA          \nNA\n     0.071\n#\n 6     0.68000000   1  NA          \nNA\n     0.055\n\n\n\n\nMulti-criteria evaluation and optimization\n\n\nDuring tuning you might want to optimize multiple, potentially conflicting, performance measures\nsimultaneously.\n\n\nIn the following example we aim to minimize both, the false positive and the false negative rates\n(\nfpr\n and \nfnr\n).\nWe again tune the hyperparameters of an SVM (function \nksvm\n) with a radial\nbasis kernel and use the \nsonar classification task\n for illustration.\nAs search strategy we choose a random search.\n\n\nFor all available multi-criteria tuning algorithms see \nTuneMultiCritControl\n.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x)\n)\nctrl = makeTuneMultiCritControlRandom(maxit = 30L)\nrdesc = makeResampleDesc(\nHoldout\n)\nres = tuneParamsMultiCrit(\nclassif.ksvm\n, task = sonar.task, resampling = rdesc, par.set = ps,\n  measures = list(fpr, fnr), control = ctrl, show.info = FALSE)\nres\n#\n Tune multicrit result:\n#\n Points on front: 2\n\nhead(as.data.frame(trafoOptPath(res$opt.path)))\n#\n              C        sigma fpr.test.mean fnr.test.mean dob eol\n#\n 1 2.837139e-02  0.004605846          1.00    0.00000000   1  NA\n#\n 2 8.161350e+00 10.073402485          1.00    0.00000000   2  NA\n#\n 3 2.947371e+03  0.023696559          0.15    0.03333333   3  NA\n#\n 4 5.020557e-01  0.279973960          1.00    0.00000000   4  NA\n#\n 5 8.642356e+01 47.600399172          1.00    0.00000000   5  NA\n#\n 6 3.661447e-04  0.715765529          1.00    0.00000000   6  NA\n#\n   error.message exec.time\n#\n 1          \nNA\n     0.043\n#\n 2          \nNA\n     0.044\n#\n 3          \nNA\n     0.056\n#\n 4          \nNA\n     0.053\n#\n 5          \nNA\n     0.053\n#\n 6          \nNA\n     0.044\n\n\n\n\nThe results can be visualized with function \nplotTuneMultiCritResult\n.\nThe plot shows the false positive and false negative rates for all parameter settings evaluated\nduring tuning. Points on the Pareto front are slightly increased.\n\n\nplotTuneMultiCritResult(res)\n\n\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -12, upper = 12, trafo = function(x) 2^x), \n  makeDiscreteParam(\nkernel\n, values = c(\nvanilladot\n, \npolydot\n, \nrbfdot\n)), \n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x, \n    requires = quote(kernel == \nrbfdot\n)), \n  makeIntegerParam(\ndegree\n, lower = 2L, upper = 5L, \n    requires = quote(kernel == \npolydot\n)) \n) \nctrl = makeTuneControlIrace(maxExperiments = 200L) \nrdesc = makeResampleDesc(\nHoldout\n) \nres = tuneParams(\nclassif.ksvm\n, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE) \nprint(head(as.data.frame(res$opt.path))) \nbase.learners = list( \n  makeLearner(\nclassif.ksvm\n), \n  makeLearner(\nclassif.randomForest\n) \n) \nlrn = makeModelMultiplexer(base.learners) \nps = makeModelMultiplexerParamSet(lrn, \n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x), \n  makeIntegerParam(\nntree\n, lower = 1L, upper = 500L) \n) \nprint(ps) \n\nrdesc = makeResampleDesc(\nCV\n, iters = 2L) \nctrl = makeTuneControlIrace(maxExperiments = 200L) \nres = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE) \nprint(head(as.data.frame(res$opt.path))) \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -12, upper = 12, trafo = function(x) 2^x), \n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x) \n) \nctrl = makeTuneMultiCritControlRandom(maxit = 30L) \nrdesc = makeResampleDesc(\nHoldout\n) \nres = tuneParamsMultiCrit(\nclassif.ksvm\n, task = sonar.task, resampling = rdesc, par.set = ps, \n  measures = list(fpr, fnr), control = ctrl, show.info = FALSE) \nres \n\nhead(as.data.frame(trafoOptPath(res$opt.path))) \nplotTuneMultiCritResult(res)", 
            "title": "Advanced Tuning"
        }, 
        {
            "location": "/advanced_tune/index.html#advanced-tuning", 
            "text": "", 
            "title": "Advanced Tuning"
        }, 
        {
            "location": "/advanced_tune/index.html#iterated-f-racing-for-mixed-spaces-and-dependencies", 
            "text": "The package supports a larger number of tuning algorithms, which can all be looked up and\nselected via  TuneControl . One of the cooler algorithms is iterated F-racing from the irace  package (technical description  here ). This not only works for arbitrary parameter types (numeric, integer,\ndiscrete, logical), but also for so-called dependent / hierarchical parameters:  ps = makeParamSet(\n  makeNumericParam( C , lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeDiscreteParam( kernel , values = c( vanilladot ,  polydot ,  rbfdot )),\n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x,\n    requires = quote(kernel ==  rbfdot )),\n  makeIntegerParam( degree , lower = 2L, upper = 5L,\n    requires = quote(kernel ==  polydot ))\n)\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nrdesc = makeResampleDesc( Holdout )\nres = tuneParams( classif.ksvm , iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\nprint(head(as.data.frame(res$opt.path)))\n#             C     kernel     sigma degree mmce.test.mean dob eol\n#  1 -2.8894525    polydot        NA      4           0.06   1  NA\n#  2 -2.6793542 vanilladot        NA     NA           0.04   1  NA\n#  3 -0.7855061     rbfdot 11.049783     NA           0.68   1  NA\n#  4  1.7678978    polydot        NA      5           0.14   1  NA\n#  5  9.7729840 vanilladot        NA     NA           0.04   1  NA\n#  6 -2.7930352     rbfdot -7.198476     NA           0.36   1  NA\n#    error.message exec.time\n#  1           NA      0.026\n#  2           NA      0.025\n#  3           NA      0.025\n#  4           NA      0.024\n#  5           NA      0.023\n#  6           NA      0.032  See how we made the kernel parameters like  sigma  and  degree  dependent on the  kernel \nselection parameters? This approach allows you to tune parameters of multiple kernels at once,\nefficiently concentrating on the ones which work best for your given data set.", 
            "title": "Iterated F-Racing for mixed spaces and dependencies"
        }, 
        {
            "location": "/advanced_tune/index.html#tuning-across-whole-model-spaces-with-modelmultiplexer", 
            "text": "We can now take the following example even one step further. If we use the ModelMultiplexer  we can tune over different model classes at once,\njust as we did with the SVM kernels above.  base.learners = list(\n  makeLearner( classif.ksvm ),\n  makeLearner( classif.randomForest )\n)\nlrn = makeModelMultiplexer(base.learners)  Function  makeModelMultiplexerParamSet  offers a simple way to construct a parameter set for tuning:\nThe parameter names are prefixed automatically and the  requires  element is set, too,\nto make all parameters subordinate to  selected.learner .  ps = makeModelMultiplexerParamSet(lrn,\n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeIntegerParam( ntree , lower = 1L, upper = 500L)\n)\nprint(ps)\n#                                 Type len Def\n#  selected.learner           discrete   -   -\n#  classif.ksvm.sigma          numeric   -   -\n#  classif.randomForest.ntree  integer   -   -\n#                                                        Constr Req Tunable\n#  selected.learner           classif.ksvm,classif.randomForest   -    TRUE\n#  classif.ksvm.sigma                                 -12 to 12   Y    TRUE\n#  classif.randomForest.ntree                          1 to 500   Y    TRUE\n#                             Trafo\n#  selected.learner               -\n#  classif.ksvm.sigma             Y\n#  classif.randomForest.ntree     -\n\nrdesc = makeResampleDesc( CV , iters = 2L)\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nres = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\nprint(head(as.data.frame(res$opt.path)))\n#        selected.learner classif.ksvm.sigma classif.randomForest.ntree\n#  1 classif.randomForest                 NA                        273\n#  2         classif.ksvm           10.53605                         NA\n#  3         classif.ksvm          -11.79057                         NA\n#  4         classif.ksvm           10.42478                         NA\n#  5 classif.randomForest                 NA                        394\n#  6         classif.ksvm           11.02356                         NA\n#    mmce.test.mean dob eol error.message exec.time\n#  1     0.04666667   1  NA           NA      0.084\n#  2     0.68000000   1  NA           NA      0.062\n#  3     0.52666667   1  NA           NA      0.060\n#  4     0.68000000   1  NA           NA      0.056\n#  5     0.04666667   1  NA           NA      0.071\n#  6     0.68000000   1  NA           NA      0.055", 
            "title": "Tuning across whole model spaces with ModelMultiplexer"
        }, 
        {
            "location": "/advanced_tune/index.html#multi-criteria-evaluation-and-optimization", 
            "text": "During tuning you might want to optimize multiple, potentially conflicting, performance measures\nsimultaneously.  In the following example we aim to minimize both, the false positive and the false negative rates\n( fpr  and  fnr ).\nWe again tune the hyperparameters of an SVM (function  ksvm ) with a radial\nbasis kernel and use the  sonar classification task  for illustration.\nAs search strategy we choose a random search.  For all available multi-criteria tuning algorithms see  TuneMultiCritControl .  ps = makeParamSet(\n  makeNumericParam( C , lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x)\n)\nctrl = makeTuneMultiCritControlRandom(maxit = 30L)\nrdesc = makeResampleDesc( Holdout )\nres = tuneParamsMultiCrit( classif.ksvm , task = sonar.task, resampling = rdesc, par.set = ps,\n  measures = list(fpr, fnr), control = ctrl, show.info = FALSE)\nres\n#  Tune multicrit result:\n#  Points on front: 2\n\nhead(as.data.frame(trafoOptPath(res$opt.path)))\n#               C        sigma fpr.test.mean fnr.test.mean dob eol\n#  1 2.837139e-02  0.004605846          1.00    0.00000000   1  NA\n#  2 8.161350e+00 10.073402485          1.00    0.00000000   2  NA\n#  3 2.947371e+03  0.023696559          0.15    0.03333333   3  NA\n#  4 5.020557e-01  0.279973960          1.00    0.00000000   4  NA\n#  5 8.642356e+01 47.600399172          1.00    0.00000000   5  NA\n#  6 3.661447e-04  0.715765529          1.00    0.00000000   6  NA\n#    error.message exec.time\n#  1           NA      0.043\n#  2           NA      0.044\n#  3           NA      0.056\n#  4           NA      0.053\n#  5           NA      0.053\n#  6           NA      0.044  The results can be visualized with function  plotTuneMultiCritResult .\nThe plot shows the false positive and false negative rates for all parameter settings evaluated\nduring tuning. Points on the Pareto front are slightly increased.  plotTuneMultiCritResult(res)", 
            "title": "Multi-criteria evaluation and optimization"
        }, 
        {
            "location": "/advanced_tune/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ps = makeParamSet( \n  makeNumericParam( C , lower = -12, upper = 12, trafo = function(x) 2^x), \n  makeDiscreteParam( kernel , values = c( vanilladot ,  polydot ,  rbfdot )), \n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x, \n    requires = quote(kernel ==  rbfdot )), \n  makeIntegerParam( degree , lower = 2L, upper = 5L, \n    requires = quote(kernel ==  polydot )) \n) \nctrl = makeTuneControlIrace(maxExperiments = 200L) \nrdesc = makeResampleDesc( Holdout ) \nres = tuneParams( classif.ksvm , iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE) \nprint(head(as.data.frame(res$opt.path))) \nbase.learners = list( \n  makeLearner( classif.ksvm ), \n  makeLearner( classif.randomForest ) \n) \nlrn = makeModelMultiplexer(base.learners) \nps = makeModelMultiplexerParamSet(lrn, \n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x), \n  makeIntegerParam( ntree , lower = 1L, upper = 500L) \n) \nprint(ps) \n\nrdesc = makeResampleDesc( CV , iters = 2L) \nctrl = makeTuneControlIrace(maxExperiments = 200L) \nres = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE) \nprint(head(as.data.frame(res$opt.path))) \nps = makeParamSet( \n  makeNumericParam( C , lower = -12, upper = 12, trafo = function(x) 2^x), \n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x) \n) \nctrl = makeTuneMultiCritControlRandom(maxit = 30L) \nrdesc = makeResampleDesc( Holdout ) \nres = tuneParamsMultiCrit( classif.ksvm , task = sonar.task, resampling = rdesc, par.set = ps, \n  measures = list(fpr, fnr), control = ctrl, show.info = FALSE) \nres \n\nhead(as.data.frame(trafoOptPath(res$opt.path))) \nplotTuneMultiCritResult(res)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/feature_selection/index.html", 
            "text": "Feature Selection\n\n\nOften, data sets include a large number of features.\nThe technique of extracting a subset of relevant features is called feature selection.\nFeature selection can enhance the interpretability of the model, speed up the learning\nprocess and improve the learner performance.\nThere exist different approaches to identify the relevant features.\n\nmlr\n supports \nfilter\n and \nwrapper methods\n.\n\n\nFilter methods\n\n\nFilter methods assign an importance value to each feature.\nBased on these values the features can be ranked and a feature subset can be selected.\n\n\nCalculating the feature importance\n\n\nDifferent methods for calculating the feature importance are built into \nmlr\n's function\n\ngenerateFilterValuesData\n (\ngetFilterValues\n has been deprecated in favor of \ngenerateFilterValuesData\n.). Currently, classification, regression and survival analysis tasks\nare supported. A table showing all available methods can be found \nhere\n.\n\n\nFunction \ngenerateFilterValuesData\n requires the \nTask\n and a character string specifying the filter\nmethod.\n\n\nfv = generateFilterValuesData(iris.task, method = \ninformation.gain\n)\nfv\n#\n FilterValues:\n#\n Task: iris_example\n#\n           name    type information.gain\n#\n 1 Sepal.Length numeric        0.4521286\n#\n 2  Sepal.Width numeric        0.2672750\n#\n 3 Petal.Length numeric        0.9402853\n#\n 4  Petal.Width numeric        0.9554360\n\n\n\n\nfv\n is a \nFilterValues\n object and \nfv$data\n contains a \ndata.frame\n\nthat gives the importance values for all features. Optionally, a vector of filter methods can be\npassed.\n\n\nfv2 = generateFilterValuesData(iris.task, method = c(\ninformation.gain\n, \nchi.squared\n))\nfv2$data\n#\n           name    type information.gain chi.squared\n#\n 1 Sepal.Length numeric        0.4521286   0.6288067\n#\n 2  Sepal.Width numeric        0.2672750   0.4922162\n#\n 3 Petal.Length numeric        0.9402853   0.9346311\n#\n 4  Petal.Width numeric        0.9554360   0.9432359\n\n\n\n\nA bar plot of importance values for the individual features can be obtained using\nfunction \nplotFilterValues\n.\n\n\nplotFilterValues(fv2)\n\n\n\n\n\n\nBy default \nplotFilterValues\n will create facetted subplots if multiple filter methods are passed as input to\n\ngenerateFilterValuesData\n.\n\n\nThere is also an experimental \nggvis\n plotting function, \nplotFilterValuesGGVIS\n. This takes the same\narguments as \nplotFilterValues\n and produces a \nshiny\n application\nthat allows the interactive selection of the displayed filter method, the number of features selected, and the sorting method (e.g., ascending or descending).\n\n\nplotFilterValuesGGVIS(fv2)\n\n\n\n\nAccording to the \n\"information.gain\"\n measure, \nPetal.Width\n and \nPetal.Length\n\ncontain the most information about the target variable \nSpecies\n.\n\n\nSelecting a feature subset\n\n\nWith \nmlr\n's function \nfilterFeatures\n you can create a new \nTask\n by leaving out\nfeatures of lower importance.\n\n\nThere are several ways to select a feature subset based on feature importance values:\n\n\n\n\nKeep a certain \nabsolute number\n (\nabs\n) of features with highest importance.\n\n\nKeep a certain \npercentage\n (\nperc\n) of features with highest importance.\n\n\nKeep all features whose importance exceeds a certain \nthreshold value\n (\nthreshold\n).\n\n\n\n\nFunction \nfilterFeatures\n supports these three methods as shown in the following example.\nMoreover, you can either specify the \nmethod\n for calculating the feature importance or you can\nuse previously computed importance values via argument \nfval\n.\n\n\n## Keep the 2 most important features\nfiltered.task = filterFeatures(iris.task, method = \ninformation.gain\n, abs = 2)\n\n## Keep the 25% most important features\nfiltered.task = filterFeatures(iris.task, fval = fv, perc = 0.25)\n\n## Keep all features with importance greater than 0.5\nfiltered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5)\nfiltered.task\n#\n Supervised task: iris_example\n#\n Type: classif\n#\n Target: Species\n#\n Observations: 150\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n           2           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 3\n#\n     setosa versicolor  virginica \n#\n         50         50         50 \n#\n Positive class: NA\n\n\n\n\nFuse a learner with a filter method\n\n\nOften feature selection based on a filter method is part of the data preprocessing and in\na subsequent step a learning method is applied to the filtered data.\nIn a proper experimental setup you might want to automate the selection of the\nfeatures so that it can be part of the validation method of your choice.\nA \nLearner\n can be fused with a filter method by function \nmakeFilterWrapper\n.\nThe resulting \nLearner\n has the additional class attribute \nFilterWrapper\n.\n\n\nIn the following example we calculate the 10-fold cross-validated error rate (\nmmce\n)\nof the \nk nearest neighbor classifier\n with preceding feature selection on the\n\niris\n data set.\nWe use \n\"information.gain\"\n as importance measure and select the 2 features with\nhighest importance.\nIn each resampling iteration feature selection is carried out on the corresponding training\ndata set before fitting the learner.\n\n\nlrn = makeFilterWrapper(learner = \nclassif.fnn\n, fw.method = \ninformation.gain\n, fw.abs = 2)\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nr = resample(learner = lrn, task = iris.task, resampling = rdesc, show.info = FALSE, models = TRUE)\nr$aggr\n#\n mmce.test.mean \n#\n           0.04\n\n\n\n\nYou may want to know which features have been used. Luckily, we have called\n\nresample\n with the argument \nmodels = TRUE\n, which means that \nr$models\n\ncontains a \nlist\n of \nmodels\n fitted in the individual resampling iterations.\nIn order to access the selected feature subsets we can call \ngetFilteredFeatures\n on each model.\n\n\nsfeats = sapply(r$models, getFilteredFeatures)\ntable(sfeats)\n#\n sfeats\n#\n Petal.Length  Petal.Width \n#\n           10           10\n\n\n\n\nThe selection of features seems to be very stable.\nThe features \nSepal.Length\n and \nSepal.Width\n did not make it into a single fold.\n\n\nTuning the size of the feature subset\n\n\nIn the above examples the number/percentage of features to select or the threshold value\nhave been arbitrarily chosen.\nIf filtering is a preprocessing step before applying a learning method optimal values\nwith regard to the learner performance can be found by \ntuning\n.\n\n\nIn the following regression example we consider the \nBostonHousing\n data set.\nWe use a \nlinear regression model\n and determine the optimal percentage value for feature selection\nsuch that the 3-fold cross-validated \nmean squared error\n of the learner is minimal.\nAs search strategy for tuning a grid search is used.\n\n\nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n)\nps = makeParamSet(makeDiscreteParam(\nfw.perc\n, values = seq(0.2, 0.5, 0.05)))\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nres = tuneParams(lrn, task = bh.task, resampling = rdesc, par.set = ps,\n  control = makeTuneControlGrid())\n#\n [Tune] Started tuning learner regr.lm.filtered for parameter set:\n#\n             Type len Def                         Constr Req Tunable Trafo\n#\n fw.perc discrete   -   - 0.2,0.25,0.3,0.35,0.4,0.45,0.5   -    TRUE     -\n#\n With control class: TuneControlGrid\n#\n Imputation value: Inf\n#\n [Tune-x] 1: fw.perc=0.2\n#\n [Tune-y] 1: mse.test.mean=40.5957841; time: 0.0 min\n#\n [Tune-x] 2: fw.perc=0.25\n#\n [Tune-y] 2: mse.test.mean=40.5957841; time: 0.0 min\n#\n [Tune-x] 3: fw.perc=0.3\n#\n [Tune-y] 3: mse.test.mean=37.0559228; time: 0.0 min\n#\n [Tune-x] 4: fw.perc=0.35\n#\n [Tune-y] 4: mse.test.mean=35.8371232; time: 0.0 min\n#\n [Tune-x] 5: fw.perc=0.4\n#\n [Tune-y] 5: mse.test.mean=35.8371232; time: 0.0 min\n#\n [Tune-x] 6: fw.perc=0.45\n#\n [Tune-y] 6: mse.test.mean=27.3995509; time: 0.0 min\n#\n [Tune-x] 7: fw.perc=0.5\n#\n [Tune-y] 7: mse.test.mean=27.3995509; time: 0.0 min\n#\n [Tune] Result: fw.perc=0.5 : mse.test.mean=27.3995509\nres\n#\n Tune result:\n#\n Op. pars: fw.perc=0.5\n#\n mse.test.mean=27.3995509\n\n\n\n\nThe performance of all percentage values visited during tuning is:\n\n\nas.data.frame(res$opt.path)\n#\n   fw.perc mse.test.mean dob eol error.message exec.time\n#\n 1     0.2      40.59578   1  NA          \nNA\n     0.191\n#\n 2    0.25      40.59578   2  NA          \nNA\n     0.203\n#\n 3     0.3      37.05592   3  NA          \nNA\n     0.276\n#\n 4    0.35      35.83712   4  NA          \nNA\n     0.195\n#\n 5     0.4      35.83712   5  NA          \nNA\n     0.210\n#\n 6    0.45      27.39955   6  NA          \nNA\n     0.193\n#\n 7     0.5      27.39955   7  NA          \nNA\n     0.193\n\n\n\n\nThe optimal percentage and the corresponding performance can be accessed as follows:\n\n\nres$x\n#\n $fw.perc\n#\n [1] 0.5\nres$y\n#\n mse.test.mean \n#\n      27.39955\n\n\n\n\nAfter tuning we can generate a new wrapped learner with the optimal percentage value for\nfurther use.\n\n\nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n, fw.perc = res$x$fw.perc)\nmod = train(lrn, bh.task)\nmod\n#\n Model for learner.id=regr.lm.filtered; learner.class=FilterWrapper\n#\n Trained on: task.id = BostonHousing-example; obs = 506; features = 13\n#\n Hyperparameters: fw.method=chi.squared,fw.perc=0.5\n\ngetFilteredFeatures(mod)\n#\n [1] \ncrim\n  \nzn\n    \nrm\n    \ndis\n   \nrad\n   \nlstat\n\n\n\n\n\nHere is another example using \nmulti-criteria tuning\n.\nWe consider \nlinear discriminant analysis\n with precedent feature selection based on\nthe Chi-squared statistic of independence (\n\"chi.squared\"\n) on the \nSonar\n\ndata set and tune the threshold value.\nDuring tuning both, the false positive and the false negative rate (\nfpr\n and\n\nfnr\n), are minimized. As search strategy we choose a random search\n(see \nmakeTuneMultiCritControlRandom\n).\n\n\nlrn = makeFilterWrapper(learner = \nclassif.lda\n, fw.method = \nchi.squared\n)\nps = makeParamSet(makeNumericParam(\nfw.threshold\n, lower = 0.1, upper = 0.9))\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nres = tuneParamsMultiCrit(lrn, task = sonar.task, resampling = rdesc, par.set = ps,\n  measures = list(fpr, fnr), control = makeTuneMultiCritControlRandom(maxit = 50L),\n  show.info = FALSE)\nres\n#\n Tune multicrit result:\n#\n Points on front: 13\nhead(as.data.frame(res$opt.path))\n#\n   fw.threshold fpr.test.mean fnr.test.mean dob eol error.message exec.time\n#\n 1    0.4892321     0.3092818     0.2639033   1  NA          \nNA\n     1.637\n#\n 2    0.2481696     0.2045499     0.2319697   2  NA          \nNA\n     1.700\n#\n 3    0.7691875     0.5128000     0.3459740   3  NA          \nNA\n     1.758\n#\n 4    0.1470133     0.2045499     0.2319697   4  NA          \nNA\n     1.762\n#\n 5    0.5958241     0.5028216     0.5239538   5  NA          \nNA\n     1.629\n#\n 6    0.6892421     0.6323959     0.4480808   6  NA          \nNA\n     1.875\n\n\n\n\nThe results can be visualized with function \nplotTuneMultiCritResult\n.\nThe plot shows the false positive and false negative rates for all parameter values visited\nduring tuning. The size of the points on the Pareto front is slightly increased.\n\n\nplotTuneMultiCritResult(res)\n\n\n\n\n\n\nWrapper methods\n\n\nWrapper methods use the performance of a learning algorithm to assess the usefulness of\na feature set.\nIn order to select a feature subset a learner is trained repeatedly on different feature subsets\nand the subset which leads to the best learner performance is chosen.\n\n\nIn order to use the wrapper approach we have to decide:\n\n\n\n\nHow to assess the performance: This involves choosing a performance measure that serves\n  as feature selection criterion and a resampling strategy.\n\n\nWhich learning method to use.\n\n\nHow to search the space of possible feature subsets.\n\n\n\n\nThe search strategy is defined by functions following the naming convention\n\nmakeFeatSelControl\nsearch_strategy\n.\nThe following search strategies are available:\n\n\n\n\nExhaustive search (\nmakeFeatSelControlExhaustive\n),\n\n\nGenetic algorithm (\nmakeFeatSelControlGA\n),\n\n\nRandom search (\nmakeFeatSelControlRandom\n),\n\n\nDeterministic forward or backward search (\nmakeFeatSelControlSequential\n).\n\n\n\n\nSelect a feature subset\n\n\nFeature selection can be conducted with function \nselectFeatures\n.\n\n\nIn the following example we perform an exhaustive search on the\n\nWisconsin Prognostic Breast Cancer\n data set.\nAs learning method we use the \nCox proportional hazards model\n.\nThe performance is assessed by the holdout estimate of the concordance index\n(\ncindex\n).\n\n\n## Specify the search strategy\nctrl = makeFeatSelControlRandom(maxit = 20L)\nctrl\n#\n FeatSel control: FeatSelControlRandom\n#\n Same resampling instance: TRUE\n#\n Imputation value: \nworst\n\n#\n Max. features: \nnot used\n\n#\n Max. iterations: 20\n#\n Tune threshold: FALSE\n#\n Further arguments: prob=0.5\n\n\n\n\nctrl\n is a \nFeatSelControl\n object that contains information about the search strategy\nand potential parameter values.\n\n\n## Resample description\nrdesc = makeResampleDesc(\nHoldout\n)\n\n## Select features\nsfeats = selectFeatures(learner = \nsurv.coxph\n, task = wpbc.task, resampling = rdesc,\n  control = ctrl, show.info = FALSE)\nsfeats\n#\n FeatSel result:\n#\n Features (17): mean_radius, mean_area, mean_smoothness, mean_concavepoints, mean_symmetry, mean_fractaldim, SE_texture, SE_perimeter, SE_smoothness, SE_compactness, SE_concavity, SE_concavepoints, worst_area, worst_compactness, worst_concavepoints, tsize, pnodes\n#\n cindex.test.mean=0.7137990\n\n\n\n\nsfeats\nis a \nFeatSelResult\n object.\nThe selected features and the corresponding performance can be accessed as follows:\n\n\nsfeats$x\n#\n  [1] \nmean_radius\n         \nmean_area\n           \nmean_smoothness\n    \n#\n  [4] \nmean_concavepoints\n  \nmean_symmetry\n       \nmean_fractaldim\n    \n#\n  [7] \nSE_texture\n          \nSE_perimeter\n        \nSE_smoothness\n      \n#\n [10] \nSE_compactness\n      \nSE_concavity\n        \nSE_concavepoints\n   \n#\n [13] \nworst_area\n          \nworst_compactness\n   \nworst_concavepoints\n\n#\n [16] \ntsize\n               \npnodes\n\nsfeats$y\n#\n cindex.test.mean \n#\n         0.713799\n\n\n\n\nIn a second example we fit a simple linear regression model to the \nBostonHousing\n\ndata set and use a sequential search to find a feature set that minimizes the mean squared\nerror (\nmse\n).\n\nmethod = \"sfs\"\n indicates that we want to conduct a sequential forward search where features\nare added to the model until the performance cannot be improved anymore.\nSee the documentation page \nmakeFeatSelControlSequential\n for other available\nsequential search methods.\nThe search is stopped if the improvement is smaller than \nalpha = 0.02\n.\n\n\n## Specify the search strategy\nctrl = makeFeatSelControlSequential(method = \nsfs\n, alpha = 0.02)\n\n## Select features\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nsfeats = selectFeatures(learner = \nregr.lm\n, task = bh.task, resampling = rdesc, control = ctrl,\n  show.info = FALSE)\nsfeats\n#\n FeatSel result:\n#\n Features (11): crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat\n#\n mse.test.mean=23.7319094\n\n\n\n\nFurther information about the sequential feature selection process can be obtained by\nfunction \nanalyzeFeatSelResult\n.\n\n\nanalyzeFeatSelResult(sfeats)\n#\n Features         : 11\n#\n Performance      : mse.test.mean=23.7319094\n#\n crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat\n#\n \n#\n Path to optimum:\n#\n - Features:    0  Init   :                       Perf = 84.831  Diff: NA  *\n#\n - Features:    1  Add    : lstat                 Perf = 38.894  Diff: 45.936  *\n#\n - Features:    2  Add    : rm                    Perf = 31.279  Diff: 7.6156  *\n#\n - Features:    3  Add    : ptratio               Perf = 28.108  Diff: 3.1703  *\n#\n - Features:    4  Add    : dis                   Perf = 27.48  Diff: 0.62813  *\n#\n - Features:    5  Add    : nox                   Perf = 26.079  Diff: 1.4008  *\n#\n - Features:    6  Add    : b                     Perf = 25.563  Diff: 0.51594  *\n#\n - Features:    7  Add    : chas                  Perf = 25.132  Diff: 0.43097  *\n#\n - Features:    8  Add    : zn                    Perf = 24.792  Diff: 0.34018  *\n#\n - Features:    9  Add    : rad                   Perf = 24.599  Diff: 0.19327  *\n#\n - Features:   10  Add    : tax                   Perf = 24.082  Diff: 0.51706  *\n#\n - Features:   11  Add    : crim                  Perf = 23.732  Diff: 0.35  *\n#\n \n#\n Stopped, because no improving feature was found.\n\n\n\n\nFuse a learner with feature selection\n\n\nA \nLearner\n can be fused with a feature selection strategy (i.e., a search\nstrategy, a performance measure and a resampling strategy) by function \nmakeFeatSelWrapper\n.\nDuring training features are selected according to the specified selection scheme. Then, the\nlearner is trained on the selected feature subset.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nlrn = makeFeatSelWrapper(\nsurv.coxph\n, resampling = rdesc,\n  control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE)\nmod = train(lrn, task = wpbc.task)\nmod\n#\n Model for learner.id=surv.coxph.featsel; learner.class=FeatSelWrapper\n#\n Trained on: task.id = wpbc-example; obs = 194; features = 32\n#\n Hyperparameters:\n\n\n\n\nThe result of the feature selection can be extracted by function \ngetFeatSelResult\n.\n\n\nsfeats = getFeatSelResult(mod)\nsfeats\n#\n FeatSel result:\n#\n Features (19): mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness, mean_compactness, mean_concavepoints, mean_fractaldim, SE_compactness, SE_concavity, SE_concavepoints, SE_symmetry, worst_texture, worst_perimeter, worst_area, worst_concavepoints, worst_symmetry, tsize, pnodes\n#\n cindex.test.mean=0.6308754\n\n\n\n\nThe selected features are:\n\n\nsfeats$x\n#\n  [1] \nmean_radius\n         \nmean_texture\n        \nmean_perimeter\n     \n#\n  [4] \nmean_area\n           \nmean_smoothness\n     \nmean_compactness\n   \n#\n  [7] \nmean_concavepoints\n  \nmean_fractaldim\n     \nSE_compactness\n     \n#\n [10] \nSE_concavity\n        \nSE_concavepoints\n    \nSE_symmetry\n        \n#\n [13] \nworst_texture\n       \nworst_perimeter\n     \nworst_area\n         \n#\n [16] \nworst_concavepoints\n \nworst_symmetry\n      \ntsize\n              \n#\n [19] \npnodes\n\n\n\n\n\nThe 5-fold cross-validated performance of the learner specified above can be computed as\nfollows:\n\n\nout.rdesc = makeResampleDesc(\nCV\n, iters = 5)\n\nr = resample(learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE,\n  show.info = FALSE)\nr$aggr\n#\n cindex.test.mean \n#\n         0.632357\n\n\n\n\nThe selected feature sets in the individual resampling iterations can be extracted as follows:\n\n\nlapply(r$models, getFeatSelResult)\n#\n [[1]]\n#\n FeatSel result:\n#\n Features (18): mean_texture, mean_area, mean_smoothness, mean_compactness, mean_concavity, mean_symmetry, SE_radius, SE_compactness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_radius, worst_smoothness, worst_compactness, worst_concavity, worst_symmetry, tsize, pnodes\n#\n cindex.test.mean=0.6599048\n#\n \n#\n [[2]]\n#\n FeatSel result:\n#\n Features (12): mean_area, mean_compactness, mean_symmetry, mean_fractaldim, SE_perimeter, SE_area, SE_concavity, SE_symmetry, worst_texture, worst_smoothness, worst_fractaldim, tsize\n#\n cindex.test.mean=0.6524759\n#\n \n#\n [[3]]\n#\n FeatSel result:\n#\n Features (14): mean_compactness, mean_symmetry, mean_fractaldim, SE_radius, SE_perimeter, SE_smoothness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_concavity, worst_concavepoints, worst_symmetry, worst_fractaldim, pnodes\n#\n cindex.test.mean=0.6074810\n#\n \n#\n [[4]]\n#\n FeatSel result:\n#\n Features (18): mean_radius, mean_texture, mean_perimeter, mean_compactness, mean_concavity, SE_texture, SE_area, SE_smoothness, SE_concavity, SE_symmetry, SE_fractaldim, worst_radius, worst_compactness, worst_concavepoints, worst_symmetry, worst_fractaldim, tsize, pnodes\n#\n cindex.test.mean=0.6526122\n#\n \n#\n [[5]]\n#\n FeatSel result:\n#\n Features (14): mean_radius, mean_texture, mean_compactness, mean_concavepoints, mean_symmetry, SE_texture, SE_compactness, SE_symmetry, SE_fractaldim, worst_radius, worst_smoothness, worst_compactness, worst_concavity, pnodes\n#\n cindex.test.mean=0.6262729\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nfv = generateFilterValuesData(iris.task, method = \ninformation.gain\n) \nfv \nfv2 = generateFilterValuesData(iris.task, method = c(\ninformation.gain\n, \nchi.squared\n)) \nfv2$data \nplotFilterValues(fv2) \n## plotFilterValuesGGVIS(fv2) \n## Keep the 2 most important features \nfiltered.task = filterFeatures(iris.task, method = \ninformation.gain\n, abs = 2) \n\n## Keep the 25% most important features \nfiltered.task = filterFeatures(iris.task, fval = fv, perc = 0.25) \n\n## Keep all features with importance greater than 0.5 \nfiltered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5) \nfiltered.task \nlrn = makeFilterWrapper(learner = \nclassif.fnn\n, fw.method = \ninformation.gain\n, fw.abs = 2) \nrdesc = makeResampleDesc(\nCV\n, iters = 10) \nr = resample(learner = lrn, task = iris.task, resampling = rdesc, show.info = FALSE, models = TRUE) \nr$aggr \nsfeats = sapply(r$models, getFilteredFeatures) \ntable(sfeats) \nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n) \nps = makeParamSet(makeDiscreteParam(\nfw.perc\n, values = seq(0.2, 0.5, 0.05))) \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nres = tuneParams(lrn, task = bh.task, resampling = rdesc, par.set = ps, \n  control = makeTuneControlGrid()) \nres \nas.data.frame(res$opt.path) \nres$x \nres$y \nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n, fw.perc = res$x$fw.perc) \nmod = train(lrn, bh.task) \nmod \n\ngetFilteredFeatures(mod) \nlrn = makeFilterWrapper(learner = \nclassif.lda\n, fw.method = \nchi.squared\n) \nps = makeParamSet(makeNumericParam(\nfw.threshold\n, lower = 0.1, upper = 0.9)) \nrdesc = makeResampleDesc(\nCV\n, iters = 10) \nres = tuneParamsMultiCrit(lrn, task = sonar.task, resampling = rdesc, par.set = ps, \n  measures = list(fpr, fnr), control = makeTuneMultiCritControlRandom(maxit = 50L), \n  show.info = FALSE) \nres \nhead(as.data.frame(res$opt.path)) \nplotTuneMultiCritResult(res) \n## Specify the search strategy \nctrl = makeFeatSelControlRandom(maxit = 20L) \nctrl \n## Resample description \nrdesc = makeResampleDesc(\nHoldout\n) \n\n## Select features \nsfeats = selectFeatures(learner = \nsurv.coxph\n, task = wpbc.task, resampling = rdesc, \n  control = ctrl, show.info = FALSE) \nsfeats \nsfeats$x \nsfeats$y \n## Specify the search strategy \nctrl = makeFeatSelControlSequential(method = \nsfs\n, alpha = 0.02) \n\n## Select features \nrdesc = makeResampleDesc(\nCV\n, iters = 10) \nsfeats = selectFeatures(learner = \nregr.lm\n, task = bh.task, resampling = rdesc, control = ctrl, \n  show.info = FALSE) \nsfeats \nanalyzeFeatSelResult(sfeats) \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nlrn = makeFeatSelWrapper(\nsurv.coxph\n, resampling = rdesc, \n  control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE) \nmod = train(lrn, task = wpbc.task) \nmod \nsfeats = getFeatSelResult(mod) \nsfeats \nsfeats$x \nout.rdesc = makeResampleDesc(\nCV\n, iters = 5) \n\nr = resample(learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE, \n  show.info = FALSE) \nr$aggr \nlapply(r$models, getFeatSelResult)", 
            "title": "Feature Selection"
        }, 
        {
            "location": "/feature_selection/index.html#feature-selection", 
            "text": "Often, data sets include a large number of features.\nThe technique of extracting a subset of relevant features is called feature selection.\nFeature selection can enhance the interpretability of the model, speed up the learning\nprocess and improve the learner performance.\nThere exist different approaches to identify the relevant features. mlr  supports  filter  and  wrapper methods .", 
            "title": "Feature Selection"
        }, 
        {
            "location": "/feature_selection/index.html#filter-methods", 
            "text": "Filter methods assign an importance value to each feature.\nBased on these values the features can be ranked and a feature subset can be selected.", 
            "title": "Filter methods"
        }, 
        {
            "location": "/feature_selection/index.html#calculating-the-feature-importance", 
            "text": "Different methods for calculating the feature importance are built into  mlr 's function generateFilterValuesData  ( getFilterValues  has been deprecated in favor of  generateFilterValuesData .). Currently, classification, regression and survival analysis tasks\nare supported. A table showing all available methods can be found  here .  Function  generateFilterValuesData  requires the  Task  and a character string specifying the filter\nmethod.  fv = generateFilterValuesData(iris.task, method =  information.gain )\nfv\n#  FilterValues:\n#  Task: iris_example\n#            name    type information.gain\n#  1 Sepal.Length numeric        0.4521286\n#  2  Sepal.Width numeric        0.2672750\n#  3 Petal.Length numeric        0.9402853\n#  4  Petal.Width numeric        0.9554360  fv  is a  FilterValues  object and  fv$data  contains a  data.frame \nthat gives the importance values for all features. Optionally, a vector of filter methods can be\npassed.  fv2 = generateFilterValuesData(iris.task, method = c( information.gain ,  chi.squared ))\nfv2$data\n#            name    type information.gain chi.squared\n#  1 Sepal.Length numeric        0.4521286   0.6288067\n#  2  Sepal.Width numeric        0.2672750   0.4922162\n#  3 Petal.Length numeric        0.9402853   0.9346311\n#  4  Petal.Width numeric        0.9554360   0.9432359  A bar plot of importance values for the individual features can be obtained using\nfunction  plotFilterValues .  plotFilterValues(fv2)   By default  plotFilterValues  will create facetted subplots if multiple filter methods are passed as input to generateFilterValuesData .  There is also an experimental  ggvis  plotting function,  plotFilterValuesGGVIS . This takes the same\narguments as  plotFilterValues  and produces a  shiny  application\nthat allows the interactive selection of the displayed filter method, the number of features selected, and the sorting method (e.g., ascending or descending).  plotFilterValuesGGVIS(fv2)  According to the  \"information.gain\"  measure,  Petal.Width  and  Petal.Length \ncontain the most information about the target variable  Species .", 
            "title": "Calculating the feature importance"
        }, 
        {
            "location": "/feature_selection/index.html#selecting-a-feature-subset", 
            "text": "With  mlr 's function  filterFeatures  you can create a new  Task  by leaving out\nfeatures of lower importance.  There are several ways to select a feature subset based on feature importance values:   Keep a certain  absolute number  ( abs ) of features with highest importance.  Keep a certain  percentage  ( perc ) of features with highest importance.  Keep all features whose importance exceeds a certain  threshold value  ( threshold ).   Function  filterFeatures  supports these three methods as shown in the following example.\nMoreover, you can either specify the  method  for calculating the feature importance or you can\nuse previously computed importance values via argument  fval .  ## Keep the 2 most important features\nfiltered.task = filterFeatures(iris.task, method =  information.gain , abs = 2)\n\n## Keep the 25% most important features\nfiltered.task = filterFeatures(iris.task, fval = fv, perc = 0.25)\n\n## Keep all features with importance greater than 0.5\nfiltered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5)\nfiltered.task\n#  Supervised task: iris_example\n#  Type: classif\n#  Target: Species\n#  Observations: 150\n#  Features:\n#     numerics     factors     ordered functionals \n#            2           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 3\n#      setosa versicolor  virginica \n#          50         50         50 \n#  Positive class: NA", 
            "title": "Selecting a feature subset"
        }, 
        {
            "location": "/feature_selection/index.html#fuse-a-learner-with-a-filter-method", 
            "text": "Often feature selection based on a filter method is part of the data preprocessing and in\na subsequent step a learning method is applied to the filtered data.\nIn a proper experimental setup you might want to automate the selection of the\nfeatures so that it can be part of the validation method of your choice.\nA  Learner  can be fused with a filter method by function  makeFilterWrapper .\nThe resulting  Learner  has the additional class attribute  FilterWrapper .  In the following example we calculate the 10-fold cross-validated error rate ( mmce )\nof the  k nearest neighbor classifier  with preceding feature selection on the iris  data set.\nWe use  \"information.gain\"  as importance measure and select the 2 features with\nhighest importance.\nIn each resampling iteration feature selection is carried out on the corresponding training\ndata set before fitting the learner.  lrn = makeFilterWrapper(learner =  classif.fnn , fw.method =  information.gain , fw.abs = 2)\nrdesc = makeResampleDesc( CV , iters = 10)\nr = resample(learner = lrn, task = iris.task, resampling = rdesc, show.info = FALSE, models = TRUE)\nr$aggr\n#  mmce.test.mean \n#            0.04  You may want to know which features have been used. Luckily, we have called resample  with the argument  models = TRUE , which means that  r$models \ncontains a  list  of  models  fitted in the individual resampling iterations.\nIn order to access the selected feature subsets we can call  getFilteredFeatures  on each model.  sfeats = sapply(r$models, getFilteredFeatures)\ntable(sfeats)\n#  sfeats\n#  Petal.Length  Petal.Width \n#            10           10  The selection of features seems to be very stable.\nThe features  Sepal.Length  and  Sepal.Width  did not make it into a single fold.", 
            "title": "Fuse a learner with a filter method"
        }, 
        {
            "location": "/feature_selection/index.html#tuning-the-size-of-the-feature-subset", 
            "text": "In the above examples the number/percentage of features to select or the threshold value\nhave been arbitrarily chosen.\nIf filtering is a preprocessing step before applying a learning method optimal values\nwith regard to the learner performance can be found by  tuning .  In the following regression example we consider the  BostonHousing  data set.\nWe use a  linear regression model  and determine the optimal percentage value for feature selection\nsuch that the 3-fold cross-validated  mean squared error  of the learner is minimal.\nAs search strategy for tuning a grid search is used.  lrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared )\nps = makeParamSet(makeDiscreteParam( fw.perc , values = seq(0.2, 0.5, 0.05)))\nrdesc = makeResampleDesc( CV , iters = 3)\nres = tuneParams(lrn, task = bh.task, resampling = rdesc, par.set = ps,\n  control = makeTuneControlGrid())\n#  [Tune] Started tuning learner regr.lm.filtered for parameter set:\n#              Type len Def                         Constr Req Tunable Trafo\n#  fw.perc discrete   -   - 0.2,0.25,0.3,0.35,0.4,0.45,0.5   -    TRUE     -\n#  With control class: TuneControlGrid\n#  Imputation value: Inf\n#  [Tune-x] 1: fw.perc=0.2\n#  [Tune-y] 1: mse.test.mean=40.5957841; time: 0.0 min\n#  [Tune-x] 2: fw.perc=0.25\n#  [Tune-y] 2: mse.test.mean=40.5957841; time: 0.0 min\n#  [Tune-x] 3: fw.perc=0.3\n#  [Tune-y] 3: mse.test.mean=37.0559228; time: 0.0 min\n#  [Tune-x] 4: fw.perc=0.35\n#  [Tune-y] 4: mse.test.mean=35.8371232; time: 0.0 min\n#  [Tune-x] 5: fw.perc=0.4\n#  [Tune-y] 5: mse.test.mean=35.8371232; time: 0.0 min\n#  [Tune-x] 6: fw.perc=0.45\n#  [Tune-y] 6: mse.test.mean=27.3995509; time: 0.0 min\n#  [Tune-x] 7: fw.perc=0.5\n#  [Tune-y] 7: mse.test.mean=27.3995509; time: 0.0 min\n#  [Tune] Result: fw.perc=0.5 : mse.test.mean=27.3995509\nres\n#  Tune result:\n#  Op. pars: fw.perc=0.5\n#  mse.test.mean=27.3995509  The performance of all percentage values visited during tuning is:  as.data.frame(res$opt.path)\n#    fw.perc mse.test.mean dob eol error.message exec.time\n#  1     0.2      40.59578   1  NA           NA      0.191\n#  2    0.25      40.59578   2  NA           NA      0.203\n#  3     0.3      37.05592   3  NA           NA      0.276\n#  4    0.35      35.83712   4  NA           NA      0.195\n#  5     0.4      35.83712   5  NA           NA      0.210\n#  6    0.45      27.39955   6  NA           NA      0.193\n#  7     0.5      27.39955   7  NA           NA      0.193  The optimal percentage and the corresponding performance can be accessed as follows:  res$x\n#  $fw.perc\n#  [1] 0.5\nres$y\n#  mse.test.mean \n#       27.39955  After tuning we can generate a new wrapped learner with the optimal percentage value for\nfurther use.  lrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared , fw.perc = res$x$fw.perc)\nmod = train(lrn, bh.task)\nmod\n#  Model for learner.id=regr.lm.filtered; learner.class=FilterWrapper\n#  Trained on: task.id = BostonHousing-example; obs = 506; features = 13\n#  Hyperparameters: fw.method=chi.squared,fw.perc=0.5\n\ngetFilteredFeatures(mod)\n#  [1]  crim    zn      rm      dis     rad     lstat   Here is another example using  multi-criteria tuning .\nWe consider  linear discriminant analysis  with precedent feature selection based on\nthe Chi-squared statistic of independence ( \"chi.squared\" ) on the  Sonar \ndata set and tune the threshold value.\nDuring tuning both, the false positive and the false negative rate ( fpr  and fnr ), are minimized. As search strategy we choose a random search\n(see  makeTuneMultiCritControlRandom ).  lrn = makeFilterWrapper(learner =  classif.lda , fw.method =  chi.squared )\nps = makeParamSet(makeNumericParam( fw.threshold , lower = 0.1, upper = 0.9))\nrdesc = makeResampleDesc( CV , iters = 10)\nres = tuneParamsMultiCrit(lrn, task = sonar.task, resampling = rdesc, par.set = ps,\n  measures = list(fpr, fnr), control = makeTuneMultiCritControlRandom(maxit = 50L),\n  show.info = FALSE)\nres\n#  Tune multicrit result:\n#  Points on front: 13\nhead(as.data.frame(res$opt.path))\n#    fw.threshold fpr.test.mean fnr.test.mean dob eol error.message exec.time\n#  1    0.4892321     0.3092818     0.2639033   1  NA           NA      1.637\n#  2    0.2481696     0.2045499     0.2319697   2  NA           NA      1.700\n#  3    0.7691875     0.5128000     0.3459740   3  NA           NA      1.758\n#  4    0.1470133     0.2045499     0.2319697   4  NA           NA      1.762\n#  5    0.5958241     0.5028216     0.5239538   5  NA           NA      1.629\n#  6    0.6892421     0.6323959     0.4480808   6  NA           NA      1.875  The results can be visualized with function  plotTuneMultiCritResult .\nThe plot shows the false positive and false negative rates for all parameter values visited\nduring tuning. The size of the points on the Pareto front is slightly increased.  plotTuneMultiCritResult(res)", 
            "title": "Tuning the size of the feature subset"
        }, 
        {
            "location": "/feature_selection/index.html#wrapper-methods", 
            "text": "Wrapper methods use the performance of a learning algorithm to assess the usefulness of\na feature set.\nIn order to select a feature subset a learner is trained repeatedly on different feature subsets\nand the subset which leads to the best learner performance is chosen.  In order to use the wrapper approach we have to decide:   How to assess the performance: This involves choosing a performance measure that serves\n  as feature selection criterion and a resampling strategy.  Which learning method to use.  How to search the space of possible feature subsets.   The search strategy is defined by functions following the naming convention makeFeatSelControl search_strategy .\nThe following search strategies are available:   Exhaustive search ( makeFeatSelControlExhaustive ),  Genetic algorithm ( makeFeatSelControlGA ),  Random search ( makeFeatSelControlRandom ),  Deterministic forward or backward search ( makeFeatSelControlSequential ).", 
            "title": "Wrapper methods"
        }, 
        {
            "location": "/feature_selection/index.html#select-a-feature-subset", 
            "text": "Feature selection can be conducted with function  selectFeatures .  In the following example we perform an exhaustive search on the Wisconsin Prognostic Breast Cancer  data set.\nAs learning method we use the  Cox proportional hazards model .\nThe performance is assessed by the holdout estimate of the concordance index\n( cindex ).  ## Specify the search strategy\nctrl = makeFeatSelControlRandom(maxit = 20L)\nctrl\n#  FeatSel control: FeatSelControlRandom\n#  Same resampling instance: TRUE\n#  Imputation value:  worst \n#  Max. features:  not used \n#  Max. iterations: 20\n#  Tune threshold: FALSE\n#  Further arguments: prob=0.5  ctrl  is a  FeatSelControl  object that contains information about the search strategy\nand potential parameter values.  ## Resample description\nrdesc = makeResampleDesc( Holdout )\n\n## Select features\nsfeats = selectFeatures(learner =  surv.coxph , task = wpbc.task, resampling = rdesc,\n  control = ctrl, show.info = FALSE)\nsfeats\n#  FeatSel result:\n#  Features (17): mean_radius, mean_area, mean_smoothness, mean_concavepoints, mean_symmetry, mean_fractaldim, SE_texture, SE_perimeter, SE_smoothness, SE_compactness, SE_concavity, SE_concavepoints, worst_area, worst_compactness, worst_concavepoints, tsize, pnodes\n#  cindex.test.mean=0.7137990  sfeats is a  FeatSelResult  object.\nThe selected features and the corresponding performance can be accessed as follows:  sfeats$x\n#   [1]  mean_radius           mean_area             mean_smoothness     \n#   [4]  mean_concavepoints    mean_symmetry         mean_fractaldim     \n#   [7]  SE_texture            SE_perimeter          SE_smoothness       \n#  [10]  SE_compactness        SE_concavity          SE_concavepoints    \n#  [13]  worst_area            worst_compactness     worst_concavepoints \n#  [16]  tsize                 pnodes \nsfeats$y\n#  cindex.test.mean \n#          0.713799  In a second example we fit a simple linear regression model to the  BostonHousing \ndata set and use a sequential search to find a feature set that minimizes the mean squared\nerror ( mse ). method = \"sfs\"  indicates that we want to conduct a sequential forward search where features\nare added to the model until the performance cannot be improved anymore.\nSee the documentation page  makeFeatSelControlSequential  for other available\nsequential search methods.\nThe search is stopped if the improvement is smaller than  alpha = 0.02 .  ## Specify the search strategy\nctrl = makeFeatSelControlSequential(method =  sfs , alpha = 0.02)\n\n## Select features\nrdesc = makeResampleDesc( CV , iters = 10)\nsfeats = selectFeatures(learner =  regr.lm , task = bh.task, resampling = rdesc, control = ctrl,\n  show.info = FALSE)\nsfeats\n#  FeatSel result:\n#  Features (11): crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat\n#  mse.test.mean=23.7319094  Further information about the sequential feature selection process can be obtained by\nfunction  analyzeFeatSelResult .  analyzeFeatSelResult(sfeats)\n#  Features         : 11\n#  Performance      : mse.test.mean=23.7319094\n#  crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat\n#  \n#  Path to optimum:\n#  - Features:    0  Init   :                       Perf = 84.831  Diff: NA  *\n#  - Features:    1  Add    : lstat                 Perf = 38.894  Diff: 45.936  *\n#  - Features:    2  Add    : rm                    Perf = 31.279  Diff: 7.6156  *\n#  - Features:    3  Add    : ptratio               Perf = 28.108  Diff: 3.1703  *\n#  - Features:    4  Add    : dis                   Perf = 27.48  Diff: 0.62813  *\n#  - Features:    5  Add    : nox                   Perf = 26.079  Diff: 1.4008  *\n#  - Features:    6  Add    : b                     Perf = 25.563  Diff: 0.51594  *\n#  - Features:    7  Add    : chas                  Perf = 25.132  Diff: 0.43097  *\n#  - Features:    8  Add    : zn                    Perf = 24.792  Diff: 0.34018  *\n#  - Features:    9  Add    : rad                   Perf = 24.599  Diff: 0.19327  *\n#  - Features:   10  Add    : tax                   Perf = 24.082  Diff: 0.51706  *\n#  - Features:   11  Add    : crim                  Perf = 23.732  Diff: 0.35  *\n#  \n#  Stopped, because no improving feature was found.", 
            "title": "Select a feature subset"
        }, 
        {
            "location": "/feature_selection/index.html#fuse-a-learner-with-feature-selection", 
            "text": "A  Learner  can be fused with a feature selection strategy (i.e., a search\nstrategy, a performance measure and a resampling strategy) by function  makeFeatSelWrapper .\nDuring training features are selected according to the specified selection scheme. Then, the\nlearner is trained on the selected feature subset.  rdesc = makeResampleDesc( CV , iters = 3)\nlrn = makeFeatSelWrapper( surv.coxph , resampling = rdesc,\n  control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE)\nmod = train(lrn, task = wpbc.task)\nmod\n#  Model for learner.id=surv.coxph.featsel; learner.class=FeatSelWrapper\n#  Trained on: task.id = wpbc-example; obs = 194; features = 32\n#  Hyperparameters:  The result of the feature selection can be extracted by function  getFeatSelResult .  sfeats = getFeatSelResult(mod)\nsfeats\n#  FeatSel result:\n#  Features (19): mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness, mean_compactness, mean_concavepoints, mean_fractaldim, SE_compactness, SE_concavity, SE_concavepoints, SE_symmetry, worst_texture, worst_perimeter, worst_area, worst_concavepoints, worst_symmetry, tsize, pnodes\n#  cindex.test.mean=0.6308754  The selected features are:  sfeats$x\n#   [1]  mean_radius           mean_texture          mean_perimeter      \n#   [4]  mean_area             mean_smoothness       mean_compactness    \n#   [7]  mean_concavepoints    mean_fractaldim       SE_compactness      \n#  [10]  SE_concavity          SE_concavepoints      SE_symmetry         \n#  [13]  worst_texture         worst_perimeter       worst_area          \n#  [16]  worst_concavepoints   worst_symmetry        tsize               \n#  [19]  pnodes   The 5-fold cross-validated performance of the learner specified above can be computed as\nfollows:  out.rdesc = makeResampleDesc( CV , iters = 5)\n\nr = resample(learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE,\n  show.info = FALSE)\nr$aggr\n#  cindex.test.mean \n#          0.632357  The selected feature sets in the individual resampling iterations can be extracted as follows:  lapply(r$models, getFeatSelResult)\n#  [[1]]\n#  FeatSel result:\n#  Features (18): mean_texture, mean_area, mean_smoothness, mean_compactness, mean_concavity, mean_symmetry, SE_radius, SE_compactness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_radius, worst_smoothness, worst_compactness, worst_concavity, worst_symmetry, tsize, pnodes\n#  cindex.test.mean=0.6599048\n#  \n#  [[2]]\n#  FeatSel result:\n#  Features (12): mean_area, mean_compactness, mean_symmetry, mean_fractaldim, SE_perimeter, SE_area, SE_concavity, SE_symmetry, worst_texture, worst_smoothness, worst_fractaldim, tsize\n#  cindex.test.mean=0.6524759\n#  \n#  [[3]]\n#  FeatSel result:\n#  Features (14): mean_compactness, mean_symmetry, mean_fractaldim, SE_radius, SE_perimeter, SE_smoothness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_concavity, worst_concavepoints, worst_symmetry, worst_fractaldim, pnodes\n#  cindex.test.mean=0.6074810\n#  \n#  [[4]]\n#  FeatSel result:\n#  Features (18): mean_radius, mean_texture, mean_perimeter, mean_compactness, mean_concavity, SE_texture, SE_area, SE_smoothness, SE_concavity, SE_symmetry, SE_fractaldim, worst_radius, worst_compactness, worst_concavepoints, worst_symmetry, worst_fractaldim, tsize, pnodes\n#  cindex.test.mean=0.6526122\n#  \n#  [[5]]\n#  FeatSel result:\n#  Features (14): mean_radius, mean_texture, mean_compactness, mean_concavepoints, mean_symmetry, SE_texture, SE_compactness, SE_symmetry, SE_fractaldim, worst_radius, worst_smoothness, worst_compactness, worst_concavity, pnodes\n#  cindex.test.mean=0.6262729", 
            "title": "Fuse a learner with feature selection"
        }, 
        {
            "location": "/feature_selection/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  fv = generateFilterValuesData(iris.task, method =  information.gain ) \nfv \nfv2 = generateFilterValuesData(iris.task, method = c( information.gain ,  chi.squared )) \nfv2$data \nplotFilterValues(fv2) \n## plotFilterValuesGGVIS(fv2) \n## Keep the 2 most important features \nfiltered.task = filterFeatures(iris.task, method =  information.gain , abs = 2) \n\n## Keep the 25% most important features \nfiltered.task = filterFeatures(iris.task, fval = fv, perc = 0.25) \n\n## Keep all features with importance greater than 0.5 \nfiltered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5) \nfiltered.task \nlrn = makeFilterWrapper(learner =  classif.fnn , fw.method =  information.gain , fw.abs = 2) \nrdesc = makeResampleDesc( CV , iters = 10) \nr = resample(learner = lrn, task = iris.task, resampling = rdesc, show.info = FALSE, models = TRUE) \nr$aggr \nsfeats = sapply(r$models, getFilteredFeatures) \ntable(sfeats) \nlrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared ) \nps = makeParamSet(makeDiscreteParam( fw.perc , values = seq(0.2, 0.5, 0.05))) \nrdesc = makeResampleDesc( CV , iters = 3) \nres = tuneParams(lrn, task = bh.task, resampling = rdesc, par.set = ps, \n  control = makeTuneControlGrid()) \nres \nas.data.frame(res$opt.path) \nres$x \nres$y \nlrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared , fw.perc = res$x$fw.perc) \nmod = train(lrn, bh.task) \nmod \n\ngetFilteredFeatures(mod) \nlrn = makeFilterWrapper(learner =  classif.lda , fw.method =  chi.squared ) \nps = makeParamSet(makeNumericParam( fw.threshold , lower = 0.1, upper = 0.9)) \nrdesc = makeResampleDesc( CV , iters = 10) \nres = tuneParamsMultiCrit(lrn, task = sonar.task, resampling = rdesc, par.set = ps, \n  measures = list(fpr, fnr), control = makeTuneMultiCritControlRandom(maxit = 50L), \n  show.info = FALSE) \nres \nhead(as.data.frame(res$opt.path)) \nplotTuneMultiCritResult(res) \n## Specify the search strategy \nctrl = makeFeatSelControlRandom(maxit = 20L) \nctrl \n## Resample description \nrdesc = makeResampleDesc( Holdout ) \n\n## Select features \nsfeats = selectFeatures(learner =  surv.coxph , task = wpbc.task, resampling = rdesc, \n  control = ctrl, show.info = FALSE) \nsfeats \nsfeats$x \nsfeats$y \n## Specify the search strategy \nctrl = makeFeatSelControlSequential(method =  sfs , alpha = 0.02) \n\n## Select features \nrdesc = makeResampleDesc( CV , iters = 10) \nsfeats = selectFeatures(learner =  regr.lm , task = bh.task, resampling = rdesc, control = ctrl, \n  show.info = FALSE) \nsfeats \nanalyzeFeatSelResult(sfeats) \nrdesc = makeResampleDesc( CV , iters = 3) \nlrn = makeFeatSelWrapper( surv.coxph , resampling = rdesc, \n  control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE) \nmod = train(lrn, task = wpbc.task) \nmod \nsfeats = getFeatSelResult(mod) \nsfeats \nsfeats$x \nout.rdesc = makeResampleDesc( CV , iters = 5) \n\nr = resample(learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE, \n  show.info = FALSE) \nr$aggr \nlapply(r$models, getFeatSelResult)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/nested_resampling/index.html", 
            "text": "Nested Resampling\n\n\nIn order to obtain honest performance estimates for a learner all parts of the model building\nlike preprocessing and model selection steps should be included in the resampling, i.e.,\nrepeated for every pair of training/test data.\nFor steps that themselves require resampling like \nparameter tuning\n or\n\nfeature selection\n (via the wrapper approach) this results in two\nnested resampling loops.\n\n\n\n\nThe graphic above illustrates nested resampling for parameter tuning with 3-fold cross-validation\nin the outer and 4-fold cross-validation in the inner loop.\n\n\nIn the outer resampling loop, we have three pairs of training/test sets.\nOn each of these outer training sets parameter tuning is done, thereby executing the inner\nresampling loop.\nThis way, we get one set of selected hyperparameters for each outer training set.\nThen the learner is fitted on each outer training set using the corresponding selected\nhyperparameters and its performance is evaluated on the outer test sets.\n\n\nIn \nmlr\n, you can get nested resampling for free without programming any looping by\nusing the \nwrapper functionality\n. This works as follows:\n\n\n\n\nGenerate a wrapped \nLearner\n via function \nmakeTuneWrapper\n or \nmakeFeatSelWrapper\n.\n   Specify the inner resampling strategy using their \nresampling\n argument.\n\n\nCall function \nresample\n (see also the section about \nresampling\n) and\n   pass the outer resampling strategy to its \nresampling\n argument.\n\n\n\n\nYou can freely combine different inner and outer resampling strategies.\n\n\nThe outer strategy can be a resample description (\nResampleDesc\n) or a\nresample instance (\nResampleInstance\n).\nA common setup is prediction and performance evaluation on a fixed outer test set. This can\nbe achieved by using function \nmakeFixedHoldoutInstance\n to generate the outer\n\nResampleInstance\n.\n\n\nThe inner resampling strategy should preferably be a \nResampleDesc\n, as the sizes\nof the outer training sets might differ.\nPer default, the inner resample description is instantiated once for every outer training set.\nThis way during tuning/feature selection all parameter or feature sets are compared\non the same inner training/test sets to reduce variance.\nYou can also turn this off using the \nsame.resampling.instance\n argument of \nmakeTuneControl*\n\nor \nmakeFeatSelControl*\n.\n\n\nNested resampling is computationally expensive.\nFor this reason in the examples shown below we use relatively small search spaces and a low\nnumber of resampling iterations. In practice, you normally have to increase both.\nAs this is computationally intensive you might want to have a look at section\n\nparallelization\n.\n\n\nTuning\n\n\nAs you might recall from the tutorial page about \ntuning\n, you need to define a search space by\nfunction \nmakeParamSet\n, a search strategy by \nmakeTuneControl*\n,\nand a method to evaluate hyperparameter settings (i.e., the inner resampling strategy and a performance measure).\n\n\nBelow is a classification example.\nWe evaluate the performance of a support vector machine (\nksvm\n) with tuned\ncost parameter \nC\n and RBF kernel parameter \nsigma\n.\nWe use 3-fold cross-validation in the outer and subsampling with 2 iterations in the inner\nloop.\nFor tuning a grid search is used to find the hyperparameters with lowest error rate\n(\nmmce\n is the default measure for classification).\nThe wrapped \nLearner\n is generated by calling \nmakeTuneWrapper\n.\n\n\nNote that in practice the parameter set should be larger.\nA common recommendation is \n2^(-12:12)\n for both \nC\n and \nsigma\n.\n\n\n## Tuning in inner resampling loop\nps = makeParamSet(\n  makeDiscreteParam(\nC\n, values = 2^(-2:2)),\n  makeDiscreteParam(\nsigma\n, values = 2^(-2:2))\n)\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc(\nSubsample\n, iters = 2)\nlrn = makeTuneWrapper(\nclassif.ksvm\n, resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)\n\n## Outer resampling loop\nouter = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE)\n\nr\n#\n Resample Result\n#\n Task: iris_example\n#\n Learner: classif.ksvm.tuned\n#\n Aggr perf: mmce.test.mean=0.0533333\n#\n Runtime: 3.40427\n\n\n\n\nYou can obtain the error rates on the 3 outer test sets by:\n\n\nr$measures.test\n#\n   iter mmce\n#\n 1    1 0.02\n#\n 2    2 0.06\n#\n 3    3 0.08\n\n\n\n\nAccessing the tuning result\n\n\nWe have kept the results of the tuning for further evaluations.\nFor example one might want to find out, if the best obtained configurations vary for the\ndifferent outer splits.\nAs storing entire models may be expensive (but possible by setting \nmodels = TRUE\n) we used\nthe \nextract\n option of \nresample\n.\nFunction \ngetTuneResult\n returns, among other things, the optimal hyperparameter values and\nthe \noptimization path\n for each iteration of the outer resampling loop.\nNote that the performance values shown when printing \nr$extract\n are the aggregated performances\nresulting from inner resampling on the outer training set for the best hyperparameter configurations\n(not to be confused with \nr$measures.test\n shown above).\n\n\nr$extract\n#\n [[1]]\n#\n Tune result:\n#\n Op. pars: C=2; sigma=0.25\n#\n mmce.test.mean=0.0147059\n#\n \n#\n [[2]]\n#\n Tune result:\n#\n Op. pars: C=4; sigma=0.25\n#\n mmce.test.mean=0.0000000\n#\n \n#\n [[3]]\n#\n Tune result:\n#\n Op. pars: C=4; sigma=0.25\n#\n mmce.test.mean=0.0735294\n\nnames(r$extract[[1]])\n#\n [1] \nlearner\n   \ncontrol\n   \nx\n         \ny\n         \nthreshold\n \nopt.path\n\n\n\n\n\nWe can compare the optimal parameter settings obtained in the 3 resampling iterations.\nAs you can see, the optimal configuration usually depends on the data. You may\nbe able to identify a \nrange\n of parameter settings that achieve good\nperformance though, e.g., the values for \nC\n should be at least 1 and the values\nfor \nsigma\n should be between 0 and 1.\n\n\nWith function \ngetNestedTuneResultsOptPathDf\n you can extract the optimization paths\nfor the 3 outer cross-validation iterations for further inspection and analysis.\nThese are stacked in one \ndata.frame\n with column \niter\n indicating the\nresampling iteration.\n\n\nopt.paths = getNestedTuneResultsOptPathDf(r)\nhead(opt.paths, 10)\n#\n       C sigma mmce.test.mean dob eol error.message exec.time iter\n#\n 1  0.25  0.25     0.05882353   1  NA          \nNA\n     0.042    1\n#\n 2   0.5  0.25     0.04411765   2  NA          \nNA\n     0.042    1\n#\n 3     1  0.25     0.04411765   3  NA          \nNA\n     0.042    1\n#\n 4     2  0.25     0.01470588   4  NA          \nNA\n     0.048    1\n#\n 5     4  0.25     0.05882353   5  NA          \nNA\n     0.036    1\n#\n 6  0.25   0.5     0.05882353   6  NA          \nNA\n     0.036    1\n#\n 7   0.5   0.5     0.01470588   7  NA          \nNA\n     0.036    1\n#\n 8     1   0.5     0.02941176   8  NA          \nNA\n     0.036    1\n#\n 9     2   0.5     0.01470588   9  NA          \nNA\n     0.036    1\n#\n 10    4   0.5     0.05882353  10  NA          \nNA\n     0.035    1\n\n\n\n\nBelow we visualize the \nopt.path\ns for the 3 outer resampling iterations.\n\n\ng = ggplot(opt.paths, aes(x = C, y = sigma, fill = mmce.test.mean))\ng + geom_tile() + facet_wrap(~ iter)\n\n\n\n\n\n\nAnother useful function is \ngetNestedTuneResultsX\n, which extracts the best found hyperparameter\nsettings for each outer resampling iteration.\n\n\ngetNestedTuneResultsX(r)\n#\n   C sigma\n#\n 1 2  0.25\n#\n 2 4  0.25\n#\n 3 4  0.25\n\n\n\n\nFeature selection\n\n\nAs you might recall from the section about \nfeature selection\n, \nmlr\n\nsupports the filter and the wrapper approach.\n\n\nWrapper methods\n\n\nWrapper methods use the performance of a learning algorithm to assess the usefulness of a\nfeature set. In order to select a feature subset a learner is trained repeatedly on different\nfeature subsets and the subset which leads to the best learner performance is chosen.\n\n\nFor feature selection in the inner resampling loop, you need to choose a search strategy\n(function \nmakeFeatSelControl*\n), a performance measure and the inner\nresampling strategy. Then use function \nmakeFeatSelWrapper\n to bind everything together.\n\n\nBelow we use sequential forward selection with linear regression on the\n\nBostonHousing\n data set (\nbh.task\n).\n\n\n## Feature selection in inner resampling loop\ninner = makeResampleDesc(\nCV\n, iters = 3)\nlrn = makeFeatSelWrapper(\nregr.lm\n, resampling = inner,\n  control = makeFeatSelControlSequential(method = \nsfs\n), show.info = FALSE)\n\n## Outer resampling loop\nouter = makeResampleDesc(\nSubsample\n, iters = 2)\nr = resample(learner = lrn, task = bh.task, resampling = outer, extract = getFeatSelResult,\n  show.info = FALSE)\n\nr\n#\n Resample Result\n#\n Task: BostonHousing-example\n#\n Learner: regr.lm.featsel\n#\n Aggr perf: mse.test.mean=31.6991293\n#\n Runtime: 7.93073\n\nr$measures.test\n#\n   iter      mse\n#\n 1    1 35.08611\n#\n 2    2 28.31215\n\n\n\n\nAccessing the selected features\n\n\nThe result of the feature selection can be extracted by function \ngetFeatSelResult\n.\nIt is also possible to keep whole \nmodels\n by setting \nmodels = TRUE\n\nwhen calling \nresample\n.\n\n\nr$extract\n#\n [[1]]\n#\n FeatSel result:\n#\n Features (10): crim, zn, indus, nox, rm, dis, rad, tax, ptratio, lstat\n#\n mse.test.mean=20.1593896\n#\n \n#\n [[2]]\n#\n FeatSel result:\n#\n Features (9): zn, nox, rm, dis, rad, tax, ptratio, b, lstat\n#\n mse.test.mean=22.5966796\n\n## Selected features in the first outer resampling iteration\nr$extract[[1]]$x\n#\n  [1] \ncrim\n    \nzn\n      \nindus\n   \nnox\n     \nrm\n      \ndis\n     \nrad\n    \n#\n  [8] \ntax\n     \nptratio\n \nlstat\n\n\n## Resampled performance of the selected feature subset on the first inner training set\nr$extract[[1]]$y\n#\n mse.test.mean \n#\n      20.15939\n\n\n\n\nAs for tuning, you can extract the optimization paths.\nThe resulting \ndata.frame\ns contain, among others, binary columns for\nall features, indicating if they were included in the linear regression model, and the\ncorresponding performances.\n\n\nopt.paths = lapply(r$extract, function(x) as.data.frame(x$opt.path))\nhead(opt.paths[[1]])\n#\n   crim zn indus chas nox rm age dis rad tax ptratio b lstat mse.test.mean\n#\n 1    0  0     0    0   0  0   0   0   0   0       0 0     0      80.33019\n#\n 2    1  0     0    0   0  0   0   0   0   0       0 0     0      65.95316\n#\n 3    0  1     0    0   0  0   0   0   0   0       0 0     0      69.15417\n#\n 4    0  0     1    0   0  0   0   0   0   0       0 0     0      55.75473\n#\n 5    0  0     0    1   0  0   0   0   0   0       0 0     0      80.48765\n#\n 6    0  0     0    0   1  0   0   0   0   0       0 0     0      63.06724\n#\n   dob eol error.message exec.time\n#\n 1   1   2          \nNA\n     0.021\n#\n 2   2   2          \nNA\n     0.034\n#\n 3   2   2          \nNA\n     0.033\n#\n 4   2   2          \nNA\n     0.032\n#\n 5   2   2          \nNA\n     0.035\n#\n 6   2   2          \nNA\n     0.032\n\n\n\n\nAn easy-to-read version of the optimization path for sequential feature selection can be\nobtained with function \nanalyzeFeatSelResult\n.\n\n\nanalyzeFeatSelResult(r$extract[[1]])\n#\n Features         : 10\n#\n Performance      : mse.test.mean=20.1593896\n#\n crim, zn, indus, nox, rm, dis, rad, tax, ptratio, lstat\n#\n \n#\n Path to optimum:\n#\n - Features:    0  Init   :                       Perf = 80.33  Diff: NA  *\n#\n - Features:    1  Add    : lstat                 Perf = 36.451  Diff: 43.879  *\n#\n - Features:    2  Add    : rm                    Perf = 27.289  Diff: 9.1623  *\n#\n - Features:    3  Add    : ptratio               Perf = 24.004  Diff: 3.2849  *\n#\n - Features:    4  Add    : nox                   Perf = 23.513  Diff: 0.49082  *\n#\n - Features:    5  Add    : dis                   Perf = 21.49  Diff: 2.023  *\n#\n - Features:    6  Add    : crim                  Perf = 21.12  Diff: 0.37008  *\n#\n - Features:    7  Add    : indus                 Perf = 20.82  Diff: 0.29994  *\n#\n - Features:    8  Add    : rad                   Perf = 20.609  Diff: 0.21054  *\n#\n - Features:    9  Add    : tax                   Perf = 20.209  Diff: 0.40059  *\n#\n - Features:   10  Add    : zn                    Perf = 20.159  Diff: 0.049441  *\n#\n \n#\n Stopped, because no improving feature was found.\n\n\n\n\nFilter methods with tuning\n\n\nFilter methods assign an importance value to each feature.\nBased on these values you can select a feature subset by either keeping all features with importance\nhigher than a certain threshold or by keeping a fixed number or percentage of the highest ranking features.\nOften, neither the theshold nor the number or percentage of features is known in advance\nand thus tuning is necessary.\n\n\nIn the example below the threshold value (\nfw.threshold\n) is tuned in the inner resampling loop.\nFor this purpose the base \nLearner\n \n\"regr.lm\"\n is wrapped two times.\nFirst, \nmakeFilterWrapper\n is used to fuse linear regression with a feature filtering\npreprocessing step. Then a tuning step is added by \nmakeTuneWrapper\n.\n\n\n## Tuning of the percentage of selected filters in the inner loop\nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n)\nps = makeParamSet(makeDiscreteParam(\nfw.threshold\n, values = seq(0, 1, 0.2)))\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc(\nCV\n, iters = 3)\nlrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)\n\n## Outer resampling loop\nouter = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(learner = lrn, task = bh.task, resampling = outer, models = TRUE, show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: BostonHousing-example\n#\n Learner: regr.lm.filtered.tuned\n#\n Aggr perf: mse.test.mean=25.3915155\n#\n Runtime: 3.85701\n\n\n\n\nAccessing the selected features and optimal percentage\n\n\nIn the above example we kept the complete \nmodel\ns.\n\n\nBelow are some examples that show how to extract information from the \nmodel\ns.\n\n\nr$models\n#\n [[1]]\n#\n Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper\n#\n Trained on: task.id = BostonHousing-example; obs = 337; features = 13\n#\n Hyperparameters: fw.method=chi.squared\n#\n \n#\n [[2]]\n#\n Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper\n#\n Trained on: task.id = BostonHousing-example; obs = 338; features = 13\n#\n Hyperparameters: fw.method=chi.squared\n#\n \n#\n [[3]]\n#\n Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper\n#\n Trained on: task.id = BostonHousing-example; obs = 337; features = 13\n#\n Hyperparameters: fw.method=chi.squared\n\n\n\n\nThe result of the feature selection can be extracted by function \ngetFilteredFeatures\n.\nAlmost always all 13 features are selected.\n\n\nlapply(r$models, function(x) getFilteredFeatures(x$learner.model$next.model))\n#\n [[1]]\n#\n  [1] \ncrim\n    \nzn\n      \nindus\n   \nchas\n    \nnox\n     \nrm\n      \nage\n    \n#\n  [8] \ndis\n     \nrad\n     \ntax\n     \nptratio\n \nb\n       \nlstat\n  \n#\n \n#\n [[2]]\n#\n  [1] \ncrim\n    \nzn\n      \nindus\n   \nnox\n     \nrm\n      \nage\n     \ndis\n    \n#\n  [8] \nrad\n     \ntax\n     \nptratio\n \nb\n       \nlstat\n  \n#\n \n#\n [[3]]\n#\n  [1] \ncrim\n    \nzn\n      \nindus\n   \nchas\n    \nnox\n     \nrm\n      \nage\n    \n#\n  [8] \ndis\n     \nrad\n     \ntax\n     \nptratio\n \nb\n       \nlstat\n\n\n\n\n\nBelow the \ntune results\n and \noptimization paths\n\nare accessed.\n\n\nres = lapply(r$models, getTuneResult)\nres\n#\n [[1]]\n#\n Tune result:\n#\n Op. pars: fw.threshold=0\n#\n mse.test.mean=24.8915992\n#\n \n#\n [[2]]\n#\n Tune result:\n#\n Op. pars: fw.threshold=0.4\n#\n mse.test.mean=27.1812625\n#\n \n#\n [[3]]\n#\n Tune result:\n#\n Op. pars: fw.threshold=0\n#\n mse.test.mean=19.7012695\n\nopt.paths = lapply(res, function(x) as.data.frame(x$opt.path))\nopt.paths[[1]]\n#\n   fw.threshold mse.test.mean dob eol error.message exec.time\n#\n 1            0      24.89160   1  NA          \nNA\n     0.195\n#\n 2          0.2      25.18817   2  NA          \nNA\n     0.204\n#\n 3          0.4      25.18817   3  NA          \nNA\n     0.194\n#\n 4          0.6      32.15930   4  NA          \nNA\n     0.182\n#\n 5          0.8      90.89848   5  NA          \nNA\n     0.190\n#\n 6            1      90.89848   6  NA          \nNA\n     0.193\n\n\n\n\nBenchmark experiments\n\n\nIn a benchmark experiment multiple learners are compared on one or several tasks\n(see also the section about \nbenchmarking\n).\nNested resampling in benchmark experiments is achieved the same way as in resampling:\n\n\n\n\nFirst, use \nmakeTuneWrapper\n or \nmakeFeatSelWrapper\n to generate wrapped \nLearner\ns\n  with the inner resampling strategies of your choice.\n\n\nSecond, call \nbenchmark\n and specify the outer resampling strategies for all tasks.\n\n\n\n\nThe inner resampling strategies should be \nresample descriptions\n.\nYou can use different inner resampling strategies for different wrapped learners.\nFor example it might be practical to do fewer subsampling or bootstrap iterations for slower\nlearners.\n\n\nIf you have larger benchmark experiments you might want to have a look at the section\nabout \nparallelization\n.\n\n\nAs mentioned in the section about \nbenchmark experiments\n you can also use\ndifferent resampling strategies for different learning tasks by passing a\n\nlist\n of resampling descriptions or instances to \nbenchmark\n.\n\n\nWe will see three examples to show different benchmark settings:\n\n\n\n\nTwo data sets + two classification algorithms + tuning\n\n\nOne data set + two regression algorithms + feature selection\n\n\nOne data set + two regression algorithms + feature filtering + tuning\n\n\n\n\nExample 1: Two tasks, two learners, tuning\n\n\nBelow is a benchmark experiment with two data sets, \niris\n and\n\nsonar\n, and two \nLearner\ns,\n\nksvm\n and \nkknn\n, that are both tuned.\n\n\nAs inner resampling strategies we use holdout for \nksvm\n and subsampling\nwith 3 iterations for \nkknn\n.\nAs outer resampling strategies we take holdout for the \niris\n and bootstrap\nwith 2 iterations for the \nsonar\n data (\nsonar.task\n).\nWe consider the accuracy (\nacc\n), which is used as tuning criterion, and also\ncalculate the balanced error rate (\nber\n).\n\n\n## List of learning tasks\ntasks = list(iris.task, sonar.task)\n\n## Tune svm in the inner resampling loop\nps = makeParamSet(\n  makeDiscreteParam(\nC\n, 2^(-1:1)),\n  makeDiscreteParam(\nsigma\n, 2^(-1:1)))\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc(\nHoldout\n)\nlrn1 = makeTuneWrapper(\nclassif.ksvm\n, resampling = inner, par.set = ps, control = ctrl,\n  show.info = FALSE)\n\n## Tune k-nearest neighbor in inner resampling loop\nps = makeParamSet(makeDiscreteParam(\nk\n, 3:5))\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc(\nSubsample\n, iters = 3)\nlrn2 = makeTuneWrapper(\nclassif.kknn\n, resampling = inner, par.set = ps, control = ctrl,\n  show.info = FALSE)\n\n## Learners\nlrns = list(lrn1, lrn2)\n\n## Outer resampling loop\nouter = list(makeResampleDesc(\nHoldout\n), makeResampleDesc(\nBootstrap\n, iters = 2))\nres = benchmark(lrns, tasks, outer, measures = list(acc, ber), show.info = FALSE)\nres\n#\n         task.id         learner.id acc.test.mean ber.test.mean\n#\n 1  iris_example classif.ksvm.tuned     0.9400000    0.05882353\n#\n 2  iris_example classif.kknn.tuned     0.9200000    0.08683473\n#\n 3 Sonar_example classif.ksvm.tuned     0.5289307    0.50000000\n#\n 4 Sonar_example classif.kknn.tuned     0.8077080    0.19549714\n\n\n\n\nThe \nprint\n method for the \nBenchmarkResult\n shows the aggregated performances\nfrom the outer resampling loop.\n\n\nAs you might recall, \nmlr\n offers several accessor function to extract information from\nthe benchmark result.\nThese are listed on the help page of \nBenchmarkResult\n and many examples are shown on the\ntutorial page about \nbenchmark experiments\n.\n\n\nThe performance values in individual outer resampling runs can be obtained by \ngetBMRPerformances\n.\nNote that, since we used different outer resampling strategies for the two tasks, the number\nof rows per task differ.\n\n\ngetBMRPerformances(res, as.df = TRUE)\n#\n         task.id         learner.id iter       acc        ber\n#\n 1  iris_example classif.ksvm.tuned    1 0.9400000 0.05882353\n#\n 2  iris_example classif.kknn.tuned    1 0.9200000 0.08683473\n#\n 3 Sonar_example classif.ksvm.tuned    1 0.5373134 0.50000000\n#\n 4 Sonar_example classif.ksvm.tuned    2 0.5205479 0.50000000\n#\n 5 Sonar_example classif.kknn.tuned    1 0.8208955 0.18234767\n#\n 6 Sonar_example classif.kknn.tuned    2 0.7945205 0.20864662\n\n\n\n\nThe results from the parameter tuning can be obtained through function \ngetBMRTuneResults\n.\n\n\ngetBMRTuneResults(res)\n#\n $iris_example\n#\n $iris_example$classif.ksvm.tuned\n#\n $iris_example$classif.ksvm.tuned[[1]]\n#\n Tune result:\n#\n Op. pars: C=0.5; sigma=0.5\n#\n mmce.test.mean=0.0588235\n#\n \n#\n \n#\n $iris_example$classif.kknn.tuned\n#\n $iris_example$classif.kknn.tuned[[1]]\n#\n Tune result:\n#\n Op. pars: k=3\n#\n mmce.test.mean=0.0490196\n#\n \n#\n \n#\n \n#\n $Sonar_example\n#\n $Sonar_example$classif.ksvm.tuned\n#\n $Sonar_example$classif.ksvm.tuned[[1]]\n#\n Tune result:\n#\n Op. pars: C=1; sigma=2\n#\n mmce.test.mean=0.3428571\n#\n \n#\n $Sonar_example$classif.ksvm.tuned[[2]]\n#\n Tune result:\n#\n Op. pars: C=2; sigma=0.5\n#\n mmce.test.mean=0.2000000\n#\n \n#\n \n#\n $Sonar_example$classif.kknn.tuned\n#\n $Sonar_example$classif.kknn.tuned[[1]]\n#\n Tune result:\n#\n Op. pars: k=4\n#\n mmce.test.mean=0.1095238\n#\n \n#\n $Sonar_example$classif.kknn.tuned[[2]]\n#\n Tune result:\n#\n Op. pars: k=3\n#\n mmce.test.mean=0.0666667\n\n\n\n\nAs for several other accessor functions a clearer representation as \ndata.frame\n\ncan be achieved by setting \nas.df = TRUE\n.\n\n\ngetBMRTuneResults(res, as.df = TRUE)\n#\n         task.id         learner.id iter   C sigma mmce.test.mean  k\n#\n 1  iris_example classif.ksvm.tuned    1 0.5   0.5     0.05882353 NA\n#\n 2  iris_example classif.kknn.tuned    1  NA    NA     0.04901961  3\n#\n 3 Sonar_example classif.ksvm.tuned    1 1.0   2.0     0.34285714 NA\n#\n 4 Sonar_example classif.ksvm.tuned    2 2.0   0.5     0.20000000 NA\n#\n 5 Sonar_example classif.kknn.tuned    1  NA    NA     0.10952381  4\n#\n 6 Sonar_example classif.kknn.tuned    2  NA    NA     0.06666667  3\n\n\n\n\nIt is also possible to extract the tuning results for individual tasks and learners and,\nas shown in earlier examples, inspect the \noptimization path\n.\n\n\ntune.res = getBMRTuneResults(res, task.ids = \nSonar_example\n, learner.ids = \nclassif.ksvm.tuned\n,\n  as.df = TRUE)\ntune.res\n#\n         task.id         learner.id iter C sigma mmce.test.mean\n#\n 1 Sonar_example classif.ksvm.tuned    1 1   2.0      0.3428571\n#\n 2 Sonar_example classif.ksvm.tuned    2 2   0.5      0.2000000\n\ngetNestedTuneResultsOptPathDf(res$results[[\nSonar_example\n]][[\nclassif.ksvm.tuned\n]])\n\n\n\n\nExample 2: One task, two learners, feature selection\n\n\nLet's see how we can do \nfeature selection\n in\na benchmark experiment:\n\n\n## Feature selection in inner resampling loop\nctrl = makeFeatSelControlSequential(method = \nsfs\n)\ninner = makeResampleDesc(\nSubsample\n, iters = 2)\nlrn = makeFeatSelWrapper(\nregr.lm\n, resampling = inner, control = ctrl, show.info = FALSE)\n\n## Learners\nlrns = list(\nregr.rpart\n, lrn)\n\n## Outer resampling loop\nouter = makeResampleDesc(\nSubsample\n, iters = 2)\nres = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE)\n\nres\n#\n                 task.id      learner.id mse.test.mean\n#\n 1 BostonHousing-example      regr.rpart      25.86232\n#\n 2 BostonHousing-example regr.lm.featsel      25.07465\n\n\n\n\nThe selected features can be extracted by function \ngetBMRFeatSelResults\n.\nBy default, a nested \nlist\n, with the first level indicating the task and the\nsecond level indicating the learner, is returned.\nIf only a single learner or, as in our case, a single task is considered, setting\n\ndrop = TRUE\n simplifies the result to a flat \nlist\n.\n\n\ngetBMRFeatSelResults(res)\n#\n $`BostonHousing-example`\n#\n $`BostonHousing-example`$regr.rpart\n#\n NULL\n#\n \n#\n $`BostonHousing-example`$regr.lm.featsel\n#\n $`BostonHousing-example`$regr.lm.featsel[[1]]\n#\n FeatSel result:\n#\n Features (8): crim, zn, chas, nox, rm, dis, ptratio, lstat\n#\n mse.test.mean=26.7257383\n#\n \n#\n $`BostonHousing-example`$regr.lm.featsel[[2]]\n#\n FeatSel result:\n#\n Features (10): crim, zn, nox, rm, dis, rad, tax, ptratio, b, lstat\n#\n mse.test.mean=24.2874464\ngetBMRFeatSelResults(res, drop = TRUE)\n#\n $regr.rpart\n#\n NULL\n#\n \n#\n $regr.lm.featsel\n#\n $regr.lm.featsel[[1]]\n#\n FeatSel result:\n#\n Features (8): crim, zn, chas, nox, rm, dis, ptratio, lstat\n#\n mse.test.mean=26.7257383\n#\n \n#\n $regr.lm.featsel[[2]]\n#\n FeatSel result:\n#\n Features (10): crim, zn, nox, rm, dis, rad, tax, ptratio, b, lstat\n#\n mse.test.mean=24.2874464\n\n\n\n\nYou can access results for individual learners and tasks and inspect them further.\n\n\nfeats = getBMRFeatSelResults(res, learner.id = \nregr.lm.featsel\n, drop = TRUE)\n\n## Selected features in the first outer resampling iteration\nfeats[[1]]$x\n#\n [1] \ncrim\n    \nzn\n      \nchas\n    \nnox\n     \nrm\n      \ndis\n     \nptratio\n\n#\n [8] \nlstat\n\n\n## Resampled performance of the selected feature subset on the first inner training set\nfeats[[1]]$y\n#\n mse.test.mean \n#\n      26.72574\n\n\n\n\nAs for tuning, you can extract the optimization paths. The resulting \ndata.frame\ns\ncontain, among others, binary columns for all features, indicating if they were included in the\nlinear regression model, and the corresponding performances.\n\nanalyzeFeatSelResult\n gives a clearer overview.\n\n\nopt.paths = lapply(feats, function(x) as.data.frame(x$opt.path))\nhead(opt.paths[[1]])\n#\n   crim zn indus chas nox rm age dis rad tax ptratio b lstat mse.test.mean\n#\n 1    0  0     0    0   0  0   0   0   0   0       0 0     0      90.16159\n#\n 2    1  0     0    0   0  0   0   0   0   0       0 0     0      82.85880\n#\n 3    0  1     0    0   0  0   0   0   0   0       0 0     0      79.55202\n#\n 4    0  0     1    0   0  0   0   0   0   0       0 0     0      70.02071\n#\n 5    0  0     0    1   0  0   0   0   0   0       0 0     0      86.93409\n#\n 6    0  0     0    0   1  0   0   0   0   0       0 0     0      76.32457\n#\n   dob eol error.message exec.time\n#\n 1   1   2          \nNA\n     0.015\n#\n 2   2   2          \nNA\n     0.024\n#\n 3   2   2          \nNA\n     0.023\n#\n 4   2   2          \nNA\n     0.025\n#\n 5   2   2          \nNA\n     0.025\n#\n 6   2   2          \nNA\n     0.035\n\nanalyzeFeatSelResult(feats[[1]])\n#\n Features         : 8\n#\n Performance      : mse.test.mean=26.7257383\n#\n crim, zn, chas, nox, rm, dis, ptratio, lstat\n#\n \n#\n Path to optimum:\n#\n - Features:    0  Init   :                       Perf = 90.162  Diff: NA  *\n#\n - Features:    1  Add    : lstat                 Perf = 42.646  Diff: 47.515  *\n#\n - Features:    2  Add    : ptratio               Perf = 34.52  Diff: 8.1263  *\n#\n - Features:    3  Add    : rm                    Perf = 30.454  Diff: 4.066  *\n#\n - Features:    4  Add    : dis                   Perf = 29.405  Diff: 1.0495  *\n#\n - Features:    5  Add    : nox                   Perf = 28.059  Diff: 1.3454  *\n#\n - Features:    6  Add    : chas                  Perf = 27.334  Diff: 0.72499  *\n#\n - Features:    7  Add    : zn                    Perf = 26.901  Diff: 0.43296  *\n#\n - Features:    8  Add    : crim                  Perf = 26.726  Diff: 0.17558  *\n#\n \n#\n Stopped, because no improving feature was found.\n\n\n\n\nExample 3: One task, two learners, feature filtering with tuning\n\n\nHere is a minimal example for feature filtering with tuning of the feature subset size.\n\n\n## Feature filtering with tuning in the inner resampling loop\nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n)\nps = makeParamSet(makeDiscreteParam(\nfw.abs\n, values = seq_len(getTaskNFeats(bh.task))))\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc(\nCV\n, iter = 2)\nlrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl,\n  show.info = FALSE)\n\n## Learners\nlrns = list(\nregr.rpart\n, lrn)\n\n## Outer resampling loop\nouter = makeResampleDesc(\nSubsample\n, iter = 3)\nres = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE)\n\nres\n#\n                 task.id             learner.id mse.test.mean\n#\n 1 BostonHousing-example             regr.rpart      22.11687\n#\n 2 BostonHousing-example regr.lm.filtered.tuned      23.76666\n\n\n\n\n## Performances on individual outer test data sets\ngetBMRPerformances(res, as.df = TRUE)\n#\n                 task.id             learner.id iter      mse\n#\n 1 BostonHousing-example             regr.rpart    1 23.55486\n#\n 2 BostonHousing-example             regr.rpart    2 20.03453\n#\n 3 BostonHousing-example             regr.rpart    3 22.76121\n#\n 4 BostonHousing-example regr.lm.filtered.tuned    1 27.51086\n#\n 5 BostonHousing-example regr.lm.filtered.tuned    2 24.87820\n#\n 6 BostonHousing-example regr.lm.filtered.tuned    3 18.91091\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## Tuning in inner resampling loop \nps = makeParamSet( \n  makeDiscreteParam(\nC\n, values = 2^(-2:2)), \n  makeDiscreteParam(\nsigma\n, values = 2^(-2:2)) \n) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc(\nSubsample\n, iters = 2) \nlrn = makeTuneWrapper(\nclassif.ksvm\n, resampling = inner, par.set = ps, control = ctrl, show.info = FALSE) \n\n## Outer resampling loop \nouter = makeResampleDesc(\nCV\n, iters = 3) \nr = resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE) \n\nr \nr$measures.test \nr$extract \n\nnames(r$extract[[1]]) \nopt.paths = getNestedTuneResultsOptPathDf(r) \nhead(opt.paths, 10) \ng = ggplot(opt.paths, aes(x = C, y = sigma, fill = mmce.test.mean)) \ng + geom_tile() + facet_wrap(~ iter) \ngetNestedTuneResultsX(r) \n## Feature selection in inner resampling loop \ninner = makeResampleDesc(\nCV\n, iters = 3) \nlrn = makeFeatSelWrapper(\nregr.lm\n, resampling = inner, \n  control = makeFeatSelControlSequential(method = \nsfs\n), show.info = FALSE) \n\n## Outer resampling loop \nouter = makeResampleDesc(\nSubsample\n, iters = 2) \nr = resample(learner = lrn, task = bh.task, resampling = outer, extract = getFeatSelResult, \n  show.info = FALSE) \n\nr \n\nr$measures.test \nr$extract \n\n## Selected features in the first outer resampling iteration \nr$extract[[1]]$x \n\n## Resampled performance of the selected feature subset on the first inner training set \nr$extract[[1]]$y \nopt.paths = lapply(r$extract, function(x) as.data.frame(x$opt.path)) \nhead(opt.paths[[1]]) \nanalyzeFeatSelResult(r$extract[[1]]) \n## Tuning of the percentage of selected filters in the inner loop \nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n) \nps = makeParamSet(makeDiscreteParam(\nfw.threshold\n, values = seq(0, 1, 0.2))) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc(\nCV\n, iters = 3) \nlrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl, show.info = FALSE) \n\n## Outer resampling loop \nouter = makeResampleDesc(\nCV\n, iters = 3) \nr = resample(learner = lrn, task = bh.task, resampling = outer, models = TRUE, show.info = FALSE) \nr \nr$models \nlapply(r$models, function(x) getFilteredFeatures(x$learner.model$next.model)) \nres = lapply(r$models, getTuneResult) \nres \n\nopt.paths = lapply(res, function(x) as.data.frame(x$opt.path)) \nopt.paths[[1]] \n## List of learning tasks \ntasks = list(iris.task, sonar.task) \n\n## Tune svm in the inner resampling loop \nps = makeParamSet( \n  makeDiscreteParam(\nC\n, 2^(-1:1)), \n  makeDiscreteParam(\nsigma\n, 2^(-1:1))) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc(\nHoldout\n) \nlrn1 = makeTuneWrapper(\nclassif.ksvm\n, resampling = inner, par.set = ps, control = ctrl, \n  show.info = FALSE) \n\n## Tune k-nearest neighbor in inner resampling loop \nps = makeParamSet(makeDiscreteParam(\nk\n, 3:5)) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc(\nSubsample\n, iters = 3) \nlrn2 = makeTuneWrapper(\nclassif.kknn\n, resampling = inner, par.set = ps, control = ctrl, \n  show.info = FALSE) \n\n## Learners \nlrns = list(lrn1, lrn2) \n\n## Outer resampling loop \nouter = list(makeResampleDesc(\nHoldout\n), makeResampleDesc(\nBootstrap\n, iters = 2)) \nres = benchmark(lrns, tasks, outer, measures = list(acc, ber), show.info = FALSE) \nres \ngetBMRPerformances(res, as.df = TRUE) \ngetBMRTuneResults(res) \ngetBMRTuneResults(res, as.df = TRUE) \ntune.res = getBMRTuneResults(res, task.ids = \nSonar_example\n, learner.ids = \nclassif.ksvm.tuned\n, \n  as.df = TRUE) \ntune.res \n\ngetNestedTuneResultsOptPathDf(res$results[[\nSonar_example\n]][[\nclassif.ksvm.tuned\n]]) \n## Feature selection in inner resampling loop \nctrl = makeFeatSelControlSequential(method = \nsfs\n) \ninner = makeResampleDesc(\nSubsample\n, iters = 2) \nlrn = makeFeatSelWrapper(\nregr.lm\n, resampling = inner, control = ctrl, show.info = FALSE) \n\n## Learners \nlrns = list(\nregr.rpart\n, lrn) \n\n## Outer resampling loop \nouter = makeResampleDesc(\nSubsample\n, iters = 2) \nres = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE) \n\nres \ngetBMRFeatSelResults(res) \ngetBMRFeatSelResults(res, drop = TRUE) \nfeats = getBMRFeatSelResults(res, learner.id = \nregr.lm.featsel\n, drop = TRUE) \n\n## Selected features in the first outer resampling iteration \nfeats[[1]]$x \n\n## Resampled performance of the selected feature subset on the first inner training set \nfeats[[1]]$y \nopt.paths = lapply(feats, function(x) as.data.frame(x$opt.path)) \nhead(opt.paths[[1]]) \n\nanalyzeFeatSelResult(feats[[1]]) \n## Feature filtering with tuning in the inner resampling loop \nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n) \nps = makeParamSet(makeDiscreteParam(\nfw.abs\n, values = seq_len(getTaskNFeats(bh.task)))) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc(\nCV\n, iter = 2) \nlrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl, \n  show.info = FALSE) \n\n## Learners \nlrns = list(\nregr.rpart\n, lrn) \n\n## Outer resampling loop \nouter = makeResampleDesc(\nSubsample\n, iter = 3) \nres = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE) \n\nres \n## Performances on individual outer test data sets \ngetBMRPerformances(res, as.df = TRUE)", 
            "title": "Nested Resampling"
        }, 
        {
            "location": "/nested_resampling/index.html#nested-resampling", 
            "text": "In order to obtain honest performance estimates for a learner all parts of the model building\nlike preprocessing and model selection steps should be included in the resampling, i.e.,\nrepeated for every pair of training/test data.\nFor steps that themselves require resampling like  parameter tuning  or feature selection  (via the wrapper approach) this results in two\nnested resampling loops.   The graphic above illustrates nested resampling for parameter tuning with 3-fold cross-validation\nin the outer and 4-fold cross-validation in the inner loop.  In the outer resampling loop, we have three pairs of training/test sets.\nOn each of these outer training sets parameter tuning is done, thereby executing the inner\nresampling loop.\nThis way, we get one set of selected hyperparameters for each outer training set.\nThen the learner is fitted on each outer training set using the corresponding selected\nhyperparameters and its performance is evaluated on the outer test sets.  In  mlr , you can get nested resampling for free without programming any looping by\nusing the  wrapper functionality . This works as follows:   Generate a wrapped  Learner  via function  makeTuneWrapper  or  makeFeatSelWrapper .\n   Specify the inner resampling strategy using their  resampling  argument.  Call function  resample  (see also the section about  resampling ) and\n   pass the outer resampling strategy to its  resampling  argument.   You can freely combine different inner and outer resampling strategies.  The outer strategy can be a resample description ( ResampleDesc ) or a\nresample instance ( ResampleInstance ).\nA common setup is prediction and performance evaluation on a fixed outer test set. This can\nbe achieved by using function  makeFixedHoldoutInstance  to generate the outer ResampleInstance .  The inner resampling strategy should preferably be a  ResampleDesc , as the sizes\nof the outer training sets might differ.\nPer default, the inner resample description is instantiated once for every outer training set.\nThis way during tuning/feature selection all parameter or feature sets are compared\non the same inner training/test sets to reduce variance.\nYou can also turn this off using the  same.resampling.instance  argument of  makeTuneControl* \nor  makeFeatSelControl* .  Nested resampling is computationally expensive.\nFor this reason in the examples shown below we use relatively small search spaces and a low\nnumber of resampling iterations. In practice, you normally have to increase both.\nAs this is computationally intensive you might want to have a look at section parallelization .", 
            "title": "Nested Resampling"
        }, 
        {
            "location": "/nested_resampling/index.html#tuning", 
            "text": "As you might recall from the tutorial page about  tuning , you need to define a search space by\nfunction  makeParamSet , a search strategy by  makeTuneControl* ,\nand a method to evaluate hyperparameter settings (i.e., the inner resampling strategy and a performance measure).  Below is a classification example.\nWe evaluate the performance of a support vector machine ( ksvm ) with tuned\ncost parameter  C  and RBF kernel parameter  sigma .\nWe use 3-fold cross-validation in the outer and subsampling with 2 iterations in the inner\nloop.\nFor tuning a grid search is used to find the hyperparameters with lowest error rate\n( mmce  is the default measure for classification).\nThe wrapped  Learner  is generated by calling  makeTuneWrapper .  Note that in practice the parameter set should be larger.\nA common recommendation is  2^(-12:12)  for both  C  and  sigma .  ## Tuning in inner resampling loop\nps = makeParamSet(\n  makeDiscreteParam( C , values = 2^(-2:2)),\n  makeDiscreteParam( sigma , values = 2^(-2:2))\n)\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc( Subsample , iters = 2)\nlrn = makeTuneWrapper( classif.ksvm , resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)\n\n## Outer resampling loop\nouter = makeResampleDesc( CV , iters = 3)\nr = resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE)\n\nr\n#  Resample Result\n#  Task: iris_example\n#  Learner: classif.ksvm.tuned\n#  Aggr perf: mmce.test.mean=0.0533333\n#  Runtime: 3.40427  You can obtain the error rates on the 3 outer test sets by:  r$measures.test\n#    iter mmce\n#  1    1 0.02\n#  2    2 0.06\n#  3    3 0.08", 
            "title": "Tuning"
        }, 
        {
            "location": "/nested_resampling/index.html#accessing-the-tuning-result", 
            "text": "We have kept the results of the tuning for further evaluations.\nFor example one might want to find out, if the best obtained configurations vary for the\ndifferent outer splits.\nAs storing entire models may be expensive (but possible by setting  models = TRUE ) we used\nthe  extract  option of  resample .\nFunction  getTuneResult  returns, among other things, the optimal hyperparameter values and\nthe  optimization path  for each iteration of the outer resampling loop.\nNote that the performance values shown when printing  r$extract  are the aggregated performances\nresulting from inner resampling on the outer training set for the best hyperparameter configurations\n(not to be confused with  r$measures.test  shown above).  r$extract\n#  [[1]]\n#  Tune result:\n#  Op. pars: C=2; sigma=0.25\n#  mmce.test.mean=0.0147059\n#  \n#  [[2]]\n#  Tune result:\n#  Op. pars: C=4; sigma=0.25\n#  mmce.test.mean=0.0000000\n#  \n#  [[3]]\n#  Tune result:\n#  Op. pars: C=4; sigma=0.25\n#  mmce.test.mean=0.0735294\n\nnames(r$extract[[1]])\n#  [1]  learner     control     x           y           threshold   opt.path   We can compare the optimal parameter settings obtained in the 3 resampling iterations.\nAs you can see, the optimal configuration usually depends on the data. You may\nbe able to identify a  range  of parameter settings that achieve good\nperformance though, e.g., the values for  C  should be at least 1 and the values\nfor  sigma  should be between 0 and 1.  With function  getNestedTuneResultsOptPathDf  you can extract the optimization paths\nfor the 3 outer cross-validation iterations for further inspection and analysis.\nThese are stacked in one  data.frame  with column  iter  indicating the\nresampling iteration.  opt.paths = getNestedTuneResultsOptPathDf(r)\nhead(opt.paths, 10)\n#        C sigma mmce.test.mean dob eol error.message exec.time iter\n#  1  0.25  0.25     0.05882353   1  NA           NA      0.042    1\n#  2   0.5  0.25     0.04411765   2  NA           NA      0.042    1\n#  3     1  0.25     0.04411765   3  NA           NA      0.042    1\n#  4     2  0.25     0.01470588   4  NA           NA      0.048    1\n#  5     4  0.25     0.05882353   5  NA           NA      0.036    1\n#  6  0.25   0.5     0.05882353   6  NA           NA      0.036    1\n#  7   0.5   0.5     0.01470588   7  NA           NA      0.036    1\n#  8     1   0.5     0.02941176   8  NA           NA      0.036    1\n#  9     2   0.5     0.01470588   9  NA           NA      0.036    1\n#  10    4   0.5     0.05882353  10  NA           NA      0.035    1  Below we visualize the  opt.path s for the 3 outer resampling iterations.  g = ggplot(opt.paths, aes(x = C, y = sigma, fill = mmce.test.mean))\ng + geom_tile() + facet_wrap(~ iter)   Another useful function is  getNestedTuneResultsX , which extracts the best found hyperparameter\nsettings for each outer resampling iteration.  getNestedTuneResultsX(r)\n#    C sigma\n#  1 2  0.25\n#  2 4  0.25\n#  3 4  0.25", 
            "title": "Accessing the tuning result"
        }, 
        {
            "location": "/nested_resampling/index.html#feature-selection", 
            "text": "As you might recall from the section about  feature selection ,  mlr \nsupports the filter and the wrapper approach.", 
            "title": "Feature selection"
        }, 
        {
            "location": "/nested_resampling/index.html#wrapper-methods", 
            "text": "Wrapper methods use the performance of a learning algorithm to assess the usefulness of a\nfeature set. In order to select a feature subset a learner is trained repeatedly on different\nfeature subsets and the subset which leads to the best learner performance is chosen.  For feature selection in the inner resampling loop, you need to choose a search strategy\n(function  makeFeatSelControl* ), a performance measure and the inner\nresampling strategy. Then use function  makeFeatSelWrapper  to bind everything together.  Below we use sequential forward selection with linear regression on the BostonHousing  data set ( bh.task ).  ## Feature selection in inner resampling loop\ninner = makeResampleDesc( CV , iters = 3)\nlrn = makeFeatSelWrapper( regr.lm , resampling = inner,\n  control = makeFeatSelControlSequential(method =  sfs ), show.info = FALSE)\n\n## Outer resampling loop\nouter = makeResampleDesc( Subsample , iters = 2)\nr = resample(learner = lrn, task = bh.task, resampling = outer, extract = getFeatSelResult,\n  show.info = FALSE)\n\nr\n#  Resample Result\n#  Task: BostonHousing-example\n#  Learner: regr.lm.featsel\n#  Aggr perf: mse.test.mean=31.6991293\n#  Runtime: 7.93073\n\nr$measures.test\n#    iter      mse\n#  1    1 35.08611\n#  2    2 28.31215", 
            "title": "Wrapper methods"
        }, 
        {
            "location": "/nested_resampling/index.html#accessing-the-selected-features", 
            "text": "The result of the feature selection can be extracted by function  getFeatSelResult .\nIt is also possible to keep whole  models  by setting  models = TRUE \nwhen calling  resample .  r$extract\n#  [[1]]\n#  FeatSel result:\n#  Features (10): crim, zn, indus, nox, rm, dis, rad, tax, ptratio, lstat\n#  mse.test.mean=20.1593896\n#  \n#  [[2]]\n#  FeatSel result:\n#  Features (9): zn, nox, rm, dis, rad, tax, ptratio, b, lstat\n#  mse.test.mean=22.5966796\n\n## Selected features in the first outer resampling iteration\nr$extract[[1]]$x\n#   [1]  crim      zn        indus     nox       rm        dis       rad     \n#   [8]  tax       ptratio   lstat \n\n## Resampled performance of the selected feature subset on the first inner training set\nr$extract[[1]]$y\n#  mse.test.mean \n#       20.15939  As for tuning, you can extract the optimization paths.\nThe resulting  data.frame s contain, among others, binary columns for\nall features, indicating if they were included in the linear regression model, and the\ncorresponding performances.  opt.paths = lapply(r$extract, function(x) as.data.frame(x$opt.path))\nhead(opt.paths[[1]])\n#    crim zn indus chas nox rm age dis rad tax ptratio b lstat mse.test.mean\n#  1    0  0     0    0   0  0   0   0   0   0       0 0     0      80.33019\n#  2    1  0     0    0   0  0   0   0   0   0       0 0     0      65.95316\n#  3    0  1     0    0   0  0   0   0   0   0       0 0     0      69.15417\n#  4    0  0     1    0   0  0   0   0   0   0       0 0     0      55.75473\n#  5    0  0     0    1   0  0   0   0   0   0       0 0     0      80.48765\n#  6    0  0     0    0   1  0   0   0   0   0       0 0     0      63.06724\n#    dob eol error.message exec.time\n#  1   1   2           NA      0.021\n#  2   2   2           NA      0.034\n#  3   2   2           NA      0.033\n#  4   2   2           NA      0.032\n#  5   2   2           NA      0.035\n#  6   2   2           NA      0.032  An easy-to-read version of the optimization path for sequential feature selection can be\nobtained with function  analyzeFeatSelResult .  analyzeFeatSelResult(r$extract[[1]])\n#  Features         : 10\n#  Performance      : mse.test.mean=20.1593896\n#  crim, zn, indus, nox, rm, dis, rad, tax, ptratio, lstat\n#  \n#  Path to optimum:\n#  - Features:    0  Init   :                       Perf = 80.33  Diff: NA  *\n#  - Features:    1  Add    : lstat                 Perf = 36.451  Diff: 43.879  *\n#  - Features:    2  Add    : rm                    Perf = 27.289  Diff: 9.1623  *\n#  - Features:    3  Add    : ptratio               Perf = 24.004  Diff: 3.2849  *\n#  - Features:    4  Add    : nox                   Perf = 23.513  Diff: 0.49082  *\n#  - Features:    5  Add    : dis                   Perf = 21.49  Diff: 2.023  *\n#  - Features:    6  Add    : crim                  Perf = 21.12  Diff: 0.37008  *\n#  - Features:    7  Add    : indus                 Perf = 20.82  Diff: 0.29994  *\n#  - Features:    8  Add    : rad                   Perf = 20.609  Diff: 0.21054  *\n#  - Features:    9  Add    : tax                   Perf = 20.209  Diff: 0.40059  *\n#  - Features:   10  Add    : zn                    Perf = 20.159  Diff: 0.049441  *\n#  \n#  Stopped, because no improving feature was found.", 
            "title": "Accessing the selected features"
        }, 
        {
            "location": "/nested_resampling/index.html#filter-methods-with-tuning", 
            "text": "Filter methods assign an importance value to each feature.\nBased on these values you can select a feature subset by either keeping all features with importance\nhigher than a certain threshold or by keeping a fixed number or percentage of the highest ranking features.\nOften, neither the theshold nor the number or percentage of features is known in advance\nand thus tuning is necessary.  In the example below the threshold value ( fw.threshold ) is tuned in the inner resampling loop.\nFor this purpose the base  Learner   \"regr.lm\"  is wrapped two times.\nFirst,  makeFilterWrapper  is used to fuse linear regression with a feature filtering\npreprocessing step. Then a tuning step is added by  makeTuneWrapper .  ## Tuning of the percentage of selected filters in the inner loop\nlrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared )\nps = makeParamSet(makeDiscreteParam( fw.threshold , values = seq(0, 1, 0.2)))\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc( CV , iters = 3)\nlrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)\n\n## Outer resampling loop\nouter = makeResampleDesc( CV , iters = 3)\nr = resample(learner = lrn, task = bh.task, resampling = outer, models = TRUE, show.info = FALSE)\nr\n#  Resample Result\n#  Task: BostonHousing-example\n#  Learner: regr.lm.filtered.tuned\n#  Aggr perf: mse.test.mean=25.3915155\n#  Runtime: 3.85701", 
            "title": "Filter methods with tuning"
        }, 
        {
            "location": "/nested_resampling/index.html#accessing-the-selected-features-and-optimal-percentage", 
            "text": "In the above example we kept the complete  model s.  Below are some examples that show how to extract information from the  model s.  r$models\n#  [[1]]\n#  Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper\n#  Trained on: task.id = BostonHousing-example; obs = 337; features = 13\n#  Hyperparameters: fw.method=chi.squared\n#  \n#  [[2]]\n#  Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper\n#  Trained on: task.id = BostonHousing-example; obs = 338; features = 13\n#  Hyperparameters: fw.method=chi.squared\n#  \n#  [[3]]\n#  Model for learner.id=regr.lm.filtered.tuned; learner.class=TuneWrapper\n#  Trained on: task.id = BostonHousing-example; obs = 337; features = 13\n#  Hyperparameters: fw.method=chi.squared  The result of the feature selection can be extracted by function  getFilteredFeatures .\nAlmost always all 13 features are selected.  lapply(r$models, function(x) getFilteredFeatures(x$learner.model$next.model))\n#  [[1]]\n#   [1]  crim      zn        indus     chas      nox       rm        age     \n#   [8]  dis       rad       tax       ptratio   b         lstat   \n#  \n#  [[2]]\n#   [1]  crim      zn        indus     nox       rm        age       dis     \n#   [8]  rad       tax       ptratio   b         lstat   \n#  \n#  [[3]]\n#   [1]  crim      zn        indus     chas      nox       rm        age     \n#   [8]  dis       rad       tax       ptratio   b         lstat   Below the  tune results  and  optimization paths \nare accessed.  res = lapply(r$models, getTuneResult)\nres\n#  [[1]]\n#  Tune result:\n#  Op. pars: fw.threshold=0\n#  mse.test.mean=24.8915992\n#  \n#  [[2]]\n#  Tune result:\n#  Op. pars: fw.threshold=0.4\n#  mse.test.mean=27.1812625\n#  \n#  [[3]]\n#  Tune result:\n#  Op. pars: fw.threshold=0\n#  mse.test.mean=19.7012695\n\nopt.paths = lapply(res, function(x) as.data.frame(x$opt.path))\nopt.paths[[1]]\n#    fw.threshold mse.test.mean dob eol error.message exec.time\n#  1            0      24.89160   1  NA           NA      0.195\n#  2          0.2      25.18817   2  NA           NA      0.204\n#  3          0.4      25.18817   3  NA           NA      0.194\n#  4          0.6      32.15930   4  NA           NA      0.182\n#  5          0.8      90.89848   5  NA           NA      0.190\n#  6            1      90.89848   6  NA           NA      0.193", 
            "title": "Accessing the selected features and optimal percentage"
        }, 
        {
            "location": "/nested_resampling/index.html#benchmark-experiments", 
            "text": "In a benchmark experiment multiple learners are compared on one or several tasks\n(see also the section about  benchmarking ).\nNested resampling in benchmark experiments is achieved the same way as in resampling:   First, use  makeTuneWrapper  or  makeFeatSelWrapper  to generate wrapped  Learner s\n  with the inner resampling strategies of your choice.  Second, call  benchmark  and specify the outer resampling strategies for all tasks.   The inner resampling strategies should be  resample descriptions .\nYou can use different inner resampling strategies for different wrapped learners.\nFor example it might be practical to do fewer subsampling or bootstrap iterations for slower\nlearners.  If you have larger benchmark experiments you might want to have a look at the section\nabout  parallelization .  As mentioned in the section about  benchmark experiments  you can also use\ndifferent resampling strategies for different learning tasks by passing a list  of resampling descriptions or instances to  benchmark .  We will see three examples to show different benchmark settings:   Two data sets + two classification algorithms + tuning  One data set + two regression algorithms + feature selection  One data set + two regression algorithms + feature filtering + tuning", 
            "title": "Benchmark experiments"
        }, 
        {
            "location": "/nested_resampling/index.html#example-1-two-tasks-two-learners-tuning", 
            "text": "Below is a benchmark experiment with two data sets,  iris  and sonar , and two  Learner s, ksvm  and  kknn , that are both tuned.  As inner resampling strategies we use holdout for  ksvm  and subsampling\nwith 3 iterations for  kknn .\nAs outer resampling strategies we take holdout for the  iris  and bootstrap\nwith 2 iterations for the  sonar  data ( sonar.task ).\nWe consider the accuracy ( acc ), which is used as tuning criterion, and also\ncalculate the balanced error rate ( ber ).  ## List of learning tasks\ntasks = list(iris.task, sonar.task)\n\n## Tune svm in the inner resampling loop\nps = makeParamSet(\n  makeDiscreteParam( C , 2^(-1:1)),\n  makeDiscreteParam( sigma , 2^(-1:1)))\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc( Holdout )\nlrn1 = makeTuneWrapper( classif.ksvm , resampling = inner, par.set = ps, control = ctrl,\n  show.info = FALSE)\n\n## Tune k-nearest neighbor in inner resampling loop\nps = makeParamSet(makeDiscreteParam( k , 3:5))\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc( Subsample , iters = 3)\nlrn2 = makeTuneWrapper( classif.kknn , resampling = inner, par.set = ps, control = ctrl,\n  show.info = FALSE)\n\n## Learners\nlrns = list(lrn1, lrn2)\n\n## Outer resampling loop\nouter = list(makeResampleDesc( Holdout ), makeResampleDesc( Bootstrap , iters = 2))\nres = benchmark(lrns, tasks, outer, measures = list(acc, ber), show.info = FALSE)\nres\n#          task.id         learner.id acc.test.mean ber.test.mean\n#  1  iris_example classif.ksvm.tuned     0.9400000    0.05882353\n#  2  iris_example classif.kknn.tuned     0.9200000    0.08683473\n#  3 Sonar_example classif.ksvm.tuned     0.5289307    0.50000000\n#  4 Sonar_example classif.kknn.tuned     0.8077080    0.19549714  The  print  method for the  BenchmarkResult  shows the aggregated performances\nfrom the outer resampling loop.  As you might recall,  mlr  offers several accessor function to extract information from\nthe benchmark result.\nThese are listed on the help page of  BenchmarkResult  and many examples are shown on the\ntutorial page about  benchmark experiments .  The performance values in individual outer resampling runs can be obtained by  getBMRPerformances .\nNote that, since we used different outer resampling strategies for the two tasks, the number\nof rows per task differ.  getBMRPerformances(res, as.df = TRUE)\n#          task.id         learner.id iter       acc        ber\n#  1  iris_example classif.ksvm.tuned    1 0.9400000 0.05882353\n#  2  iris_example classif.kknn.tuned    1 0.9200000 0.08683473\n#  3 Sonar_example classif.ksvm.tuned    1 0.5373134 0.50000000\n#  4 Sonar_example classif.ksvm.tuned    2 0.5205479 0.50000000\n#  5 Sonar_example classif.kknn.tuned    1 0.8208955 0.18234767\n#  6 Sonar_example classif.kknn.tuned    2 0.7945205 0.20864662  The results from the parameter tuning can be obtained through function  getBMRTuneResults .  getBMRTuneResults(res)\n#  $iris_example\n#  $iris_example$classif.ksvm.tuned\n#  $iris_example$classif.ksvm.tuned[[1]]\n#  Tune result:\n#  Op. pars: C=0.5; sigma=0.5\n#  mmce.test.mean=0.0588235\n#  \n#  \n#  $iris_example$classif.kknn.tuned\n#  $iris_example$classif.kknn.tuned[[1]]\n#  Tune result:\n#  Op. pars: k=3\n#  mmce.test.mean=0.0490196\n#  \n#  \n#  \n#  $Sonar_example\n#  $Sonar_example$classif.ksvm.tuned\n#  $Sonar_example$classif.ksvm.tuned[[1]]\n#  Tune result:\n#  Op. pars: C=1; sigma=2\n#  mmce.test.mean=0.3428571\n#  \n#  $Sonar_example$classif.ksvm.tuned[[2]]\n#  Tune result:\n#  Op. pars: C=2; sigma=0.5\n#  mmce.test.mean=0.2000000\n#  \n#  \n#  $Sonar_example$classif.kknn.tuned\n#  $Sonar_example$classif.kknn.tuned[[1]]\n#  Tune result:\n#  Op. pars: k=4\n#  mmce.test.mean=0.1095238\n#  \n#  $Sonar_example$classif.kknn.tuned[[2]]\n#  Tune result:\n#  Op. pars: k=3\n#  mmce.test.mean=0.0666667  As for several other accessor functions a clearer representation as  data.frame \ncan be achieved by setting  as.df = TRUE .  getBMRTuneResults(res, as.df = TRUE)\n#          task.id         learner.id iter   C sigma mmce.test.mean  k\n#  1  iris_example classif.ksvm.tuned    1 0.5   0.5     0.05882353 NA\n#  2  iris_example classif.kknn.tuned    1  NA    NA     0.04901961  3\n#  3 Sonar_example classif.ksvm.tuned    1 1.0   2.0     0.34285714 NA\n#  4 Sonar_example classif.ksvm.tuned    2 2.0   0.5     0.20000000 NA\n#  5 Sonar_example classif.kknn.tuned    1  NA    NA     0.10952381  4\n#  6 Sonar_example classif.kknn.tuned    2  NA    NA     0.06666667  3  It is also possible to extract the tuning results for individual tasks and learners and,\nas shown in earlier examples, inspect the  optimization path .  tune.res = getBMRTuneResults(res, task.ids =  Sonar_example , learner.ids =  classif.ksvm.tuned ,\n  as.df = TRUE)\ntune.res\n#          task.id         learner.id iter C sigma mmce.test.mean\n#  1 Sonar_example classif.ksvm.tuned    1 1   2.0      0.3428571\n#  2 Sonar_example classif.ksvm.tuned    2 2   0.5      0.2000000\n\ngetNestedTuneResultsOptPathDf(res$results[[ Sonar_example ]][[ classif.ksvm.tuned ]])", 
            "title": "Example 1: Two tasks, two learners, tuning"
        }, 
        {
            "location": "/nested_resampling/index.html#example-2-one-task-two-learners-feature-selection", 
            "text": "Let's see how we can do  feature selection  in\na benchmark experiment:  ## Feature selection in inner resampling loop\nctrl = makeFeatSelControlSequential(method =  sfs )\ninner = makeResampleDesc( Subsample , iters = 2)\nlrn = makeFeatSelWrapper( regr.lm , resampling = inner, control = ctrl, show.info = FALSE)\n\n## Learners\nlrns = list( regr.rpart , lrn)\n\n## Outer resampling loop\nouter = makeResampleDesc( Subsample , iters = 2)\nres = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE)\n\nres\n#                  task.id      learner.id mse.test.mean\n#  1 BostonHousing-example      regr.rpart      25.86232\n#  2 BostonHousing-example regr.lm.featsel      25.07465  The selected features can be extracted by function  getBMRFeatSelResults .\nBy default, a nested  list , with the first level indicating the task and the\nsecond level indicating the learner, is returned.\nIf only a single learner or, as in our case, a single task is considered, setting drop = TRUE  simplifies the result to a flat  list .  getBMRFeatSelResults(res)\n#  $`BostonHousing-example`\n#  $`BostonHousing-example`$regr.rpart\n#  NULL\n#  \n#  $`BostonHousing-example`$regr.lm.featsel\n#  $`BostonHousing-example`$regr.lm.featsel[[1]]\n#  FeatSel result:\n#  Features (8): crim, zn, chas, nox, rm, dis, ptratio, lstat\n#  mse.test.mean=26.7257383\n#  \n#  $`BostonHousing-example`$regr.lm.featsel[[2]]\n#  FeatSel result:\n#  Features (10): crim, zn, nox, rm, dis, rad, tax, ptratio, b, lstat\n#  mse.test.mean=24.2874464\ngetBMRFeatSelResults(res, drop = TRUE)\n#  $regr.rpart\n#  NULL\n#  \n#  $regr.lm.featsel\n#  $regr.lm.featsel[[1]]\n#  FeatSel result:\n#  Features (8): crim, zn, chas, nox, rm, dis, ptratio, lstat\n#  mse.test.mean=26.7257383\n#  \n#  $regr.lm.featsel[[2]]\n#  FeatSel result:\n#  Features (10): crim, zn, nox, rm, dis, rad, tax, ptratio, b, lstat\n#  mse.test.mean=24.2874464  You can access results for individual learners and tasks and inspect them further.  feats = getBMRFeatSelResults(res, learner.id =  regr.lm.featsel , drop = TRUE)\n\n## Selected features in the first outer resampling iteration\nfeats[[1]]$x\n#  [1]  crim      zn        chas      nox       rm        dis       ptratio \n#  [8]  lstat \n\n## Resampled performance of the selected feature subset on the first inner training set\nfeats[[1]]$y\n#  mse.test.mean \n#       26.72574  As for tuning, you can extract the optimization paths. The resulting  data.frame s\ncontain, among others, binary columns for all features, indicating if they were included in the\nlinear regression model, and the corresponding performances. analyzeFeatSelResult  gives a clearer overview.  opt.paths = lapply(feats, function(x) as.data.frame(x$opt.path))\nhead(opt.paths[[1]])\n#    crim zn indus chas nox rm age dis rad tax ptratio b lstat mse.test.mean\n#  1    0  0     0    0   0  0   0   0   0   0       0 0     0      90.16159\n#  2    1  0     0    0   0  0   0   0   0   0       0 0     0      82.85880\n#  3    0  1     0    0   0  0   0   0   0   0       0 0     0      79.55202\n#  4    0  0     1    0   0  0   0   0   0   0       0 0     0      70.02071\n#  5    0  0     0    1   0  0   0   0   0   0       0 0     0      86.93409\n#  6    0  0     0    0   1  0   0   0   0   0       0 0     0      76.32457\n#    dob eol error.message exec.time\n#  1   1   2           NA      0.015\n#  2   2   2           NA      0.024\n#  3   2   2           NA      0.023\n#  4   2   2           NA      0.025\n#  5   2   2           NA      0.025\n#  6   2   2           NA      0.035\n\nanalyzeFeatSelResult(feats[[1]])\n#  Features         : 8\n#  Performance      : mse.test.mean=26.7257383\n#  crim, zn, chas, nox, rm, dis, ptratio, lstat\n#  \n#  Path to optimum:\n#  - Features:    0  Init   :                       Perf = 90.162  Diff: NA  *\n#  - Features:    1  Add    : lstat                 Perf = 42.646  Diff: 47.515  *\n#  - Features:    2  Add    : ptratio               Perf = 34.52  Diff: 8.1263  *\n#  - Features:    3  Add    : rm                    Perf = 30.454  Diff: 4.066  *\n#  - Features:    4  Add    : dis                   Perf = 29.405  Diff: 1.0495  *\n#  - Features:    5  Add    : nox                   Perf = 28.059  Diff: 1.3454  *\n#  - Features:    6  Add    : chas                  Perf = 27.334  Diff: 0.72499  *\n#  - Features:    7  Add    : zn                    Perf = 26.901  Diff: 0.43296  *\n#  - Features:    8  Add    : crim                  Perf = 26.726  Diff: 0.17558  *\n#  \n#  Stopped, because no improving feature was found.", 
            "title": "Example 2: One task, two learners, feature selection"
        }, 
        {
            "location": "/nested_resampling/index.html#example-3-one-task-two-learners-feature-filtering-with-tuning", 
            "text": "Here is a minimal example for feature filtering with tuning of the feature subset size.  ## Feature filtering with tuning in the inner resampling loop\nlrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared )\nps = makeParamSet(makeDiscreteParam( fw.abs , values = seq_len(getTaskNFeats(bh.task))))\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc( CV , iter = 2)\nlrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl,\n  show.info = FALSE)\n\n## Learners\nlrns = list( regr.rpart , lrn)\n\n## Outer resampling loop\nouter = makeResampleDesc( Subsample , iter = 3)\nres = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE)\n\nres\n#                  task.id             learner.id mse.test.mean\n#  1 BostonHousing-example             regr.rpart      22.11687\n#  2 BostonHousing-example regr.lm.filtered.tuned      23.76666  ## Performances on individual outer test data sets\ngetBMRPerformances(res, as.df = TRUE)\n#                  task.id             learner.id iter      mse\n#  1 BostonHousing-example             regr.rpart    1 23.55486\n#  2 BostonHousing-example             regr.rpart    2 20.03453\n#  3 BostonHousing-example             regr.rpart    3 22.76121\n#  4 BostonHousing-example regr.lm.filtered.tuned    1 27.51086\n#  5 BostonHousing-example regr.lm.filtered.tuned    2 24.87820\n#  6 BostonHousing-example regr.lm.filtered.tuned    3 18.91091", 
            "title": "Example 3: One task, two learners, feature filtering with tuning"
        }, 
        {
            "location": "/nested_resampling/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## Tuning in inner resampling loop \nps = makeParamSet( \n  makeDiscreteParam( C , values = 2^(-2:2)), \n  makeDiscreteParam( sigma , values = 2^(-2:2)) \n) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc( Subsample , iters = 2) \nlrn = makeTuneWrapper( classif.ksvm , resampling = inner, par.set = ps, control = ctrl, show.info = FALSE) \n\n## Outer resampling loop \nouter = makeResampleDesc( CV , iters = 3) \nr = resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE) \n\nr \nr$measures.test \nr$extract \n\nnames(r$extract[[1]]) \nopt.paths = getNestedTuneResultsOptPathDf(r) \nhead(opt.paths, 10) \ng = ggplot(opt.paths, aes(x = C, y = sigma, fill = mmce.test.mean)) \ng + geom_tile() + facet_wrap(~ iter) \ngetNestedTuneResultsX(r) \n## Feature selection in inner resampling loop \ninner = makeResampleDesc( CV , iters = 3) \nlrn = makeFeatSelWrapper( regr.lm , resampling = inner, \n  control = makeFeatSelControlSequential(method =  sfs ), show.info = FALSE) \n\n## Outer resampling loop \nouter = makeResampleDesc( Subsample , iters = 2) \nr = resample(learner = lrn, task = bh.task, resampling = outer, extract = getFeatSelResult, \n  show.info = FALSE) \n\nr \n\nr$measures.test \nr$extract \n\n## Selected features in the first outer resampling iteration \nr$extract[[1]]$x \n\n## Resampled performance of the selected feature subset on the first inner training set \nr$extract[[1]]$y \nopt.paths = lapply(r$extract, function(x) as.data.frame(x$opt.path)) \nhead(opt.paths[[1]]) \nanalyzeFeatSelResult(r$extract[[1]]) \n## Tuning of the percentage of selected filters in the inner loop \nlrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared ) \nps = makeParamSet(makeDiscreteParam( fw.threshold , values = seq(0, 1, 0.2))) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc( CV , iters = 3) \nlrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl, show.info = FALSE) \n\n## Outer resampling loop \nouter = makeResampleDesc( CV , iters = 3) \nr = resample(learner = lrn, task = bh.task, resampling = outer, models = TRUE, show.info = FALSE) \nr \nr$models \nlapply(r$models, function(x) getFilteredFeatures(x$learner.model$next.model)) \nres = lapply(r$models, getTuneResult) \nres \n\nopt.paths = lapply(res, function(x) as.data.frame(x$opt.path)) \nopt.paths[[1]] \n## List of learning tasks \ntasks = list(iris.task, sonar.task) \n\n## Tune svm in the inner resampling loop \nps = makeParamSet( \n  makeDiscreteParam( C , 2^(-1:1)), \n  makeDiscreteParam( sigma , 2^(-1:1))) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc( Holdout ) \nlrn1 = makeTuneWrapper( classif.ksvm , resampling = inner, par.set = ps, control = ctrl, \n  show.info = FALSE) \n\n## Tune k-nearest neighbor in inner resampling loop \nps = makeParamSet(makeDiscreteParam( k , 3:5)) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc( Subsample , iters = 3) \nlrn2 = makeTuneWrapper( classif.kknn , resampling = inner, par.set = ps, control = ctrl, \n  show.info = FALSE) \n\n## Learners \nlrns = list(lrn1, lrn2) \n\n## Outer resampling loop \nouter = list(makeResampleDesc( Holdout ), makeResampleDesc( Bootstrap , iters = 2)) \nres = benchmark(lrns, tasks, outer, measures = list(acc, ber), show.info = FALSE) \nres \ngetBMRPerformances(res, as.df = TRUE) \ngetBMRTuneResults(res) \ngetBMRTuneResults(res, as.df = TRUE) \ntune.res = getBMRTuneResults(res, task.ids =  Sonar_example , learner.ids =  classif.ksvm.tuned , \n  as.df = TRUE) \ntune.res \n\ngetNestedTuneResultsOptPathDf(res$results[[ Sonar_example ]][[ classif.ksvm.tuned ]]) \n## Feature selection in inner resampling loop \nctrl = makeFeatSelControlSequential(method =  sfs ) \ninner = makeResampleDesc( Subsample , iters = 2) \nlrn = makeFeatSelWrapper( regr.lm , resampling = inner, control = ctrl, show.info = FALSE) \n\n## Learners \nlrns = list( regr.rpart , lrn) \n\n## Outer resampling loop \nouter = makeResampleDesc( Subsample , iters = 2) \nres = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE) \n\nres \ngetBMRFeatSelResults(res) \ngetBMRFeatSelResults(res, drop = TRUE) \nfeats = getBMRFeatSelResults(res, learner.id =  regr.lm.featsel , drop = TRUE) \n\n## Selected features in the first outer resampling iteration \nfeats[[1]]$x \n\n## Resampled performance of the selected feature subset on the first inner training set \nfeats[[1]]$y \nopt.paths = lapply(feats, function(x) as.data.frame(x$opt.path)) \nhead(opt.paths[[1]]) \n\nanalyzeFeatSelResult(feats[[1]]) \n## Feature filtering with tuning in the inner resampling loop \nlrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared ) \nps = makeParamSet(makeDiscreteParam( fw.abs , values = seq_len(getTaskNFeats(bh.task)))) \nctrl = makeTuneControlGrid() \ninner = makeResampleDesc( CV , iter = 2) \nlrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl, \n  show.info = FALSE) \n\n## Learners \nlrns = list( regr.rpart , lrn) \n\n## Outer resampling loop \nouter = makeResampleDesc( Subsample , iter = 3) \nres = benchmark(tasks = bh.task, learners = lrns, resampling = outer, show.info = FALSE) \n\nres \n## Performances on individual outer test data sets \ngetBMRPerformances(res, as.df = TRUE)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html", 
            "text": "Cost-Sensitive Classification\n\n\nIn \nregular classification\n the aim is to minimize the misclassification rate and\nthus all types of misclassification errors are deemed equally severe.\nA more general setting is \ncost-sensitive classification\n where the costs caused by different\nkinds of errors are not assumed to be equal and the objective is to minimize the expected costs.\n\n\nIn case of \nclass-dependent costs\n the costs depend on the true and predicted class label.\nThe costs \nc(k, l)\n for predicting class \nk\n if the true label is \nl\n are usually organized\ninto a \nK \\times K\n cost matrix where \nK\n is the number of classes.\nNaturally, it is assumed that the cost of predicting the correct class label \ny\n is minimal\n(that is \nc(y, y) \\leq c(k, y)\n for all \nk = 1,\\ldots,K\n).\n\n\nA further generalization of this scenario are \nexample-dependent misclassification costs\n where\neach example \n(x, y)\n is coupled with an individual cost vector of length \nK\n. Its \nk\n-th\ncomponent expresses the cost of assigning \nx\n to class \nk\n.\nA real-world example is fraud detection where the costs do not only depend on the true and\npredicted status fraud/non-fraud, but also on the amount of money involved in each case.\nNaturally, the cost of predicting the true class label \ny\n is assumed to be minimum.\nThe true class labels are redundant information, as they can be easily inferred from the\ncost vectors.\nMoreover, given the cost vector, the expected costs do not depend on the true class label \ny\n.\nThe classification problem is therefore completely defined by the feature values \nx\n and the\ncorresponding cost vectors.\n\n\nIn the following we show ways to handle cost-sensitive classification problems in \nmlr\n.\nSome of the functionality is currently experimental, and there may be changes in the future.\n\n\nClass-dependent misclassification costs\n\n\nThere are some classification methods that can accomodate misclassification costs\ndirectly.\nOne example is \nrpart\n.\n\n\nAlternatively, we can use cost-insensitive methods and manipulate the predictions or the\ntraining data in order to take misclassification costs into account.\n\nmlr\n supports \nthresholding\n and \nrebalancing\n.\n\n\n\n\n\n\nThresholding\n:\n  The thresholds used to turn posterior probabilities into class labels are chosen such that\n  the costs are minimized.\n  This requires a \nLearner\n that can predict posterior probabilities.\n  During training the costs are not taken into account.\n\n\n\n\n\n\nRebalancing\n:\n  The idea is to change the proportion of the classes in the training data set in order to\n  account for costs during training, either by \nweighting\n or by \nsampling\n.\n  Rebalancing does not require that the \nLearner\n can predict probabilities.\n\n\ni. For \nweighting\n we need a \nLearner\n that supports class weights or observation\n     weights.\n\n\nii. If the \nLearner\n cannot deal with weights the proportion of classes can\n     be changed by \nover-\n and \nundersampling\n.\n\n\n\n\n\n\nWe start with binary classification problems and afterwards deal with multi-class problems.\n\n\nBinary classification problems\n\n\nThe positive and negative classes are labeled \n1\n and \n-1\n, respectively, and we consider the\nfollowing cost matrix where the rows indicate true classes and the columns predicted classes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrue/pred.\n\n\n\n\n+1\n\n\n\n\n\n\n-1\n\n\n\n\n\n\n\n\n\n\n+1\n\n\n\n\n\n\nc(+1,+1)\n\n\n\n\n\n\nc(-1,+1)\n\n\n\n\n\n\n\n\n\n\n-1\n\n\n\n\n\n\nc(+1,-1)\n\n\n\n\n\n\nc(-1,-1)\n\n\n\n\n\n\n\n\n\n\nOften, the diagonal entries are zero or the cost matrix is rescaled to achieve zeros in the diagonal\n(see for example \nO'Brien et al, 2008\n).\n\n\nA well-known cost-sensitive classification problem is posed by the\n\nGerman Credit data set\n\n(see also the \nUCI Machine Learning Repository\n).\nThe corresponding cost matrix (though \nElkan (2001)\n\nargues that this matrix is economically unreasonable) is given as:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrue/pred.\n\n\nBad\n\n\nGood\n\n\n\n\n\n\nBad\n\n\n0\n\n\n5\n\n\n\n\n\n\nGood\n\n\n1\n\n\n0\n\n\n\n\n\n\n\n\nAs in the table above, the rows indicate true and the columns predicted classes.\n\n\nIn case of class-dependent costs it is sufficient to generate an ordinary \nClassifTask\n.\nA \nCostSensTask\n is only needed if the costs are example-dependent.\nIn the \nR\n code below we create the \nClassifTask\n, remove two constant features from the\ndata set and generate the cost matrix.\nPer default, Bad is the positive class.\n\n\ndata(GermanCredit, package = \ncaret\n)\ncredit.task = makeClassifTask(data = GermanCredit, target = \nClass\n)\ncredit.task = removeConstantFeatures(credit.task)\n#\n Removing 2 columns: Purpose.Vacation,Personal.Female.Single\n\ncredit.task\n#\n Supervised task: GermanCredit\n#\n Type: classif\n#\n Target: Class\n#\n Observations: 1000\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n          59           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 2\n#\n  Bad Good \n#\n  300  700 \n#\n Positive class: Bad\n\ncosts = matrix(c(0, 1, 5, 0), 2)\ncolnames(costs) = rownames(costs) = getTaskClassLevels(credit.task)\ncosts\n#\n      Bad Good\n#\n Bad    0    5\n#\n Good   1    0\n\n\n\n\n1. Thresholding\n\n\nWe start by fitting a \nlogistic regression model\n to the\n\nGerman credit data set\n and predict posterior probabilities.\n\n\n## Train and predict posterior probabilities\nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, trace = FALSE)\nmod = train(lrn, credit.task)\npred = predict(mod, task = credit.task)\npred\n#\n Prediction: 1000 observations\n#\n predict.type: prob\n#\n threshold: Bad=0.50,Good=0.50\n#\n time: 0.02\n#\n   id truth   prob.Bad prob.Good response\n#\n 1  1  Good 0.03525092 0.9647491     Good\n#\n 2  2   Bad 0.63222363 0.3677764      Bad\n#\n 3  3  Good 0.02807414 0.9719259     Good\n#\n 4  4  Good 0.25182703 0.7481730     Good\n#\n 5  5   Bad 0.75193275 0.2480673      Bad\n#\n 6  6  Good 0.26230149 0.7376985     Good\n#\n ... (#rows: 1000, #cols: 5)\n\n\n\n\nThe default thresholds for both classes are 0.5.\nBut according to the cost matrix we should predict class Good only if we are very sure that Good\nis indeed the correct label. Therefore we should increase the threshold for class Good and decrease the\nthreshold for class Bad.\n\n\ni. Theoretical thresholding\n\n\nThe theoretical threshold for the \npositive\n class can be calculated from the cost matrix as\n\nt^* = \\frac{c(+1,-1) - c(-1,-1)}{c(+1,-1) - c(+1,+1) + c(-1,+1) - c(-1,-1)}.\n\nFor more details see \nElkan (2001)\n.\n\n\nBelow the theoretical threshold for the \nGerman credit example\n\nis calculated and used to predict class labels.\nSince the diagonal of the cost matrix is zero the formula given above simplifies accordingly.\n\n\n## Calculate the theoretical threshold for the positive class\nth = costs[2,1]/(costs[2,1] + costs[1,2])\nth\n#\n [1] 0.1666667\n\n\n\n\nAs you may recall you can change thresholds in \nmlr\n either before training by using the\n\npredict.threshold\n option of \nmakeLearner\n or after prediction by calling \nsetThreshold\n\non the \nPrediction\n object.\n\n\nAs we already have a prediction we use the \nsetThreshold\n function. It returns an altered\n\nPrediction\n object with class predictions for the theoretical threshold.\n\n\n## Predict class labels according to the theoretical threshold\npred.th = setThreshold(pred, th)\npred.th\n#\n Prediction: 1000 observations\n#\n predict.type: prob\n#\n threshold: Bad=0.17,Good=0.83\n#\n time: 0.02\n#\n   id truth   prob.Bad prob.Good response\n#\n 1  1  Good 0.03525092 0.9647491     Good\n#\n 2  2   Bad 0.63222363 0.3677764      Bad\n#\n 3  3  Good 0.02807414 0.9719259     Good\n#\n 4  4  Good 0.25182703 0.7481730      Bad\n#\n 5  5   Bad 0.75193275 0.2480673      Bad\n#\n 6  6  Good 0.26230149 0.7376985      Bad\n#\n ... (#rows: 1000, #cols: 5)\n\n\n\n\nIn order to calculate the average costs over the entire data set we first need to create a new\nperformance \nMeasure\n. This can be done through function \nmakeCostMeasure\n.\nIt is expected that the rows of the cost matrix indicate true and the columns predicted\nclass labels.\n\n\ncredit.costs = makeCostMeasure(id = \ncredit.costs\n, name = \nCredit costs\n, costs = costs,\n  best = 0, worst = 5)\ncredit.costs\n#\n Name: Credit costs\n#\n Performance measure: credit.costs\n#\n Properties: classif,classif.multi,req.pred,req.truth,predtype.response,predtype.prob\n#\n Minimize: TRUE\n#\n Best: 0; Worst: 5\n#\n Aggregated by: test.mean\n#\n Arguments: \nunnamed\n=\nmatrix\n, \nunnamed\n=\nfunction\n\n#\n Note:\n\n\n\n\nThen the average costs can be computed by function \nperformance\n.\nBelow we compare the average costs and the error rate (\nmmce\n) of the learning algorithm\nwith both default thresholds 0.5 and theoretical thresholds.\n\n\n## Performance with default thresholds 0.5\nperformance(pred, measures = list(credit.costs, mmce))\n#\n credit.costs         mmce \n#\n        0.774        0.214\n\n## Performance with theoretical thresholds\nperformance(pred.th, measures = list(credit.costs, mmce))\n#\n credit.costs         mmce \n#\n        0.478        0.346\n\n\n\n\nThese performance values may be overly optimistic as we used the same data set for training\nand prediction, and resampling strategies should be preferred.\nIn the \nR\n code below we make use of the \npredict.threshold\n argument of \nmakeLearner\n to set\nthe threshold before doing a 3-fold cross-validation on the \ncredit.task\n.\nNote that we create a \nResampleInstance\n (\nrin\n) that is used throughout\nthe next several code chunks to get comparable performance values.\n\n\n## Cross-validated performance with theoretical thresholds\nrin = makeResampleInstance(\nCV\n, iters = 3, task = credit.task)\nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, predict.threshold = th, trace = FALSE)\nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: classif.multinom\n#\n Aggr perf: credit.costs.test.mean=0.5831280,mmce.test.mean=0.3630397\n#\n Runtime: 0.210319\n\n\n\n\nIf we are also interested in the cross-validated performance for the default threshold values\nwe can call \nsetThreshold\n on the \nresample prediction\n \nr$pred\n.\n\n\n## Cross-validated performance with default thresholds\nperformance(setThreshold(r$pred, 0.5), measures = list(credit.costs, mmce))\n#\n credit.costs         mmce \n#\n    0.8600427    0.2520215\n\n\n\n\nTheoretical thresholding is only reliable if the predicted posterior probabilities are correct.\nIf there is bias the thresholds have to be shifted accordingly.\n\n\nUseful in this regard is function \nplotThreshVsPerf\n that you can use to plot the average costs\nas well as any other performance measure versus possible threshold values for the positive\nclass in \n[0,1]\n. The underlying data is generated by \ngenerateThreshVsPerfData\n.\n\n\nThe following plots show the cross-validated costs and error rate (\nmmce\n).\nThe theoretical threshold \nth\n calculated above is indicated by the vertical line.\nAs you can see from the left-hand plot the theoretical threshold seems a bit large.\n\n\nd = generateThreshVsPerfData(r, measures = list(credit.costs, mmce))\nplotThreshVsPerf(d, mark.th = th)\n\n\n\n\n\n\nii. Empirical thresholding\n\n\nThe idea of \nempirical thresholding\n (see \nSheng and Ling, 2006\n)\nis to select cost-optimal threshold values for a given learning method based on the training data.\nIn contrast to \ntheoretical thresholding\n it suffices if the estimated posterior probabilities\nare order-correct.\n\n\nIn order to determine optimal threshold values you can use \nmlr\n's function \ntuneThreshold\n.\nAs tuning the threshold on the complete training data set can lead to overfitting, you should\nuse resampling strategies.\nBelow we perform 3-fold cross-validation and use \ntuneThreshold\n to calculate threshold values\nwith lowest average costs over the 3 test data sets.\n\n\nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, trace = FALSE)\n\n## 3-fold cross-validation\nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: classif.multinom\n#\n Aggr perf: credit.costs.test.mean=0.8600427,mmce.test.mean=0.2520215\n#\n Runtime: 0.211516\n\n## Tune the threshold based on the predicted probabilities on the 3 test data sets\ntune.res = tuneThreshold(pred = r$pred, measure = credit.costs)\ntune.res\n#\n $th\n#\n [1] 0.1874551\n#\n \n#\n $perf\n#\n credit.costs \n#\n     0.569108\n\n\n\n\ntuneThreshold\n returns the optimal threshold value for the positive class and the corresponding\nperformance.\nAs expected the tuned threshold is smaller than the theoretical threshold.\n\n\n2. Rebalancing\n\n\nIn order to minimize the average costs, observations from the less costly class should be\ngiven higher importance during training.\nThis can be achieved by \nweighting\n the classes, provided that the learner under consideration\nhas a 'class weights' or an 'observation weights' argument.\nTo find out which learning methods support either type of weights have a look at the\n\nlist of integrated learners\n in the Appendix or use \nlistLearners\n.\n\n\n## Learners that accept observation weights\nlistLearners(\nclassif\n, properties = \nweights\n)[c(\nclass\n, \npackage\n)]\n#\n                class      package\n#\n 1   classif.binomial        stats\n#\n 2 classif.blackboost mboost,party\n#\n 3        classif.C50          C50\n#\n 4    classif.cforest        party\n#\n 5      classif.ctree        party\n#\n 6   classif.cvglmnet       glmnet\n#\n ... (#rows: 25, #cols: 2)\n\n## Learners that can deal with class weights\nlistLearners(\nclassif\n, properties = \nclass.weights\n)[c(\nclass\n, \npackage\n)]\n#\n                       class   package\n#\n 1              classif.ksvm   kernlab\n#\n 2  classif.LiblineaRL1L2SVC LiblineaR\n#\n 3 classif.LiblineaRL1LogReg LiblineaR\n#\n 4  classif.LiblineaRL2L1SVC LiblineaR\n#\n 5 classif.LiblineaRL2LogReg LiblineaR\n#\n 6    classif.LiblineaRL2SVC LiblineaR\n#\n ... (#rows: 9, #cols: 2)\n\n\n\n\nAlternatively, \nover- and undersampling\n techniques can be used.\n\n\ni. Weighting\n\n\nJust as \ntheoretical thresholds\n, \ntheoretical weights\n can be calculated from the\ncost matrix.\nIf \nt\n indicates the target threshold and \nt_0\n the original threshold for the positive class the\nproportion of observations in the positive class has to be multiplied by\n\n\\frac{1-t}{t} \\frac{t_0}{1-t_0}.\n\nAlternatively, the proportion of observations in the negative class can be multiplied by\nthe inverse.\nA proof is given by \nElkan (2001)\n.\n\n\nIn most cases, the original threshold is \nt_0 = 0.5\n and thus the second factor vanishes.\nIf additionally the target threshold \nt\n equals the theoretical threshold \nt^*\n the\nproportion of observations in the positive class has to be multiplied by\n\n\\frac{1-t^*}{t^*} = \\frac{c(-1,+1) - c(+1,+1)}{c(+1,-1) - c(-1,-1)}.\n\n\n\n\nFor the \ncredit example\n the theoretical threshold corresponds to a\nweight of 5 for the positive class.\n\n\n## Weight for positive class corresponding to theoretical treshold\nw = (1 - th)/th\nw\n#\n [1] 5\n\n\n\n\nA unified and convenient way to assign class weights to a \nLearner\n (and tune\nthem) is provided by function \nmakeWeightedClassesWrapper\n. The class weights are specified\nusing argument \nwcw.weight\n.\nFor learners that support observation weights a suitable weight vector is then generated\ninternally during training or resampling.\nIf the learner can deal with class weights, the weights are basically passed on to the\nappropriate learner parameter. The advantage of using the wrapper in this case is the unified\nway to specify the class weights.\n\n\nBelow is an example using learner \n\"classif.multinom\"\n (\nmultinom\n from\npackage \nnnet\n) which accepts observation weights.\nFor binary classification problems it is sufficient to specify the weight \nw\n for the positive\nclass. The negative class then automatically receives weight 1.\n\n\n## Weighted learner\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn, wcw.weight = w)\nlrn\n#\n Learner weightedclasses.classif.multinom from package nnet\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: WeightedClassesWrapper\n#\n Properties: twoclass,multiclass,numerics,factors,prob\n#\n Predict-Type: response\n#\n Hyperparameters: trace=FALSE,wcw.weight=5\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: weightedclasses.classif.multinom\n#\n Aggr perf: credit.costs.test.mean=0.5851031,mmce.test.mean=0.3530387\n#\n Runtime: 0.247982\n\n\n\n\nFor classification methods like \n\"classif.ksvm\"\n (the support vector machine\n\nksvm\n in package \nkernlab\n) that support class weights you can pass them\ndirectly.\n\n\nlrn = makeLearner(\nclassif.ksvm\n, class.weights = c(Bad = w, Good = 1))\n\n\n\n\nOr, more conveniently, you can again use \nmakeWeightedClassesWrapper\n.\n\n\nlrn = makeWeightedClassesWrapper(\nclassif.ksvm\n, wcw.weight = w)\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: weightedclasses.classif.ksvm\n#\n Aggr perf: credit.costs.test.mean=0.6520802,mmce.test.mean=0.3360276\n#\n Runtime: 0.385736\n\n\n\n\nJust like the theoretical threshold, the theoretical weights may not always be suitable,\ntherefore you can tune the weight for the positive class as shown in the following example.\nCalculating the theoretical weight beforehand may help to narrow down the search interval.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn)\nps = makeParamSet(makeDiscreteParam(\nwcw.weight\n, seq(4, 12, 0.5)))\nctrl = makeTuneControlGrid()\ntune.res = tuneParams(lrn, credit.task, resampling = rin, par.set = ps,\n  measures = list(credit.costs, mmce), control = ctrl, show.info = FALSE)\ntune.res\n#\n Tune result:\n#\n Op. pars: wcw.weight=7.5\n#\n credit.costs.test.mean=0.5590681,mmce.test.mean=0.3950118\n\nas.data.frame(tune.res$opt.path)[1:3]\n#\n    wcw.weight credit.costs.test.mean mmce.test.mean\n#\n 1           4              0.5961231      0.3280466\n#\n 2         4.5              0.5960931      0.3440327\n#\n 3           5              0.5851031      0.3530387\n#\n 4         5.5              0.5711010      0.3590327\n#\n 5           6              0.5761030      0.3720307\n#\n 6         6.5              0.5851091      0.3850287\n#\n 7           7              0.5830951      0.3950148\n#\n 8         7.5              0.5590681      0.3950118\n#\n 9           8              0.5700671      0.4060108\n#\n 10        8.5              0.5760671      0.4120108\n#\n 11          9              0.5780631      0.4180108\n#\n 12        9.5              0.5790581      0.4230098\n#\n 13         10              0.5850641      0.4290158\n#\n 14       10.5              0.5890771      0.4370209\n#\n 15         11              0.5770891      0.4410249\n#\n 16       11.5              0.5780901      0.4420259\n#\n 17         12              0.5770801      0.4450199\n\n\n\n\nii. Over- and undersampling\n\n\nIf the \nLearner\n supports neither observation nor class weights the proportions\nof the classes in the training data can be changed by over- or undersampling.\n\n\nIn the \nGermanCredit data set\n the positive class Bad should receive\na theoretical weight of \nw = (1 - th)/th = 5\n.\nThis can be achieved by oversampling class Bad with a \nrate\n of 5 or by undersampling\nclass Good with a \nrate\n of 1/5 (using functions \noversample\n or \nundersample\n).\n\n\ncredit.task.over = oversample(credit.task, rate = w, cl = \nBad\n)\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nmod = train(lrn, credit.task.over)\npred = predict(mod, task = credit.task)\nperformance(pred, measures = list(credit.costs, mmce))\n#\n credit.costs         mmce \n#\n        0.441        0.325\n\n\n\n\nNote that in the above example the learner was trained on the oversampled task \ncredit.task.over\n.\nIn order to get the training performance on the original task predictions were calculated for \ncredit.task\n.\n\n\nWe usually prefer resampled performance values, but simply calling \nresample\n on the oversampled\ntask does not work since predictions have to be based on the original task.\nThe solution is to create a wrapped \nLearner\n via function\n\nmakeOversampleWrapper\n.\nInternally, \noversample\n is called before training, but predictions are done on the original data.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeOversampleWrapper(lrn, osw.rate = w, osw.cl = \nBad\n)\nlrn\n#\n Learner classif.multinom.oversampled from package mlr,nnet\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: OversampleWrapper\n#\n Properties: numerics,factors,weights,prob,twoclass,multiclass\n#\n Predict-Type: response\n#\n Hyperparameters: trace=FALSE,osw.rate=5,osw.cl=Bad\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: classif.multinom.oversampled\n#\n Aggr perf: credit.costs.test.mean=0.5681190,mmce.test.mean=0.3360426\n#\n Runtime: 0.421018\n\n\n\n\nOf course, we can also tune the oversampling rate. For this purpose we again have to create\nan \nOversampleWrapper\n.\nOptimal values for parameter \nosw.rate\n can be obtained using function \ntuneParams\n.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeOversampleWrapper(lrn, osw.cl = \nBad\n)\nps = makeParamSet(makeDiscreteParam(\nosw.rate\n, seq(3, 7, 0.25)))\nctrl = makeTuneControlGrid()\ntune.res = tuneParams(lrn, credit.task, rin, par.set = ps, measures = list(credit.costs, mmce),\n  control = ctrl, show.info = FALSE)\ntune.res\n#\n Tune result:\n#\n Op. pars: osw.rate=6.75\n#\n credit.costs.test.mean=0.5700821,mmce.test.mean=0.3860178\n\n\n\n\nMulti-class problems\n\n\nWe consider the \nwaveform\n data set from package \nmlbench\n and\nadd an artificial cost matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrue/pred.\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n1\n\n\n0\n\n\n30\n\n\n80\n\n\n\n\n\n\n2\n\n\n5\n\n\n0\n\n\n4\n\n\n\n\n\n\n3\n\n\n10\n\n\n8\n\n\n0\n\n\n\n\n\n\n\n\nWe start by creating the \nTask\n, the cost matrix and the corresponding performance measure.\n\n\n## Task\ndf = mlbench::mlbench.waveform(500)\nwf.task = makeClassifTask(id = \nwaveform\n, data = as.data.frame(df), target = \nclasses\n)\n\n## Cost matrix\ncosts = matrix(c(0, 5, 10, 30, 0, 8, 80, 4, 0), 3)\ncolnames(costs) = rownames(costs) = getTaskClassLevels(wf.task)\n\n## Performance measure\nwf.costs = makeCostMeasure(id = \nwf.costs\n, name = \nWaveform costs\n, costs = costs,\n  best = 0, worst = 10)\n\n\n\n\nIn the multi-class case, both, \nthresholding\n and \nrebalancing\n correspond to cost matrices\nof a certain structure where \nc(k,l) = c(l)\n for \nk\n, \nl = 1, \\ldots, K\n, \nk \\neq l\n.\nThis condition means that the cost of misclassifying an observation is independent of the\npredicted class label\n(see \nDomingos, 1999\n).\nGiven a cost matrix of this type, theoretical thresholds and weights can be derived\nin a similar manner as in the binary case.\nObviously, the cost matrix given above does not have this special structure.\n\n\n1. Thresholding\n\n\nGiven a vector of positive threshold values as long as the number of classes \nK\n, the predicted\nprobabilities for all classes are adjusted by dividing them by the corresponding threshold value.\nThen the class with the highest adjusted probability is predicted.\nThis way, as in the binary case, classes with a low threshold are preferred to classes\nwith a larger threshold.\n\n\nAgain this can be done by function \nsetThreshold\n as shown in the following example (or\nalternatively by the \npredict.threshold\n option of \nmakeLearner\n).\nNote that the threshold vector needs to have names that correspond to the class labels.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nrin = makeResampleInstance(\nCV\n, iters = 3, task = wf.task)\nr = resample(lrn, wf.task, rin, measures = list(wf.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: waveform\n#\n Learner: classif.rpart\n#\n Aggr perf: wf.costs.test.mean=7.9568814,mmce.test.mean=0.3180386\n#\n Runtime: 0.0731192\n\n## Calculate thresholds as 1/(average costs of true classes)\nth = 2/rowSums(costs)\nnames(th) = getTaskClassLevels(wf.task)\nth\n#\n          1          2          3 \n#\n 0.01818182 0.22222222 0.11111111\n\npred.th = setThreshold(r$pred, threshold = th)\nperformance(pred.th, measures = list(wf.costs, mmce))\n#\n  wf.costs      mmce \n#\n 6.1248707 0.4699998\n\n\n\n\nThe threshold vector \nth\n in the above example is chosen according to the average costs\nof the true classes 55, 4.5 and 9.\nMore exactly, \nth\n corresponds to an artificial cost matrix of the structure mentioned\nabove with off-diagonal elements \nc(2,1) = c(3,1) = 55\n, \nc(1,2) = c(3,2) = 4.5\n and\n\nc(1,3) = c(2,3) = 9\n.\nThis threshold vector may be not optimal but leads to smaller total costs on the data set than\nthe default.\n\n\nii. Empirical thresholding\n\n\nAs in the binary case it is possible to tune the threshold vector using function \ntuneThreshold\n.\nSince the scaling of the threshold vector does not change the predicted class labels\n\ntuneThreshold\n returns threshold values that lie in [0,1] and sum to unity.\n\n\ntune.res = tuneThreshold(pred = r$pred, measure = wf.costs)\ntune.res\n#\n $th\n#\n          1          2          3 \n#\n 0.03481266 0.32529865 0.63988869 \n#\n \n#\n $perf\n#\n [1] 4.711613\n\n\n\n\nFor comparison we show the standardized version of the theoretically motivated threshold\nvector chosen above.\n\n\nth/sum(th)\n#\n          1          2          3 \n#\n 0.05172414 0.63218391 0.31609195\n\n\n\n\n2. Rebalancing\n\n\ni. Weighting\n\n\nIn the multi-class case you have to pass a vector of weights as long as the number of classes\n\nK\n to function \nmakeWeightedClassesWrapper\n.\nThe weight vector can be tuned using function \ntuneParams\n.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn)\n\nps = makeParamSet(makeNumericVectorParam(\nwcw.weight\n, len = 3, lower = 0, upper = 1))\nctrl = makeTuneControlRandom()\n\ntune.res = tuneParams(lrn, wf.task, resampling = rin, par.set = ps,\n  measures = list(wf.costs, mmce), control = ctrl, show.info = FALSE)\ntune.res\n#\n Tune result:\n#\n Op. pars: wcw.weight=0.673,0.0513,0.114\n#\n wf.costs.test.mean=3.2633889,mmce.test.mean=0.2179496\n\n\n\n\nExample-dependent misclassification costs\n\n\nIn case of example-dependent costs we have to create a special \nTask\n via function\n\nmakeCostSensTask\n.\nFor this purpose the feature values \nx\n and an \nn \\times K\n\n\ncost\n matrix that contains\nthe cost vectors for all \nn\n examples in the data set are required.\n\n\nWe use the \niris\n data and generate an artificial cost matrix\n(see \nBeygelzimer et al., 2005\n).\n\n\ndf = iris\ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] + runif(150, 0, 10)\ncolnames(cost) = levels(iris$Species)\nrownames(cost) = rownames(iris)\ndf$Species = NULL\n\ncostsens.task = makeCostSensTask(id = \niris\n, data = df, cost = cost)\ncostsens.task\n#\n Supervised task: iris\n#\n Type: costsens\n#\n Observations: 150\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n           4           0           0           0 \n#\n Missings: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 3\n#\n setosa, versicolor, virginica\n\n\n\n\nmlr\n provides several \nwrappers\n to turn regular classification or regression methods\ninto \nLearner\ns that can deal with example-dependent costs.\n\n\n\n\nmakeCostSensClassifWrapper\n (wraps a classification \nLearner\n):\n  This is a naive approach where the costs are coerced into class labels by choosing the\n  class label with minimum cost for each example. Then a regular classification method is\n  used.\n\n\nmakeCostSensRegrWrapper\n (wraps a regression \nLearner\n):\n  An individual regression model is fitted for the costs of each class.\n  In the prediction step first the costs are predicted for all classes and then the class with\n  the lowest predicted costs is selected.\n\n\nmakeCostSensWeightedPairsWrapper\n (wraps a classification \nLearner\n):\n  This is also known as \ncost-sensitive one-vs-one\n (CS-OVO) and the most sophisticated of\n  the currently supported methods.\n  For each pair of classes, a binary classifier is fitted.\n  For each observation the class label is defined as the element of the pair with minimal costs.\n  During fitting, the observations are weighted with the absolute difference in costs.\n  Prediction is performed by simple voting.\n\n\n\n\nIn the following example we use the third method. We create the wrapped \nLearner\n\nand train it on the \nCostSensTask\n defined above.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeCostSensWeightedPairsWrapper(lrn)\nlrn\n#\n Learner costsens.classif.multinom from package nnet\n#\n Type: costsens\n#\n Name: ; Short name: \n#\n Class: CostSensWeightedPairsWrapper\n#\n Properties: twoclass,multiclass,numerics,factors\n#\n Predict-Type: response\n#\n Hyperparameters: trace=FALSE\n\nmod = train(lrn, costsens.task)\nmod\n#\n Model for learner.id=costsens.classif.multinom; learner.class=CostSensWeightedPairsWrapper\n#\n Trained on: task.id = iris; obs = 150; features = 4\n#\n Hyperparameters: trace=FALSE\n\n\n\n\nThe models corresponding to the individual pairs can be accessed by function\n\ngetLearnerModel\n.\n\n\ngetLearnerModel(mod)\n#\n [[1]]\n#\n Model for learner.id=classif.multinom; learner.class=classif.multinom\n#\n Trained on: task.id = feats; obs = 150; features = 4\n#\n Hyperparameters: trace=FALSE\n#\n \n#\n [[2]]\n#\n Model for learner.id=classif.multinom; learner.class=classif.multinom\n#\n Trained on: task.id = feats; obs = 150; features = 4\n#\n Hyperparameters: trace=FALSE\n#\n \n#\n [[3]]\n#\n Model for learner.id=classif.multinom; learner.class=classif.multinom\n#\n Trained on: task.id = feats; obs = 150; features = 4\n#\n Hyperparameters: trace=FALSE\n\n\n\n\nmlr\n provides some performance measures for example-specific cost-sensitive classification.\nIn the following example we calculate the mean costs of the predicted class labels\n(\nmeancosts\n) and the misclassification penalty (\nmcp\n).\nThe latter measure is the average difference between the costs caused by the predicted\nclass labels, i.e., \nmeancosts\n, and the costs resulting from choosing the\nclass with lowest cost for each observation.\nIn order to compute these measures the costs for the test observations are required and\ntherefore the \nTask\n has to be passed to \nperformance\n.\n\n\npred = predict(mod, task = costsens.task)\npred\n#\n Prediction: 150 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.05\n#\n   id response\n#\n 1  1   setosa\n#\n 2  2   setosa\n#\n 3  3   setosa\n#\n 4  4   setosa\n#\n 5  5   setosa\n#\n 6  6   setosa\n#\n ... (#rows: 150, #cols: 2)\n\nperformance(pred, measures = list(meancosts, mcp), task = costsens.task)\n#\n meancosts       mcp \n#\n  151.0839  146.2973\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\ndata(GermanCredit, package = \ncaret\n) \ncredit.task = makeClassifTask(data = GermanCredit, target = \nClass\n) \ncredit.task = removeConstantFeatures(credit.task) \n\ncredit.task \n\ncosts = matrix(c(0, 1, 5, 0), 2) \ncolnames(costs) = rownames(costs) = getTaskClassLevels(credit.task) \ncosts \n## Train and predict posterior probabilities \nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, trace = FALSE) \nmod = train(lrn, credit.task) \npred = predict(mod, task = credit.task) \npred \n## Calculate the theoretical threshold for the positive class \nth = costs[2,1]/(costs[2,1] + costs[1,2]) \nth \n## Predict class labels according to the theoretical threshold \npred.th = setThreshold(pred, th) \npred.th \ncredit.costs = makeCostMeasure(id = \ncredit.costs\n, name = \nCredit costs\n, costs = costs, \n  best = 0, worst = 5) \ncredit.costs \n## Performance with default thresholds 0.5 \nperformance(pred, measures = list(credit.costs, mmce)) \n\n## Performance with theoretical thresholds \nperformance(pred.th, measures = list(credit.costs, mmce)) \n## Cross-validated performance with theoretical thresholds \nrin = makeResampleInstance(\nCV\n, iters = 3, task = credit.task) \nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, predict.threshold = th, trace = FALSE) \nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \n## Cross-validated performance with default thresholds \nperformance(setThreshold(r$pred, 0.5), measures = list(credit.costs, mmce)) \nd = generateThreshVsPerfData(r, measures = list(credit.costs, mmce)) \nplotThreshVsPerf(d, mark.th = th) \nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, trace = FALSE) \n\n## 3-fold cross-validation \nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \n\n## Tune the threshold based on the predicted probabilities on the 3 test data sets \ntune.res = tuneThreshold(pred = r$pred, measure = credit.costs) \ntune.res \n## Learners that accept observation weights \nlistLearners(\nclassif\n, properties = \nweights\n)[c(\nclass\n, \npackage\n)] \n\n## Learners that can deal with class weights \nlistLearners(\nclassif\n, properties = \nclass.weights\n)[c(\nclass\n, \npackage\n)] \n## Weight for positive class corresponding to theoretical treshold \nw = (1 - th)/th \nw \n## Weighted learner \nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE) \nlrn = makeWeightedClassesWrapper(lrn, wcw.weight = w) \nlrn \n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \nlrn = makeLearner(\nclassif.ksvm\n, class.weights = c(Bad = w, Good = 1)) \nlrn = makeWeightedClassesWrapper(\nclassif.ksvm\n, wcw.weight = w) \nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE) \nlrn = makeWeightedClassesWrapper(lrn) \nps = makeParamSet(makeDiscreteParam(\nwcw.weight\n, seq(4, 12, 0.5))) \nctrl = makeTuneControlGrid() \ntune.res = tuneParams(lrn, credit.task, resampling = rin, par.set = ps, \n  measures = list(credit.costs, mmce), control = ctrl, show.info = FALSE) \ntune.res \n\nas.data.frame(tune.res$opt.path)[1:3] \ncredit.task.over = oversample(credit.task, rate = w, cl = \nBad\n) \nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE) \nmod = train(lrn, credit.task.over) \npred = predict(mod, task = credit.task) \nperformance(pred, measures = list(credit.costs, mmce)) \nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE) \nlrn = makeOversampleWrapper(lrn, osw.rate = w, osw.cl = \nBad\n) \nlrn \n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE) \nlrn = makeOversampleWrapper(lrn, osw.cl = \nBad\n) \nps = makeParamSet(makeDiscreteParam(\nosw.rate\n, seq(3, 7, 0.25))) \nctrl = makeTuneControlGrid() \ntune.res = tuneParams(lrn, credit.task, rin, par.set = ps, measures = list(credit.costs, mmce), \n  control = ctrl, show.info = FALSE) \ntune.res \n## Task \ndf = mlbench::mlbench.waveform(500) \nwf.task = makeClassifTask(id = \nwaveform\n, data = as.data.frame(df), target = \nclasses\n) \n\n## Cost matrix \ncosts = matrix(c(0, 5, 10, 30, 0, 8, 80, 4, 0), 3) \ncolnames(costs) = rownames(costs) = getTaskClassLevels(wf.task) \n\n## Performance measure \nwf.costs = makeCostMeasure(id = \nwf.costs\n, name = \nWaveform costs\n, costs = costs, \n  best = 0, worst = 10) \nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n) \nrin = makeResampleInstance(\nCV\n, iters = 3, task = wf.task) \nr = resample(lrn, wf.task, rin, measures = list(wf.costs, mmce), show.info = FALSE) \nr \n\n## Calculate thresholds as 1/(average costs of true classes) \nth = 2/rowSums(costs) \nnames(th) = getTaskClassLevels(wf.task) \nth \n\npred.th = setThreshold(r$pred, threshold = th) \nperformance(pred.th, measures = list(wf.costs, mmce)) \ntune.res = tuneThreshold(pred = r$pred, measure = wf.costs) \ntune.res \nth/sum(th) \nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE) \nlrn = makeWeightedClassesWrapper(lrn) \n\nps = makeParamSet(makeNumericVectorParam(\nwcw.weight\n, len = 3, lower = 0, upper = 1)) \nctrl = makeTuneControlRandom() \n\ntune.res = tuneParams(lrn, wf.task, resampling = rin, par.set = ps, \n  measures = list(wf.costs, mmce), control = ctrl, show.info = FALSE) \ntune.res \ndf = iris \ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] + runif(150, 0, 10) \ncolnames(cost) = levels(iris$Species) \nrownames(cost) = rownames(iris) \ndf$Species = NULL \n\ncostsens.task = makeCostSensTask(id = \niris\n, data = df, cost = cost) \ncostsens.task \nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE) \nlrn = makeCostSensWeightedPairsWrapper(lrn) \nlrn \n\nmod = train(lrn, costsens.task) \nmod \ngetLearnerModel(mod) \npred = predict(mod, task = costsens.task) \npred \n\nperformance(pred, measures = list(meancosts, mcp), task = costsens.task)", 
            "title": "Cost-Sensitive Classification"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#cost-sensitive-classification", 
            "text": "In  regular classification  the aim is to minimize the misclassification rate and\nthus all types of misclassification errors are deemed equally severe.\nA more general setting is  cost-sensitive classification  where the costs caused by different\nkinds of errors are not assumed to be equal and the objective is to minimize the expected costs.  In case of  class-dependent costs  the costs depend on the true and predicted class label.\nThe costs  c(k, l)  for predicting class  k  if the true label is  l  are usually organized\ninto a  K \\times K  cost matrix where  K  is the number of classes.\nNaturally, it is assumed that the cost of predicting the correct class label  y  is minimal\n(that is  c(y, y) \\leq c(k, y)  for all  k = 1,\\ldots,K ).  A further generalization of this scenario are  example-dependent misclassification costs  where\neach example  (x, y)  is coupled with an individual cost vector of length  K . Its  k -th\ncomponent expresses the cost of assigning  x  to class  k .\nA real-world example is fraud detection where the costs do not only depend on the true and\npredicted status fraud/non-fraud, but also on the amount of money involved in each case.\nNaturally, the cost of predicting the true class label  y  is assumed to be minimum.\nThe true class labels are redundant information, as they can be easily inferred from the\ncost vectors.\nMoreover, given the cost vector, the expected costs do not depend on the true class label  y .\nThe classification problem is therefore completely defined by the feature values  x  and the\ncorresponding cost vectors.  In the following we show ways to handle cost-sensitive classification problems in  mlr .\nSome of the functionality is currently experimental, and there may be changes in the future.", 
            "title": "Cost-Sensitive Classification"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#class-dependent-misclassification-costs", 
            "text": "There are some classification methods that can accomodate misclassification costs\ndirectly.\nOne example is  rpart .  Alternatively, we can use cost-insensitive methods and manipulate the predictions or the\ntraining data in order to take misclassification costs into account. mlr  supports  thresholding  and  rebalancing .    Thresholding :\n  The thresholds used to turn posterior probabilities into class labels are chosen such that\n  the costs are minimized.\n  This requires a  Learner  that can predict posterior probabilities.\n  During training the costs are not taken into account.    Rebalancing :\n  The idea is to change the proportion of the classes in the training data set in order to\n  account for costs during training, either by  weighting  or by  sampling .\n  Rebalancing does not require that the  Learner  can predict probabilities.  i. For  weighting  we need a  Learner  that supports class weights or observation\n     weights.  ii. If the  Learner  cannot deal with weights the proportion of classes can\n     be changed by  over-  and  undersampling .    We start with binary classification problems and afterwards deal with multi-class problems.", 
            "title": "Class-dependent misclassification costs"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#binary-classification-problems", 
            "text": "The positive and negative classes are labeled  1  and  -1 , respectively, and we consider the\nfollowing cost matrix where the rows indicate true classes and the columns predicted classes:            true/pred.   +1    -1      +1    c(+1,+1)    c(-1,+1)      -1    c(+1,-1)    c(-1,-1)      Often, the diagonal entries are zero or the cost matrix is rescaled to achieve zeros in the diagonal\n(see for example  O'Brien et al, 2008 ).  A well-known cost-sensitive classification problem is posed by the German Credit data set \n(see also the  UCI Machine Learning Repository ).\nThe corresponding cost matrix (though  Elkan (2001) \nargues that this matrix is economically unreasonable) is given as:            true/pred.  Bad  Good    Bad  0  5    Good  1  0     As in the table above, the rows indicate true and the columns predicted classes.  In case of class-dependent costs it is sufficient to generate an ordinary  ClassifTask .\nA  CostSensTask  is only needed if the costs are example-dependent.\nIn the  R  code below we create the  ClassifTask , remove two constant features from the\ndata set and generate the cost matrix.\nPer default, Bad is the positive class.  data(GermanCredit, package =  caret )\ncredit.task = makeClassifTask(data = GermanCredit, target =  Class )\ncredit.task = removeConstantFeatures(credit.task)\n#  Removing 2 columns: Purpose.Vacation,Personal.Female.Single\n\ncredit.task\n#  Supervised task: GermanCredit\n#  Type: classif\n#  Target: Class\n#  Observations: 1000\n#  Features:\n#     numerics     factors     ordered functionals \n#           59           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 2\n#   Bad Good \n#   300  700 \n#  Positive class: Bad\n\ncosts = matrix(c(0, 1, 5, 0), 2)\ncolnames(costs) = rownames(costs) = getTaskClassLevels(credit.task)\ncosts\n#       Bad Good\n#  Bad    0    5\n#  Good   1    0", 
            "title": "Binary classification problems"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#1-thresholding", 
            "text": "We start by fitting a  logistic regression model  to the German credit data set  and predict posterior probabilities.  ## Train and predict posterior probabilities\nlrn = makeLearner( classif.multinom , predict.type =  prob , trace = FALSE)\nmod = train(lrn, credit.task)\npred = predict(mod, task = credit.task)\npred\n#  Prediction: 1000 observations\n#  predict.type: prob\n#  threshold: Bad=0.50,Good=0.50\n#  time: 0.02\n#    id truth   prob.Bad prob.Good response\n#  1  1  Good 0.03525092 0.9647491     Good\n#  2  2   Bad 0.63222363 0.3677764      Bad\n#  3  3  Good 0.02807414 0.9719259     Good\n#  4  4  Good 0.25182703 0.7481730     Good\n#  5  5   Bad 0.75193275 0.2480673      Bad\n#  6  6  Good 0.26230149 0.7376985     Good\n#  ... (#rows: 1000, #cols: 5)  The default thresholds for both classes are 0.5.\nBut according to the cost matrix we should predict class Good only if we are very sure that Good\nis indeed the correct label. Therefore we should increase the threshold for class Good and decrease the\nthreshold for class Bad.", 
            "title": "1. Thresholding"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#i-theoretical-thresholding", 
            "text": "The theoretical threshold for the  positive  class can be calculated from the cost matrix as t^* = \\frac{c(+1,-1) - c(-1,-1)}{c(+1,-1) - c(+1,+1) + c(-1,+1) - c(-1,-1)}. \nFor more details see  Elkan (2001) .  Below the theoretical threshold for the  German credit example \nis calculated and used to predict class labels.\nSince the diagonal of the cost matrix is zero the formula given above simplifies accordingly.  ## Calculate the theoretical threshold for the positive class\nth = costs[2,1]/(costs[2,1] + costs[1,2])\nth\n#  [1] 0.1666667  As you may recall you can change thresholds in  mlr  either before training by using the predict.threshold  option of  makeLearner  or after prediction by calling  setThreshold \non the  Prediction  object.  As we already have a prediction we use the  setThreshold  function. It returns an altered Prediction  object with class predictions for the theoretical threshold.  ## Predict class labels according to the theoretical threshold\npred.th = setThreshold(pred, th)\npred.th\n#  Prediction: 1000 observations\n#  predict.type: prob\n#  threshold: Bad=0.17,Good=0.83\n#  time: 0.02\n#    id truth   prob.Bad prob.Good response\n#  1  1  Good 0.03525092 0.9647491     Good\n#  2  2   Bad 0.63222363 0.3677764      Bad\n#  3  3  Good 0.02807414 0.9719259     Good\n#  4  4  Good 0.25182703 0.7481730      Bad\n#  5  5   Bad 0.75193275 0.2480673      Bad\n#  6  6  Good 0.26230149 0.7376985      Bad\n#  ... (#rows: 1000, #cols: 5)  In order to calculate the average costs over the entire data set we first need to create a new\nperformance  Measure . This can be done through function  makeCostMeasure .\nIt is expected that the rows of the cost matrix indicate true and the columns predicted\nclass labels.  credit.costs = makeCostMeasure(id =  credit.costs , name =  Credit costs , costs = costs,\n  best = 0, worst = 5)\ncredit.costs\n#  Name: Credit costs\n#  Performance measure: credit.costs\n#  Properties: classif,classif.multi,req.pred,req.truth,predtype.response,predtype.prob\n#  Minimize: TRUE\n#  Best: 0; Worst: 5\n#  Aggregated by: test.mean\n#  Arguments:  unnamed = matrix ,  unnamed = function \n#  Note:  Then the average costs can be computed by function  performance .\nBelow we compare the average costs and the error rate ( mmce ) of the learning algorithm\nwith both default thresholds 0.5 and theoretical thresholds.  ## Performance with default thresholds 0.5\nperformance(pred, measures = list(credit.costs, mmce))\n#  credit.costs         mmce \n#         0.774        0.214\n\n## Performance with theoretical thresholds\nperformance(pred.th, measures = list(credit.costs, mmce))\n#  credit.costs         mmce \n#         0.478        0.346  These performance values may be overly optimistic as we used the same data set for training\nand prediction, and resampling strategies should be preferred.\nIn the  R  code below we make use of the  predict.threshold  argument of  makeLearner  to set\nthe threshold before doing a 3-fold cross-validation on the  credit.task .\nNote that we create a  ResampleInstance  ( rin ) that is used throughout\nthe next several code chunks to get comparable performance values.  ## Cross-validated performance with theoretical thresholds\nrin = makeResampleInstance( CV , iters = 3, task = credit.task)\nlrn = makeLearner( classif.multinom , predict.type =  prob , predict.threshold = th, trace = FALSE)\nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: classif.multinom\n#  Aggr perf: credit.costs.test.mean=0.5831280,mmce.test.mean=0.3630397\n#  Runtime: 0.210319  If we are also interested in the cross-validated performance for the default threshold values\nwe can call  setThreshold  on the  resample prediction   r$pred .  ## Cross-validated performance with default thresholds\nperformance(setThreshold(r$pred, 0.5), measures = list(credit.costs, mmce))\n#  credit.costs         mmce \n#     0.8600427    0.2520215  Theoretical thresholding is only reliable if the predicted posterior probabilities are correct.\nIf there is bias the thresholds have to be shifted accordingly.  Useful in this regard is function  plotThreshVsPerf  that you can use to plot the average costs\nas well as any other performance measure versus possible threshold values for the positive\nclass in  [0,1] . The underlying data is generated by  generateThreshVsPerfData .  The following plots show the cross-validated costs and error rate ( mmce ).\nThe theoretical threshold  th  calculated above is indicated by the vertical line.\nAs you can see from the left-hand plot the theoretical threshold seems a bit large.  d = generateThreshVsPerfData(r, measures = list(credit.costs, mmce))\nplotThreshVsPerf(d, mark.th = th)", 
            "title": "i. Theoretical thresholding"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#ii-empirical-thresholding", 
            "text": "The idea of  empirical thresholding  (see  Sheng and Ling, 2006 )\nis to select cost-optimal threshold values for a given learning method based on the training data.\nIn contrast to  theoretical thresholding  it suffices if the estimated posterior probabilities\nare order-correct.  In order to determine optimal threshold values you can use  mlr 's function  tuneThreshold .\nAs tuning the threshold on the complete training data set can lead to overfitting, you should\nuse resampling strategies.\nBelow we perform 3-fold cross-validation and use  tuneThreshold  to calculate threshold values\nwith lowest average costs over the 3 test data sets.  lrn = makeLearner( classif.multinom , predict.type =  prob , trace = FALSE)\n\n## 3-fold cross-validation\nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: classif.multinom\n#  Aggr perf: credit.costs.test.mean=0.8600427,mmce.test.mean=0.2520215\n#  Runtime: 0.211516\n\n## Tune the threshold based on the predicted probabilities on the 3 test data sets\ntune.res = tuneThreshold(pred = r$pred, measure = credit.costs)\ntune.res\n#  $th\n#  [1] 0.1874551\n#  \n#  $perf\n#  credit.costs \n#      0.569108  tuneThreshold  returns the optimal threshold value for the positive class and the corresponding\nperformance.\nAs expected the tuned threshold is smaller than the theoretical threshold.", 
            "title": "ii. Empirical thresholding"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#2-rebalancing", 
            "text": "In order to minimize the average costs, observations from the less costly class should be\ngiven higher importance during training.\nThis can be achieved by  weighting  the classes, provided that the learner under consideration\nhas a 'class weights' or an 'observation weights' argument.\nTo find out which learning methods support either type of weights have a look at the list of integrated learners  in the Appendix or use  listLearners .  ## Learners that accept observation weights\nlistLearners( classif , properties =  weights )[c( class ,  package )]\n#                 class      package\n#  1   classif.binomial        stats\n#  2 classif.blackboost mboost,party\n#  3        classif.C50          C50\n#  4    classif.cforest        party\n#  5      classif.ctree        party\n#  6   classif.cvglmnet       glmnet\n#  ... (#rows: 25, #cols: 2)\n\n## Learners that can deal with class weights\nlistLearners( classif , properties =  class.weights )[c( class ,  package )]\n#                        class   package\n#  1              classif.ksvm   kernlab\n#  2  classif.LiblineaRL1L2SVC LiblineaR\n#  3 classif.LiblineaRL1LogReg LiblineaR\n#  4  classif.LiblineaRL2L1SVC LiblineaR\n#  5 classif.LiblineaRL2LogReg LiblineaR\n#  6    classif.LiblineaRL2SVC LiblineaR\n#  ... (#rows: 9, #cols: 2)  Alternatively,  over- and undersampling  techniques can be used.", 
            "title": "2. Rebalancing"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#i-weighting", 
            "text": "Just as  theoretical thresholds ,  theoretical weights  can be calculated from the\ncost matrix.\nIf  t  indicates the target threshold and  t_0  the original threshold for the positive class the\nproportion of observations in the positive class has to be multiplied by \\frac{1-t}{t} \\frac{t_0}{1-t_0}. \nAlternatively, the proportion of observations in the negative class can be multiplied by\nthe inverse.\nA proof is given by  Elkan (2001) .  In most cases, the original threshold is  t_0 = 0.5  and thus the second factor vanishes.\nIf additionally the target threshold  t  equals the theoretical threshold  t^*  the\nproportion of observations in the positive class has to be multiplied by \\frac{1-t^*}{t^*} = \\frac{c(-1,+1) - c(+1,+1)}{c(+1,-1) - c(-1,-1)}.   For the  credit example  the theoretical threshold corresponds to a\nweight of 5 for the positive class.  ## Weight for positive class corresponding to theoretical treshold\nw = (1 - th)/th\nw\n#  [1] 5  A unified and convenient way to assign class weights to a  Learner  (and tune\nthem) is provided by function  makeWeightedClassesWrapper . The class weights are specified\nusing argument  wcw.weight .\nFor learners that support observation weights a suitable weight vector is then generated\ninternally during training or resampling.\nIf the learner can deal with class weights, the weights are basically passed on to the\nappropriate learner parameter. The advantage of using the wrapper in this case is the unified\nway to specify the class weights.  Below is an example using learner  \"classif.multinom\"  ( multinom  from\npackage  nnet ) which accepts observation weights.\nFor binary classification problems it is sufficient to specify the weight  w  for the positive\nclass. The negative class then automatically receives weight 1.  ## Weighted learner\nlrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn, wcw.weight = w)\nlrn\n#  Learner weightedclasses.classif.multinom from package nnet\n#  Type: classif\n#  Name: ; Short name: \n#  Class: WeightedClassesWrapper\n#  Properties: twoclass,multiclass,numerics,factors,prob\n#  Predict-Type: response\n#  Hyperparameters: trace=FALSE,wcw.weight=5\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: weightedclasses.classif.multinom\n#  Aggr perf: credit.costs.test.mean=0.5851031,mmce.test.mean=0.3530387\n#  Runtime: 0.247982  For classification methods like  \"classif.ksvm\"  (the support vector machine ksvm  in package  kernlab ) that support class weights you can pass them\ndirectly.  lrn = makeLearner( classif.ksvm , class.weights = c(Bad = w, Good = 1))  Or, more conveniently, you can again use  makeWeightedClassesWrapper .  lrn = makeWeightedClassesWrapper( classif.ksvm , wcw.weight = w)\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: weightedclasses.classif.ksvm\n#  Aggr perf: credit.costs.test.mean=0.6520802,mmce.test.mean=0.3360276\n#  Runtime: 0.385736  Just like the theoretical threshold, the theoretical weights may not always be suitable,\ntherefore you can tune the weight for the positive class as shown in the following example.\nCalculating the theoretical weight beforehand may help to narrow down the search interval.  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn)\nps = makeParamSet(makeDiscreteParam( wcw.weight , seq(4, 12, 0.5)))\nctrl = makeTuneControlGrid()\ntune.res = tuneParams(lrn, credit.task, resampling = rin, par.set = ps,\n  measures = list(credit.costs, mmce), control = ctrl, show.info = FALSE)\ntune.res\n#  Tune result:\n#  Op. pars: wcw.weight=7.5\n#  credit.costs.test.mean=0.5590681,mmce.test.mean=0.3950118\n\nas.data.frame(tune.res$opt.path)[1:3]\n#     wcw.weight credit.costs.test.mean mmce.test.mean\n#  1           4              0.5961231      0.3280466\n#  2         4.5              0.5960931      0.3440327\n#  3           5              0.5851031      0.3530387\n#  4         5.5              0.5711010      0.3590327\n#  5           6              0.5761030      0.3720307\n#  6         6.5              0.5851091      0.3850287\n#  7           7              0.5830951      0.3950148\n#  8         7.5              0.5590681      0.3950118\n#  9           8              0.5700671      0.4060108\n#  10        8.5              0.5760671      0.4120108\n#  11          9              0.5780631      0.4180108\n#  12        9.5              0.5790581      0.4230098\n#  13         10              0.5850641      0.4290158\n#  14       10.5              0.5890771      0.4370209\n#  15         11              0.5770891      0.4410249\n#  16       11.5              0.5780901      0.4420259\n#  17         12              0.5770801      0.4450199", 
            "title": "i. Weighting"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#ii-over-and-undersampling", 
            "text": "If the  Learner  supports neither observation nor class weights the proportions\nof the classes in the training data can be changed by over- or undersampling.  In the  GermanCredit data set  the positive class Bad should receive\na theoretical weight of  w = (1 - th)/th = 5 .\nThis can be achieved by oversampling class Bad with a  rate  of 5 or by undersampling\nclass Good with a  rate  of 1/5 (using functions  oversample  or  undersample ).  credit.task.over = oversample(credit.task, rate = w, cl =  Bad )\nlrn = makeLearner( classif.multinom , trace = FALSE)\nmod = train(lrn, credit.task.over)\npred = predict(mod, task = credit.task)\nperformance(pred, measures = list(credit.costs, mmce))\n#  credit.costs         mmce \n#         0.441        0.325  Note that in the above example the learner was trained on the oversampled task  credit.task.over .\nIn order to get the training performance on the original task predictions were calculated for  credit.task .  We usually prefer resampled performance values, but simply calling  resample  on the oversampled\ntask does not work since predictions have to be based on the original task.\nThe solution is to create a wrapped  Learner  via function makeOversampleWrapper .\nInternally,  oversample  is called before training, but predictions are done on the original data.  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeOversampleWrapper(lrn, osw.rate = w, osw.cl =  Bad )\nlrn\n#  Learner classif.multinom.oversampled from package mlr,nnet\n#  Type: classif\n#  Name: ; Short name: \n#  Class: OversampleWrapper\n#  Properties: numerics,factors,weights,prob,twoclass,multiclass\n#  Predict-Type: response\n#  Hyperparameters: trace=FALSE,osw.rate=5,osw.cl=Bad\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: classif.multinom.oversampled\n#  Aggr perf: credit.costs.test.mean=0.5681190,mmce.test.mean=0.3360426\n#  Runtime: 0.421018  Of course, we can also tune the oversampling rate. For this purpose we again have to create\nan  OversampleWrapper .\nOptimal values for parameter  osw.rate  can be obtained using function  tuneParams .  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeOversampleWrapper(lrn, osw.cl =  Bad )\nps = makeParamSet(makeDiscreteParam( osw.rate , seq(3, 7, 0.25)))\nctrl = makeTuneControlGrid()\ntune.res = tuneParams(lrn, credit.task, rin, par.set = ps, measures = list(credit.costs, mmce),\n  control = ctrl, show.info = FALSE)\ntune.res\n#  Tune result:\n#  Op. pars: osw.rate=6.75\n#  credit.costs.test.mean=0.5700821,mmce.test.mean=0.3860178", 
            "title": "ii. Over- and undersampling"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#multi-class-problems", 
            "text": "We consider the  waveform  data set from package  mlbench  and\nadd an artificial cost matrix:             true/pred.  1  2  3    1  0  30  80    2  5  0  4    3  10  8  0     We start by creating the  Task , the cost matrix and the corresponding performance measure.  ## Task\ndf = mlbench::mlbench.waveform(500)\nwf.task = makeClassifTask(id =  waveform , data = as.data.frame(df), target =  classes )\n\n## Cost matrix\ncosts = matrix(c(0, 5, 10, 30, 0, 8, 80, 4, 0), 3)\ncolnames(costs) = rownames(costs) = getTaskClassLevels(wf.task)\n\n## Performance measure\nwf.costs = makeCostMeasure(id =  wf.costs , name =  Waveform costs , costs = costs,\n  best = 0, worst = 10)  In the multi-class case, both,  thresholding  and  rebalancing  correspond to cost matrices\nof a certain structure where  c(k,l) = c(l)  for  k ,  l = 1, \\ldots, K ,  k \\neq l .\nThis condition means that the cost of misclassifying an observation is independent of the\npredicted class label\n(see  Domingos, 1999 ).\nGiven a cost matrix of this type, theoretical thresholds and weights can be derived\nin a similar manner as in the binary case.\nObviously, the cost matrix given above does not have this special structure.", 
            "title": "Multi-class problems"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#1-thresholding_1", 
            "text": "Given a vector of positive threshold values as long as the number of classes  K , the predicted\nprobabilities for all classes are adjusted by dividing them by the corresponding threshold value.\nThen the class with the highest adjusted probability is predicted.\nThis way, as in the binary case, classes with a low threshold are preferred to classes\nwith a larger threshold.  Again this can be done by function  setThreshold  as shown in the following example (or\nalternatively by the  predict.threshold  option of  makeLearner ).\nNote that the threshold vector needs to have names that correspond to the class labels.  lrn = makeLearner( classif.rpart , predict.type =  prob )\nrin = makeResampleInstance( CV , iters = 3, task = wf.task)\nr = resample(lrn, wf.task, rin, measures = list(wf.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: waveform\n#  Learner: classif.rpart\n#  Aggr perf: wf.costs.test.mean=7.9568814,mmce.test.mean=0.3180386\n#  Runtime: 0.0731192\n\n## Calculate thresholds as 1/(average costs of true classes)\nth = 2/rowSums(costs)\nnames(th) = getTaskClassLevels(wf.task)\nth\n#           1          2          3 \n#  0.01818182 0.22222222 0.11111111\n\npred.th = setThreshold(r$pred, threshold = th)\nperformance(pred.th, measures = list(wf.costs, mmce))\n#   wf.costs      mmce \n#  6.1248707 0.4699998  The threshold vector  th  in the above example is chosen according to the average costs\nof the true classes 55, 4.5 and 9.\nMore exactly,  th  corresponds to an artificial cost matrix of the structure mentioned\nabove with off-diagonal elements  c(2,1) = c(3,1) = 55 ,  c(1,2) = c(3,2) = 4.5  and c(1,3) = c(2,3) = 9 .\nThis threshold vector may be not optimal but leads to smaller total costs on the data set than\nthe default.", 
            "title": "1. Thresholding"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#ii-empirical-thresholding_1", 
            "text": "As in the binary case it is possible to tune the threshold vector using function  tuneThreshold .\nSince the scaling of the threshold vector does not change the predicted class labels tuneThreshold  returns threshold values that lie in [0,1] and sum to unity.  tune.res = tuneThreshold(pred = r$pred, measure = wf.costs)\ntune.res\n#  $th\n#           1          2          3 \n#  0.03481266 0.32529865 0.63988869 \n#  \n#  $perf\n#  [1] 4.711613  For comparison we show the standardized version of the theoretically motivated threshold\nvector chosen above.  th/sum(th)\n#           1          2          3 \n#  0.05172414 0.63218391 0.31609195", 
            "title": "ii. Empirical thresholding"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#2-rebalancing_1", 
            "text": "", 
            "title": "2. Rebalancing"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#i-weighting_1", 
            "text": "In the multi-class case you have to pass a vector of weights as long as the number of classes K  to function  makeWeightedClassesWrapper .\nThe weight vector can be tuned using function  tuneParams .  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn)\n\nps = makeParamSet(makeNumericVectorParam( wcw.weight , len = 3, lower = 0, upper = 1))\nctrl = makeTuneControlRandom()\n\ntune.res = tuneParams(lrn, wf.task, resampling = rin, par.set = ps,\n  measures = list(wf.costs, mmce), control = ctrl, show.info = FALSE)\ntune.res\n#  Tune result:\n#  Op. pars: wcw.weight=0.673,0.0513,0.114\n#  wf.costs.test.mean=3.2633889,mmce.test.mean=0.2179496", 
            "title": "i. Weighting"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#example-dependent-misclassification-costs", 
            "text": "In case of example-dependent costs we have to create a special  Task  via function makeCostSensTask .\nFor this purpose the feature values  x  and an  n \\times K  cost  matrix that contains\nthe cost vectors for all  n  examples in the data set are required.  We use the  iris  data and generate an artificial cost matrix\n(see  Beygelzimer et al., 2005 ).  df = iris\ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] + runif(150, 0, 10)\ncolnames(cost) = levels(iris$Species)\nrownames(cost) = rownames(iris)\ndf$Species = NULL\n\ncostsens.task = makeCostSensTask(id =  iris , data = df, cost = cost)\ncostsens.task\n#  Supervised task: iris\n#  Type: costsens\n#  Observations: 150\n#  Features:\n#     numerics     factors     ordered functionals \n#            4           0           0           0 \n#  Missings: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 3\n#  setosa, versicolor, virginica  mlr  provides several  wrappers  to turn regular classification or regression methods\ninto  Learner s that can deal with example-dependent costs.   makeCostSensClassifWrapper  (wraps a classification  Learner ):\n  This is a naive approach where the costs are coerced into class labels by choosing the\n  class label with minimum cost for each example. Then a regular classification method is\n  used.  makeCostSensRegrWrapper  (wraps a regression  Learner ):\n  An individual regression model is fitted for the costs of each class.\n  In the prediction step first the costs are predicted for all classes and then the class with\n  the lowest predicted costs is selected.  makeCostSensWeightedPairsWrapper  (wraps a classification  Learner ):\n  This is also known as  cost-sensitive one-vs-one  (CS-OVO) and the most sophisticated of\n  the currently supported methods.\n  For each pair of classes, a binary classifier is fitted.\n  For each observation the class label is defined as the element of the pair with minimal costs.\n  During fitting, the observations are weighted with the absolute difference in costs.\n  Prediction is performed by simple voting.   In the following example we use the third method. We create the wrapped  Learner \nand train it on the  CostSensTask  defined above.  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeCostSensWeightedPairsWrapper(lrn)\nlrn\n#  Learner costsens.classif.multinom from package nnet\n#  Type: costsens\n#  Name: ; Short name: \n#  Class: CostSensWeightedPairsWrapper\n#  Properties: twoclass,multiclass,numerics,factors\n#  Predict-Type: response\n#  Hyperparameters: trace=FALSE\n\nmod = train(lrn, costsens.task)\nmod\n#  Model for learner.id=costsens.classif.multinom; learner.class=CostSensWeightedPairsWrapper\n#  Trained on: task.id = iris; obs = 150; features = 4\n#  Hyperparameters: trace=FALSE  The models corresponding to the individual pairs can be accessed by function getLearnerModel .  getLearnerModel(mod)\n#  [[1]]\n#  Model for learner.id=classif.multinom; learner.class=classif.multinom\n#  Trained on: task.id = feats; obs = 150; features = 4\n#  Hyperparameters: trace=FALSE\n#  \n#  [[2]]\n#  Model for learner.id=classif.multinom; learner.class=classif.multinom\n#  Trained on: task.id = feats; obs = 150; features = 4\n#  Hyperparameters: trace=FALSE\n#  \n#  [[3]]\n#  Model for learner.id=classif.multinom; learner.class=classif.multinom\n#  Trained on: task.id = feats; obs = 150; features = 4\n#  Hyperparameters: trace=FALSE  mlr  provides some performance measures for example-specific cost-sensitive classification.\nIn the following example we calculate the mean costs of the predicted class labels\n( meancosts ) and the misclassification penalty ( mcp ).\nThe latter measure is the average difference between the costs caused by the predicted\nclass labels, i.e.,  meancosts , and the costs resulting from choosing the\nclass with lowest cost for each observation.\nIn order to compute these measures the costs for the test observations are required and\ntherefore the  Task  has to be passed to  performance .  pred = predict(mod, task = costsens.task)\npred\n#  Prediction: 150 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.05\n#    id response\n#  1  1   setosa\n#  2  2   setosa\n#  3  3   setosa\n#  4  4   setosa\n#  5  5   setosa\n#  6  6   setosa\n#  ... (#rows: 150, #cols: 2)\n\nperformance(pred, measures = list(meancosts, mcp), task = costsens.task)\n#  meancosts       mcp \n#   151.0839  146.2973", 
            "title": "Example-dependent misclassification costs"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  data(GermanCredit, package =  caret ) \ncredit.task = makeClassifTask(data = GermanCredit, target =  Class ) \ncredit.task = removeConstantFeatures(credit.task) \n\ncredit.task \n\ncosts = matrix(c(0, 1, 5, 0), 2) \ncolnames(costs) = rownames(costs) = getTaskClassLevels(credit.task) \ncosts \n## Train and predict posterior probabilities \nlrn = makeLearner( classif.multinom , predict.type =  prob , trace = FALSE) \nmod = train(lrn, credit.task) \npred = predict(mod, task = credit.task) \npred \n## Calculate the theoretical threshold for the positive class \nth = costs[2,1]/(costs[2,1] + costs[1,2]) \nth \n## Predict class labels according to the theoretical threshold \npred.th = setThreshold(pred, th) \npred.th \ncredit.costs = makeCostMeasure(id =  credit.costs , name =  Credit costs , costs = costs, \n  best = 0, worst = 5) \ncredit.costs \n## Performance with default thresholds 0.5 \nperformance(pred, measures = list(credit.costs, mmce)) \n\n## Performance with theoretical thresholds \nperformance(pred.th, measures = list(credit.costs, mmce)) \n## Cross-validated performance with theoretical thresholds \nrin = makeResampleInstance( CV , iters = 3, task = credit.task) \nlrn = makeLearner( classif.multinom , predict.type =  prob , predict.threshold = th, trace = FALSE) \nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \n## Cross-validated performance with default thresholds \nperformance(setThreshold(r$pred, 0.5), measures = list(credit.costs, mmce)) \nd = generateThreshVsPerfData(r, measures = list(credit.costs, mmce)) \nplotThreshVsPerf(d, mark.th = th) \nlrn = makeLearner( classif.multinom , predict.type =  prob , trace = FALSE) \n\n## 3-fold cross-validation \nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \n\n## Tune the threshold based on the predicted probabilities on the 3 test data sets \ntune.res = tuneThreshold(pred = r$pred, measure = credit.costs) \ntune.res \n## Learners that accept observation weights \nlistLearners( classif , properties =  weights )[c( class ,  package )] \n\n## Learners that can deal with class weights \nlistLearners( classif , properties =  class.weights )[c( class ,  package )] \n## Weight for positive class corresponding to theoretical treshold \nw = (1 - th)/th \nw \n## Weighted learner \nlrn = makeLearner( classif.multinom , trace = FALSE) \nlrn = makeWeightedClassesWrapper(lrn, wcw.weight = w) \nlrn \n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \nlrn = makeLearner( classif.ksvm , class.weights = c(Bad = w, Good = 1)) \nlrn = makeWeightedClassesWrapper( classif.ksvm , wcw.weight = w) \nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \nlrn = makeLearner( classif.multinom , trace = FALSE) \nlrn = makeWeightedClassesWrapper(lrn) \nps = makeParamSet(makeDiscreteParam( wcw.weight , seq(4, 12, 0.5))) \nctrl = makeTuneControlGrid() \ntune.res = tuneParams(lrn, credit.task, resampling = rin, par.set = ps, \n  measures = list(credit.costs, mmce), control = ctrl, show.info = FALSE) \ntune.res \n\nas.data.frame(tune.res$opt.path)[1:3] \ncredit.task.over = oversample(credit.task, rate = w, cl =  Bad ) \nlrn = makeLearner( classif.multinom , trace = FALSE) \nmod = train(lrn, credit.task.over) \npred = predict(mod, task = credit.task) \nperformance(pred, measures = list(credit.costs, mmce)) \nlrn = makeLearner( classif.multinom , trace = FALSE) \nlrn = makeOversampleWrapper(lrn, osw.rate = w, osw.cl =  Bad ) \nlrn \n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE) \nr \nlrn = makeLearner( classif.multinom , trace = FALSE) \nlrn = makeOversampleWrapper(lrn, osw.cl =  Bad ) \nps = makeParamSet(makeDiscreteParam( osw.rate , seq(3, 7, 0.25))) \nctrl = makeTuneControlGrid() \ntune.res = tuneParams(lrn, credit.task, rin, par.set = ps, measures = list(credit.costs, mmce), \n  control = ctrl, show.info = FALSE) \ntune.res \n## Task \ndf = mlbench::mlbench.waveform(500) \nwf.task = makeClassifTask(id =  waveform , data = as.data.frame(df), target =  classes ) \n\n## Cost matrix \ncosts = matrix(c(0, 5, 10, 30, 0, 8, 80, 4, 0), 3) \ncolnames(costs) = rownames(costs) = getTaskClassLevels(wf.task) \n\n## Performance measure \nwf.costs = makeCostMeasure(id =  wf.costs , name =  Waveform costs , costs = costs, \n  best = 0, worst = 10) \nlrn = makeLearner( classif.rpart , predict.type =  prob ) \nrin = makeResampleInstance( CV , iters = 3, task = wf.task) \nr = resample(lrn, wf.task, rin, measures = list(wf.costs, mmce), show.info = FALSE) \nr \n\n## Calculate thresholds as 1/(average costs of true classes) \nth = 2/rowSums(costs) \nnames(th) = getTaskClassLevels(wf.task) \nth \n\npred.th = setThreshold(r$pred, threshold = th) \nperformance(pred.th, measures = list(wf.costs, mmce)) \ntune.res = tuneThreshold(pred = r$pred, measure = wf.costs) \ntune.res \nth/sum(th) \nlrn = makeLearner( classif.multinom , trace = FALSE) \nlrn = makeWeightedClassesWrapper(lrn) \n\nps = makeParamSet(makeNumericVectorParam( wcw.weight , len = 3, lower = 0, upper = 1)) \nctrl = makeTuneControlRandom() \n\ntune.res = tuneParams(lrn, wf.task, resampling = rin, par.set = ps, \n  measures = list(wf.costs, mmce), control = ctrl, show.info = FALSE) \ntune.res \ndf = iris \ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] + runif(150, 0, 10) \ncolnames(cost) = levels(iris$Species) \nrownames(cost) = rownames(iris) \ndf$Species = NULL \n\ncostsens.task = makeCostSensTask(id =  iris , data = df, cost = cost) \ncostsens.task \nlrn = makeLearner( classif.multinom , trace = FALSE) \nlrn = makeCostSensWeightedPairsWrapper(lrn) \nlrn \n\nmod = train(lrn, costsens.task) \nmod \ngetLearnerModel(mod) \npred = predict(mod, task = costsens.task) \npred \n\nperformance(pred, measures = list(meancosts, mcp), task = costsens.task)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/over_and_undersampling/index.html", 
            "text": "Imbalanced Classification Problems\n\n\nIn case of \nbinary classification\n strongly imbalanced classes\noften lead to unsatisfactory results regarding the prediction of new\nobservations, especially for the small class.\nIn this context \nimbalanced classes\n simply means that the number of\nobservations of one class (usu. positive or majority class) by far exceeds\nthe number of observations of the other class (usu. negative or minority class).\nThis setting can be observed fairly often in practice and in various disciplines\nlike credit scoring, fraud detection, medical diagnostics or churn management.\n\n\nMost classification methods work best when the number of observations per\nclass are roughly equal. The problem with \nimbalanced classes\n is that because\nof the dominance of the majority class classifiers tend to ignore cases of\nthe minority class as noise and therefore predict the majority class far more\noften. In order to lay more weight on the cases of the minority class, there are\nnumerous correction methods which tackle the \nimbalanced classification problem\n.\nThese methods can generally be divided into \ncost- and sampling-based approaches\n.\nBelow all methods supported by \nmlr\n are introduced.\n\n\nSampling-based approaches\n\n\nThe basic idea of \nsampling methods\n is to simply adjust the proportion of\nthe classes in order to increase the weight of the minority class observations\nwithin the model.\n\n\nThe \nsampling-based approaches\n can be divided further into three different categories:\n\n\n\n\n\n\nUndersampling methods\n:\n   Elimination of randomly chosen cases of the majority class to decrease their\n   effect on the classifier. All cases of the minority class are kept.\n\n\n\n\n\n\nOversampling methods\n:\n   Generation of additional cases (copies, artificial observations) of the minority\n   class to increase their effect on the classifier. All cases of the majority\n   class are kept.\n\n\n\n\n\n\nHybrid methods\n:\n   Mixture of under- and oversampling strategies.\n\n\n\n\n\n\nAll these methods directly access the underlying data and \"rearrange\" it.\nIn this way the sampling is done as part of the \npreprocesssing\n and can therefore\nbe combined with every appropriate classifier.\n\n\nmlr\n currently supports the first two approaches.\n\n\n(Simple) over- and undersampling\n\n\nAs mentioned above \nundersampling\n always refers to the majority class, while\n\noversampling\n affects the minority class. By the use of \nundersampling\n, randomly\nchosen observations of the majority class are eliminated. Through (simple)\n\noversampling\n all observations of the minority class are considered at least\nonce when fitting the model. In addition, exact copies of minority class cases are created\nby random sampling with repetitions.\n\n\nFirst, let's take a look at the effect for a classification \ntask\n.\nBased on a simulated \nClassifTask\n with imbalanced classes two new\ntasks (\ntask.over\n, \ntask.under\n) are created via \nmlr\n functions\n\noversample\n and \nundersample\n, respectively.\n\n\ndata.imbal.train = rbind(\n  data.frame(x = rnorm(100, mean = 1), class = \nA\n),\n  data.frame(x = rnorm(5000, mean = 2), class = \nB\n)\n)\ntask = makeClassifTask(data = data.imbal.train, target = \nclass\n)\ntask.over = oversample(task, rate = 8)\ntask.under = undersample(task, rate = 1/8)\n\ntable(getTaskTargets(task))\n#\n \n#\n    A    B \n#\n  100 5000\n\ntable(getTaskTargets(task.over))\n#\n \n#\n    A    B \n#\n  800 5000\n\ntable(getTaskTargets(task.under))\n#\n \n#\n   A   B \n#\n 100 625\n\n\n\n\nPlease note that the \nundersampling rate\n has to be between 0 and 1, where 1 means\nno undersampling and 0.5 implies a reduction of the majority class size to 50 percent.\nCorrespondingly, the \noversampling rate\n must be greater or equal to 1,\nwhere 1 means no oversampling and 2 would result in doubling the minority\nclass size.\n\n\nAs a result the \nperformance\n should improve if the model is applied to new data.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, task)\nmod.over = train(lrn, task.over)\nmod.under = train(lrn, task.under)\ndata.imbal.test = rbind(\n  data.frame(x = rnorm(10, mean = 1), class = \nA\n),\n  data.frame(x = rnorm(500, mean = 2), class = \nB\n)\n)\n\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#\n       mmce        ber        auc \n#\n 0.01960784 0.50000000 0.50000000\n\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#\n       mmce        ber        auc \n#\n 0.04509804 0.41500000 0.58500000\n\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#\n       mmce        ber        auc \n#\n 0.05098039 0.41800000 0.70550000\n\n\n\n\nIn this case the \nperformance measure\n has to be considered very carefully.\nAs the \nmisclassification rate\n (\nmmce\n) evaluates the overall\naccuracy of the predictions, the \nbalanced error rate\n (\nber\n) and\n\narea under the ROC Curve\n (\nauc\n) \nmight be more suitable here, as the misclassifications within each class\nare separately taken into account.\n\n\nOver- and undersampling wrappers\n\n\nAlternatively, \nmlr\n also offers the integration of over- and undersampling\nvia a \nwrapper approach\n. This way\nover- and undersampling can be applied to already existing \nlearners\n\nto extend their functionality. \n\n\nThe example given above is repeated once again, but this time with extended\nlearners instead of modified tasks (see \nmakeOversampleWrapper\n\nand \nmakeUndersampleWrapper\n).\nJust like before the \nundersampling rate\n has to be between 0 and 1, while the\n\noversampling rate\n has a lower boundary of 1.\n\n\nlrn.over = makeOversampleWrapper(lrn, osw.rate = 8)\nlrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8)\nmod = train(lrn, task)\nmod.over = train(lrn.over, task)\nmod.under = train(lrn.under, task)\n\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#\n       mmce        ber        auc \n#\n 0.01960784 0.50000000 0.50000000\n\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#\n       mmce        ber        auc \n#\n 0.03333333 0.40900000 0.72020000\n\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#\n       mmce        ber        auc \n#\n 0.04509804 0.41500000 0.71660000\n\n\n\n\nExtensions to oversampling\n\n\nTwo extensions to (simple) oversampling are available in \nmlr\n.\n\n\n1. SMOTE (Synthetic Minority Oversampling Technique)\n\n\nAs the duplicating of the minority class observations can lead to overfitting,\nwithin \nSMOTE\n the \"new cases\" are constructed in a different way. For each\nnew observation, one randomly chosen minority class observation as well as\none of its \nrandomly chosen next neighbours\n are interpolated, so that finally\na new \nartificial observation\n of the minority class is created.\nThe \nsmote\n function in \nmlr\n handles numeric as well as factor features, as\nthe gower distance is used for nearest neighbour calculation. The factor level\nof the new artificial case is sampled from the given levels of the two\ninput observations.\n\n\nAnalogous to oversampling, \nSMOTE preprocessing\n is possible via modification\nof the task.\n\n\ntask.smote = smote(task, rate = 8, nn = 5)\ntable(getTaskTargets(task))\n#\n \n#\n    A    B \n#\n  100 5000\n\ntable(getTaskTargets(task.smote))\n#\n \n#\n    A    B \n#\n  800 5000\n\n\n\n\nAlternatively, a new wrapped learner can be created via \nmakeSMOTEWrapper\n.\n\n\nlrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5)\nmod.smote = train(lrn.smote, task)\nperformance(predict(mod.smote, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#\n       mmce        ber        auc \n#\n 0.04509804 0.41500000 0.71660000\n\n\n\n\nBy default the number of nearest neighbours considered within the algorithm is\nset to 5.\n\n\n2. Overbagging\n\n\nAnother extension of oversampling consists in the combination of sampling with\nthe \nbagging approach\n. For each iteration of the bagging process,\nminority class observations are oversampled with a given rate in \nobw.rate\n.\nThe majority class cases can either all be taken into account for each\niteration (\nobw.maxcl = \"all\"\n) or bootstrapped with replacement to increase\nvariability between training data sets during iterations (\nobw.maxcl = \"boot\"\n).\n\n\nThe construction of the \nOverbagging Wrapper\n works similar\nto \nmakeBaggingWrapper\n.\nFirst an existing \nmlr\n learner has to be passed to \nmakeOverBaggingWrapper\n.\nThe number of iterations or fitted models can be set via \nobw.iters\n.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nresponse\n)\nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)\n\n\n\n\nFor \nbinary classification\n the prediction is based on majority voting to create\na discrete label. Corresponding probabilities are predicted by considering\nthe proportions of all the predicted labels.\nPlease note that the benefit of the sampling process is \nhighly dependent\n\non the specific learner as shown in the following example.\n\n\nFirst, let's take a look at the tree learner with and without overbagging:\n\n\nlrn = setPredictType(lrn, \nprob\n)\nrdesc = makeResampleDesc(\nCV\n, iters = 5)\nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE,\n  measures = list(mmce, ber, auc))\nr1$aggr\n#\n mmce.test.mean  ber.test.mean  auc.test.mean \n#\n     0.01960784     0.50000000     0.50000000\n\nobw.lrn = setPredictType(obw.lrn, \nprob\n)\nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,\n  measures = list(mmce, ber, auc))\nr2$aggr\n#\n mmce.test.mean  ber.test.mean  auc.test.mean \n#\n     0.04470588     0.43611719     0.58535862\n\n\n\n\nNow let's consider a \nrandom forest\n as initial learner:\n\n\nlrn = makeLearner(\nclassif.randomForest\n)\nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)\n\nlrn = setPredictType(lrn, \nprob\n)\nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE,\n  measures = list(mmce, ber, auc))\nr1$aggr\n#\n mmce.test.mean  ber.test.mean  auc.test.mean \n#\n     0.03509804     0.46089748     0.58514212\n\nobw.lrn = setPredictType(obw.lrn, \nprob\n)\nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,\n  measures = list(mmce, ber, auc))\nr2$aggr\n#\n mmce.test.mean  ber.test.mean  auc.test.mean \n#\n     0.04098039     0.45961754     0.54926842\n\n\n\n\nWhile \noverbagging\n slighty improves the performance of the \ndecision tree\n,\nthe auc decreases in the second example when additional overbagging is applied.\nAs the \nrandom forest\n itself is already a strong learner (and a bagged one\nas well), a further bagging step isn't very helpful here and usually won't\nimprove the model.\n\n\nCost-based approaches\n\n\nIn contrast to sampling, \ncost-based approaches\n usually require particular \nlearners, which can deal with different \nclass-dependent costs\n\n(\nCost-Sensitive Classification\n).\n\n\nWeighted classes wrapper\n\n\nAnother approach independent of the underlying classifier is to \nassign the costs as \nclass weights\n, so that each observation receives a weight,\ndepending on the class it belongs to. Similar to the sampling-based approaches,\nthe effect of the minority class observations is thereby increased simply by a\nhigher weight of these instances and vice versa for majority class observations.\n\n\nIn this way every learner which supports weights can be extended through\nthe \nwrapper approach\n.\nIf the learner does not have a direct parameter for class weights,\nbut supports observation weights, the weights depending on the class are\ninternally set in the wrapper.\n\n\nlrn = makeLearner(\nclassif.logreg\n)\nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)\n\n\n\n\nFor binary classification, the single number passed to the classifier corresponds\nto the weight of the positive / majority class, while the negative / minority \nclass receives a weight of 1. So actually, no real costs are used within\nthis approach, but the cost ratio is taken into account.\n\n\nIf the underlying learner already has a parameter for class weighting (e.g.,\n\nclass.weights\n in \n\"classif.ksvm\"\n), the \nwcw.weight\n is basically passed\nto the specific class weighting parameter.\n\n\nlrn = makeLearner(\nclassif.ksvm\n)\nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\ndata.imbal.train = rbind( \n  data.frame(x = rnorm(100, mean = 1), class = \nA\n), \n  data.frame(x = rnorm(5000, mean = 2), class = \nB\n) \n) \ntask = makeClassifTask(data = data.imbal.train, target = \nclass\n) \ntask.over = oversample(task, rate = 8) \ntask.under = undersample(task, rate = 1/8) \n\ntable(getTaskTargets(task)) \n\ntable(getTaskTargets(task.over)) \n\ntable(getTaskTargets(task.under)) \nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n) \nmod = train(lrn, task) \nmod.over = train(lrn, task.over) \nmod.under = train(lrn, task.under) \ndata.imbal.test = rbind( \n  data.frame(x = rnorm(10, mean = 1), class = \nA\n), \n  data.frame(x = rnorm(500, mean = 2), class = \nB\n) \n) \n\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \n\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \n\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \nlrn.over = makeOversampleWrapper(lrn, osw.rate = 8) \nlrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8) \nmod = train(lrn, task) \nmod.over = train(lrn.over, task) \nmod.under = train(lrn.under, task) \n\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \n\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \n\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \ntask.smote = smote(task, rate = 8, nn = 5) \ntable(getTaskTargets(task)) \n\ntable(getTaskTargets(task.smote)) \nlrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5) \nmod.smote = train(lrn.smote, task) \nperformance(predict(mod.smote, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nresponse\n) \nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3) \nlrn = setPredictType(lrn, \nprob\n) \nrdesc = makeResampleDesc(\nCV\n, iters = 5) \nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, \n  measures = list(mmce, ber, auc)) \nr1$aggr \n\nobw.lrn = setPredictType(obw.lrn, \nprob\n) \nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE, \n  measures = list(mmce, ber, auc)) \nr2$aggr \nlrn = makeLearner(\nclassif.randomForest\n) \nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3) \n\nlrn = setPredictType(lrn, \nprob\n) \nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, \n  measures = list(mmce, ber, auc)) \nr1$aggr \n\nobw.lrn = setPredictType(obw.lrn, \nprob\n) \nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE, \n  measures = list(mmce, ber, auc)) \nr2$aggr \nlrn = makeLearner(\nclassif.logreg\n) \nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01) \nlrn = makeLearner(\nclassif.ksvm\n) \nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)", 
            "title": "Imbalanced Classification Problems"
        }, 
        {
            "location": "/over_and_undersampling/index.html#imbalanced-classification-problems", 
            "text": "In case of  binary classification  strongly imbalanced classes\noften lead to unsatisfactory results regarding the prediction of new\nobservations, especially for the small class.\nIn this context  imbalanced classes  simply means that the number of\nobservations of one class (usu. positive or majority class) by far exceeds\nthe number of observations of the other class (usu. negative or minority class).\nThis setting can be observed fairly often in practice and in various disciplines\nlike credit scoring, fraud detection, medical diagnostics or churn management.  Most classification methods work best when the number of observations per\nclass are roughly equal. The problem with  imbalanced classes  is that because\nof the dominance of the majority class classifiers tend to ignore cases of\nthe minority class as noise and therefore predict the majority class far more\noften. In order to lay more weight on the cases of the minority class, there are\nnumerous correction methods which tackle the  imbalanced classification problem .\nThese methods can generally be divided into  cost- and sampling-based approaches .\nBelow all methods supported by  mlr  are introduced.", 
            "title": "Imbalanced Classification Problems"
        }, 
        {
            "location": "/over_and_undersampling/index.html#sampling-based-approaches", 
            "text": "The basic idea of  sampling methods  is to simply adjust the proportion of\nthe classes in order to increase the weight of the minority class observations\nwithin the model.  The  sampling-based approaches  can be divided further into three different categories:    Undersampling methods :\n   Elimination of randomly chosen cases of the majority class to decrease their\n   effect on the classifier. All cases of the minority class are kept.    Oversampling methods :\n   Generation of additional cases (copies, artificial observations) of the minority\n   class to increase their effect on the classifier. All cases of the majority\n   class are kept.    Hybrid methods :\n   Mixture of under- and oversampling strategies.    All these methods directly access the underlying data and \"rearrange\" it.\nIn this way the sampling is done as part of the  preprocesssing  and can therefore\nbe combined with every appropriate classifier.  mlr  currently supports the first two approaches.", 
            "title": "Sampling-based approaches"
        }, 
        {
            "location": "/over_and_undersampling/index.html#simple-over-and-undersampling", 
            "text": "As mentioned above  undersampling  always refers to the majority class, while oversampling  affects the minority class. By the use of  undersampling , randomly\nchosen observations of the majority class are eliminated. Through (simple) oversampling  all observations of the minority class are considered at least\nonce when fitting the model. In addition, exact copies of minority class cases are created\nby random sampling with repetitions.  First, let's take a look at the effect for a classification  task .\nBased on a simulated  ClassifTask  with imbalanced classes two new\ntasks ( task.over ,  task.under ) are created via  mlr  functions oversample  and  undersample , respectively.  data.imbal.train = rbind(\n  data.frame(x = rnorm(100, mean = 1), class =  A ),\n  data.frame(x = rnorm(5000, mean = 2), class =  B )\n)\ntask = makeClassifTask(data = data.imbal.train, target =  class )\ntask.over = oversample(task, rate = 8)\ntask.under = undersample(task, rate = 1/8)\n\ntable(getTaskTargets(task))\n#  \n#     A    B \n#   100 5000\n\ntable(getTaskTargets(task.over))\n#  \n#     A    B \n#   800 5000\n\ntable(getTaskTargets(task.under))\n#  \n#    A   B \n#  100 625  Please note that the  undersampling rate  has to be between 0 and 1, where 1 means\nno undersampling and 0.5 implies a reduction of the majority class size to 50 percent.\nCorrespondingly, the  oversampling rate  must be greater or equal to 1,\nwhere 1 means no oversampling and 2 would result in doubling the minority\nclass size.  As a result the  performance  should improve if the model is applied to new data.  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, task)\nmod.over = train(lrn, task.over)\nmod.under = train(lrn, task.under)\ndata.imbal.test = rbind(\n  data.frame(x = rnorm(10, mean = 1), class =  A ),\n  data.frame(x = rnorm(500, mean = 2), class =  B )\n)\n\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#        mmce        ber        auc \n#  0.01960784 0.50000000 0.50000000\n\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#        mmce        ber        auc \n#  0.04509804 0.41500000 0.58500000\n\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#        mmce        ber        auc \n#  0.05098039 0.41800000 0.70550000  In this case the  performance measure  has to be considered very carefully.\nAs the  misclassification rate  ( mmce ) evaluates the overall\naccuracy of the predictions, the  balanced error rate  ( ber ) and area under the ROC Curve  ( auc ) \nmight be more suitable here, as the misclassifications within each class\nare separately taken into account.", 
            "title": "(Simple) over- and undersampling"
        }, 
        {
            "location": "/over_and_undersampling/index.html#over-and-undersampling-wrappers", 
            "text": "Alternatively,  mlr  also offers the integration of over- and undersampling\nvia a  wrapper approach . This way\nover- and undersampling can be applied to already existing  learners \nto extend their functionality.   The example given above is repeated once again, but this time with extended\nlearners instead of modified tasks (see  makeOversampleWrapper \nand  makeUndersampleWrapper ).\nJust like before the  undersampling rate  has to be between 0 and 1, while the oversampling rate  has a lower boundary of 1.  lrn.over = makeOversampleWrapper(lrn, osw.rate = 8)\nlrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8)\nmod = train(lrn, task)\nmod.over = train(lrn.over, task)\nmod.under = train(lrn.under, task)\n\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#        mmce        ber        auc \n#  0.01960784 0.50000000 0.50000000\n\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#        mmce        ber        auc \n#  0.03333333 0.40900000 0.72020000\n\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#        mmce        ber        auc \n#  0.04509804 0.41500000 0.71660000", 
            "title": "Over- and undersampling wrappers"
        }, 
        {
            "location": "/over_and_undersampling/index.html#extensions-to-oversampling", 
            "text": "Two extensions to (simple) oversampling are available in  mlr .", 
            "title": "Extensions to oversampling"
        }, 
        {
            "location": "/over_and_undersampling/index.html#1-smote-synthetic-minority-oversampling-technique", 
            "text": "As the duplicating of the minority class observations can lead to overfitting,\nwithin  SMOTE  the \"new cases\" are constructed in a different way. For each\nnew observation, one randomly chosen minority class observation as well as\none of its  randomly chosen next neighbours  are interpolated, so that finally\na new  artificial observation  of the minority class is created.\nThe  smote  function in  mlr  handles numeric as well as factor features, as\nthe gower distance is used for nearest neighbour calculation. The factor level\nof the new artificial case is sampled from the given levels of the two\ninput observations.  Analogous to oversampling,  SMOTE preprocessing  is possible via modification\nof the task.  task.smote = smote(task, rate = 8, nn = 5)\ntable(getTaskTargets(task))\n#  \n#     A    B \n#   100 5000\n\ntable(getTaskTargets(task.smote))\n#  \n#     A    B \n#   800 5000  Alternatively, a new wrapped learner can be created via  makeSMOTEWrapper .  lrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5)\nmod.smote = train(lrn.smote, task)\nperformance(predict(mod.smote, newdata = data.imbal.test), measures = list(mmce, ber, auc))\n#        mmce        ber        auc \n#  0.04509804 0.41500000 0.71660000  By default the number of nearest neighbours considered within the algorithm is\nset to 5.", 
            "title": "1. SMOTE (Synthetic Minority Oversampling Technique)"
        }, 
        {
            "location": "/over_and_undersampling/index.html#2-overbagging", 
            "text": "Another extension of oversampling consists in the combination of sampling with\nthe  bagging approach . For each iteration of the bagging process,\nminority class observations are oversampled with a given rate in  obw.rate .\nThe majority class cases can either all be taken into account for each\niteration ( obw.maxcl = \"all\" ) or bootstrapped with replacement to increase\nvariability between training data sets during iterations ( obw.maxcl = \"boot\" ).  The construction of the  Overbagging Wrapper  works similar\nto  makeBaggingWrapper .\nFirst an existing  mlr  learner has to be passed to  makeOverBaggingWrapper .\nThe number of iterations or fitted models can be set via  obw.iters .  lrn = makeLearner( classif.rpart , predict.type =  response )\nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)  For  binary classification  the prediction is based on majority voting to create\na discrete label. Corresponding probabilities are predicted by considering\nthe proportions of all the predicted labels.\nPlease note that the benefit of the sampling process is  highly dependent \non the specific learner as shown in the following example.  First, let's take a look at the tree learner with and without overbagging:  lrn = setPredictType(lrn,  prob )\nrdesc = makeResampleDesc( CV , iters = 5)\nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE,\n  measures = list(mmce, ber, auc))\nr1$aggr\n#  mmce.test.mean  ber.test.mean  auc.test.mean \n#      0.01960784     0.50000000     0.50000000\n\nobw.lrn = setPredictType(obw.lrn,  prob )\nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,\n  measures = list(mmce, ber, auc))\nr2$aggr\n#  mmce.test.mean  ber.test.mean  auc.test.mean \n#      0.04470588     0.43611719     0.58535862  Now let's consider a  random forest  as initial learner:  lrn = makeLearner( classif.randomForest )\nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)\n\nlrn = setPredictType(lrn,  prob )\nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE,\n  measures = list(mmce, ber, auc))\nr1$aggr\n#  mmce.test.mean  ber.test.mean  auc.test.mean \n#      0.03509804     0.46089748     0.58514212\n\nobw.lrn = setPredictType(obw.lrn,  prob )\nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,\n  measures = list(mmce, ber, auc))\nr2$aggr\n#  mmce.test.mean  ber.test.mean  auc.test.mean \n#      0.04098039     0.45961754     0.54926842  While  overbagging  slighty improves the performance of the  decision tree ,\nthe auc decreases in the second example when additional overbagging is applied.\nAs the  random forest  itself is already a strong learner (and a bagged one\nas well), a further bagging step isn't very helpful here and usually won't\nimprove the model.", 
            "title": "2. Overbagging"
        }, 
        {
            "location": "/over_and_undersampling/index.html#cost-based-approaches", 
            "text": "In contrast to sampling,  cost-based approaches  usually require particular \nlearners, which can deal with different  class-dependent costs \n( Cost-Sensitive Classification ).", 
            "title": "Cost-based approaches"
        }, 
        {
            "location": "/over_and_undersampling/index.html#weighted-classes-wrapper", 
            "text": "Another approach independent of the underlying classifier is to \nassign the costs as  class weights , so that each observation receives a weight,\ndepending on the class it belongs to. Similar to the sampling-based approaches,\nthe effect of the minority class observations is thereby increased simply by a\nhigher weight of these instances and vice versa for majority class observations.  In this way every learner which supports weights can be extended through\nthe  wrapper approach .\nIf the learner does not have a direct parameter for class weights,\nbut supports observation weights, the weights depending on the class are\ninternally set in the wrapper.  lrn = makeLearner( classif.logreg )\nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)  For binary classification, the single number passed to the classifier corresponds\nto the weight of the positive / majority class, while the negative / minority \nclass receives a weight of 1. So actually, no real costs are used within\nthis approach, but the cost ratio is taken into account.  If the underlying learner already has a parameter for class weighting (e.g., class.weights  in  \"classif.ksvm\" ), the  wcw.weight  is basically passed\nto the specific class weighting parameter.  lrn = makeLearner( classif.ksvm )\nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)", 
            "title": "Weighted classes wrapper"
        }, 
        {
            "location": "/over_and_undersampling/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  data.imbal.train = rbind( \n  data.frame(x = rnorm(100, mean = 1), class =  A ), \n  data.frame(x = rnorm(5000, mean = 2), class =  B ) \n) \ntask = makeClassifTask(data = data.imbal.train, target =  class ) \ntask.over = oversample(task, rate = 8) \ntask.under = undersample(task, rate = 1/8) \n\ntable(getTaskTargets(task)) \n\ntable(getTaskTargets(task.over)) \n\ntable(getTaskTargets(task.under)) \nlrn = makeLearner( classif.rpart , predict.type =  prob ) \nmod = train(lrn, task) \nmod.over = train(lrn, task.over) \nmod.under = train(lrn, task.under) \ndata.imbal.test = rbind( \n  data.frame(x = rnorm(10, mean = 1), class =  A ), \n  data.frame(x = rnorm(500, mean = 2), class =  B ) \n) \n\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \n\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \n\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \nlrn.over = makeOversampleWrapper(lrn, osw.rate = 8) \nlrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8) \nmod = train(lrn, task) \nmod.over = train(lrn.over, task) \nmod.under = train(lrn.under, task) \n\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \n\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \n\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \ntask.smote = smote(task, rate = 8, nn = 5) \ntable(getTaskTargets(task)) \n\ntable(getTaskTargets(task.smote)) \nlrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5) \nmod.smote = train(lrn.smote, task) \nperformance(predict(mod.smote, newdata = data.imbal.test), measures = list(mmce, ber, auc)) \nlrn = makeLearner( classif.rpart , predict.type =  response ) \nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3) \nlrn = setPredictType(lrn,  prob ) \nrdesc = makeResampleDesc( CV , iters = 5) \nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, \n  measures = list(mmce, ber, auc)) \nr1$aggr \n\nobw.lrn = setPredictType(obw.lrn,  prob ) \nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE, \n  measures = list(mmce, ber, auc)) \nr2$aggr \nlrn = makeLearner( classif.randomForest ) \nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3) \n\nlrn = setPredictType(lrn,  prob ) \nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, \n  measures = list(mmce, ber, auc)) \nr1$aggr \n\nobw.lrn = setPredictType(obw.lrn,  prob ) \nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE, \n  measures = list(mmce, ber, auc)) \nr2$aggr \nlrn = makeLearner( classif.logreg ) \nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01) \nlrn = makeLearner( classif.ksvm ) \nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/roc_analysis/index.html", 
            "text": "ROC Analysis and Performance Curves\n\n\nFor binary scoring classifiers a \nthreshold\n (or \ncutoff\n) value\ncontrols how predicted posterior probabilities are converted into class labels.\nROC curves and other performance plots serve to visualize and analyse the relationship between\none or two performance measures and the threshold.\n\n\nThis page is mainly devoted to \nreceiver operating characteristic\n (ROC) curves that\nplot the \ntrue positive rate\n (sensitivity) on the vertical axis against the \nfalse positive rate\n\n(1 - specificity, fall-out) on the horizontal axis for all possible threshold values.\nCreating other performance plots like \nlift charts\n or \nprecision/recall graphs\n works\nanalogously and is shown briefly.\n\n\nIn addition to performance visualization ROC curves are helpful in\n\n\n\n\ndetermining an optimal decision threshold for given class prior probabilities and\n  misclassification costs (for alternatives see also the pages about\n  \ncost-sensitive classification\n and\n  \nimbalanced classification problems\n in this tutorial),\n\n\nidentifying regions where one classifier outperforms another and building suitable multi-classifier\n  systems,\n\n\nobtaining calibrated estimates of the posterior probabilities.\n\n\n\n\nFor more information see the tutorials and introductory papers by\n\nFawcett (2004)\n,\n\nFawcett (2006)\n\nas well as \nFlach (ICML 2004)\n.\n\n\nIn many applications as, e.g., diagnostic tests or spam detection, there is uncertainty\nabout the class priors or the misclassification costs at the time of prediction, for example\nbecause it's hard to quantify the costs or because costs and class priors vary over time.\nUnder these circumstances the classifier is expected to work well for a whole range of\ndecision thresholds and the area under the ROC curve (AUC) provides a scalar performance\nmeasure for comparing and selecting classifiers.\n\nmlr\n provides the AUC for binary classification (\nauc\n) and also several\ngeneralizations of the AUC to the\nmulti-class case (e.g., \nmulticlass.au1p\n, \nmulticlass.au1u\n based\non \nFerri et al. (2009)\n).\n\n\nmlr\n offers three ways to plot ROC and other performance curves.\n\n\n\n\nFunction \nplotROCCurves\n can, based on the output of \ngenerateThreshVsPerfData\n,\n   plot performance curves for any pair of \nperformance measures\n available in\n   \nmlr\n.\n\n\nmlr\n offers an interface to package \nROCR\n through function \nasROCRPrediction\n.\n\n\nmlr\n's function \nplotViperCharts\n provides an interface to\n   \nViperCharts\n.\n\n\n\n\nWith \nmlr\n version 2.8 functions \ngenerateROCRCurvesData\n, \nplotROCRCurves\n, and \nplotROCRCurvesGGVIS\n\nwere deprecated.\n\n\nBelow are some examples that demonstrate the three possible ways.\nNote that you can only use \nlearners\n that are capable of predicting probabilities.\nHave a look at the \nlearner table in the Appendix\n\nor run \nlistLearners(\"classif\", properties = c(\"twoclass\", \"prob\"))\n to get a list of all\nlearners that support this.\n\n\nPerformance plots with plotROCCurves\n\n\nAs you might recall \ngenerateThreshVsPerfData\n calculates one or several performance measures\nfor a sequence of decision thresholds from 0 to 1.\nIt provides S3 methods for objects of class \nPrediction\n, \nResampleResult\n\nand \nBenchmarkResult\n (resulting from \npredict\n, \nresample\n\nor \nbenchmark\n). \nplotROCCurves\n plots the result of \ngenerateThreshVsPerfData\n using \nggplot2\n.\n\n\nExample 1: Single predictions\n\n\nWe consider the \nSonar\n data set from package \nmlbench\n, which poses a\nbinary classification problem (\nsonar.task\n) and apply \nlinear discriminant analysis\n.\n\n\nn = getTaskSize(sonar.task)\ntrain.set = sample(n, size = round(2/3 * n))\ntest.set = setdiff(seq_len(n), train.set)\n\nlrn1 = makeLearner(\nclassif.lda\n, predict.type = \nprob\n)\nmod1 = train(lrn1, sonar.task, subset = train.set)\npred1 = predict(mod1, task = sonar.task, subset = test.set)\n\n\n\n\nSince we want to plot ROC curves we calculate the false and true positive rates (\nfpr\n\nand \ntpr\n).\nAdditionally, we also compute error rates (\nmmce\n).\n\n\ndf = generateThreshVsPerfData(pred1, measures = list(fpr, tpr, mmce))\n\n\n\n\ngenerateThreshVsPerfData\n returns an object of class \nThreshVsPerfData\n\nwhich contains the performance values in the \n$data\n element.\n\n\nPer default, \nplotROCCurves\n plots the performance values of the first two measures passed\nto \ngenerateThreshVsPerfData\n. The first is shown on the x-axis, the second on the y-axis.\nMoreover, a diagonal line that represents the performance of a random classifier is added.\nYou can remove the diagonal by setting \ndiagonal = FALSE\n.\n\n\nplotROCCurves(df)\n\n\n\n\n\n\nThe corresponding area under curve (\nauc\n) can be calculated as usual by calling\n\nperformance\n.\n\n\nperformance(pred1, auc)\n#\n      auc \n#\n 0.847973\n\n\n\n\nplotROCCurves\n always requires a pair of performance measures that are plotted against\neach other.\nIf you want to plot individual measures versus the decision threshold you can use function\n\nplotThreshVsPerf\n.\n\n\nplotThreshVsPerf(df)\n\n\n\n\n\n\nAdditional to \nlinear discriminant analysis\n we try a support vector machine\nwith RBF kernel (\nksvm\n).\n\n\nlrn2 = makeLearner(\nclassif.ksvm\n, predict.type = \nprob\n)\nmod2 = train(lrn2, sonar.task, subset = train.set)\npred2 = predict(mod2, task = sonar.task, subset = test.set)\n\n\n\n\nIn order to compare the performance of the two learners you might want to display the two\ncorresponding ROC curves in one plot.\nFor this purpose just pass a named \nlist\n of \nPrediction\ns to \ngenerateThreshVsPerfData\n.\n\n\ndf = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(fpr, tpr))\nplotROCCurves(df)\n\n\n\n\n\n\nIt's clear from the plot above that \nksvm\n has a slightly higher AUC than\n\nlda\n.\n\n\nperformance(pred2, auc)\n#\n       auc \n#\n 0.9214527\n\n\n\n\nBased on the \n$data\n member of \ndf\n you can easily generate custom plots.\nBelow the curves for the two learners are superposed.\n\n\nqplot(x = fpr, y = tpr, color = learner, data = df$data, geom = \npath\n)\n\n\n\n\n\n\nIt is easily possible to generate other performance plots by passing the appropriate\nperformance measures to \ngenerateThreshVsPerfData\n and \nplotROCCurves\n.\nBelow, we generate a \nprecision/recall graph\n (precision = positive predictive value = ppv,\nrecall = tpr) and a \nsensitivity/specificity plot\n (sensitivity = tpr, specificity = tnr).\n\n\ndf = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(ppv, tpr, tnr))\n\n## Precision/recall graph\nplotROCCurves(df, measures = list(tpr, ppv), diagonal = FALSE)\n\n## Sensitivity/specificity plot\nplotROCCurves(df, measures = list(tnr, tpr), diagonal = FALSE)\n\n\n\n\n\n\nExample 2: Benchmark experiment\n\n\nThe analysis in the example above can be improved a little.\nInstead of writing individual code for training/prediction of each learner, which can become\ntedious very quickly, we can use function \nbenchmark\n (see also\n\nBenchmark Experiments\n) and, ideally, the support vector machine\nshould have been \ntuned\n.\n\n\nWe again consider the \nSonar\n data set and apply \nlda\n\nas well as \nksvm\n.\nWe first generate a \ntuning wrapper\n for \nksvm\n.\nThe cost parameter is tuned on a (for demonstration purposes small) parameter grid.\nWe assume that we are interested in a good performance over the complete threshold range\nand therefore tune with regard to the \nauc\n.\nThe error rate (\nmmce\n) for a threshold value of 0.5 is reported as well.\n\n\n## Tune wrapper for ksvm\nrdesc.inner = makeResampleDesc(\nHoldout\n)\nms = list(auc, mmce)\nps = makeParamSet(\n  makeDiscreteParam(\nC\n, 2^(-1:1))\n)\nctrl = makeTuneControlGrid()\nlrn2 = makeTuneWrapper(lrn2, rdesc.inner, ms, ps, ctrl, show.info = FALSE)\n\n\n\n\nBelow the actual benchmark experiment is conducted.\nAs resampling strategy we use 5-fold cross-validation and again calculate the \nauc\n\nas well as the error rate (for a threshold/cutoff value of 0.5).\n\n\n## Benchmark experiment\nlrns = list(lrn1, lrn2)\nrdesc.outer = makeResampleDesc(\nCV\n, iters = 5)\n\nbmr = benchmark(lrns, tasks = sonar.task, resampling = rdesc.outer, measures = ms, show.info = FALSE)\nbmr\n#\n         task.id         learner.id auc.test.mean mmce.test.mean\n#\n 1 Sonar_example        classif.lda     0.7835442      0.2592334\n#\n 2 Sonar_example classif.ksvm.tuned     0.9454418      0.1390244\n\n\n\n\nCalling \ngenerateThreshVsPerfData\n and \nplotROCCurves\n on the \nbenchmark result\n\nproduces a plot with ROC curves for all learners in the experiment.\n\n\ndf = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce))\nplotROCCurves(df)\n\n\n\n\n\n\nPer default, \ngenerateThreshVsPerfData\n calculates aggregated performances according to the\nchosen resampling strategy (5-fold cross-validation) and aggregation scheme\n(\ntest.mean\n) for each threshold in the sequence.\nThis way we get \nthreshold-averaged\n ROC curves.\n\n\nIf you want to plot the individual ROC curves for each resample iteration set \naggregate = FALSE\n.\n\n\ndf = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce), aggregate = FALSE)\nplotROCCurves(df)\n\n\n\n\n\n\nThe same applies for \nplotThreshVsPerf\n.\n\n\nplotThreshVsPerf(df) +\n  theme(strip.text.x = element_text(size = 7))\n\n\n\n\n\n\nAn alternative to averaging is to just merge the 5 test folds and draw a single ROC curve.\nMerging can be achieved by manually changing the \nclass\n attribute of\nthe prediction objects from \nResamplePrediction\n to \nPrediction\n.\n\n\nBelow, the predictions are extracted from the \nBenchmarkResult\n via function \ngetBMRPredictions\n,\nthe \nclass\n is changed and the ROC curves are created.\n\n\nAveraging methods are normally preferred\n(cp. \nFawcett, 2006\n),\nas they permit to assess the variability, which is needed to properly compare classifier\nperformance.\n\n\n## Extract predictions\npreds = getBMRPredictions(bmr, drop = TRUE)\n\n## Change the class attribute\npreds2 = lapply(preds, function(x) {class(x) = \nPrediction\n; return(x)})\n\n## Draw ROC curves\ndf = generateThreshVsPerfData(preds2, measures = list(fpr, tpr, mmce))\nplotROCCurves(df)\n\n\n\n\n\n\nAgain, you can easily create other standard evaluation plots by passing the appropriate\nperformance measures to \ngenerateThreshVsPerfData\n and \nplotROCCurves\n.\n\n\nPerformance plots with asROCRPrediction\n\n\nDrawing performance plots with package \nROCR\n works through three basic commands:\n\n\n\n\nROCR::prediction\n: Create a \nROCR\n \nprediction\n object.\n\n\nROCR::performance\n: Calculate one or more performance measures for the\n   given prediction object.\n\n\nROCR::plot\n: Generate the performance plot.\n\n\n\n\nmlr\n's function \nasROCRPrediction\n converts an \nmlr\n \nPrediction\n object to\na \nROCR\n \nprediction\n object, so you can easily generate\nperformance plots by doing steps 2. and 3. yourself.\n\nROCR\n's \nplot\n method has some nice features which are not (yet)\navailable in \nplotROCCurves\n, for example plotting the convex hull of the ROC curves.\nSome examples are shown below.\n\n\nExample 1: Single predictions (continued)\n\n\nWe go back to out first example where we trained and predicted \nlda\n on the\n\nsonar classification task\n.\n\n\nn = getTaskSize(sonar.task)\ntrain.set = sample(n, size = round(2/3 * n))\ntest.set = setdiff(seq_len(n), train.set)\n\n## Train and predict linear discriminant analysis\nlrn1 = makeLearner(\nclassif.lda\n, predict.type = \nprob\n)\nmod1 = train(lrn1, sonar.task, subset = train.set)\npred1 = predict(mod1, task = sonar.task, subset = test.set)\n\n\n\n\nBelow we use \nasROCRPrediction\n to convert the lda prediction, let \nROCR\n calculate the\ntrue and false positive rate and plot the ROC curve.\n\n\n## Convert prediction\nROCRpred1 = asROCRPrediction(pred1)\n\n## Calculate true and false positive rate\nROCRperf1 = ROCR::performance(ROCRpred1, \ntpr\n, \nfpr\n)\n\n## Draw ROC curve\nROCR::plot(ROCRperf1)\n\n\n\n\n\n\nBelow is the same ROC curve, but we make use of some more graphical parameters:\nThe ROC curve is color-coded by the threshold and selected threshold values are printed on\nthe curve. Additionally, the convex hull (black broken line) of the ROC curve is drawn.\n\n\n## Draw ROC curve\nROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2)\n\n## Draw convex hull of ROC curve\nch = ROCR::performance(ROCRpred1, \nrch\n)\nROCR::plot(ch, add = TRUE, lty = 2)\n\n\n\n\n\n\nExample 2: Benchmark experiments (continued)\n\n\nWe again consider the benchmark experiment conducted earlier.\nWe first extract the predictions by \ngetBMRPredictions\n and then convert them via function\n\nasROCRPrediction\n.\n\n\n## Extract predictions\npreds = getBMRPredictions(bmr, drop = TRUE)\n\n## Convert predictions\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate true and false positive rate\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \ntpr\n, \nfpr\n))\n\n\n\n\nWe draw the vertically averaged ROC curves (solid lines) as well as the ROC curves for\nthe individual resampling iterations (broken lines).\nMoreover, standard error bars are plotted for selected true positive rates (0.1, 0.2, ..., 0.9).\nSee \nROCR\n's \nplot\n function for details.\n\n\n## lda average ROC curve\nplot(ROCRperfs[[1]], col = \nblue\n, avg = \nvertical\n, spread.estimate = \nstderror\n,\n  show.spread.at = seq(0.1, 0.8, 0.1), plotCI.col = \nblue\n, plotCI.lwd = 2, lwd = 2)\n## lda individual ROC curves\nplot(ROCRperfs[[1]], col = \nblue\n, lty = 2, lwd = 0.25, add = TRUE)\n\n## ksvm average ROC curve\nplot(ROCRperfs[[2]], col = \nred\n, avg = \nvertical\n, spread.estimate = \nstderror\n,\n  show.spread.at = seq(0.1, 0.6, 0.1), plotCI.col = \nred\n, plotCI.lwd = 2, lwd = 2, add = TRUE)\n## ksvm individual ROC curves\nplot(ROCRperfs[[2]], col = \nred\n, lty = 2, lwd = 0.25, add = TRUE)\n\nlegend(\nbottomright\n, legend = getBMRLearnerIds(bmr), lty = 1, lwd = 2, col = c(\nblue\n, \nred\n))\n\n\n\n\n\n\nIn order to create other evaluation plots like \nprecision/recall graphs\n you just have to\nchange the performance measures when calling \nROCR::performance\n.\n(Note that you have to use the measures provided by \nROCR\n listed \nhere\n\nand not \nmlr\n's performance measures.)\n\n\n## Extract and convert predictions\npreds = getBMRPredictions(bmr, drop = TRUE)\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate precision and recall\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \nprec\n, \nrec\n))\n\n## Draw performance plot\nplot(ROCRperfs[[1]], col = \nblue\n, avg = \nthreshold\n)\nplot(ROCRperfs[[2]], col = \nred\n, avg = \nthreshold\n, add = TRUE)\nlegend(\nbottomleft\n, legend = getBMRLearnerIds(bmr), lty = 1, col = c(\nblue\n, \nred\n))\n\n\n\n\n\n\nIf you want to plot a performance measure versus the threshold, specify only one measure when\ncalling \nROCR::performance\n.\nBelow the average accuracy over the 5 cross-validation iterations is plotted against the\nthreshold. Moreover, boxplots for certain threshold values (0.1, 0.2, ..., 0.9) are drawn.\n\n\n## Extract and convert predictions\npreds = getBMRPredictions(bmr, drop = TRUE)\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate accuracy\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \nacc\n))\n\n## Plot accuracy versus threshold\nplot(ROCRperfs[[1]], avg = \nvertical\n, spread.estimate = \nboxplot\n, lwd = 2, col = \nblue\n,\n  show.spread.at = seq(0.1, 0.9, 0.1), ylim = c(0,1), xlab = \nThreshold\n)\n\n\n\n\n\n\nViper charts\n\n\nmlr\n also supports \nViperCharts\n for plotting ROC and other performance\ncurves. Like \ngenerateThreshVsPerfData\n it has S3 methods for objects of class \nPrediction\n,\n\nResampleResult\n and \nBenchmarkResult\n.\nBelow plots for the benchmark experiment (Example 2) are generated.\n\n\nz = plotViperCharts(bmr, chart = \nrocc\n, browse = FALSE)\n\n\n\n\nYou can see the plot created this way \nhere\n.\nNote that besides ROC curves you get several other plots like lift charts or cost curves.\nFor details, see \nplotViperCharts\n.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nn = getTaskSize(sonar.task) \ntrain.set = sample(n, size = round(2/3 * n)) \ntest.set = setdiff(seq_len(n), train.set) \n\nlrn1 = makeLearner(\nclassif.lda\n, predict.type = \nprob\n) \nmod1 = train(lrn1, sonar.task, subset = train.set) \npred1 = predict(mod1, task = sonar.task, subset = test.set) \ndf = generateThreshVsPerfData(pred1, measures = list(fpr, tpr, mmce)) \nplotROCCurves(df) \nperformance(pred1, auc) \nplotThreshVsPerf(df) \nlrn2 = makeLearner(\nclassif.ksvm\n, predict.type = \nprob\n) \nmod2 = train(lrn2, sonar.task, subset = train.set) \npred2 = predict(mod2, task = sonar.task, subset = test.set) \ndf = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(fpr, tpr)) \nplotROCCurves(df) \nperformance(pred2, auc) \nqplot(x = fpr, y = tpr, color = learner, data = df$data, geom = \npath\n) \ndf = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(ppv, tpr, tnr)) \n\n## Precision/recall graph \nplotROCCurves(df, measures = list(tpr, ppv), diagonal = FALSE) \n\n## Sensitivity/specificity plot \nplotROCCurves(df, measures = list(tnr, tpr), diagonal = FALSE) \n## Tune wrapper for ksvm \nrdesc.inner = makeResampleDesc(\nHoldout\n) \nms = list(auc, mmce) \nps = makeParamSet( \n  makeDiscreteParam(\nC\n, 2^(-1:1)) \n) \nctrl = makeTuneControlGrid() \nlrn2 = makeTuneWrapper(lrn2, rdesc.inner, ms, ps, ctrl, show.info = FALSE) \n## Benchmark experiment \nlrns = list(lrn1, lrn2) \nrdesc.outer = makeResampleDesc(\nCV\n, iters = 5) \n\nbmr = benchmark(lrns, tasks = sonar.task, resampling = rdesc.outer, measures = ms, show.info = FALSE) \nbmr \ndf = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce)) \nplotROCCurves(df) \ndf = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce), aggregate = FALSE) \nplotROCCurves(df) \nplotThreshVsPerf(df) + \n  theme(strip.text.x = element_text(size = 7)) \n## Extract predictions \npreds = getBMRPredictions(bmr, drop = TRUE) \n\n## Change the class attribute \npreds2 = lapply(preds, function(x) {class(x) = \nPrediction\n; return(x)}) \n\n## Draw ROC curves \ndf = generateThreshVsPerfData(preds2, measures = list(fpr, tpr, mmce)) \nplotROCCurves(df) \nn = getTaskSize(sonar.task) \ntrain.set = sample(n, size = round(2/3 * n)) \ntest.set = setdiff(seq_len(n), train.set) \n\n## Train and predict linear discriminant analysis \nlrn1 = makeLearner(\nclassif.lda\n, predict.type = \nprob\n) \nmod1 = train(lrn1, sonar.task, subset = train.set) \npred1 = predict(mod1, task = sonar.task, subset = test.set) \n## Convert prediction \nROCRpred1 = asROCRPrediction(pred1) \n\n## Calculate true and false positive rate \nROCRperf1 = ROCR::performance(ROCRpred1, \ntpr\n, \nfpr\n) \n\n## Draw ROC curve \nROCR::plot(ROCRperf1) \n## Draw ROC curve \nROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2) \n\n## Draw convex hull of ROC curve \nch = ROCR::performance(ROCRpred1, \nrch\n) \nROCR::plot(ch, add = TRUE, lty = 2) \n## Extract predictions \npreds = getBMRPredictions(bmr, drop = TRUE) \n\n## Convert predictions \nROCRpreds = lapply(preds, asROCRPrediction) \n\n## Calculate true and false positive rate \nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \ntpr\n, \nfpr\n)) \n## lda average ROC curve \nplot(ROCRperfs[[1]], col = \nblue\n, avg = \nvertical\n, spread.estimate = \nstderror\n, \n  show.spread.at = seq(0.1, 0.8, 0.1), plotCI.col = \nblue\n, plotCI.lwd = 2, lwd = 2) \n## lda individual ROC curves \nplot(ROCRperfs[[1]], col = \nblue\n, lty = 2, lwd = 0.25, add = TRUE) \n\n## ksvm average ROC curve \nplot(ROCRperfs[[2]], col = \nred\n, avg = \nvertical\n, spread.estimate = \nstderror\n, \n  show.spread.at = seq(0.1, 0.6, 0.1), plotCI.col = \nred\n, plotCI.lwd = 2, lwd = 2, add = TRUE) \n## ksvm individual ROC curves \nplot(ROCRperfs[[2]], col = \nred\n, lty = 2, lwd = 0.25, add = TRUE) \n\nlegend(\nbottomright\n, legend = getBMRLearnerIds(bmr), lty = 1, lwd = 2, col = c(\nblue\n, \nred\n)) \n## Extract and convert predictions \npreds = getBMRPredictions(bmr, drop = TRUE) \nROCRpreds = lapply(preds, asROCRPrediction) \n\n## Calculate precision and recall \nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \nprec\n, \nrec\n)) \n\n## Draw performance plot \nplot(ROCRperfs[[1]], col = \nblue\n, avg = \nthreshold\n) \nplot(ROCRperfs[[2]], col = \nred\n, avg = \nthreshold\n, add = TRUE) \nlegend(\nbottomleft\n, legend = getBMRLearnerIds(bmr), lty = 1, col = c(\nblue\n, \nred\n)) \n## Extract and convert predictions \npreds = getBMRPredictions(bmr, drop = TRUE) \nROCRpreds = lapply(preds, asROCRPrediction) \n\n## Calculate accuracy \nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \nacc\n)) \n\n## Plot accuracy versus threshold \nplot(ROCRperfs[[1]], avg = \nvertical\n, spread.estimate = \nboxplot\n, lwd = 2, col = \nblue\n, \n  show.spread.at = seq(0.1, 0.9, 0.1), ylim = c(0,1), xlab = \nThreshold\n) \nz = plotViperCharts(bmr, chart = \nrocc\n, browse = FALSE)", 
            "title": "ROC Analysis"
        }, 
        {
            "location": "/roc_analysis/index.html#roc-analysis-and-performance-curves", 
            "text": "For binary scoring classifiers a  threshold  (or  cutoff ) value\ncontrols how predicted posterior probabilities are converted into class labels.\nROC curves and other performance plots serve to visualize and analyse the relationship between\none or two performance measures and the threshold.  This page is mainly devoted to  receiver operating characteristic  (ROC) curves that\nplot the  true positive rate  (sensitivity) on the vertical axis against the  false positive rate \n(1 - specificity, fall-out) on the horizontal axis for all possible threshold values.\nCreating other performance plots like  lift charts  or  precision/recall graphs  works\nanalogously and is shown briefly.  In addition to performance visualization ROC curves are helpful in   determining an optimal decision threshold for given class prior probabilities and\n  misclassification costs (for alternatives see also the pages about\n   cost-sensitive classification  and\n   imbalanced classification problems  in this tutorial),  identifying regions where one classifier outperforms another and building suitable multi-classifier\n  systems,  obtaining calibrated estimates of the posterior probabilities.   For more information see the tutorials and introductory papers by Fawcett (2004) , Fawcett (2006) \nas well as  Flach (ICML 2004) .  In many applications as, e.g., diagnostic tests or spam detection, there is uncertainty\nabout the class priors or the misclassification costs at the time of prediction, for example\nbecause it's hard to quantify the costs or because costs and class priors vary over time.\nUnder these circumstances the classifier is expected to work well for a whole range of\ndecision thresholds and the area under the ROC curve (AUC) provides a scalar performance\nmeasure for comparing and selecting classifiers. mlr  provides the AUC for binary classification ( auc ) and also several\ngeneralizations of the AUC to the\nmulti-class case (e.g.,  multiclass.au1p ,  multiclass.au1u  based\non  Ferri et al. (2009) ).  mlr  offers three ways to plot ROC and other performance curves.   Function  plotROCCurves  can, based on the output of  generateThreshVsPerfData ,\n   plot performance curves for any pair of  performance measures  available in\n    mlr .  mlr  offers an interface to package  ROCR  through function  asROCRPrediction .  mlr 's function  plotViperCharts  provides an interface to\n    ViperCharts .   With  mlr  version 2.8 functions  generateROCRCurvesData ,  plotROCRCurves , and  plotROCRCurvesGGVIS \nwere deprecated.  Below are some examples that demonstrate the three possible ways.\nNote that you can only use  learners  that are capable of predicting probabilities.\nHave a look at the  learner table in the Appendix \nor run  listLearners(\"classif\", properties = c(\"twoclass\", \"prob\"))  to get a list of all\nlearners that support this.", 
            "title": "ROC Analysis and Performance Curves"
        }, 
        {
            "location": "/roc_analysis/index.html#performance-plots-with-plotroccurves", 
            "text": "As you might recall  generateThreshVsPerfData  calculates one or several performance measures\nfor a sequence of decision thresholds from 0 to 1.\nIt provides S3 methods for objects of class  Prediction ,  ResampleResult \nand  BenchmarkResult  (resulting from  predict ,  resample \nor  benchmark ).  plotROCCurves  plots the result of  generateThreshVsPerfData  using  ggplot2 .", 
            "title": "Performance plots with plotROCCurves"
        }, 
        {
            "location": "/roc_analysis/index.html#example-1-single-predictions", 
            "text": "We consider the  Sonar  data set from package  mlbench , which poses a\nbinary classification problem ( sonar.task ) and apply  linear discriminant analysis .  n = getTaskSize(sonar.task)\ntrain.set = sample(n, size = round(2/3 * n))\ntest.set = setdiff(seq_len(n), train.set)\n\nlrn1 = makeLearner( classif.lda , predict.type =  prob )\nmod1 = train(lrn1, sonar.task, subset = train.set)\npred1 = predict(mod1, task = sonar.task, subset = test.set)  Since we want to plot ROC curves we calculate the false and true positive rates ( fpr \nand  tpr ).\nAdditionally, we also compute error rates ( mmce ).  df = generateThreshVsPerfData(pred1, measures = list(fpr, tpr, mmce))  generateThreshVsPerfData  returns an object of class  ThreshVsPerfData \nwhich contains the performance values in the  $data  element.  Per default,  plotROCCurves  plots the performance values of the first two measures passed\nto  generateThreshVsPerfData . The first is shown on the x-axis, the second on the y-axis.\nMoreover, a diagonal line that represents the performance of a random classifier is added.\nYou can remove the diagonal by setting  diagonal = FALSE .  plotROCCurves(df)   The corresponding area under curve ( auc ) can be calculated as usual by calling performance .  performance(pred1, auc)\n#       auc \n#  0.847973  plotROCCurves  always requires a pair of performance measures that are plotted against\neach other.\nIf you want to plot individual measures versus the decision threshold you can use function plotThreshVsPerf .  plotThreshVsPerf(df)   Additional to  linear discriminant analysis  we try a support vector machine\nwith RBF kernel ( ksvm ).  lrn2 = makeLearner( classif.ksvm , predict.type =  prob )\nmod2 = train(lrn2, sonar.task, subset = train.set)\npred2 = predict(mod2, task = sonar.task, subset = test.set)  In order to compare the performance of the two learners you might want to display the two\ncorresponding ROC curves in one plot.\nFor this purpose just pass a named  list  of  Prediction s to  generateThreshVsPerfData .  df = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(fpr, tpr))\nplotROCCurves(df)   It's clear from the plot above that  ksvm  has a slightly higher AUC than lda .  performance(pred2, auc)\n#        auc \n#  0.9214527  Based on the  $data  member of  df  you can easily generate custom plots.\nBelow the curves for the two learners are superposed.  qplot(x = fpr, y = tpr, color = learner, data = df$data, geom =  path )   It is easily possible to generate other performance plots by passing the appropriate\nperformance measures to  generateThreshVsPerfData  and  plotROCCurves .\nBelow, we generate a  precision/recall graph  (precision = positive predictive value = ppv,\nrecall = tpr) and a  sensitivity/specificity plot  (sensitivity = tpr, specificity = tnr).  df = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(ppv, tpr, tnr))\n\n## Precision/recall graph\nplotROCCurves(df, measures = list(tpr, ppv), diagonal = FALSE)\n\n## Sensitivity/specificity plot\nplotROCCurves(df, measures = list(tnr, tpr), diagonal = FALSE)", 
            "title": "Example 1: Single predictions"
        }, 
        {
            "location": "/roc_analysis/index.html#example-2-benchmark-experiment", 
            "text": "The analysis in the example above can be improved a little.\nInstead of writing individual code for training/prediction of each learner, which can become\ntedious very quickly, we can use function  benchmark  (see also Benchmark Experiments ) and, ideally, the support vector machine\nshould have been  tuned .  We again consider the  Sonar  data set and apply  lda \nas well as  ksvm .\nWe first generate a  tuning wrapper  for  ksvm .\nThe cost parameter is tuned on a (for demonstration purposes small) parameter grid.\nWe assume that we are interested in a good performance over the complete threshold range\nand therefore tune with regard to the  auc .\nThe error rate ( mmce ) for a threshold value of 0.5 is reported as well.  ## Tune wrapper for ksvm\nrdesc.inner = makeResampleDesc( Holdout )\nms = list(auc, mmce)\nps = makeParamSet(\n  makeDiscreteParam( C , 2^(-1:1))\n)\nctrl = makeTuneControlGrid()\nlrn2 = makeTuneWrapper(lrn2, rdesc.inner, ms, ps, ctrl, show.info = FALSE)  Below the actual benchmark experiment is conducted.\nAs resampling strategy we use 5-fold cross-validation and again calculate the  auc \nas well as the error rate (for a threshold/cutoff value of 0.5).  ## Benchmark experiment\nlrns = list(lrn1, lrn2)\nrdesc.outer = makeResampleDesc( CV , iters = 5)\n\nbmr = benchmark(lrns, tasks = sonar.task, resampling = rdesc.outer, measures = ms, show.info = FALSE)\nbmr\n#          task.id         learner.id auc.test.mean mmce.test.mean\n#  1 Sonar_example        classif.lda     0.7835442      0.2592334\n#  2 Sonar_example classif.ksvm.tuned     0.9454418      0.1390244  Calling  generateThreshVsPerfData  and  plotROCCurves  on the  benchmark result \nproduces a plot with ROC curves for all learners in the experiment.  df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce))\nplotROCCurves(df)   Per default,  generateThreshVsPerfData  calculates aggregated performances according to the\nchosen resampling strategy (5-fold cross-validation) and aggregation scheme\n( test.mean ) for each threshold in the sequence.\nThis way we get  threshold-averaged  ROC curves.  If you want to plot the individual ROC curves for each resample iteration set  aggregate = FALSE .  df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce), aggregate = FALSE)\nplotROCCurves(df)   The same applies for  plotThreshVsPerf .  plotThreshVsPerf(df) +\n  theme(strip.text.x = element_text(size = 7))   An alternative to averaging is to just merge the 5 test folds and draw a single ROC curve.\nMerging can be achieved by manually changing the  class  attribute of\nthe prediction objects from  ResamplePrediction  to  Prediction .  Below, the predictions are extracted from the  BenchmarkResult  via function  getBMRPredictions ,\nthe  class  is changed and the ROC curves are created.  Averaging methods are normally preferred\n(cp.  Fawcett, 2006 ),\nas they permit to assess the variability, which is needed to properly compare classifier\nperformance.  ## Extract predictions\npreds = getBMRPredictions(bmr, drop = TRUE)\n\n## Change the class attribute\npreds2 = lapply(preds, function(x) {class(x) =  Prediction ; return(x)})\n\n## Draw ROC curves\ndf = generateThreshVsPerfData(preds2, measures = list(fpr, tpr, mmce))\nplotROCCurves(df)   Again, you can easily create other standard evaluation plots by passing the appropriate\nperformance measures to  generateThreshVsPerfData  and  plotROCCurves .", 
            "title": "Example 2: Benchmark experiment"
        }, 
        {
            "location": "/roc_analysis/index.html#performance-plots-with-asrocrprediction", 
            "text": "Drawing performance plots with package  ROCR  works through three basic commands:   ROCR::prediction : Create a  ROCR   prediction  object.  ROCR::performance : Calculate one or more performance measures for the\n   given prediction object.  ROCR::plot : Generate the performance plot.   mlr 's function  asROCRPrediction  converts an  mlr   Prediction  object to\na  ROCR   prediction  object, so you can easily generate\nperformance plots by doing steps 2. and 3. yourself. ROCR 's  plot  method has some nice features which are not (yet)\navailable in  plotROCCurves , for example plotting the convex hull of the ROC curves.\nSome examples are shown below.", 
            "title": "Performance plots with asROCRPrediction"
        }, 
        {
            "location": "/roc_analysis/index.html#example-1-single-predictions-continued", 
            "text": "We go back to out first example where we trained and predicted  lda  on the sonar classification task .  n = getTaskSize(sonar.task)\ntrain.set = sample(n, size = round(2/3 * n))\ntest.set = setdiff(seq_len(n), train.set)\n\n## Train and predict linear discriminant analysis\nlrn1 = makeLearner( classif.lda , predict.type =  prob )\nmod1 = train(lrn1, sonar.task, subset = train.set)\npred1 = predict(mod1, task = sonar.task, subset = test.set)  Below we use  asROCRPrediction  to convert the lda prediction, let  ROCR  calculate the\ntrue and false positive rate and plot the ROC curve.  ## Convert prediction\nROCRpred1 = asROCRPrediction(pred1)\n\n## Calculate true and false positive rate\nROCRperf1 = ROCR::performance(ROCRpred1,  tpr ,  fpr )\n\n## Draw ROC curve\nROCR::plot(ROCRperf1)   Below is the same ROC curve, but we make use of some more graphical parameters:\nThe ROC curve is color-coded by the threshold and selected threshold values are printed on\nthe curve. Additionally, the convex hull (black broken line) of the ROC curve is drawn.  ## Draw ROC curve\nROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2)\n\n## Draw convex hull of ROC curve\nch = ROCR::performance(ROCRpred1,  rch )\nROCR::plot(ch, add = TRUE, lty = 2)", 
            "title": "Example 1: Single predictions (continued)"
        }, 
        {
            "location": "/roc_analysis/index.html#example-2-benchmark-experiments-continued", 
            "text": "We again consider the benchmark experiment conducted earlier.\nWe first extract the predictions by  getBMRPredictions  and then convert them via function asROCRPrediction .  ## Extract predictions\npreds = getBMRPredictions(bmr, drop = TRUE)\n\n## Convert predictions\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate true and false positive rate\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  tpr ,  fpr ))  We draw the vertically averaged ROC curves (solid lines) as well as the ROC curves for\nthe individual resampling iterations (broken lines).\nMoreover, standard error bars are plotted for selected true positive rates (0.1, 0.2, ..., 0.9).\nSee  ROCR 's  plot  function for details.  ## lda average ROC curve\nplot(ROCRperfs[[1]], col =  blue , avg =  vertical , spread.estimate =  stderror ,\n  show.spread.at = seq(0.1, 0.8, 0.1), plotCI.col =  blue , plotCI.lwd = 2, lwd = 2)\n## lda individual ROC curves\nplot(ROCRperfs[[1]], col =  blue , lty = 2, lwd = 0.25, add = TRUE)\n\n## ksvm average ROC curve\nplot(ROCRperfs[[2]], col =  red , avg =  vertical , spread.estimate =  stderror ,\n  show.spread.at = seq(0.1, 0.6, 0.1), plotCI.col =  red , plotCI.lwd = 2, lwd = 2, add = TRUE)\n## ksvm individual ROC curves\nplot(ROCRperfs[[2]], col =  red , lty = 2, lwd = 0.25, add = TRUE)\n\nlegend( bottomright , legend = getBMRLearnerIds(bmr), lty = 1, lwd = 2, col = c( blue ,  red ))   In order to create other evaluation plots like  precision/recall graphs  you just have to\nchange the performance measures when calling  ROCR::performance .\n(Note that you have to use the measures provided by  ROCR  listed  here \nand not  mlr 's performance measures.)  ## Extract and convert predictions\npreds = getBMRPredictions(bmr, drop = TRUE)\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate precision and recall\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  prec ,  rec ))\n\n## Draw performance plot\nplot(ROCRperfs[[1]], col =  blue , avg =  threshold )\nplot(ROCRperfs[[2]], col =  red , avg =  threshold , add = TRUE)\nlegend( bottomleft , legend = getBMRLearnerIds(bmr), lty = 1, col = c( blue ,  red ))   If you want to plot a performance measure versus the threshold, specify only one measure when\ncalling  ROCR::performance .\nBelow the average accuracy over the 5 cross-validation iterations is plotted against the\nthreshold. Moreover, boxplots for certain threshold values (0.1, 0.2, ..., 0.9) are drawn.  ## Extract and convert predictions\npreds = getBMRPredictions(bmr, drop = TRUE)\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate accuracy\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  acc ))\n\n## Plot accuracy versus threshold\nplot(ROCRperfs[[1]], avg =  vertical , spread.estimate =  boxplot , lwd = 2, col =  blue ,\n  show.spread.at = seq(0.1, 0.9, 0.1), ylim = c(0,1), xlab =  Threshold )", 
            "title": "Example 2: Benchmark experiments (continued)"
        }, 
        {
            "location": "/roc_analysis/index.html#viper-charts", 
            "text": "mlr  also supports  ViperCharts  for plotting ROC and other performance\ncurves. Like  generateThreshVsPerfData  it has S3 methods for objects of class  Prediction , ResampleResult  and  BenchmarkResult .\nBelow plots for the benchmark experiment (Example 2) are generated.  z = plotViperCharts(bmr, chart =  rocc , browse = FALSE)  You can see the plot created this way  here .\nNote that besides ROC curves you get several other plots like lift charts or cost curves.\nFor details, see  plotViperCharts .", 
            "title": "Viper charts"
        }, 
        {
            "location": "/roc_analysis/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  n = getTaskSize(sonar.task) \ntrain.set = sample(n, size = round(2/3 * n)) \ntest.set = setdiff(seq_len(n), train.set) \n\nlrn1 = makeLearner( classif.lda , predict.type =  prob ) \nmod1 = train(lrn1, sonar.task, subset = train.set) \npred1 = predict(mod1, task = sonar.task, subset = test.set) \ndf = generateThreshVsPerfData(pred1, measures = list(fpr, tpr, mmce)) \nplotROCCurves(df) \nperformance(pred1, auc) \nplotThreshVsPerf(df) \nlrn2 = makeLearner( classif.ksvm , predict.type =  prob ) \nmod2 = train(lrn2, sonar.task, subset = train.set) \npred2 = predict(mod2, task = sonar.task, subset = test.set) \ndf = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(fpr, tpr)) \nplotROCCurves(df) \nperformance(pred2, auc) \nqplot(x = fpr, y = tpr, color = learner, data = df$data, geom =  path ) \ndf = generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(ppv, tpr, tnr)) \n\n## Precision/recall graph \nplotROCCurves(df, measures = list(tpr, ppv), diagonal = FALSE) \n\n## Sensitivity/specificity plot \nplotROCCurves(df, measures = list(tnr, tpr), diagonal = FALSE) \n## Tune wrapper for ksvm \nrdesc.inner = makeResampleDesc( Holdout ) \nms = list(auc, mmce) \nps = makeParamSet( \n  makeDiscreteParam( C , 2^(-1:1)) \n) \nctrl = makeTuneControlGrid() \nlrn2 = makeTuneWrapper(lrn2, rdesc.inner, ms, ps, ctrl, show.info = FALSE) \n## Benchmark experiment \nlrns = list(lrn1, lrn2) \nrdesc.outer = makeResampleDesc( CV , iters = 5) \n\nbmr = benchmark(lrns, tasks = sonar.task, resampling = rdesc.outer, measures = ms, show.info = FALSE) \nbmr \ndf = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce)) \nplotROCCurves(df) \ndf = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce), aggregate = FALSE) \nplotROCCurves(df) \nplotThreshVsPerf(df) + \n  theme(strip.text.x = element_text(size = 7)) \n## Extract predictions \npreds = getBMRPredictions(bmr, drop = TRUE) \n\n## Change the class attribute \npreds2 = lapply(preds, function(x) {class(x) =  Prediction ; return(x)}) \n\n## Draw ROC curves \ndf = generateThreshVsPerfData(preds2, measures = list(fpr, tpr, mmce)) \nplotROCCurves(df) \nn = getTaskSize(sonar.task) \ntrain.set = sample(n, size = round(2/3 * n)) \ntest.set = setdiff(seq_len(n), train.set) \n\n## Train and predict linear discriminant analysis \nlrn1 = makeLearner( classif.lda , predict.type =  prob ) \nmod1 = train(lrn1, sonar.task, subset = train.set) \npred1 = predict(mod1, task = sonar.task, subset = test.set) \n## Convert prediction \nROCRpred1 = asROCRPrediction(pred1) \n\n## Calculate true and false positive rate \nROCRperf1 = ROCR::performance(ROCRpred1,  tpr ,  fpr ) \n\n## Draw ROC curve \nROCR::plot(ROCRperf1) \n## Draw ROC curve \nROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2) \n\n## Draw convex hull of ROC curve \nch = ROCR::performance(ROCRpred1,  rch ) \nROCR::plot(ch, add = TRUE, lty = 2) \n## Extract predictions \npreds = getBMRPredictions(bmr, drop = TRUE) \n\n## Convert predictions \nROCRpreds = lapply(preds, asROCRPrediction) \n\n## Calculate true and false positive rate \nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  tpr ,  fpr )) \n## lda average ROC curve \nplot(ROCRperfs[[1]], col =  blue , avg =  vertical , spread.estimate =  stderror , \n  show.spread.at = seq(0.1, 0.8, 0.1), plotCI.col =  blue , plotCI.lwd = 2, lwd = 2) \n## lda individual ROC curves \nplot(ROCRperfs[[1]], col =  blue , lty = 2, lwd = 0.25, add = TRUE) \n\n## ksvm average ROC curve \nplot(ROCRperfs[[2]], col =  red , avg =  vertical , spread.estimate =  stderror , \n  show.spread.at = seq(0.1, 0.6, 0.1), plotCI.col =  red , plotCI.lwd = 2, lwd = 2, add = TRUE) \n## ksvm individual ROC curves \nplot(ROCRperfs[[2]], col =  red , lty = 2, lwd = 0.25, add = TRUE) \n\nlegend( bottomright , legend = getBMRLearnerIds(bmr), lty = 1, lwd = 2, col = c( blue ,  red )) \n## Extract and convert predictions \npreds = getBMRPredictions(bmr, drop = TRUE) \nROCRpreds = lapply(preds, asROCRPrediction) \n\n## Calculate precision and recall \nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  prec ,  rec )) \n\n## Draw performance plot \nplot(ROCRperfs[[1]], col =  blue , avg =  threshold ) \nplot(ROCRperfs[[2]], col =  red , avg =  threshold , add = TRUE) \nlegend( bottomleft , legend = getBMRLearnerIds(bmr), lty = 1, col = c( blue ,  red )) \n## Extract and convert predictions \npreds = getBMRPredictions(bmr, drop = TRUE) \nROCRpreds = lapply(preds, asROCRPrediction) \n\n## Calculate accuracy \nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  acc )) \n\n## Plot accuracy versus threshold \nplot(ROCRperfs[[1]], avg =  vertical , spread.estimate =  boxplot , lwd = 2, col =  blue , \n  show.spread.at = seq(0.1, 0.9, 0.1), ylim = c(0,1), xlab =  Threshold ) \nz = plotViperCharts(bmr, chart =  rocc , browse = FALSE)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/multilabel/index.html", 
            "text": "Multilabel Classification\n\n\nMultilabel classification is a classification problem where multiple target labels can be\nassigned to each observation instead of only one like in multiclass classification.\n\n\nTwo different approaches exist for multilabel classification. \nProblem transformation\nmethods\n try to transform the multilabel classification into binary or multiclass\nclassification problems. \nAlgorithm adaptation methods\n adapt multiclass algorithms\nso they can be applied directly to the problem.\n\n\nCreating a task\n\n\nThe first thing you have to do for multilabel classification in \nmlr\n is to\nget your data in the right format. You need a \ndata.frame\n which\nconsists of the features and a logical vector for each label which indicates if the\nlabel is present in the observation or not. After that you can create a\n\nMultilabelTask\n like a normal \nClassifTask\n. Instead of one\ntarget name you have to specify a vector of targets which correspond to the names of\nlogical variables in the \ndata.frame\n. In the following example\nwe get the yeast data frame from the already existing \nyeast.task\n, extract\nthe 14 label names and create the task again.\n\n\nyeast = getTaskData(yeast.task)\nlabels = colnames(yeast)[1:14]\nyeast.task = makeMultilabelTask(id = \nmulti\n, data = yeast, target = labels)\nyeast.task\n#\n Supervised task: multi\n#\n Type: multilabel\n#\n Target: label1,label2,label3,label4,label5,label6,label7,label8,label9,label10,label11,label12,label13,label14\n#\n Observations: 2417\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n         103           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 14\n#\n  label1  label2  label3  label4  label5  label6  label7  label8  label9 \n#\n     762    1038     983     862     722     597     428     480     178 \n#\n label10 label11 label12 label13 label14 \n#\n     253     289    1816    1799      34\n\n\n\n\nConstructing a learner\n\n\nMultilabel classification in \nmlr\n can currently be done in two ways:\n\n\n\n\n\n\nAlgorithm adaptation methods: Treat the whole problem with a specific algorithm.\n\n\n\n\n\n\nProblem transformation methods: Transform the problem, so that simple binary classification algorithms can be applied.\n\n\n\n\n\n\nAlgorithm adaptation methods\n\n\nCurrently the available algorithm adaptation methods in \nR\n are the multivariate random forest in the \nrandomForestSRC\n package and the random ferns multilabel algorithm in the \nrFerns\n package. You can create the learner for these algorithms\nlike in multiclass classification problems.\n\n\nlrn.rfsrc = makeLearner(\nmultilabel.randomForestSRC\n)\nlrn.rFerns = makeLearner(\nmultilabel.rFerns\n)\nlrn.rFerns\n#\n Learner multilabel.rFerns from package rFerns\n#\n Type: multilabel\n#\n Name: Random ferns; Short name: rFerns\n#\n Class: multilabel.rFerns\n#\n Properties: numerics,factors,ordered\n#\n Predict-Type: response\n#\n Hyperparameters:\n\n\n\n\nProblem transformation methods\n\n\nFor generating a wrapped multilabel learner first create a binary (or multiclass)\nclassification learner with \nmakeLearner\n. Afterwards apply a function like\n\nmakeMultilabelBinaryRelevanceWrapper\n, \nmakeMultilabelClassifierChainsWrapper\n, \nmakeMultilabelNestedStackingWrapper\n, \n\nmakeMultilabelDBRWrapper\n or \nmakeMultilabelStackingWrapper\n on the learner to convert it to a learner that uses the respective problem transformation method.\n\n\nYou can also generate a binary relevance learner directly, as you can see in the example.\n\n\nlrn.br = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nlrn.br = makeMultilabelBinaryRelevanceWrapper(lrn.br)\nlrn.br\n#\n Learner multilabel.binaryRelevance.classif.rpart from package rpart\n#\n Type: multilabel\n#\n Name: ; Short name: \n#\n Class: MultilabelBinaryRelevanceWrapper\n#\n Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass\n#\n Predict-Type: prob\n#\n Hyperparameters: xval=0\n\nlrn.br2 = makeMultilabelBinaryRelevanceWrapper(\nclassif.rpart\n)\nlrn.br2\n#\n Learner multilabel.binaryRelevance.classif.rpart from package rpart\n#\n Type: multilabel\n#\n Name: ; Short name: \n#\n Class: MultilabelBinaryRelevanceWrapper\n#\n Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass\n#\n Predict-Type: response\n#\n Hyperparameters: xval=0\n\n\n\n\nThe different methods are shortly described in the following. \n\n\nBinary relevance\n\n\nThis problem transformation method converts the multilabel problem to binary\nclassification problems for each\nlabel and applies a simple binary classificator on these. In \nmlr\n this can be done by\nconverting your binary learner to a wrapped binary relevance multilabel learner.\n\n\nClassifier chains\n\n\nTrains consecutively the labels with the input data. The input data in each step is augmented\nby the already trained labels (with the real observed values).\nTherefore an order of the labels has to be specified. At prediction time the labels are\npredicted in the same order as while training. The required labels in the input data are\ngiven by the previous done prediction of the respective label.\n\n\nNested stacking\n\n\nSame as classifier chains, but the labels in the input data are not the real ones, but\nestimations of the labels obtained by the already trained learners.\n\n\nDependent binary relevance\n\n\nEach label is trained with the real observed values of all other labels. In prediction phase\nfor a label the other necessary labels are obtained in a previous step by a base learner like\nthe binary relevance method.\n\n\nStacking\n\n\nSame as the dependent binary relevance method, but in the training phase the labels used as\ninput for each label are obtained by the binary relevance method.\n\n\nTrain\n\n\nYou can \ntrain\n a model as usual with a multilabel learner and a\nmultilabel task as input. You can also pass \nsubset\n and \nweights\n arguments if the\nlearner supports this.\n\n\nmod = train(lrn.br, yeast.task)\nmod = train(lrn.br, yeast.task, subset = 1:1500, weights = rep(1/1500, 1500))\nmod\n#\n Model for learner.id=multilabel.binaryRelevance.classif.rpart; learner.class=MultilabelBinaryRelevanceWrapper\n#\n Trained on: task.id = multi; obs = 1500; features = 103\n#\n Hyperparameters: xval=0\n\nmod2 = train(lrn.rfsrc, yeast.task, subset = 1:100)\nmod2\n#\n Model for learner.id=multilabel.randomForestSRC; learner.class=multilabel.randomForestSRC\n#\n Trained on: task.id = multi; obs = 100; features = 103\n#\n Hyperparameters: na.action=na.impute\n\n\n\n\nPredict\n\n\nPrediction can be done as usual in \nmlr\n with \npredict\n and by\npassing a trained model\nand either the task to the \ntask\n argument or some new data to the \nnewdata\n\nargument. As always you can specify a \nsubset\n of the data\nwhich should be predicted.\n\n\npred = predict(mod, task = yeast.task, subset = 1:10)\npred = predict(mod, newdata = yeast[1501:1600,])\nnames(as.data.frame(pred))\n#\n  [1] \ntruth.label1\n     \ntruth.label2\n     \ntruth.label3\n    \n#\n  [4] \ntruth.label4\n     \ntruth.label5\n     \ntruth.label6\n    \n#\n  [7] \ntruth.label7\n     \ntruth.label8\n     \ntruth.label9\n    \n#\n [10] \ntruth.label10\n    \ntruth.label11\n    \ntruth.label12\n   \n#\n [13] \ntruth.label13\n    \ntruth.label14\n    \nprob.label1\n     \n#\n [16] \nprob.label2\n      \nprob.label3\n      \nprob.label4\n     \n#\n [19] \nprob.label5\n      \nprob.label6\n      \nprob.label7\n     \n#\n [22] \nprob.label8\n      \nprob.label9\n      \nprob.label10\n    \n#\n [25] \nprob.label11\n     \nprob.label12\n     \nprob.label13\n    \n#\n [28] \nprob.label14\n     \nresponse.label1\n  \nresponse.label2\n \n#\n [31] \nresponse.label3\n  \nresponse.label4\n  \nresponse.label5\n \n#\n [34] \nresponse.label6\n  \nresponse.label7\n  \nresponse.label8\n \n#\n [37] \nresponse.label9\n  \nresponse.label10\n \nresponse.label11\n\n#\n [40] \nresponse.label12\n \nresponse.label13\n \nresponse.label14\n\n\npred2 = predict(mod2, task = yeast.task)\nnames(as.data.frame(pred2))\n#\n  [1] \nid\n               \ntruth.label1\n     \ntruth.label2\n    \n#\n  [4] \ntruth.label3\n     \ntruth.label4\n     \ntruth.label5\n    \n#\n  [7] \ntruth.label6\n     \ntruth.label7\n     \ntruth.label8\n    \n#\n [10] \ntruth.label9\n     \ntruth.label10\n    \ntruth.label11\n   \n#\n [13] \ntruth.label12\n    \ntruth.label13\n    \ntruth.label14\n   \n#\n [16] \nresponse.label1\n  \nresponse.label2\n  \nresponse.label3\n \n#\n [19] \nresponse.label4\n  \nresponse.label5\n  \nresponse.label6\n \n#\n [22] \nresponse.label7\n  \nresponse.label8\n  \nresponse.label9\n \n#\n [25] \nresponse.label10\n \nresponse.label11\n \nresponse.label12\n\n#\n [28] \nresponse.label13\n \nresponse.label14\n\n\n\n\n\nDepending on the chosen \npredict.type\n of the learner you get true and predicted values and\npossibly probabilities for each class label.\nThese can be extracted by the usual accessor functions \ngetPredictionTruth\n, \ngetPredictionResponse\n\nand \ngetPredictionProbabilities\n.\n\n\nPerformance\n\n\nThe performance of your prediction can be assessed via function \nperformance\n.\nYou can specify via the \nmeasures\n argument which \nmeasure(s)\n to calculate.\nThe default measure for multilabel classification is the Hamming loss (\nmultilabel.hamloss\n).\nAll available measures for multilabel classification can be shown by \nlistMeasures\n and found\nin the \ntable of performance measures\n and the \nmeasures\n documentation page.\n\n\nperformance(pred)\n#\n multilabel.hamloss \n#\n          0.2257143\n\nperformance(pred2, measures = list(multilabel.subset01, multilabel.hamloss, multilabel.acc,\n  multilabel.f1, timepredict))\n#\n multilabel.subset01  multilabel.hamloss      multilabel.acc \n#\n           0.8642946           0.2043560           0.4639072 \n#\n       multilabel.f1         timepredict \n#\n           0.5730995           5.3650000\n\nlistMeasures(\nmultilabel\n)\n#\n  [1] \nfeatperc\n            \nmultilabel.tpr\n      \nmultilabel.hamloss\n \n#\n  [4] \nmultilabel.subset01\n \ntimeboth\n            \ntimetrain\n          \n#\n  [7] \ntimepredict\n         \nmultilabel.ppv\n      \nmultilabel.f1\n      \n#\n [10] \nmultilabel.acc\n\n\n\n\n\nResampling\n\n\nFor evaluating the overall performance of the learning algorithm you can do some\n\nresampling\n. As usual you have to define a resampling strategy, either\nvia \nmakeResampleDesc\n or \nmakeResampleInstance\n. After that you can run the \nresample\n\nfunction. Below the default measure Hamming loss is calculated.\n\n\nrdesc = makeResampleDesc(method = \nCV\n, stratify = FALSE, iters = 3)\nr = resample(learner = lrn.br, task = yeast.task, resampling = rdesc, show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: multi\n#\n Learner: multilabel.binaryRelevance.classif.rpart\n#\n Aggr perf: multilabel.hamloss.test.mean=0.2220267\n#\n Runtime: 5.85074\n\nr = resample(learner = lrn.rFerns, task = yeast.task, resampling = rdesc, show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: multi\n#\n Learner: multilabel.rFerns\n#\n Aggr perf: multilabel.hamloss.test.mean=0.4745012\n#\n Runtime: 0.605497\n\n\n\n\nBinary performance\n\n\nIf you want to calculate a binary\nperformance measure like, e.g., the \naccuracy\n, the \nmmce\n\nor the \nauc\n for each label, you can use function\n\ngetMultilabelBinaryPerformances\n.\nYou can apply this function to any multilabel prediction, e.g., also on the resample\nmultilabel prediction. For calculating the \nauc\n you need\npredicted probabilities.\n\n\ngetMultilabelBinaryPerformances(pred, measures = list(acc, mmce, auc))\n#\n         acc.test.mean mmce.test.mean auc.test.mean\n#\n label1           0.75           0.25     0.6321925\n#\n label2           0.64           0.36     0.6547917\n#\n label3           0.68           0.32     0.7118227\n#\n label4           0.69           0.31     0.6764835\n#\n label5           0.73           0.27     0.6676923\n#\n label6           0.70           0.30     0.6417739\n#\n label7           0.81           0.19     0.5968750\n#\n label8           0.73           0.27     0.5164474\n#\n label9           0.89           0.11     0.4688458\n#\n label10          0.86           0.14     0.3996463\n#\n label11          0.85           0.15     0.5000000\n#\n label12          0.76           0.24     0.5330667\n#\n label13          0.75           0.25     0.5938610\n#\n label14          1.00           0.00            NA\n\ngetMultilabelBinaryPerformances(r$pred, measures = list(acc, mmce))\n#\n         acc.test.mean mmce.test.mean\n#\n label1     0.69880017      0.3011998\n#\n label2     0.58543649      0.4145635\n#\n label3     0.70086885      0.2991312\n#\n label4     0.71493587      0.2850641\n#\n label5     0.71162598      0.2883740\n#\n label6     0.58874638      0.4112536\n#\n label7     0.53165081      0.4683492\n#\n label8     0.53496070      0.4650393\n#\n label9     0.30740588      0.6925941\n#\n label10    0.44269756      0.5573024\n#\n label11    0.45097228      0.5490277\n#\n label12    0.53330575      0.4666942\n#\n label13    0.53868432      0.4613157\n#\n label14    0.01696318      0.9830368\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nyeast = getTaskData(yeast.task) \nlabels = colnames(yeast)[1:14] \nyeast.task = makeMultilabelTask(id = \nmulti\n, data = yeast, target = labels) \nyeast.task \nlrn.rfsrc = makeLearner(\nmultilabel.randomForestSRC\n) \nlrn.rFerns = makeLearner(\nmultilabel.rFerns\n) \nlrn.rFerns \nlrn.br = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n) \nlrn.br = makeMultilabelBinaryRelevanceWrapper(lrn.br) \nlrn.br \n\nlrn.br2 = makeMultilabelBinaryRelevanceWrapper(\nclassif.rpart\n) \nlrn.br2 \nmod = train(lrn.br, yeast.task) \nmod = train(lrn.br, yeast.task, subset = 1:1500, weights = rep(1/1500, 1500)) \nmod \n\nmod2 = train(lrn.rfsrc, yeast.task, subset = 1:100) \nmod2 \npred = predict(mod, task = yeast.task, subset = 1:10) \npred = predict(mod, newdata = yeast[1501:1600,]) \nnames(as.data.frame(pred)) \n\npred2 = predict(mod2, task = yeast.task) \nnames(as.data.frame(pred2)) \nperformance(pred) \n\nperformance(pred2, measures = list(multilabel.subset01, multilabel.hamloss, multilabel.acc, \n  multilabel.f1, timepredict)) \n\nlistMeasures(\nmultilabel\n) \nrdesc = makeResampleDesc(method = \nCV\n, stratify = FALSE, iters = 3) \nr = resample(learner = lrn.br, task = yeast.task, resampling = rdesc, show.info = FALSE) \nr \n\nr = resample(learner = lrn.rFerns, task = yeast.task, resampling = rdesc, show.info = FALSE) \nr \ngetMultilabelBinaryPerformances(pred, measures = list(acc, mmce, auc)) \n\ngetMultilabelBinaryPerformances(r$pred, measures = list(acc, mmce))", 
            "title": "Multilabel Classification"
        }, 
        {
            "location": "/multilabel/index.html#multilabel-classification", 
            "text": "Multilabel classification is a classification problem where multiple target labels can be\nassigned to each observation instead of only one like in multiclass classification.  Two different approaches exist for multilabel classification.  Problem transformation\nmethods  try to transform the multilabel classification into binary or multiclass\nclassification problems.  Algorithm adaptation methods  adapt multiclass algorithms\nso they can be applied directly to the problem.", 
            "title": "Multilabel Classification"
        }, 
        {
            "location": "/multilabel/index.html#creating-a-task", 
            "text": "The first thing you have to do for multilabel classification in  mlr  is to\nget your data in the right format. You need a  data.frame  which\nconsists of the features and a logical vector for each label which indicates if the\nlabel is present in the observation or not. After that you can create a MultilabelTask  like a normal  ClassifTask . Instead of one\ntarget name you have to specify a vector of targets which correspond to the names of\nlogical variables in the  data.frame . In the following example\nwe get the yeast data frame from the already existing  yeast.task , extract\nthe 14 label names and create the task again.  yeast = getTaskData(yeast.task)\nlabels = colnames(yeast)[1:14]\nyeast.task = makeMultilabelTask(id =  multi , data = yeast, target = labels)\nyeast.task\n#  Supervised task: multi\n#  Type: multilabel\n#  Target: label1,label2,label3,label4,label5,label6,label7,label8,label9,label10,label11,label12,label13,label14\n#  Observations: 2417\n#  Features:\n#     numerics     factors     ordered functionals \n#          103           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 14\n#   label1  label2  label3  label4  label5  label6  label7  label8  label9 \n#      762    1038     983     862     722     597     428     480     178 \n#  label10 label11 label12 label13 label14 \n#      253     289    1816    1799      34", 
            "title": "Creating a task"
        }, 
        {
            "location": "/multilabel/index.html#constructing-a-learner", 
            "text": "Multilabel classification in  mlr  can currently be done in two ways:    Algorithm adaptation methods: Treat the whole problem with a specific algorithm.    Problem transformation methods: Transform the problem, so that simple binary classification algorithms can be applied.", 
            "title": "Constructing a learner"
        }, 
        {
            "location": "/multilabel/index.html#algorithm-adaptation-methods", 
            "text": "Currently the available algorithm adaptation methods in  R  are the multivariate random forest in the  randomForestSRC  package and the random ferns multilabel algorithm in the  rFerns  package. You can create the learner for these algorithms\nlike in multiclass classification problems.  lrn.rfsrc = makeLearner( multilabel.randomForestSRC )\nlrn.rFerns = makeLearner( multilabel.rFerns )\nlrn.rFerns\n#  Learner multilabel.rFerns from package rFerns\n#  Type: multilabel\n#  Name: Random ferns; Short name: rFerns\n#  Class: multilabel.rFerns\n#  Properties: numerics,factors,ordered\n#  Predict-Type: response\n#  Hyperparameters:", 
            "title": "Algorithm adaptation methods"
        }, 
        {
            "location": "/multilabel/index.html#problem-transformation-methods", 
            "text": "For generating a wrapped multilabel learner first create a binary (or multiclass)\nclassification learner with  makeLearner . Afterwards apply a function like makeMultilabelBinaryRelevanceWrapper ,  makeMultilabelClassifierChainsWrapper ,  makeMultilabelNestedStackingWrapper ,  makeMultilabelDBRWrapper  or  makeMultilabelStackingWrapper  on the learner to convert it to a learner that uses the respective problem transformation method.  You can also generate a binary relevance learner directly, as you can see in the example.  lrn.br = makeLearner( classif.rpart , predict.type =  prob )\nlrn.br = makeMultilabelBinaryRelevanceWrapper(lrn.br)\nlrn.br\n#  Learner multilabel.binaryRelevance.classif.rpart from package rpart\n#  Type: multilabel\n#  Name: ; Short name: \n#  Class: MultilabelBinaryRelevanceWrapper\n#  Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass\n#  Predict-Type: prob\n#  Hyperparameters: xval=0\n\nlrn.br2 = makeMultilabelBinaryRelevanceWrapper( classif.rpart )\nlrn.br2\n#  Learner multilabel.binaryRelevance.classif.rpart from package rpart\n#  Type: multilabel\n#  Name: ; Short name: \n#  Class: MultilabelBinaryRelevanceWrapper\n#  Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass\n#  Predict-Type: response\n#  Hyperparameters: xval=0  The different methods are shortly described in the following.", 
            "title": "Problem transformation methods"
        }, 
        {
            "location": "/multilabel/index.html#binary-relevance", 
            "text": "This problem transformation method converts the multilabel problem to binary\nclassification problems for each\nlabel and applies a simple binary classificator on these. In  mlr  this can be done by\nconverting your binary learner to a wrapped binary relevance multilabel learner.", 
            "title": "Binary relevance"
        }, 
        {
            "location": "/multilabel/index.html#classifier-chains", 
            "text": "Trains consecutively the labels with the input data. The input data in each step is augmented\nby the already trained labels (with the real observed values).\nTherefore an order of the labels has to be specified. At prediction time the labels are\npredicted in the same order as while training. The required labels in the input data are\ngiven by the previous done prediction of the respective label.", 
            "title": "Classifier chains"
        }, 
        {
            "location": "/multilabel/index.html#nested-stacking", 
            "text": "Same as classifier chains, but the labels in the input data are not the real ones, but\nestimations of the labels obtained by the already trained learners.", 
            "title": "Nested stacking"
        }, 
        {
            "location": "/multilabel/index.html#dependent-binary-relevance", 
            "text": "Each label is trained with the real observed values of all other labels. In prediction phase\nfor a label the other necessary labels are obtained in a previous step by a base learner like\nthe binary relevance method.", 
            "title": "Dependent binary relevance"
        }, 
        {
            "location": "/multilabel/index.html#stacking", 
            "text": "Same as the dependent binary relevance method, but in the training phase the labels used as\ninput for each label are obtained by the binary relevance method.", 
            "title": "Stacking"
        }, 
        {
            "location": "/multilabel/index.html#train", 
            "text": "You can  train  a model as usual with a multilabel learner and a\nmultilabel task as input. You can also pass  subset  and  weights  arguments if the\nlearner supports this.  mod = train(lrn.br, yeast.task)\nmod = train(lrn.br, yeast.task, subset = 1:1500, weights = rep(1/1500, 1500))\nmod\n#  Model for learner.id=multilabel.binaryRelevance.classif.rpart; learner.class=MultilabelBinaryRelevanceWrapper\n#  Trained on: task.id = multi; obs = 1500; features = 103\n#  Hyperparameters: xval=0\n\nmod2 = train(lrn.rfsrc, yeast.task, subset = 1:100)\nmod2\n#  Model for learner.id=multilabel.randomForestSRC; learner.class=multilabel.randomForestSRC\n#  Trained on: task.id = multi; obs = 100; features = 103\n#  Hyperparameters: na.action=na.impute", 
            "title": "Train"
        }, 
        {
            "location": "/multilabel/index.html#predict", 
            "text": "Prediction can be done as usual in  mlr  with  predict  and by\npassing a trained model\nand either the task to the  task  argument or some new data to the  newdata \nargument. As always you can specify a  subset  of the data\nwhich should be predicted.  pred = predict(mod, task = yeast.task, subset = 1:10)\npred = predict(mod, newdata = yeast[1501:1600,])\nnames(as.data.frame(pred))\n#   [1]  truth.label1       truth.label2       truth.label3     \n#   [4]  truth.label4       truth.label5       truth.label6     \n#   [7]  truth.label7       truth.label8       truth.label9     \n#  [10]  truth.label10      truth.label11      truth.label12    \n#  [13]  truth.label13      truth.label14      prob.label1      \n#  [16]  prob.label2        prob.label3        prob.label4      \n#  [19]  prob.label5        prob.label6        prob.label7      \n#  [22]  prob.label8        prob.label9        prob.label10     \n#  [25]  prob.label11       prob.label12       prob.label13     \n#  [28]  prob.label14       response.label1    response.label2  \n#  [31]  response.label3    response.label4    response.label5  \n#  [34]  response.label6    response.label7    response.label8  \n#  [37]  response.label9    response.label10   response.label11 \n#  [40]  response.label12   response.label13   response.label14 \n\npred2 = predict(mod2, task = yeast.task)\nnames(as.data.frame(pred2))\n#   [1]  id                 truth.label1       truth.label2     \n#   [4]  truth.label3       truth.label4       truth.label5     \n#   [7]  truth.label6       truth.label7       truth.label8     \n#  [10]  truth.label9       truth.label10      truth.label11    \n#  [13]  truth.label12      truth.label13      truth.label14    \n#  [16]  response.label1    response.label2    response.label3  \n#  [19]  response.label4    response.label5    response.label6  \n#  [22]  response.label7    response.label8    response.label9  \n#  [25]  response.label10   response.label11   response.label12 \n#  [28]  response.label13   response.label14   Depending on the chosen  predict.type  of the learner you get true and predicted values and\npossibly probabilities for each class label.\nThese can be extracted by the usual accessor functions  getPredictionTruth ,  getPredictionResponse \nand  getPredictionProbabilities .", 
            "title": "Predict"
        }, 
        {
            "location": "/multilabel/index.html#performance", 
            "text": "The performance of your prediction can be assessed via function  performance .\nYou can specify via the  measures  argument which  measure(s)  to calculate.\nThe default measure for multilabel classification is the Hamming loss ( multilabel.hamloss ).\nAll available measures for multilabel classification can be shown by  listMeasures  and found\nin the  table of performance measures  and the  measures  documentation page.  performance(pred)\n#  multilabel.hamloss \n#           0.2257143\n\nperformance(pred2, measures = list(multilabel.subset01, multilabel.hamloss, multilabel.acc,\n  multilabel.f1, timepredict))\n#  multilabel.subset01  multilabel.hamloss      multilabel.acc \n#            0.8642946           0.2043560           0.4639072 \n#        multilabel.f1         timepredict \n#            0.5730995           5.3650000\n\nlistMeasures( multilabel )\n#   [1]  featperc              multilabel.tpr        multilabel.hamloss  \n#   [4]  multilabel.subset01   timeboth              timetrain           \n#   [7]  timepredict           multilabel.ppv        multilabel.f1       \n#  [10]  multilabel.acc", 
            "title": "Performance"
        }, 
        {
            "location": "/multilabel/index.html#resampling", 
            "text": "For evaluating the overall performance of the learning algorithm you can do some resampling . As usual you have to define a resampling strategy, either\nvia  makeResampleDesc  or  makeResampleInstance . After that you can run the  resample \nfunction. Below the default measure Hamming loss is calculated.  rdesc = makeResampleDesc(method =  CV , stratify = FALSE, iters = 3)\nr = resample(learner = lrn.br, task = yeast.task, resampling = rdesc, show.info = FALSE)\nr\n#  Resample Result\n#  Task: multi\n#  Learner: multilabel.binaryRelevance.classif.rpart\n#  Aggr perf: multilabel.hamloss.test.mean=0.2220267\n#  Runtime: 5.85074\n\nr = resample(learner = lrn.rFerns, task = yeast.task, resampling = rdesc, show.info = FALSE)\nr\n#  Resample Result\n#  Task: multi\n#  Learner: multilabel.rFerns\n#  Aggr perf: multilabel.hamloss.test.mean=0.4745012\n#  Runtime: 0.605497", 
            "title": "Resampling"
        }, 
        {
            "location": "/multilabel/index.html#binary-performance", 
            "text": "If you want to calculate a binary\nperformance measure like, e.g., the  accuracy , the  mmce \nor the  auc  for each label, you can use function getMultilabelBinaryPerformances .\nYou can apply this function to any multilabel prediction, e.g., also on the resample\nmultilabel prediction. For calculating the  auc  you need\npredicted probabilities.  getMultilabelBinaryPerformances(pred, measures = list(acc, mmce, auc))\n#          acc.test.mean mmce.test.mean auc.test.mean\n#  label1           0.75           0.25     0.6321925\n#  label2           0.64           0.36     0.6547917\n#  label3           0.68           0.32     0.7118227\n#  label4           0.69           0.31     0.6764835\n#  label5           0.73           0.27     0.6676923\n#  label6           0.70           0.30     0.6417739\n#  label7           0.81           0.19     0.5968750\n#  label8           0.73           0.27     0.5164474\n#  label9           0.89           0.11     0.4688458\n#  label10          0.86           0.14     0.3996463\n#  label11          0.85           0.15     0.5000000\n#  label12          0.76           0.24     0.5330667\n#  label13          0.75           0.25     0.5938610\n#  label14          1.00           0.00            NA\n\ngetMultilabelBinaryPerformances(r$pred, measures = list(acc, mmce))\n#          acc.test.mean mmce.test.mean\n#  label1     0.69880017      0.3011998\n#  label2     0.58543649      0.4145635\n#  label3     0.70086885      0.2991312\n#  label4     0.71493587      0.2850641\n#  label5     0.71162598      0.2883740\n#  label6     0.58874638      0.4112536\n#  label7     0.53165081      0.4683492\n#  label8     0.53496070      0.4650393\n#  label9     0.30740588      0.6925941\n#  label10    0.44269756      0.5573024\n#  label11    0.45097228      0.5490277\n#  label12    0.53330575      0.4666942\n#  label13    0.53868432      0.4613157\n#  label14    0.01696318      0.9830368", 
            "title": "Binary performance"
        }, 
        {
            "location": "/multilabel/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  yeast = getTaskData(yeast.task) \nlabels = colnames(yeast)[1:14] \nyeast.task = makeMultilabelTask(id =  multi , data = yeast, target = labels) \nyeast.task \nlrn.rfsrc = makeLearner( multilabel.randomForestSRC ) \nlrn.rFerns = makeLearner( multilabel.rFerns ) \nlrn.rFerns \nlrn.br = makeLearner( classif.rpart , predict.type =  prob ) \nlrn.br = makeMultilabelBinaryRelevanceWrapper(lrn.br) \nlrn.br \n\nlrn.br2 = makeMultilabelBinaryRelevanceWrapper( classif.rpart ) \nlrn.br2 \nmod = train(lrn.br, yeast.task) \nmod = train(lrn.br, yeast.task, subset = 1:1500, weights = rep(1/1500, 1500)) \nmod \n\nmod2 = train(lrn.rfsrc, yeast.task, subset = 1:100) \nmod2 \npred = predict(mod, task = yeast.task, subset = 1:10) \npred = predict(mod, newdata = yeast[1501:1600,]) \nnames(as.data.frame(pred)) \n\npred2 = predict(mod2, task = yeast.task) \nnames(as.data.frame(pred2)) \nperformance(pred) \n\nperformance(pred2, measures = list(multilabel.subset01, multilabel.hamloss, multilabel.acc, \n  multilabel.f1, timepredict)) \n\nlistMeasures( multilabel ) \nrdesc = makeResampleDesc(method =  CV , stratify = FALSE, iters = 3) \nr = resample(learner = lrn.br, task = yeast.task, resampling = rdesc, show.info = FALSE) \nr \n\nr = resample(learner = lrn.rFerns, task = yeast.task, resampling = rdesc, show.info = FALSE) \nr \ngetMultilabelBinaryPerformances(pred, measures = list(acc, mmce, auc)) \n\ngetMultilabelBinaryPerformances(r$pred, measures = list(acc, mmce))", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/learning_curve/index.html", 
            "text": "Learning Curve Analysis\n\n\nTo analyze how the increase of observations in the training set improves the performance of\na learner the \nlearning curve\n is an appropriate visual tool.\nThe experiment is conducted with an increasing subsample size and the performance is measured.\nIn the plot the x-axis represents the relative subsample size whereas the y-axis represents\nthe performance.\n\n\nNote that this function internally uses \nbenchmark\n in combination with \nmakeDownsampleWrapper\n,\nso for every run new observations are drawn.\nThus the results are noisy.\nTo reduce noise increase the number of resampling iterations.\nYou can define the resampling method in the \nresampling\n argument of \ngenerateLearningCurveData\n.\nIt is also possible to pass a \nResampleInstance\n (which is a result\nof \nmakeResampleInstance\n) to make resampling consistent for all passed learners and each\nstep of increasing the number of observations.\n\n\nPlotting the learning curve\n\n\nThe \nmlr\n function \ngenerateLearningCurveData\n can generate the data for \nlearning curves\n\nfor multiple \nlearners\n and multiple \nperformance measures\n\nat once.\nWith \nplotLearningCurve\n the result of \ngenerateLearningCurveData\n can be plotted using \nggplot2\n.\n\nplotLearningCurve\n has an argument \nfacet\n which can be either \n\"measure\"\n or \n\"learner\"\n. By default\n\nfacet = \"measure\"\n and facetted subplots are created for each measure input to \ngenerateLearningCurveData\n.\nIf \nfacet = \"measure\"\n learners are mapped to color, and vice versa.\n\n\nr = generateLearningCurveData(\n  learners = c(\nclassif.rpart\n, \nclassif.knn\n),\n  task = sonar.task,\n  percs = seq(0.1, 1, by = 0.2),\n  measures = list(tp, fp, tn, fn),\n  resampling = makeResampleDesc(method = \nCV\n, iters = 5),\n  show.info = FALSE)\nplotLearningCurve(r)\n\n\n\n\n\n\nWhat happens in \ngenerateLearningCurveData\n is the following:\nEach learner will be internally wrapped in a \nDownsampleWrapper\n.\nTo measure the performance at the first step of \npercs\n, say \n0.1\n, first the data will be\nsplit into a \ntraining\n and a \ntest set\n according to the given \nresampling strategy\n.\nThen a random sample containing 10% of the observations of the \ntraining set\n will be drawn\nand used to train the learner.\nThe performance will be measured on the \ncomplete test set\n.\nThese steps will be repeated as defined by the given \nresampling method\n and for each value\nof \npercs\n.\n\n\nIn the first example we simply passed a vector of learner names to [generateLearningCurveData].\nAs usual, you can also create the learners beforehand and provide a \nlist\n\nof \nLearner\n objects, or even pass a mixed \nlist\n of \nLearner\n\nobjects and strings.\nMake sure that all learners have unique \nid\ns.\n\n\nlrns = list(\n  makeLearner(cl = \nclassif.ksvm\n, id = \nksvm1\n, sigma = 0.2, C = 2),\n  makeLearner(cl = \nclassif.ksvm\n, id = \nksvm2\n, sigma = 0.1, C = 1),\n  \nclassif.randomForest\n\n)\nrin = makeResampleDesc(method = \nCV\n, iters = 5)\nlc = generateLearningCurveData(learners = lrns, task = sonar.task,\n  percs = seq(0.1, 1, by = 0.1), measures = acc,\n  resampling = rin, show.info = FALSE)\nplotLearningCurve(lc)\n\n\n\n\n\n\nWe can display performance on the train set as well as the test set:\n\n\nrin2 = makeResampleDesc(method = \nCV\n, iters = 5, predict = \nboth\n)\nlc2 = generateLearningCurveData(learners = lrns, task = sonar.task,\n  percs = seq(0.1, 1, by = 0.1),\n  measures = list(acc, setAggregation(acc, train.mean)), resampling = rin2,\n  show.info = FALSE)\nplotLearningCurve(lc2, facet = \nlearner\n)\n\n\n\n\n\n\nThere is also an experimental \nggvis\n plotting function, \nplotLearningCurveGGVIS\n. Instead of the \nfacet\n\nargument to \nplotLearningCurve\n there is an argument \ninteraction\n which plays a similar role. As subplots\nare not available in \nggvis\n, measures or learners are mapped to an interactive sidebar which allows selection\nof the displayed measures or learners. The other feature is mapped to color.\n\n\nplotLearningCurveGGVIS(lc2, interaction = \nmeasure\n)\nplotLearningCurveGGVIS(lc2, interaction = \nlearner\n)\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nr = generateLearningCurveData( \n  learners = c(\nclassif.rpart\n, \nclassif.knn\n), \n  task = sonar.task, \n  percs = seq(0.1, 1, by = 0.2), \n  measures = list(tp, fp, tn, fn), \n  resampling = makeResampleDesc(method = \nCV\n, iters = 5), \n  show.info = FALSE) \nplotLearningCurve(r) \nlrns = list( \n  makeLearner(cl = \nclassif.ksvm\n, id = \nksvm1\n, sigma = 0.2, C = 2), \n  makeLearner(cl = \nclassif.ksvm\n, id = \nksvm2\n, sigma = 0.1, C = 1), \n  \nclassif.randomForest\n \n) \nrin = makeResampleDesc(method = \nCV\n, iters = 5) \nlc = generateLearningCurveData(learners = lrns, task = sonar.task, \n  percs = seq(0.1, 1, by = 0.1), measures = acc, \n  resampling = rin, show.info = FALSE) \nplotLearningCurve(lc) \nrin2 = makeResampleDesc(method = \nCV\n, iters = 5, predict = \nboth\n) \nlc2 = generateLearningCurveData(learners = lrns, task = sonar.task, \n  percs = seq(0.1, 1, by = 0.1), \n  measures = list(acc, setAggregation(acc, train.mean)), resampling = rin2, \n  show.info = FALSE) \nplotLearningCurve(lc2, facet = \nlearner\n) \n## plotLearningCurveGGVIS(lc2, interaction = \nmeasure\n) \n## plotLearningCurveGGVIS(lc2, interaction = \nlearner\n)", 
            "title": "Learning Curves"
        }, 
        {
            "location": "/learning_curve/index.html#learning-curve-analysis", 
            "text": "To analyze how the increase of observations in the training set improves the performance of\na learner the  learning curve  is an appropriate visual tool.\nThe experiment is conducted with an increasing subsample size and the performance is measured.\nIn the plot the x-axis represents the relative subsample size whereas the y-axis represents\nthe performance.  Note that this function internally uses  benchmark  in combination with  makeDownsampleWrapper ,\nso for every run new observations are drawn.\nThus the results are noisy.\nTo reduce noise increase the number of resampling iterations.\nYou can define the resampling method in the  resampling  argument of  generateLearningCurveData .\nIt is also possible to pass a  ResampleInstance  (which is a result\nof  makeResampleInstance ) to make resampling consistent for all passed learners and each\nstep of increasing the number of observations.", 
            "title": "Learning Curve Analysis"
        }, 
        {
            "location": "/learning_curve/index.html#plotting-the-learning-curve", 
            "text": "The  mlr  function  generateLearningCurveData  can generate the data for  learning curves \nfor multiple  learners  and multiple  performance measures \nat once.\nWith  plotLearningCurve  the result of  generateLearningCurveData  can be plotted using  ggplot2 . plotLearningCurve  has an argument  facet  which can be either  \"measure\"  or  \"learner\" . By default facet = \"measure\"  and facetted subplots are created for each measure input to  generateLearningCurveData .\nIf  facet = \"measure\"  learners are mapped to color, and vice versa.  r = generateLearningCurveData(\n  learners = c( classif.rpart ,  classif.knn ),\n  task = sonar.task,\n  percs = seq(0.1, 1, by = 0.2),\n  measures = list(tp, fp, tn, fn),\n  resampling = makeResampleDesc(method =  CV , iters = 5),\n  show.info = FALSE)\nplotLearningCurve(r)   What happens in  generateLearningCurveData  is the following:\nEach learner will be internally wrapped in a  DownsampleWrapper .\nTo measure the performance at the first step of  percs , say  0.1 , first the data will be\nsplit into a  training  and a  test set  according to the given  resampling strategy .\nThen a random sample containing 10% of the observations of the  training set  will be drawn\nand used to train the learner.\nThe performance will be measured on the  complete test set .\nThese steps will be repeated as defined by the given  resampling method  and for each value\nof  percs .  In the first example we simply passed a vector of learner names to [generateLearningCurveData].\nAs usual, you can also create the learners beforehand and provide a  list \nof  Learner  objects, or even pass a mixed  list  of  Learner \nobjects and strings.\nMake sure that all learners have unique  id s.  lrns = list(\n  makeLearner(cl =  classif.ksvm , id =  ksvm1 , sigma = 0.2, C = 2),\n  makeLearner(cl =  classif.ksvm , id =  ksvm2 , sigma = 0.1, C = 1),\n   classif.randomForest \n)\nrin = makeResampleDesc(method =  CV , iters = 5)\nlc = generateLearningCurveData(learners = lrns, task = sonar.task,\n  percs = seq(0.1, 1, by = 0.1), measures = acc,\n  resampling = rin, show.info = FALSE)\nplotLearningCurve(lc)   We can display performance on the train set as well as the test set:  rin2 = makeResampleDesc(method =  CV , iters = 5, predict =  both )\nlc2 = generateLearningCurveData(learners = lrns, task = sonar.task,\n  percs = seq(0.1, 1, by = 0.1),\n  measures = list(acc, setAggregation(acc, train.mean)), resampling = rin2,\n  show.info = FALSE)\nplotLearningCurve(lc2, facet =  learner )   There is also an experimental  ggvis  plotting function,  plotLearningCurveGGVIS . Instead of the  facet \nargument to  plotLearningCurve  there is an argument  interaction  which plays a similar role. As subplots\nare not available in  ggvis , measures or learners are mapped to an interactive sidebar which allows selection\nof the displayed measures or learners. The other feature is mapped to color.  plotLearningCurveGGVIS(lc2, interaction =  measure )\nplotLearningCurveGGVIS(lc2, interaction =  learner )", 
            "title": "Plotting the learning curve"
        }, 
        {
            "location": "/learning_curve/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  r = generateLearningCurveData( \n  learners = c( classif.rpart ,  classif.knn ), \n  task = sonar.task, \n  percs = seq(0.1, 1, by = 0.2), \n  measures = list(tp, fp, tn, fn), \n  resampling = makeResampleDesc(method =  CV , iters = 5), \n  show.info = FALSE) \nplotLearningCurve(r) \nlrns = list( \n  makeLearner(cl =  classif.ksvm , id =  ksvm1 , sigma = 0.2, C = 2), \n  makeLearner(cl =  classif.ksvm , id =  ksvm2 , sigma = 0.1, C = 1), \n   classif.randomForest  \n) \nrin = makeResampleDesc(method =  CV , iters = 5) \nlc = generateLearningCurveData(learners = lrns, task = sonar.task, \n  percs = seq(0.1, 1, by = 0.1), measures = acc, \n  resampling = rin, show.info = FALSE) \nplotLearningCurve(lc) \nrin2 = makeResampleDesc(method =  CV , iters = 5, predict =  both ) \nlc2 = generateLearningCurveData(learners = lrns, task = sonar.task, \n  percs = seq(0.1, 1, by = 0.1), \n  measures = list(acc, setAggregation(acc, train.mean)), resampling = rin2, \n  show.info = FALSE) \nplotLearningCurve(lc2, facet =  learner ) \n## plotLearningCurveGGVIS(lc2, interaction =  measure ) \n## plotLearningCurveGGVIS(lc2, interaction =  learner )", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/partial_dependence/index.html", 
            "text": "Exploring Learner Predictions\n\n\nLearners use features to learn a prediction function and make predictions, but the effect of those features is often not apparent.\n\nmlr\n can estimate the partial dependence of a learned function on a subset of the feature space using\n\ngeneratePartialDependenceData\n.\n\n\nPartial dependence plots reduce the potentially high dimensional function estimated by the\nlearner, and display a marginalized version of this function in a lower dimensional space.\nFor example suppose \nY = f(X) + \\epsilon\n, where \n\\mathbb{E}[\\epsilon|X] = 0\n. With \n(X, Y)\n\npairs drawn independently from this statistical model, a learner may estimate \n\\hat{f}\n, which,\nif \nX\n is high dimensional, can be uninterpretable. Suppose we want to approximate the relationship\nbetween some subset of \nX\n. We partition \nX\n into two sets, \nX_s\n and \nX_c\n such that\n\nX = X_s \\cup X_c\n, where \nX_s\n is a subset of \nX\n of interest.\n\n\nThe partial dependence of \nf\n on \nX_s\n is\n\n\n\n\nf_{X_s} = \\mathbb{E}_{X_c}f(X_s, X_c).\n\n\n\n\n\n\nX_c\n is integrated out. We use the following estimator:\n\n\n\n\n\\hat{f}_{X_s} = \\frac{1}{N} \\sum_{i = 1}^N \\hat{f}(X_s, x_{ic}).\n\n\n\n\nThe individual conditional expectation of an observation can also be estimated using the above\nalgorithm absent the averaging, giving \n\\hat{f}^{(i)}_{X_s}\n. This allows the discovery of\nfeatures of \n\\hat{f}\n that may be obscured by an aggregated summary of \n\\hat{f}\n.\n\n\nThe partial derivative of the partial dependence function, \n\\frac{\\partial \\hat{f}_{X_s}}{\\partial X_s}\n,\nand the individual conditional expectation function, \n\\frac{\\partial \\hat{f}^{(i)}_{X_s}}{\\partial X_s}\n,\ncan also be computed. For regression and survival tasks the partial derivative of a single\nfeature \nX_s\n is the gradient of the partial dependence function, and for classification tasks\nwhere the learner can output class probabilities the Jacobian. Note that if the learner produces\ndiscontinuous partial dependence (e.g., piecewise constant functions such as decision trees,\nensembles of decision trees, etc.) the derivative will be 0 (where the function is not changing)\nor trending towards positive or negative infinity (at the discontinuities where the derivative\nis undefined). Plotting the partial dependence function of such learners may give the impression\nthat the function is not discontinuous because the prediction grid is not composed of all\ndiscontinuous points in the predictor space. This results in a line interpolating that makes\nthe function appear to be piecewise linear (where the derivative would be defined except at\nthe boundaries of each piece).\n\n\nThe partial derivative can be informative regarding the additivity of the learned function\nin certain features. If \n\\hat{f}^{(i)}_{X_s}\n is an additive function in a feature \nX_s\n,\nthen its partial derivative will not depend on any other features (\nX_c\n) that may have been\nused by the learner. Variation in the estimated partial derivative indicates that there is a\nregion of interaction between \nX_s\n and \nX_c\n in \n\\hat{f}\n. Similarly, instead of using the mean to estimate\nthe expected value of the function at different values of \nX_s\n, instead computing the variance\ncan highlight regions of interaction between \nX_s\n and \nX_c\n.\n\n\nSee \nGoldstein, Kapelner, Bleich, and Pitkin (2014)\n for more\ndetails and their package \nICEbox\n for the original implementation. The algorithm works for\nany supervised learner with classification, regression, and survival tasks.\n\n\nGenerating partial dependences\n\n\nOur implementation, following \nmlr\n's \nvisualization\n pattern, consists\nof the above mentioned function \ngeneratePartialDependenceData\n, as well as two visualization\nfunctions, \nplotPartialDependence\n and \nplotPartialDependenceGGVIS\n. The former generates\ninput (objects of class \nPartialDependenceData\n) for the latter.\n\n\nThe first step executed by \ngeneratePartialDependenceData\n is to generate a feature grid for\nevery element of the character vector \nfeatures\n passed. The data are given by the \ninput\n\nargument, which can be a \nTask\n or a \ndata.frame\n. The feature grid can be generated in\nseveral ways. A uniformly spaced grid of length \ngridsize\n (default 10) from the empirical\nminimum to the empirical maximum is created by default, but arguments \nfmin\n and \nfmax\n may\nbe used to override the empirical default (the lengths of \nfmin\n and \nfmax\n must match the\nlength of \nfeatures\n). Alternatively the feature data can be resampled, either by using a\nbootstrap or by subsampling.\n\n\nlrn.classif = makeLearner(\nclassif.ksvm\n, predict.type = \nprob\n)\nfit.classif = train(lrn.classif, iris.task)\npd = generatePartialDependenceData(fit.classif, iris.task, \nPetal.Width\n)\npd\n#\n PartialDependenceData\n#\n Task: iris_example\n#\n Features: Petal.Width\n#\n Target: setosa, versicolor, virginica\n#\n Derivative: FALSE\n#\n Interaction: FALSE\n#\n Individual: FALSE\n#\n    Class Probability Petal.Width\n#\n 1 setosa   0.4983925   0.1000000\n#\n 2 setosa   0.4441165   0.3666667\n#\n 3 setosa   0.3808075   0.6333333\n#\n 4 setosa   0.3250243   0.9000000\n#\n 5 setosa   0.2589014   1.1666667\n#\n 6 setosa   0.1870692   1.4333333\n#\n ... (#rows: 30, #cols: 3)\n\n\n\n\nAs noted above, \nX_s\n does not have to be unidimensional. If it is not, the \ninteraction\n\nflag must be set to \nTRUE\n. Then the individual feature grids are combined using the Cartesian\nproduct, and the estimator above is applied, producing the partial dependence for every combination\nof unique feature values. If the \ninteraction\n flag is \nFALSE\n (the default) then by default\n\nX_s\n is assumed unidimensional, and partial dependencies are generated for each feature separately.\nThe resulting output when \ninteraction = FALSE\n has a column for each feature, and \nNA\n where\nthe feature was not used.\n\n\npd.lst = generatePartialDependenceData(fit.classif, iris.task, c(\nPetal.Width\n, \nPetal.Length\n), FALSE)\nhead(pd.lst$data)\n#\n    Class Probability Petal.Width Petal.Length\n#\n 1 setosa   0.4983925   0.1000000           NA\n#\n 2 setosa   0.4441165   0.3666667           NA\n#\n 3 setosa   0.3808075   0.6333333           NA\n#\n 4 setosa   0.3250243   0.9000000           NA\n#\n 5 setosa   0.2589014   1.1666667           NA\n#\n 6 setosa   0.1870692   1.4333333           NA\n\ntail(pd.lst$data)\n#\n        Class Probability Petal.Width Petal.Length\n#\n 55 virginica   0.2006336          NA     3.622222\n#\n 56 virginica   0.3114545          NA     4.277778\n#\n 57 virginica   0.4404613          NA     4.933333\n#\n 58 virginica   0.6005358          NA     5.588889\n#\n 59 virginica   0.7099841          NA     6.244444\n#\n 60 virginica   0.7242584          NA     6.900000\n\n\n\n\npd.int = generatePartialDependenceData(fit.classif, iris.task, c(\nPetal.Width\n, \nPetal.Length\n), TRUE)\npd.int\n#\n PartialDependenceData\n#\n Task: iris_example\n#\n Features: Petal.Width, Petal.Length\n#\n Target: setosa, versicolor, virginica\n#\n Derivative: FALSE\n#\n Interaction: TRUE\n#\n Individual: FALSE\n#\n    Class Probability Petal.Width Petal.Length\n#\n 1 setosa   0.6885025   0.1000000            1\n#\n 2 setosa   0.6824560   0.3666667            1\n#\n 3 setosa   0.6459476   0.6333333            1\n#\n 4 setosa   0.5750861   0.9000000            1\n#\n 5 setosa   0.4745925   1.1666667            1\n#\n 6 setosa   0.3749285   1.4333333            1\n#\n ... (#rows: 300, #cols: 4)\n\n\n\n\nAt each step in the estimation of \n\\hat{f}_{X_s}\n a set of predictions of length \nN\n is generated.\nBy default the mean prediction is used. For classification where \npredict.type = \"prob\"\n this\nentails the mean class probabilities. However, other summaries of the predictions may be used.\nFor regression and survival tasks the function used here must either return one number or three,\nand, if the latter, the numbers must be sorted lowest to highest. For classification tasks\nthe function must return a number for each level of the target feature.\n\n\nAs noted, the \nfun\n argument can be a function which returns three numbers (sorted low to high)\nfor a regression task. This allows further exploration of relative feature importance. If a\nfeature is relatively important, the bounds are necessarily tighter because the feature accounts\nfor more of the variance of the predictions, i.e., it is \"used\" more by the learner. More directly\nsetting \nfun = var\n identifies regions of interaction between \nX_s\n and \nX_c\n.\n\n\nlrn.regr = makeLearner(\nregr.ksvm\n)\nfit.regr = train(lrn.regr, bh.task)\npd.regr = generatePartialDependenceData(fit.regr, bh.task, \nlstat\n, fun = median)\npd.regr\n#\n PartialDependenceData\n#\n Task: BostonHousing-example\n#\n Features: lstat\n#\n Target: medv\n#\n Derivative: FALSE\n#\n Interaction: FALSE\n#\n Individual: FALSE\n#\n       medv     lstat\n#\n 1 24.69031  1.730000\n#\n 2 23.72479  5.756667\n#\n 3 22.34841  9.783333\n#\n 4 20.78817 13.810000\n#\n 5 19.76183 17.836667\n#\n 6 19.33115 21.863333\n#\n ... (#rows: 10, #cols: 2)\n\n\n\n\npd.ci = generatePartialDependenceData(fit.regr, bh.task, \nlstat\n,\n  fun = function(x) quantile(x, c(.25, .5, .75)))\npd.ci\n#\n PartialDependenceData\n#\n Task: BostonHousing-example\n#\n Features: lstat\n#\n Target: medv\n#\n Derivative: FALSE\n#\n Interaction: FALSE\n#\n Individual: FALSE\n#\n       medv     lstat    lower    upper\n#\n 1 24.69031  1.730000 21.36068 29.75615\n#\n 2 23.72479  5.756667 20.80590 28.02338\n#\n 3 22.34841  9.783333 20.06507 25.22291\n#\n 4 20.78817 13.810000 18.55592 23.68100\n#\n 5 19.76183 17.836667 16.52737 22.98520\n#\n 6 19.33115 21.863333 15.14425 22.12766\n#\n ... (#rows: 10, #cols: 4)\n\n\n\n\npd.classif = generatePartialDependenceData(fit.classif, iris.task, \nPetal.Length\n, fun = median)\npd.classif\n#\n PartialDependenceData\n#\n Task: iris_example\n#\n Features: Petal.Length\n#\n Target: setosa, versicolor, virginica\n#\n Derivative: FALSE\n#\n Interaction: FALSE\n#\n Individual: FALSE\n#\n    Class Probability Petal.Length\n#\n 1 setosa  0.31008788     1.000000\n#\n 2 setosa  0.24271454     1.655556\n#\n 3 setosa  0.17126036     2.311111\n#\n 4 setosa  0.09380787     2.966667\n#\n 5 setosa  0.04579912     3.622222\n#\n 6 setosa  0.02455344     4.277778\n#\n ... (#rows: 30, #cols: 3)\n\n\n\n\nIn addition to bounds based on a summary of the distribution of the conditional expectation of each observation, learners which can estimate the variance of their predictions can also be used. The argument \nbounds\n is a numeric vector of length two which is added (so the first number should be negative) to the point prediction to produce a confidence interval for the partial dependence. The default is the .025 and .975 quantiles of the Gaussian distribution.\n\n\nfit.se = train(makeLearner(\nregr.randomForest\n, predict.type = \nse\n), bh.task)\npd.se = generatePartialDependenceData(fit.se, bh.task, c(\nlstat\n, \ncrim\n))\nhead(pd.se$data)\n#\n       medv     lstat crim    lower    upper\n#\n 1 31.45061  1.730000   NA 12.66252 50.23870\n#\n 2 26.07468  5.756667   NA 14.45816 37.69120\n#\n 3 23.50402  9.783333   NA 13.53661 33.47144\n#\n 4 22.09801 13.810000   NA 14.25053 29.94550\n#\n 5 20.44824 17.836667   NA 12.87242 28.02405\n#\n 6 19.82650 21.863333   NA 11.85056 27.80244\n\ntail(pd.se$data)\n#\n        medv lstat     crim    lower    upper\n#\n 15 21.85229    NA 39.54849 10.72496 32.97962\n#\n 16 21.83147    NA 49.43403 10.67659 32.98635\n#\n 17 21.81572    NA 59.31957 10.63741 32.99402\n#\n 18 21.79497    NA 69.20512 10.55183 33.03810\n#\n 19 21.79698    NA 79.09066 10.55663 33.03733\n#\n 20 21.79698    NA 88.97620 10.55663 33.03733\n\n\n\n\nAs previously mentioned if the aggregation function is not used, i.e., it is the identity,\nthen the conditional expectation of \n\\hat{f}^{(i)}_{X_s}\n is estimated. If \nindividual = TRUE\n\nthen \ngeneratePartialDependenceData\n returns \nn\n partial dependence estimates made at each point in\nthe prediction grid constructed from the features.\n\n\npd.ind.regr = generatePartialDependenceData(fit.regr, bh.task, \nlstat\n, individual = TRUE)\npd.ind.regr\n#\n PartialDependenceData\n#\n Task: BostonHousing-example\n#\n Features: lstat\n#\n Target: medv\n#\n Derivative: FALSE\n#\n Interaction: FALSE\n#\n Individual: TRUE\n#\n Predictions centered: FALSE\n#\n       medv     lstat idx\n#\n 1 25.66995  1.730000   1\n#\n 2 24.71747  5.756667   1\n#\n 3 23.64157  9.783333   1\n#\n 4 22.70812 13.810000   1\n#\n 5 22.00059 17.836667   1\n#\n 6 21.46195 21.863333   1\n#\n ... (#rows: 5060, #cols: 3)\n\n\n\n\nThe resulting output, particularly the element \ndata\n in the returned object, has an additional\ncolumn \nidx\n which gives the index of the observation to which the row pertains.\n\n\nFor classification tasks this index references both the class and the observation index.\n\n\npd.ind.classif = generatePartialDependenceData(fit.classif, iris.task, \nPetal.Length\n, individual = TRUE)\npd.ind.classif\n#\n PartialDependenceData\n#\n Task: iris_example\n#\n Features: Petal.Length\n#\n Target: setosa, versicolor, virginica\n#\n Derivative: FALSE\n#\n Interaction: FALSE\n#\n Individual: TRUE\n#\n Predictions centered: FALSE\n#\n    Class Probability Petal.Length      idx\n#\n 1 setosa   0.9814053            1 1.setosa\n#\n 2 setosa   0.9747355            1 2.setosa\n#\n 3 setosa   0.9815516            1 3.setosa\n#\n 4 setosa   0.9795761            1 4.setosa\n#\n 5 setosa   0.9806494            1 5.setosa\n#\n 6 setosa   0.9758763            1 6.setosa\n#\n ... (#rows: 4500, #cols: 4)\n\n\n\n\nIndividual estimates of partial dependence can also be centered by predictions made at all \nn\n observations\nfor a particular point in the prediction grid created by the features. This is controlled by\nthe argument \ncenter\n which is a list of the same length as the length of the \nfeatures\n\nargument and contains the values of the \nfeatures\n desired.\n\n\niris = getTaskData(iris.task)\npd.ind.classif = generatePartialDependenceData(fit.classif, iris.task, \nPetal.Length\n, individual = TRUE,\n  center = list(\nPetal.Length\n = min(iris$Petal.Length)))\n\n\n\n\nPartial derivatives can also be computed for individual partial dependence estimates and aggregate\npartial dependence. This is restricted to a single feature at a time. The derivatives of\nindividual partial dependence estimates can be useful in finding regions of interaction between the\nfeature for which the derivative is estimated and the features excluded.\n\n\npd.regr.der = generatePartialDependenceData(fit.regr, bh.task, \nlstat\n, derivative = TRUE)\nhead(pd.regr.der$data)\n#\n         medv     lstat\n#\n 1 -0.1792626  1.730000\n#\n 2 -0.3584207  5.756667\n#\n 3 -0.4557666  9.783333\n#\n 4 -0.4523905 13.810000\n#\n 5 -0.3700880 17.836667\n#\n 6 -0.2471346 21.863333\n\n\n\n\npd.regr.der.ind = generatePartialDependenceData(fit.regr, bh.task, \nlstat\n, derivative = TRUE,\n  individual = TRUE)\nhead(pd.regr.der.ind$data)\n#\n         medv     lstat idx\n#\n 1 -0.1931323  1.730000   1\n#\n 2 -0.2656911  5.756667   1\n#\n 3 -0.2571006  9.783333   1\n#\n 4 -0.2033080 13.810000   1\n#\n 5 -0.1511472 17.836667   1\n#\n 6 -0.1193129 21.863333   1\n\n\n\n\npd.classif.der = generatePartialDependenceData(fit.classif, iris.task, \nPetal.Width\n, derivative = TRUE)\nhead(pd.classif.der$data)\n#\n    Class Probability Petal.Width\n#\n 1 setosa  -0.1479385   0.1000000\n#\n 2 setosa  -0.2422728   0.3666667\n#\n 3 setosa  -0.2189893   0.6333333\n#\n 4 setosa  -0.2162803   0.9000000\n#\n 5 setosa  -0.2768042   1.1666667\n#\n 6 setosa  -0.2394176   1.4333333\n\n\n\n\npd.classif.der.ind = generatePartialDependenceData(fit.classif, iris.task, \nPetal.Width\n, derivative = TRUE,\n  individual = TRUE)\nhead(pd.classif.der.ind$data)\n#\n    Class Probability Petal.Width      idx\n#\n 1 setosa  0.02479474         0.1 1.setosa\n#\n 2 setosa  0.01710561         0.1 2.setosa\n#\n 3 setosa  0.01646252         0.1 3.setosa\n#\n 4 setosa  0.01530718         0.1 4.setosa\n#\n 5 setosa  0.02608577         0.1 5.setosa\n#\n 6 setosa  0.03925531         0.1 6.setosa\n\n\n\n\nFunctional ANOVA\n\n\nHooker (2004)\n proposed the decomposition of a learned function \n\\hat{f}\n as a sum of lower dimensional functions \nf(\\mathbf{x}) = g_0 + \\sum_{i = 1}^p g_{i}(X_i) + \\sum_{i \\neq j} g_{ij}(x_{ij}) + \\ldots\n where \np\n is the number of features. \ngenerateFunctionalANOVAData\n estimates the individual \ng\n functions using partial dependence. When functions depend only on one feature, they are equivalent to partial dependence, but a \ng\n function which depends on more than one feature is the \"effect\" of only those features: lower dimensional \"effects\" are removed.\n\n\n\n\n\\hat{g}_u(X) = \\frac{1}{N} \\sum_{i = 1}^N \\left( \\hat{f}(X) - \\sum_{v \\subset u} g_v(X) \\right)\n\n\n\n\nHere \nu\n is a subset of \n{1, \\ldots, p}\n. When \n|v| = 1\n\n\ng_v\n can be directly computed by computing the bivariate partial dependence of \n\\hat{f}\n on \nX_u\n and then subtracting off the univariate partial dependences of the features contained in \nv\n.\n\n\nAlthough this decomposition is generalizable to classification it is currently only available for regression tasks.\n\n\nlrn.regr = makeLearner(\nregr.ksvm\n)\nfit.regr = train(lrn.regr, bh.task)\n\nfa = generateFunctionalANOVAData(fit.regr, bh.task, \nlstat\n, depth = 1, fun = median)\nfa\n#\n FunctionalANOVAData\n#\n Task: BostonHousing-example\n#\n Features: lstat\n#\n Target: medv\n#\n \n#\n \n#\n   effect     medv     lstat\n#\n 1  lstat 24.91250  1.730000\n#\n 2  lstat 23.73349  5.756667\n#\n 3  lstat 22.35740  9.783333\n#\n 4  lstat 20.71107 13.810000\n#\n 5  lstat 19.62082 17.836667\n#\n 6  lstat 19.04515 21.863333\n#\n ... (#rows: 10, #cols: 3)\n\npd.regr = generatePartialDependenceData(fit.regr, bh.task, \nlstat\n, fun = median)\npd.regr\n#\n PartialDependenceData\n#\n Task: BostonHousing-example\n#\n Features: lstat\n#\n Target: medv\n#\n Derivative: FALSE\n#\n Interaction: FALSE\n#\n Individual: FALSE\n#\n       medv     lstat\n#\n 1 24.91250  1.730000\n#\n 2 23.73349  5.756667\n#\n 3 22.35740  9.783333\n#\n 4 20.71107 13.810000\n#\n 5 19.62082 17.836667\n#\n 6 19.04515 21.863333\n#\n ... (#rows: 10, #cols: 2)\n\n\n\n\nThe \ndepth\n argument is similar to the \ninteraction\n argument in \ngeneratePartialDependenceData\n but instead of specifying whether all of joint \"effect\" of all the \nfeatures\n is computed, it determines whether \"effects\" of all subsets of the features given the specified \ndepth\n are computed. So, for example, with \np\n features and depth 1, the univariate partial dependence is returned. If, instead, \ndepth = 2\n, then all possible bivariate functional ANOVA effects are returned. This is done by computing the univariate partial dependence for each feature and subtracting it from the bivariate partial dependence for each possible pair.\n\n\nfa.bv = generateFunctionalANOVAData(fit.regr, bh.task, c(\ncrim\n, \nlstat\n, \nage\n),\n  depth = 2)\nfa.bv\n#\n FunctionalANOVAData\n#\n Task: BostonHousing-example\n#\n Features: crim, lstat, age\n#\n Target: medv\n#\n \n#\n \n#\n       effect      medv      crim lstat age\n#\n 1 crim:lstat -22.69831  0.006320  1.73  NA\n#\n 2 crim:lstat -23.22083  9.891862  1.73  NA\n#\n 3 crim:lstat -24.84978 19.777404  1.73  NA\n#\n 4 crim:lstat -26.52861 29.662947  1.73  NA\n#\n 5 crim:lstat -27.62138 39.548489  1.73  NA\n#\n 6 crim:lstat -28.21985 49.434031  1.73  NA\n#\n ... (#rows: 300, #cols: 5)\n\nnames(table(fa.bv$data$effect)) ## interaction effects estimated\n#\n [1] \ncrim:age\n   \ncrim:lstat\n \nlstat:age\n\n\n\n\n\nPlotting partial dependences\n\n\nResults from \ngeneratePartialDependenceData\n and \ngenerateFunctionalANOVAData\n can be visualized with \nplotPartialDependence\n\nand \nplotPartialDependenceGGVIS\n.\n\n\nWith one feature and a regression task the output is a line plot, with a point for each point\nin the corresponding feature's grid.\n\n\nplotPartialDependence(pd.regr)\n\n\n\n\n\n\nWith a classification task, a line is drawn for each class, which gives the estimated partial\nprobability of that class for a particular point in the feature grid.\n\n\nplotPartialDependence(pd.classif)\n\n\n\n\n\n\nFor regression tasks, when the \nfun\n argument of \ngeneratePartialDependenceData\n is used,\nthe bounds will automatically be displayed using a gray ribbon.\n\n\nplotPartialDependence(pd.ci)\n\n\n\n\n\n\nThe same goes for plots of partial dependences where the learner has \npredict.type = \"se\"\n.\n\n\nplotPartialDependence(pd.se)\n\n\n\n\n\n\nWhen multiple features are passed to \ngeneratePartialDependenceData\n but \ninteraction = FALSE\n,\nfacetting is used to display each estimated bivariate relationship.\n\n\nplotPartialDependence(pd.lst)\n\n\n\n\n\n\nWhen \ninteraction = TRUE\n in the call to \ngeneratePartialDependenceData\n, one variable must\nbe chosen to be used for facetting, and a subplot for each value in the chosen feature's grid\nis created, wherein the other feature's partial dependences within the facetting feature's\nvalue are shown. Note that this type of plot is limited to two features.\n\n\nplotPartialDependence(pd.int, facet = \nPetal.Length\n)\n\n\n\n\n\n\nplotPartialDependenceGGVIS\n can be used similarly, however, since \nggvis\n currently lacks\nsubplotting/facetting capabilities, the argument \ninteract\n maps one feature to an interactive\nsidebar where the user can select a value of one feature.\n\n\nplotPartialDependenceGGVIS(pd.int, interact = \nPetal.Length\n)\n\n\n\n\nWhen \nindividual = TRUE\n each individual conditional expectation curve is plotted.\n\n\nplotPartialDependence(pd.ind.regr)\n\n\n\n\n\n\nWhen the individual curves are centered by subtracting the individual conditional expectations\nestimated at a particular value of \nX_s\n this results in a fixed intercept which aids in\nvisualizing variation in predictions made by \n\\hat{f}^{(i)}_{X_s}\n.\n\n\nplotPartialDependence(pd.ind.classif)\n\n\n\n\n\n\nPlotting partial derivative functions works the same as partial dependence. Below are estimates\nof the derivative of the mean aggregated partial dependence function, and the individual\npartial dependence functions for a regression and a classification task respectively.\n\n\nplotPartialDependence(pd.regr.der)\n\n\n\n\n\n\nThis suggests that \n\\hat{f}\n is not additive in \nlstat\n except in the neighborhood of \n25\n.\n\n\nplotPartialDependence(pd.regr.der.ind)\n\n\n\n\n\n\nThis suggests that \nPetal.Width\n interacts with some other feature in the neighborhood of\n\n(1.5, 2)\n for classes \"virginica\" and \"versicolor\".\n\n\nplotPartialDependence(pd.classif.der.ind)\n\n\n\n\n\n\nOutput from \ngenerateFunctionalANOVAData\n can also be plotted using \nplotPartialDependence\n.\n\n\nfa = generateFunctionalANOVAData(fit.regr, bh.task, c(\ncrim\n, \nlstat\n), depth = 1)\nplotPartialDependence(fa)\n\n\n\n\n\n\nInteractions can often be more easily visualized by using functional ANOVA.\n\n\nfa.bv = generateFunctionalANOVAData(fit.regr, bh.task, c(\ncrim\n, \nlstat\n), depth = 2)\nplotPartialDependence(fa.bv, \ntile\n)", 
            "title": "Partial Dependence Plots"
        }, 
        {
            "location": "/partial_dependence/index.html#exploring-learner-predictions", 
            "text": "Learners use features to learn a prediction function and make predictions, but the effect of those features is often not apparent. mlr  can estimate the partial dependence of a learned function on a subset of the feature space using generatePartialDependenceData .  Partial dependence plots reduce the potentially high dimensional function estimated by the\nlearner, and display a marginalized version of this function in a lower dimensional space.\nFor example suppose  Y = f(X) + \\epsilon , where  \\mathbb{E}[\\epsilon|X] = 0 . With  (X, Y) \npairs drawn independently from this statistical model, a learner may estimate  \\hat{f} , which,\nif  X  is high dimensional, can be uninterpretable. Suppose we want to approximate the relationship\nbetween some subset of  X . We partition  X  into two sets,  X_s  and  X_c  such that X = X_s \\cup X_c , where  X_s  is a subset of  X  of interest.  The partial dependence of  f  on  X_s  is   f_{X_s} = \\mathbb{E}_{X_c}f(X_s, X_c).    X_c  is integrated out. We use the following estimator:   \\hat{f}_{X_s} = \\frac{1}{N} \\sum_{i = 1}^N \\hat{f}(X_s, x_{ic}).   The individual conditional expectation of an observation can also be estimated using the above\nalgorithm absent the averaging, giving  \\hat{f}^{(i)}_{X_s} . This allows the discovery of\nfeatures of  \\hat{f}  that may be obscured by an aggregated summary of  \\hat{f} .  The partial derivative of the partial dependence function,  \\frac{\\partial \\hat{f}_{X_s}}{\\partial X_s} ,\nand the individual conditional expectation function,  \\frac{\\partial \\hat{f}^{(i)}_{X_s}}{\\partial X_s} ,\ncan also be computed. For regression and survival tasks the partial derivative of a single\nfeature  X_s  is the gradient of the partial dependence function, and for classification tasks\nwhere the learner can output class probabilities the Jacobian. Note that if the learner produces\ndiscontinuous partial dependence (e.g., piecewise constant functions such as decision trees,\nensembles of decision trees, etc.) the derivative will be 0 (where the function is not changing)\nor trending towards positive or negative infinity (at the discontinuities where the derivative\nis undefined). Plotting the partial dependence function of such learners may give the impression\nthat the function is not discontinuous because the prediction grid is not composed of all\ndiscontinuous points in the predictor space. This results in a line interpolating that makes\nthe function appear to be piecewise linear (where the derivative would be defined except at\nthe boundaries of each piece).  The partial derivative can be informative regarding the additivity of the learned function\nin certain features. If  \\hat{f}^{(i)}_{X_s}  is an additive function in a feature  X_s ,\nthen its partial derivative will not depend on any other features ( X_c ) that may have been\nused by the learner. Variation in the estimated partial derivative indicates that there is a\nregion of interaction between  X_s  and  X_c  in  \\hat{f} . Similarly, instead of using the mean to estimate\nthe expected value of the function at different values of  X_s , instead computing the variance\ncan highlight regions of interaction between  X_s  and  X_c .  See  Goldstein, Kapelner, Bleich, and Pitkin (2014)  for more\ndetails and their package  ICEbox  for the original implementation. The algorithm works for\nany supervised learner with classification, regression, and survival tasks.", 
            "title": "Exploring Learner Predictions"
        }, 
        {
            "location": "/partial_dependence/index.html#generating-partial-dependences", 
            "text": "Our implementation, following  mlr 's  visualization  pattern, consists\nof the above mentioned function  generatePartialDependenceData , as well as two visualization\nfunctions,  plotPartialDependence  and  plotPartialDependenceGGVIS . The former generates\ninput (objects of class  PartialDependenceData ) for the latter.  The first step executed by  generatePartialDependenceData  is to generate a feature grid for\nevery element of the character vector  features  passed. The data are given by the  input \nargument, which can be a  Task  or a  data.frame . The feature grid can be generated in\nseveral ways. A uniformly spaced grid of length  gridsize  (default 10) from the empirical\nminimum to the empirical maximum is created by default, but arguments  fmin  and  fmax  may\nbe used to override the empirical default (the lengths of  fmin  and  fmax  must match the\nlength of  features ). Alternatively the feature data can be resampled, either by using a\nbootstrap or by subsampling.  lrn.classif = makeLearner( classif.ksvm , predict.type =  prob )\nfit.classif = train(lrn.classif, iris.task)\npd = generatePartialDependenceData(fit.classif, iris.task,  Petal.Width )\npd\n#  PartialDependenceData\n#  Task: iris_example\n#  Features: Petal.Width\n#  Target: setosa, versicolor, virginica\n#  Derivative: FALSE\n#  Interaction: FALSE\n#  Individual: FALSE\n#     Class Probability Petal.Width\n#  1 setosa   0.4983925   0.1000000\n#  2 setosa   0.4441165   0.3666667\n#  3 setosa   0.3808075   0.6333333\n#  4 setosa   0.3250243   0.9000000\n#  5 setosa   0.2589014   1.1666667\n#  6 setosa   0.1870692   1.4333333\n#  ... (#rows: 30, #cols: 3)  As noted above,  X_s  does not have to be unidimensional. If it is not, the  interaction \nflag must be set to  TRUE . Then the individual feature grids are combined using the Cartesian\nproduct, and the estimator above is applied, producing the partial dependence for every combination\nof unique feature values. If the  interaction  flag is  FALSE  (the default) then by default X_s  is assumed unidimensional, and partial dependencies are generated for each feature separately.\nThe resulting output when  interaction = FALSE  has a column for each feature, and  NA  where\nthe feature was not used.  pd.lst = generatePartialDependenceData(fit.classif, iris.task, c( Petal.Width ,  Petal.Length ), FALSE)\nhead(pd.lst$data)\n#     Class Probability Petal.Width Petal.Length\n#  1 setosa   0.4983925   0.1000000           NA\n#  2 setosa   0.4441165   0.3666667           NA\n#  3 setosa   0.3808075   0.6333333           NA\n#  4 setosa   0.3250243   0.9000000           NA\n#  5 setosa   0.2589014   1.1666667           NA\n#  6 setosa   0.1870692   1.4333333           NA\n\ntail(pd.lst$data)\n#         Class Probability Petal.Width Petal.Length\n#  55 virginica   0.2006336          NA     3.622222\n#  56 virginica   0.3114545          NA     4.277778\n#  57 virginica   0.4404613          NA     4.933333\n#  58 virginica   0.6005358          NA     5.588889\n#  59 virginica   0.7099841          NA     6.244444\n#  60 virginica   0.7242584          NA     6.900000  pd.int = generatePartialDependenceData(fit.classif, iris.task, c( Petal.Width ,  Petal.Length ), TRUE)\npd.int\n#  PartialDependenceData\n#  Task: iris_example\n#  Features: Petal.Width, Petal.Length\n#  Target: setosa, versicolor, virginica\n#  Derivative: FALSE\n#  Interaction: TRUE\n#  Individual: FALSE\n#     Class Probability Petal.Width Petal.Length\n#  1 setosa   0.6885025   0.1000000            1\n#  2 setosa   0.6824560   0.3666667            1\n#  3 setosa   0.6459476   0.6333333            1\n#  4 setosa   0.5750861   0.9000000            1\n#  5 setosa   0.4745925   1.1666667            1\n#  6 setosa   0.3749285   1.4333333            1\n#  ... (#rows: 300, #cols: 4)  At each step in the estimation of  \\hat{f}_{X_s}  a set of predictions of length  N  is generated.\nBy default the mean prediction is used. For classification where  predict.type = \"prob\"  this\nentails the mean class probabilities. However, other summaries of the predictions may be used.\nFor regression and survival tasks the function used here must either return one number or three,\nand, if the latter, the numbers must be sorted lowest to highest. For classification tasks\nthe function must return a number for each level of the target feature.  As noted, the  fun  argument can be a function which returns three numbers (sorted low to high)\nfor a regression task. This allows further exploration of relative feature importance. If a\nfeature is relatively important, the bounds are necessarily tighter because the feature accounts\nfor more of the variance of the predictions, i.e., it is \"used\" more by the learner. More directly\nsetting  fun = var  identifies regions of interaction between  X_s  and  X_c .  lrn.regr = makeLearner( regr.ksvm )\nfit.regr = train(lrn.regr, bh.task)\npd.regr = generatePartialDependenceData(fit.regr, bh.task,  lstat , fun = median)\npd.regr\n#  PartialDependenceData\n#  Task: BostonHousing-example\n#  Features: lstat\n#  Target: medv\n#  Derivative: FALSE\n#  Interaction: FALSE\n#  Individual: FALSE\n#        medv     lstat\n#  1 24.69031  1.730000\n#  2 23.72479  5.756667\n#  3 22.34841  9.783333\n#  4 20.78817 13.810000\n#  5 19.76183 17.836667\n#  6 19.33115 21.863333\n#  ... (#rows: 10, #cols: 2)  pd.ci = generatePartialDependenceData(fit.regr, bh.task,  lstat ,\n  fun = function(x) quantile(x, c(.25, .5, .75)))\npd.ci\n#  PartialDependenceData\n#  Task: BostonHousing-example\n#  Features: lstat\n#  Target: medv\n#  Derivative: FALSE\n#  Interaction: FALSE\n#  Individual: FALSE\n#        medv     lstat    lower    upper\n#  1 24.69031  1.730000 21.36068 29.75615\n#  2 23.72479  5.756667 20.80590 28.02338\n#  3 22.34841  9.783333 20.06507 25.22291\n#  4 20.78817 13.810000 18.55592 23.68100\n#  5 19.76183 17.836667 16.52737 22.98520\n#  6 19.33115 21.863333 15.14425 22.12766\n#  ... (#rows: 10, #cols: 4)  pd.classif = generatePartialDependenceData(fit.classif, iris.task,  Petal.Length , fun = median)\npd.classif\n#  PartialDependenceData\n#  Task: iris_example\n#  Features: Petal.Length\n#  Target: setosa, versicolor, virginica\n#  Derivative: FALSE\n#  Interaction: FALSE\n#  Individual: FALSE\n#     Class Probability Petal.Length\n#  1 setosa  0.31008788     1.000000\n#  2 setosa  0.24271454     1.655556\n#  3 setosa  0.17126036     2.311111\n#  4 setosa  0.09380787     2.966667\n#  5 setosa  0.04579912     3.622222\n#  6 setosa  0.02455344     4.277778\n#  ... (#rows: 30, #cols: 3)  In addition to bounds based on a summary of the distribution of the conditional expectation of each observation, learners which can estimate the variance of their predictions can also be used. The argument  bounds  is a numeric vector of length two which is added (so the first number should be negative) to the point prediction to produce a confidence interval for the partial dependence. The default is the .025 and .975 quantiles of the Gaussian distribution.  fit.se = train(makeLearner( regr.randomForest , predict.type =  se ), bh.task)\npd.se = generatePartialDependenceData(fit.se, bh.task, c( lstat ,  crim ))\nhead(pd.se$data)\n#        medv     lstat crim    lower    upper\n#  1 31.45061  1.730000   NA 12.66252 50.23870\n#  2 26.07468  5.756667   NA 14.45816 37.69120\n#  3 23.50402  9.783333   NA 13.53661 33.47144\n#  4 22.09801 13.810000   NA 14.25053 29.94550\n#  5 20.44824 17.836667   NA 12.87242 28.02405\n#  6 19.82650 21.863333   NA 11.85056 27.80244\n\ntail(pd.se$data)\n#         medv lstat     crim    lower    upper\n#  15 21.85229    NA 39.54849 10.72496 32.97962\n#  16 21.83147    NA 49.43403 10.67659 32.98635\n#  17 21.81572    NA 59.31957 10.63741 32.99402\n#  18 21.79497    NA 69.20512 10.55183 33.03810\n#  19 21.79698    NA 79.09066 10.55663 33.03733\n#  20 21.79698    NA 88.97620 10.55663 33.03733  As previously mentioned if the aggregation function is not used, i.e., it is the identity,\nthen the conditional expectation of  \\hat{f}^{(i)}_{X_s}  is estimated. If  individual = TRUE \nthen  generatePartialDependenceData  returns  n  partial dependence estimates made at each point in\nthe prediction grid constructed from the features.  pd.ind.regr = generatePartialDependenceData(fit.regr, bh.task,  lstat , individual = TRUE)\npd.ind.regr\n#  PartialDependenceData\n#  Task: BostonHousing-example\n#  Features: lstat\n#  Target: medv\n#  Derivative: FALSE\n#  Interaction: FALSE\n#  Individual: TRUE\n#  Predictions centered: FALSE\n#        medv     lstat idx\n#  1 25.66995  1.730000   1\n#  2 24.71747  5.756667   1\n#  3 23.64157  9.783333   1\n#  4 22.70812 13.810000   1\n#  5 22.00059 17.836667   1\n#  6 21.46195 21.863333   1\n#  ... (#rows: 5060, #cols: 3)  The resulting output, particularly the element  data  in the returned object, has an additional\ncolumn  idx  which gives the index of the observation to which the row pertains.  For classification tasks this index references both the class and the observation index.  pd.ind.classif = generatePartialDependenceData(fit.classif, iris.task,  Petal.Length , individual = TRUE)\npd.ind.classif\n#  PartialDependenceData\n#  Task: iris_example\n#  Features: Petal.Length\n#  Target: setosa, versicolor, virginica\n#  Derivative: FALSE\n#  Interaction: FALSE\n#  Individual: TRUE\n#  Predictions centered: FALSE\n#     Class Probability Petal.Length      idx\n#  1 setosa   0.9814053            1 1.setosa\n#  2 setosa   0.9747355            1 2.setosa\n#  3 setosa   0.9815516            1 3.setosa\n#  4 setosa   0.9795761            1 4.setosa\n#  5 setosa   0.9806494            1 5.setosa\n#  6 setosa   0.9758763            1 6.setosa\n#  ... (#rows: 4500, #cols: 4)  Individual estimates of partial dependence can also be centered by predictions made at all  n  observations\nfor a particular point in the prediction grid created by the features. This is controlled by\nthe argument  center  which is a list of the same length as the length of the  features \nargument and contains the values of the  features  desired.  iris = getTaskData(iris.task)\npd.ind.classif = generatePartialDependenceData(fit.classif, iris.task,  Petal.Length , individual = TRUE,\n  center = list( Petal.Length  = min(iris$Petal.Length)))  Partial derivatives can also be computed for individual partial dependence estimates and aggregate\npartial dependence. This is restricted to a single feature at a time. The derivatives of\nindividual partial dependence estimates can be useful in finding regions of interaction between the\nfeature for which the derivative is estimated and the features excluded.  pd.regr.der = generatePartialDependenceData(fit.regr, bh.task,  lstat , derivative = TRUE)\nhead(pd.regr.der$data)\n#          medv     lstat\n#  1 -0.1792626  1.730000\n#  2 -0.3584207  5.756667\n#  3 -0.4557666  9.783333\n#  4 -0.4523905 13.810000\n#  5 -0.3700880 17.836667\n#  6 -0.2471346 21.863333  pd.regr.der.ind = generatePartialDependenceData(fit.regr, bh.task,  lstat , derivative = TRUE,\n  individual = TRUE)\nhead(pd.regr.der.ind$data)\n#          medv     lstat idx\n#  1 -0.1931323  1.730000   1\n#  2 -0.2656911  5.756667   1\n#  3 -0.2571006  9.783333   1\n#  4 -0.2033080 13.810000   1\n#  5 -0.1511472 17.836667   1\n#  6 -0.1193129 21.863333   1  pd.classif.der = generatePartialDependenceData(fit.classif, iris.task,  Petal.Width , derivative = TRUE)\nhead(pd.classif.der$data)\n#     Class Probability Petal.Width\n#  1 setosa  -0.1479385   0.1000000\n#  2 setosa  -0.2422728   0.3666667\n#  3 setosa  -0.2189893   0.6333333\n#  4 setosa  -0.2162803   0.9000000\n#  5 setosa  -0.2768042   1.1666667\n#  6 setosa  -0.2394176   1.4333333  pd.classif.der.ind = generatePartialDependenceData(fit.classif, iris.task,  Petal.Width , derivative = TRUE,\n  individual = TRUE)\nhead(pd.classif.der.ind$data)\n#     Class Probability Petal.Width      idx\n#  1 setosa  0.02479474         0.1 1.setosa\n#  2 setosa  0.01710561         0.1 2.setosa\n#  3 setosa  0.01646252         0.1 3.setosa\n#  4 setosa  0.01530718         0.1 4.setosa\n#  5 setosa  0.02608577         0.1 5.setosa\n#  6 setosa  0.03925531         0.1 6.setosa", 
            "title": "Generating partial dependences"
        }, 
        {
            "location": "/partial_dependence/index.html#functional-anova", 
            "text": "Hooker (2004)  proposed the decomposition of a learned function  \\hat{f}  as a sum of lower dimensional functions  f(\\mathbf{x}) = g_0 + \\sum_{i = 1}^p g_{i}(X_i) + \\sum_{i \\neq j} g_{ij}(x_{ij}) + \\ldots  where  p  is the number of features.  generateFunctionalANOVAData  estimates the individual  g  functions using partial dependence. When functions depend only on one feature, they are equivalent to partial dependence, but a  g  function which depends on more than one feature is the \"effect\" of only those features: lower dimensional \"effects\" are removed.   \\hat{g}_u(X) = \\frac{1}{N} \\sum_{i = 1}^N \\left( \\hat{f}(X) - \\sum_{v \\subset u} g_v(X) \\right)   Here  u  is a subset of  {1, \\ldots, p} . When  |v| = 1  g_v  can be directly computed by computing the bivariate partial dependence of  \\hat{f}  on  X_u  and then subtracting off the univariate partial dependences of the features contained in  v .  Although this decomposition is generalizable to classification it is currently only available for regression tasks.  lrn.regr = makeLearner( regr.ksvm )\nfit.regr = train(lrn.regr, bh.task)\n\nfa = generateFunctionalANOVAData(fit.regr, bh.task,  lstat , depth = 1, fun = median)\nfa\n#  FunctionalANOVAData\n#  Task: BostonHousing-example\n#  Features: lstat\n#  Target: medv\n#  \n#  \n#    effect     medv     lstat\n#  1  lstat 24.91250  1.730000\n#  2  lstat 23.73349  5.756667\n#  3  lstat 22.35740  9.783333\n#  4  lstat 20.71107 13.810000\n#  5  lstat 19.62082 17.836667\n#  6  lstat 19.04515 21.863333\n#  ... (#rows: 10, #cols: 3)\n\npd.regr = generatePartialDependenceData(fit.regr, bh.task,  lstat , fun = median)\npd.regr\n#  PartialDependenceData\n#  Task: BostonHousing-example\n#  Features: lstat\n#  Target: medv\n#  Derivative: FALSE\n#  Interaction: FALSE\n#  Individual: FALSE\n#        medv     lstat\n#  1 24.91250  1.730000\n#  2 23.73349  5.756667\n#  3 22.35740  9.783333\n#  4 20.71107 13.810000\n#  5 19.62082 17.836667\n#  6 19.04515 21.863333\n#  ... (#rows: 10, #cols: 2)  The  depth  argument is similar to the  interaction  argument in  generatePartialDependenceData  but instead of specifying whether all of joint \"effect\" of all the  features  is computed, it determines whether \"effects\" of all subsets of the features given the specified  depth  are computed. So, for example, with  p  features and depth 1, the univariate partial dependence is returned. If, instead,  depth = 2 , then all possible bivariate functional ANOVA effects are returned. This is done by computing the univariate partial dependence for each feature and subtracting it from the bivariate partial dependence for each possible pair.  fa.bv = generateFunctionalANOVAData(fit.regr, bh.task, c( crim ,  lstat ,  age ),\n  depth = 2)\nfa.bv\n#  FunctionalANOVAData\n#  Task: BostonHousing-example\n#  Features: crim, lstat, age\n#  Target: medv\n#  \n#  \n#        effect      medv      crim lstat age\n#  1 crim:lstat -22.69831  0.006320  1.73  NA\n#  2 crim:lstat -23.22083  9.891862  1.73  NA\n#  3 crim:lstat -24.84978 19.777404  1.73  NA\n#  4 crim:lstat -26.52861 29.662947  1.73  NA\n#  5 crim:lstat -27.62138 39.548489  1.73  NA\n#  6 crim:lstat -28.21985 49.434031  1.73  NA\n#  ... (#rows: 300, #cols: 5)\n\nnames(table(fa.bv$data$effect)) ## interaction effects estimated\n#  [1]  crim:age     crim:lstat   lstat:age", 
            "title": "Functional ANOVA"
        }, 
        {
            "location": "/partial_dependence/index.html#plotting-partial-dependences", 
            "text": "Results from  generatePartialDependenceData  and  generateFunctionalANOVAData  can be visualized with  plotPartialDependence \nand  plotPartialDependenceGGVIS .  With one feature and a regression task the output is a line plot, with a point for each point\nin the corresponding feature's grid.  plotPartialDependence(pd.regr)   With a classification task, a line is drawn for each class, which gives the estimated partial\nprobability of that class for a particular point in the feature grid.  plotPartialDependence(pd.classif)   For regression tasks, when the  fun  argument of  generatePartialDependenceData  is used,\nthe bounds will automatically be displayed using a gray ribbon.  plotPartialDependence(pd.ci)   The same goes for plots of partial dependences where the learner has  predict.type = \"se\" .  plotPartialDependence(pd.se)   When multiple features are passed to  generatePartialDependenceData  but  interaction = FALSE ,\nfacetting is used to display each estimated bivariate relationship.  plotPartialDependence(pd.lst)   When  interaction = TRUE  in the call to  generatePartialDependenceData , one variable must\nbe chosen to be used for facetting, and a subplot for each value in the chosen feature's grid\nis created, wherein the other feature's partial dependences within the facetting feature's\nvalue are shown. Note that this type of plot is limited to two features.  plotPartialDependence(pd.int, facet =  Petal.Length )   plotPartialDependenceGGVIS  can be used similarly, however, since  ggvis  currently lacks\nsubplotting/facetting capabilities, the argument  interact  maps one feature to an interactive\nsidebar where the user can select a value of one feature.  plotPartialDependenceGGVIS(pd.int, interact =  Petal.Length )  When  individual = TRUE  each individual conditional expectation curve is plotted.  plotPartialDependence(pd.ind.regr)   When the individual curves are centered by subtracting the individual conditional expectations\nestimated at a particular value of  X_s  this results in a fixed intercept which aids in\nvisualizing variation in predictions made by  \\hat{f}^{(i)}_{X_s} .  plotPartialDependence(pd.ind.classif)   Plotting partial derivative functions works the same as partial dependence. Below are estimates\nof the derivative of the mean aggregated partial dependence function, and the individual\npartial dependence functions for a regression and a classification task respectively.  plotPartialDependence(pd.regr.der)   This suggests that  \\hat{f}  is not additive in  lstat  except in the neighborhood of  25 .  plotPartialDependence(pd.regr.der.ind)   This suggests that  Petal.Width  interacts with some other feature in the neighborhood of (1.5, 2)  for classes \"virginica\" and \"versicolor\".  plotPartialDependence(pd.classif.der.ind)   Output from  generateFunctionalANOVAData  can also be plotted using  plotPartialDependence .  fa = generateFunctionalANOVAData(fit.regr, bh.task, c( crim ,  lstat ), depth = 1)\nplotPartialDependence(fa)   Interactions can often be more easily visualized by using functional ANOVA.  fa.bv = generateFunctionalANOVAData(fit.regr, bh.task, c( crim ,  lstat ), depth = 2)\nplotPartialDependence(fa.bv,  tile )", 
            "title": "Plotting partial dependences"
        }, 
        {
            "location": "/classifier_calibration/index.html", 
            "text": "Classifier Calibration\n\n\nA classifier is \"calibrated\" when the predicted probability of a class matches the expected\nfrequency of that class. \nmlr\n can visualize this by plotting estimated class probabilities\n(which are discretized) against the observed frequency of said class in the data using\n\ngenerateCalibrationData\n and \nplotCalibration\n.\n\n\ngenerateCalibrationData\n takes as input \nPrediction\n, \nResampleResult\n, \nBenchmarkResult\n,\nor a named list of \nPrediction\n or \nResampleResult\n objects on a classification (multiclass\nor binary) task with learner(s) that are capable of outputting probabiliites (i.e., learners\nmust be constructed with \npredict.type = \"prob\"\n). The result is an object of class\n\nCalibrationData\n which has elements \nproportion\n, \ndata\n, and\n\ntask\n. \nproportion\n gives the proportion of observations labelled with a given class for\neach predicted probability bin (e.g., for observations which are predicted to have class \"A\"\nwith probability \n(0, 0.1]\n, what is the proportion of said observations which have class \"A\"?).\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, task = sonar.task)\npred = predict(mod, task = sonar.task)\ncal = generateCalibrationData(pred)\ncal$proportion\n#\n      Learner       bin Class Proportion\n#\n 1 prediction (0.1,0.2]     M  0.1060606\n#\n 2 prediction (0.7,0.8]     M  0.7333333\n#\n 3 prediction   [0,0.1]     M  0.0000000\n#\n 4 prediction   (0.9,1]     M  0.9333333\n#\n 5 prediction (0.2,0.3]     M  0.2727273\n#\n 6 prediction (0.4,0.5]     M  0.4615385\n#\n 7 prediction (0.8,0.9]     M  0.0000000\n#\n 8 prediction (0.5,0.6]     M  0.0000000\n\n\n\n\nThe manner in which the predicted probabilities are discretized is controlled by two arguments:\n\nbreaks\n and \ngroups\n. By default \nbreaks = \"Sturges\"\n which uses the Sturges algorithm in\n\nhist\n. This argument can specify other algorithms available in\n\nhist\n, it can be a numeric vector specifying breakpoints for \ncut\n,\nor a single integer specifying the number of bins to create (which are evenly spaced).\nAlternatively, \ngroups\n can be set to a positive integer value (by default \ngroups = NULL\n)\nin which case \ncut2\n is used to create bins with an approximately equal number\nof observations in each bin.\n\n\ncal = generateCalibrationData(pred, groups = 3)\ncal$proportion\n#\n      Learner           bin Class Proportion\n#\n 1 prediction [0.000,0.267)     M 0.08860759\n#\n 2 prediction [0.267,0.925)     M 0.51282051\n#\n 3 prediction [0.925,1.000]     M 0.93333333\n\n\n\n\nCalibrationData\n objects can be plotted using \nplotCalibration\n.\n\nplotCalibration\n by default plots a reference line which shows perfect calibration and a\n\"rag\" plot, which is a rug plot on the top and bottom of the graph, where the top pertains\nto \"positive\" cases, where the predicted class matches the observed class, and the bottom\npertains to \"negative\" cases, where the predicted class does not match the observed class.\nPerfect classifier performance would result in all the positive cases clustering in the top\nright (i.e., the correct classes are predicted with high probability) and the negative cases\nclustering in the bottom left.\n\n\nplotCalibration(cal)\n\n\n\n\n\n\nBecause of the discretization of the probabilities, sometimes it is advantageous to smooth\nthe calibration plot. Though \nsmooth = FALSE\n by default, setting this option to \nTRUE\n\nreplaces the estimated proportions with a loess smoother.\n\n\ncal = generateCalibrationData(pred)\nplotCalibration(cal, smooth = TRUE)\n\n\n\n\n\n\nAll of the above functionality works with multi-class classification as well.\n\n\nlrns = list(\n  makeLearner(\nclassif.randomForest\n, predict.type = \nprob\n),\n  makeLearner(\nclassif.nnet\n, predict.type = \nprob\n, trace = FALSE)\n)\nmod = lapply(lrns, train, task = iris.task)\npred = lapply(mod, predict, task = iris.task)\nnames(pred) = c(\nrandomForest\n, \nnnet\n)\ncal = generateCalibrationData(pred, breaks = c(0, .3, .6, 1))\nplotCalibration(cal)\n\n\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n) \nmod = train(lrn, task = sonar.task) \npred = predict(mod, task = sonar.task) \ncal = generateCalibrationData(pred) \ncal$proportion \ncal = generateCalibrationData(pred, groups = 3) \ncal$proportion \nplotCalibration(cal) \ncal = generateCalibrationData(pred) \nplotCalibration(cal, smooth = TRUE) \nlrns = list( \n  makeLearner(\nclassif.randomForest\n, predict.type = \nprob\n), \n  makeLearner(\nclassif.nnet\n, predict.type = \nprob\n, trace = FALSE) \n) \nmod = lapply(lrns, train, task = iris.task) \npred = lapply(mod, predict, task = iris.task) \nnames(pred) = c(\nrandomForest\n, \nnnet\n) \ncal = generateCalibrationData(pred, breaks = c(0, .3, .6, 1)) \nplotCalibration(cal)", 
            "title": "Classifier Calibration Plots"
        }, 
        {
            "location": "/classifier_calibration/index.html#classifier-calibration", 
            "text": "A classifier is \"calibrated\" when the predicted probability of a class matches the expected\nfrequency of that class.  mlr  can visualize this by plotting estimated class probabilities\n(which are discretized) against the observed frequency of said class in the data using generateCalibrationData  and  plotCalibration .  generateCalibrationData  takes as input  Prediction ,  ResampleResult ,  BenchmarkResult ,\nor a named list of  Prediction  or  ResampleResult  objects on a classification (multiclass\nor binary) task with learner(s) that are capable of outputting probabiliites (i.e., learners\nmust be constructed with  predict.type = \"prob\" ). The result is an object of class CalibrationData  which has elements  proportion ,  data , and task .  proportion  gives the proportion of observations labelled with a given class for\neach predicted probability bin (e.g., for observations which are predicted to have class \"A\"\nwith probability  (0, 0.1] , what is the proportion of said observations which have class \"A\"?).  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, task = sonar.task)\npred = predict(mod, task = sonar.task)\ncal = generateCalibrationData(pred)\ncal$proportion\n#       Learner       bin Class Proportion\n#  1 prediction (0.1,0.2]     M  0.1060606\n#  2 prediction (0.7,0.8]     M  0.7333333\n#  3 prediction   [0,0.1]     M  0.0000000\n#  4 prediction   (0.9,1]     M  0.9333333\n#  5 prediction (0.2,0.3]     M  0.2727273\n#  6 prediction (0.4,0.5]     M  0.4615385\n#  7 prediction (0.8,0.9]     M  0.0000000\n#  8 prediction (0.5,0.6]     M  0.0000000  The manner in which the predicted probabilities are discretized is controlled by two arguments: breaks  and  groups . By default  breaks = \"Sturges\"  which uses the Sturges algorithm in hist . This argument can specify other algorithms available in hist , it can be a numeric vector specifying breakpoints for  cut ,\nor a single integer specifying the number of bins to create (which are evenly spaced).\nAlternatively,  groups  can be set to a positive integer value (by default  groups = NULL )\nin which case  cut2  is used to create bins with an approximately equal number\nof observations in each bin.  cal = generateCalibrationData(pred, groups = 3)\ncal$proportion\n#       Learner           bin Class Proportion\n#  1 prediction [0.000,0.267)     M 0.08860759\n#  2 prediction [0.267,0.925)     M 0.51282051\n#  3 prediction [0.925,1.000]     M 0.93333333  CalibrationData  objects can be plotted using  plotCalibration . plotCalibration  by default plots a reference line which shows perfect calibration and a\n\"rag\" plot, which is a rug plot on the top and bottom of the graph, where the top pertains\nto \"positive\" cases, where the predicted class matches the observed class, and the bottom\npertains to \"negative\" cases, where the predicted class does not match the observed class.\nPerfect classifier performance would result in all the positive cases clustering in the top\nright (i.e., the correct classes are predicted with high probability) and the negative cases\nclustering in the bottom left.  plotCalibration(cal)   Because of the discretization of the probabilities, sometimes it is advantageous to smooth\nthe calibration plot. Though  smooth = FALSE  by default, setting this option to  TRUE \nreplaces the estimated proportions with a loess smoother.  cal = generateCalibrationData(pred)\nplotCalibration(cal, smooth = TRUE)   All of the above functionality works with multi-class classification as well.  lrns = list(\n  makeLearner( classif.randomForest , predict.type =  prob ),\n  makeLearner( classif.nnet , predict.type =  prob , trace = FALSE)\n)\nmod = lapply(lrns, train, task = iris.task)\npred = lapply(mod, predict, task = iris.task)\nnames(pred) = c( randomForest ,  nnet )\ncal = generateCalibrationData(pred, breaks = c(0, .3, .6, 1))\nplotCalibration(cal)", 
            "title": "Classifier Calibration"
        }, 
        {
            "location": "/classifier_calibration/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  lrn = makeLearner( classif.rpart , predict.type =  prob ) \nmod = train(lrn, task = sonar.task) \npred = predict(mod, task = sonar.task) \ncal = generateCalibrationData(pred) \ncal$proportion \ncal = generateCalibrationData(pred, groups = 3) \ncal$proportion \nplotCalibration(cal) \ncal = generateCalibrationData(pred) \nplotCalibration(cal, smooth = TRUE) \nlrns = list( \n  makeLearner( classif.randomForest , predict.type =  prob ), \n  makeLearner( classif.nnet , predict.type =  prob , trace = FALSE) \n) \nmod = lapply(lrns, train, task = iris.task) \npred = lapply(mod, predict, task = iris.task) \nnames(pred) = c( randomForest ,  nnet ) \ncal = generateCalibrationData(pred, breaks = c(0, .3, .6, 1)) \nplotCalibration(cal)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/hyperpar_tuning_effects/index.html", 
            "text": "Evaluating Hyperparameter Tuning\n\n\nAs mentioned on the \nTuning\n tutorial page, tuning a machine learning algorithm\ntypically involves:\n\n\n\n\nthe hyperparameter search space:\n\n\n\n\n## ex: create a search space for the C hyperparameter from 0.01 to 0.1\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = 0.01, upper = 0.1)\n)\n\n\n\n\n\n\nthe optimization algorithm (aka tuning method):\n\n\n\n\n## ex: random search with 100 iterations\nctrl = makeTuneControlRandom(maxit = 100L)\n\n\n\n\n\n\nan evaluation method, i.e., a resampling strategy and a performance measure:\n\n\n\n\n## ex: 2-fold CV\nrdesc = makeResampleDesc(\nCV\n, iters = 2L)\n\n\n\n\nAfter tuning, you may want to evaluate the tuning process in order to answer questions such as:\n\n\n\n\nHow does varying the value of a hyperparameter change the performance of the machine learning\n  algorithm?\n\n\nWhat's the relative importance of each hyperparameter?\n\n\nHow did the optimization algorithm (prematurely) converge?\n\n\n\n\nmlr\n provides methods to generate and plot the data in order to evaluate the effect of\nhyperparameter tuning.\n\n\nGenerating hyperparameter tuning data\n\n\nmlr\n separates the generation of the data from the plotting of the data in case the user\nwishes to use the data in a custom way downstream.\n\n\nThe \ngenerateHyperParsEffectData\n method takes the tuning result along with 2 additional\narguments: \ntrafo\n and \ninclude.diagnostics\n. The \ntrafo\n argument will convert the\nhyperparameter data to be on the transformed scale in case a transformation was used when\ncreating the parameter (as in the case below). The \ninclude.diagnostics\n argument will tell\n\nmlr\n whether to include the eol and any error messages from the learner.\n\n\nBelow we perform random search on the \nC\n parameter for SVM on the famous\n\nPima Indians\n dataset.\nWe generate the hyperparameter effect data so that the \nC\n parameter is on the transformed\nscale and we do not include diagnostic data:\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlRandom(maxit = 100L)\nrdesc = makeResampleDesc(\nCV\n, iters = 2L)\nres = tuneParams(\nclassif.ksvm\n, task = pid.task, control = ctrl,\n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE)\ngenerateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE)\n#\n HyperParsEffectData:\n#\n Hyperparameters: C\n#\n Measures: acc.test.mean,mmce.test.mean\n#\n Optimizer: TuneControlRandom\n#\n Nested CV Used: FALSE\n#\n Snapshot of data:\n#\n            C acc.test.mean mmce.test.mean iteration exec.time\n#\n 1  0.3770897     0.7695312      0.2304688         1     0.063\n#\n 2  3.4829323     0.7526042      0.2473958         2     0.062\n#\n 3  2.2050176     0.7630208      0.2369792         3     0.061\n#\n 4 24.9285221     0.7070312      0.2929688         4     0.068\n#\n 5  0.2092395     0.7539062      0.2460938         5     0.071\n#\n 6  0.1495099     0.7395833      0.2604167         6     0.067\n\n\n\n\nAs a reminder from the \nresampling\n tutorial, if we wanted to generate data on\nthe training set as well as the validation set, we only need to make a few minor changes:\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlRandom(maxit = 100L)\nrdesc = makeResampleDesc(\nCV\n, iters = 2L, predict = \nboth\n)\nres = tuneParams(\nclassif.ksvm\n, task = pid.task, control = ctrl,\n  measures = list(acc, setAggregation(acc, train.mean), mmce, setAggregation(mmce,\n    train.mean)), resampling = rdesc, par.set = ps, show.info = FALSE)\ngenerateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE)\n#\n HyperParsEffectData:\n#\n Hyperparameters: C\n#\n Measures: acc.test.mean,acc.train.mean,mmce.test.mean,mmce.train.mean\n#\n Optimizer: TuneControlRandom\n#\n Nested CV Used: FALSE\n#\n Snapshot of data:\n#\n            C acc.test.mean acc.train.mean mmce.test.mean mmce.train.mean\n#\n 1 0.03518875     0.6510417      0.6510417      0.3489583       0.3489583\n#\n 2 0.17104229     0.7356771      0.7721354      0.2643229       0.2278646\n#\n 3 4.35326556     0.7304688      0.8828125      0.2695312       0.1171875\n#\n 4 0.33644238     0.7486979      0.8138021      0.2513021       0.1861979\n#\n 5 1.28168692     0.7500000      0.8476562      0.2500000       0.1523438\n#\n 6 7.36607693     0.7239583      0.8932292      0.2760417       0.1067708\n#\n   iteration exec.time\n#\n 1         1     0.103\n#\n 2         2     0.095\n#\n 3         3     0.088\n#\n 4         4     0.087\n#\n 5         5     0.087\n#\n 6         6     0.087\n\n\n\n\nIn the example below, we perform grid search on the \nC\n parameter for SVM on the Pima Indians\ndataset using nested cross validation. We generate the hyperparameter effect data so that\nthe \nC\n parameter is on the untransformed scale and we do not include diagnostic data. As\nyou can see below, nested cross validation is supported without any extra work by the user,\nallowing the user to obtain an unbiased estimator for the performance.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc(\nCV\n, iters = 2L)\nlrn = makeTuneWrapper(\nclassif.ksvm\n, control = ctrl,\n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE)\nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE)\ngenerateHyperParsEffectData(res)\n#\n HyperParsEffectData:\n#\n Hyperparameters: C\n#\n Measures: acc.test.mean,mmce.test.mean\n#\n Optimizer: TuneControlGrid\n#\n Nested CV Used: TRUE\n#\n Snapshot of data:\n#\n            C acc.test.mean mmce.test.mean iteration exec.time\n#\n 1 -5.0000000     0.6640625      0.3359375         1     0.046\n#\n 2 -3.8888889     0.6640625      0.3359375         2     0.044\n#\n 3 -2.7777778     0.6822917      0.3177083         3     0.044\n#\n 4 -1.6666667     0.7473958      0.2526042         4     0.045\n#\n 5 -0.5555556     0.7708333      0.2291667         5     0.044\n#\n 6  0.5555556     0.7682292      0.2317708         6     0.045\n#\n   nested_cv_run\n#\n 1             1\n#\n 2             1\n#\n 3             1\n#\n 4             1\n#\n 5             1\n#\n 6             1\n\n\n\n\nAfter generating the hyperparameter effect data, the next step is to visualize it. \nmlr\n\nhas several methods built-in to visualize the data, meant to support the needs of the\nresearcher and the engineer in industry. The next few sections will walk through the\nvisualization support for several use-cases.\n\n\nVisualizing the effect of a single hyperparameter\n\n\nIn a situation when the user is tuning a single hyperparameter for a learner, the user may\nwish to plot the performance of the learner against the values of the hyperparameter.\n\n\nIn the example below, we tune the number of clusters against the silhouette\nscore on the Pima dataset. We specify the x-axis with the \nx\n argument and the y-axis with\nthe \ny\n argument. If the \nplot.type\n argument is not specified, \nmlr\n will attempt to plot\na scatterplot by default. Since \nplotHyperParsEffect\n returns a \nggplot\n\nobject, we can easily customize it to our liking!\n\n\nps = makeParamSet(\n  makeDiscreteParam(\ncenters\n, values = 3:10)\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc(\nHoldout\n)\nres = tuneParams(\ncluster.kmeans\n, task = mtcars.task, control = ctrl,\n  measures = silhouette, resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x = \ncenters\n, y = \nsilhouette.test.mean\n)\n## add our own touches to the plot\nplt + geom_point(colour = \nred\n) +\n  ggtitle(\nEvaluating Number of Cluster Centers on mtcars\n) +\n  scale_x_continuous(breaks = 3:10) +\n  theme_bw()\n\n\n\n\n\n\nIn the example below, we tune SVM with the \nC\n hyperparameter on the Pima dataset. We will\nuse simulated annealing optimizer, so we are interested in seeing if the optimization\nalgorithm actually improves with iterations. By default, \nmlr\n only plots improvements to\nthe global optimum.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlGenSA(budget = 100L)\nrdesc = makeResampleDesc(\nHoldout\n)\nres = tuneParams(\nclassif.ksvm\n, task = pid.task, control = ctrl,\n  resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x = \niteration\n, y = \nmmce.test.mean\n,\n  plot.type = \nline\n)\nplt + ggtitle(\nAnalyzing convergence of simulated annealing\n) +\n  theme_minimal()\n\n\n\n\n\n\nIn the case of a learner crash, \nmlr\n will impute the crash with the worst value graphically\nand indicate the point. In the example below, we give the \nC\n parameter negative values,\nwhich will result in a learner crash for SVM.\n\n\nps = makeParamSet(\n  makeDiscreteParam(\nC\n, values = c(-1, -0.5, 0.5, 1, 1.5))\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc(\nCV\n, iters = 2L)\nres = tuneParams(\nclassif.ksvm\n, task = pid.task, control = ctrl,\n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x = \nC\n, y = \nacc.test.mean\n)\nplt + ggtitle(\nSVM learner crashes with negative C\n) +\n  theme_bw()\n\n\n\n\n\n\nThe example below uses \nnested cross validation\n with an outer loop\nof 2 runs. \nmlr\n indicates each run within the visualization.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc(\nHoldout\n)\nlrn = makeTuneWrapper(\nclassif.ksvm\n, control = ctrl,\n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE)\nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplotHyperParsEffect(data, x = \nC\n, y = \nacc.test.mean\n, plot.type = \nline\n)\n\n\n\n\n\n\nVisualizing the effect of 2 hyperparameters\n\n\nIn the case of tuning 2 hyperparameters simultaneously, \nmlr\n provides the ability to plot\na heatmap and contour plot in addition to a scatterplot or line.\n\n\nIn the example below, we tune the \nC\n and \nsigma\n parameters for SVM on the Pima dataset. We use\ninterpolation to produce a regular grid for plotting the heatmap. The \ninterpolation\n argument\naccepts any regression learner from \nmlr\n to perform the interpolation. The \nz\n argument\nwill be used to fill the heatmap or color lines, depending on the \nplot.type\n used.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeNumericParam(\nsigma\n, lower = -5, upper = 5, trafo = function(x) 2^x))\nctrl = makeTuneControlRandom(maxit = 100L)\nrdesc = makeResampleDesc(\nHoldout\n)\nlearn = makeLearner(\nclassif.ksvm\n, par.vals = list(kernel = \nrbfdot\n))\nres = tuneParams(learn, task = pid.task, control = ctrl, measures = acc,\n  resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n,\n  plot.type = \nheatmap\n, interpolate = \nregr.earth\n)\nmin_plt = min(data$data$acc.test.mean, na.rm = TRUE)\nmax_plt = max(data$data$acc.test.mean, na.rm = TRUE)\nmed_plt = mean(c(min_plt, max_plt))\nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),\n  low = \nblue\n, mid = \nwhite\n, high = \nred\n, midpoint = med_plt)\n\n\n\n\n\n\nWe can use the \nshow.experiments\n argument in order to visualize which points were\nspecifically passed to the learner in the original experiment and which points were\ninterpolated by \nmlr\n:\n\n\nplt = plotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n,\n  plot.type = \nheatmap\n, interpolate = \nregr.earth\n, show.experiments = TRUE)\nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),\n  low = \nblue\n, mid = \nwhite\n, high = \nred\n, midpoint = med_plt)\n\n\n\n\n\n\nWe can also visualize how long the optimizer takes to reach an optima for the same example:\n\n\nplotHyperParsEffect(data, x = \niteration\n, y = \nacc.test.mean\n,\n  plot.type = \nline\n)\n\n\n\n\n\n\nIn the case where we are tuning 2 hyperparameters and we have a learner crash, \nmlr\n will\nindicate the respective points and impute them with the worst value. In the example below,\nwe tune \nC\n and \nsigma\n, forcing \nC\n to be negative for some instances which will crash SVM.\nWe perform interpolation to get a regular grid in order to plot a heatmap. We can see that\nthe interpolation creates axis parallel lines resulting from the learner crashes.\n\n\nps = makeParamSet(\n  makeDiscreteParam(\nC\n, values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5)),\n  makeDiscreteParam(\nsigma\n, values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5)))\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc(\nHoldout\n)\nlearn = makeLearner(\nclassif.ksvm\n, par.vals = list(kernel = \nrbfdot\n))\nres = tuneParams(learn, task = pid.task, control = ctrl, measures = acc,\n  resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n,\n  plot.type = \nheatmap\n, interpolate = \nregr.earth\n)\n\n\n\n\n\n\nA slightly more complicated example is using nested cross validation while simultaneously\ntuning 2 hyperparameters. In order to plot a heatmap in this case, \nmlr\n will aggregate\neach of the nested runs by a user-specified function. The default function is \nmean\n. As\nexpected, we can still take advantage of interpolation.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeNumericParam(\nsigma\n, lower = -5, upper = 5, trafo = function(x) 2^x))\nctrl = makeTuneControlRandom(maxit = 100)\nrdesc = makeResampleDesc(\nHoldout\n)\nlearn = makeLearner(\nclassif.ksvm\n, par.vals = list(kernel = \nrbfdot\n))\nlrn = makeTuneWrapper(learn, control = ctrl, measures = list(acc, mmce),\n  resampling = rdesc, par.set = ps, show.info = FALSE)\nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n,\n  plot.type = \nheatmap\n, interpolate = \nregr.earth\n, show.experiments = TRUE,\n  nested.agg = mean)\nmin_plt = min(plt$data$acc.test.mean, na.rm = TRUE)\nmax_plt = max(plt$data$acc.test.mean, na.rm = TRUE)\nmed_plt = mean(c(min_plt, max_plt))\nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),\n  low = \nred\n, mid = \nwhite\n, high = \nblue\n, midpoint = med_plt)\n\n\n\n\n\n\nVisualizing the effects of more than 2 hyperparameters\n\n\nIn order to visualize the result when tuning 3 or more hyperparameters simultaneously we\ncan take advantage of \npartial dependence plots\n to show how the\nperformance depends on a one- or two-dimensional subset of the hyperparameters.\nBelow we tune three hyperparameters \nC\n, \nsigma\n, and \ndegree\n of an SVM with Bessel kernel\nand set the \npartial.dep\n flag to \nTRUE\n to indicate that we intend to calculate partial\ndependences.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeNumericParam(\nsigma\n, lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeDiscreteParam(\ndegree\n, values = 2:5))\nctrl = makeTuneControlRandom(maxit = 100L)\nrdesc = makeResampleDesc(\nHoldout\n, predict = \nboth\n)\nlearn = makeLearner(\nclassif.ksvm\n, par.vals = list(kernel = \nbesseldot\n))\nres = tuneParams(learn, task = pid.task, control = ctrl,\n  measures = list(acc, setAggregation(acc, train.mean)), resampling = rdesc,\n  par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res, partial.dep = TRUE)\n\n\n\n\nYou can generate a plot for a single hyperparameter like \nC\n as shown below.\nThe \npartial.dep.learn\n can be any regression \nLearner\n in \nmlr\n and is used\nto regress the attained performance values on the values of the 3 hyperparameters visited\nduring tuning. The fitted model serves as basis for calculating partial dependences.\n\n\nplotHyperParsEffect(data, x = \nC\n, y = \nacc.test.mean\n, plot.type = \nline\n,\n  partial.dep.learn = \nregr.randomForest\n)\n\n\n\n\n\n\nWe can also look at two hyperparameters simultaneously, for example \nC\n and \nsigma\n.\n\n\nplotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n,\n  plot.type = \nheatmap\n, partial.dep.learn = \nregr.randomForest\n)\n\n\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\n## ex: create a search space for the C hyperparameter from 0.01 to 0.1 \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = 0.01, upper = 0.1) \n) \n## ex: random search with 100 iterations \nctrl = makeTuneControlRandom(maxit = 100L) \n## ex: 2-fold CV \nrdesc = makeResampleDesc(\nCV\n, iters = 2L) \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc(\nCV\n, iters = 2L) \nres = tuneParams(\nclassif.ksvm\n, task = pid.task, control = ctrl, \n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) \ngenerateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE) \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc(\nCV\n, iters = 2L, predict = \nboth\n) \nres = tuneParams(\nclassif.ksvm\n, task = pid.task, control = ctrl, \n  measures = list(acc, setAggregation(acc, train.mean), mmce, setAggregation(mmce, \n    train.mean)), resampling = rdesc, par.set = ps, show.info = FALSE) \ngenerateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE) \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc(\nCV\n, iters = 2L) \nlrn = makeTuneWrapper(\nclassif.ksvm\n, control = ctrl, \n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) \nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) \ngenerateHyperParsEffectData(res) \nps = makeParamSet( \n  makeDiscreteParam(\ncenters\n, values = 3:10) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc(\nHoldout\n) \nres = tuneParams(\ncluster.kmeans\n, task = mtcars.task, control = ctrl, \n  measures = silhouette, resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x = \ncenters\n, y = \nsilhouette.test.mean\n) \n## add our own touches to the plot \nplt + geom_point(colour = \nred\n) + \n  ggtitle(\nEvaluating Number of Cluster Centers on mtcars\n) + \n  scale_x_continuous(breaks = 3:10) + \n  theme_bw() \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlGenSA(budget = 100L) \nrdesc = makeResampleDesc(\nHoldout\n) \nres = tuneParams(\nclassif.ksvm\n, task = pid.task, control = ctrl, \n  resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x = \niteration\n, y = \nmmce.test.mean\n, \n  plot.type = \nline\n) \nplt + ggtitle(\nAnalyzing convergence of simulated annealing\n) + \n  theme_minimal() \nps = makeParamSet( \n  makeDiscreteParam(\nC\n, values = c(-1, -0.5, 0.5, 1, 1.5)) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc(\nCV\n, iters = 2L) \nres = tuneParams(\nclassif.ksvm\n, task = pid.task, control = ctrl, \n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x = \nC\n, y = \nacc.test.mean\n) \nplt + ggtitle(\nSVM learner crashes with negative C\n) + \n  theme_bw() \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc(\nHoldout\n) \nlrn = makeTuneWrapper(\nclassif.ksvm\n, control = ctrl, \n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) \nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplotHyperParsEffect(data, x = \nC\n, y = \nacc.test.mean\n, plot.type = \nline\n) \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x), \n  makeNumericParam(\nsigma\n, lower = -5, upper = 5, trafo = function(x) 2^x)) \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc(\nHoldout\n) \nlearn = makeLearner(\nclassif.ksvm\n, par.vals = list(kernel = \nrbfdot\n)) \nres = tuneParams(learn, task = pid.task, control = ctrl, measures = acc, \n  resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n, \n  plot.type = \nheatmap\n, interpolate = \nregr.earth\n) \nmin_plt = min(data$data$acc.test.mean, na.rm = TRUE) \nmax_plt = max(data$data$acc.test.mean, na.rm = TRUE) \nmed_plt = mean(c(min_plt, max_plt)) \nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5), \n  low = \nblue\n, mid = \nwhite\n, high = \nred\n, midpoint = med_plt) \nplt = plotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n, \n  plot.type = \nheatmap\n, interpolate = \nregr.earth\n, show.experiments = TRUE) \nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5), \n  low = \nblue\n, mid = \nwhite\n, high = \nred\n, midpoint = med_plt) \nplotHyperParsEffect(data, x = \niteration\n, y = \nacc.test.mean\n, \n  plot.type = \nline\n) \nps = makeParamSet( \n  makeDiscreteParam(\nC\n, values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5)), \n  makeDiscreteParam(\nsigma\n, values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5))) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc(\nHoldout\n) \nlearn = makeLearner(\nclassif.ksvm\n, par.vals = list(kernel = \nrbfdot\n)) \nres = tuneParams(learn, task = pid.task, control = ctrl, measures = acc, \n  resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n, \n  plot.type = \nheatmap\n, interpolate = \nregr.earth\n) \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x), \n  makeNumericParam(\nsigma\n, lower = -5, upper = 5, trafo = function(x) 2^x)) \nctrl = makeTuneControlRandom(maxit = 100) \nrdesc = makeResampleDesc(\nHoldout\n) \nlearn = makeLearner(\nclassif.ksvm\n, par.vals = list(kernel = \nrbfdot\n)) \nlrn = makeTuneWrapper(learn, control = ctrl, measures = list(acc, mmce), \n  resampling = rdesc, par.set = ps, show.info = FALSE) \nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n, \n  plot.type = \nheatmap\n, interpolate = \nregr.earth\n, show.experiments = TRUE, \n  nested.agg = mean) \nmin_plt = min(plt$data$acc.test.mean, na.rm = TRUE) \nmax_plt = max(plt$data$acc.test.mean, na.rm = TRUE) \nmed_plt = mean(c(min_plt, max_plt)) \nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5), \n  low = \nred\n, mid = \nwhite\n, high = \nblue\n, midpoint = med_plt) \nps = makeParamSet( \n  makeNumericParam(\nC\n, lower = -5, upper = 5, trafo = function(x) 2^x), \n  makeNumericParam(\nsigma\n, lower = -5, upper = 5, trafo = function(x) 2^x), \n  makeDiscreteParam(\ndegree\n, values = 2:5)) \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc(\nHoldout\n, predict = \nboth\n) \nlearn = makeLearner(\nclassif.ksvm\n, par.vals = list(kernel = \nbesseldot\n)) \nres = tuneParams(learn, task = pid.task, control = ctrl, \n  measures = list(acc, setAggregation(acc, train.mean)), resampling = rdesc, \n  par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res, partial.dep = TRUE) \nplotHyperParsEffect(data, x = \nC\n, y = \nacc.test.mean\n, plot.type = \nline\n, \n  partial.dep.learn = \nregr.randomForest\n) \nplotHyperParsEffect(data, x = \nC\n, y = \nsigma\n, z = \nacc.test.mean\n, \n  plot.type = \nheatmap\n, partial.dep.learn = \nregr.randomForest\n)", 
            "title": "Hyperparameter Tuning Effects"
        }, 
        {
            "location": "/hyperpar_tuning_effects/index.html#evaluating-hyperparameter-tuning", 
            "text": "As mentioned on the  Tuning  tutorial page, tuning a machine learning algorithm\ntypically involves:   the hyperparameter search space:   ## ex: create a search space for the C hyperparameter from 0.01 to 0.1\nps = makeParamSet(\n  makeNumericParam( C , lower = 0.01, upper = 0.1)\n)   the optimization algorithm (aka tuning method):   ## ex: random search with 100 iterations\nctrl = makeTuneControlRandom(maxit = 100L)   an evaluation method, i.e., a resampling strategy and a performance measure:   ## ex: 2-fold CV\nrdesc = makeResampleDesc( CV , iters = 2L)  After tuning, you may want to evaluate the tuning process in order to answer questions such as:   How does varying the value of a hyperparameter change the performance of the machine learning\n  algorithm?  What's the relative importance of each hyperparameter?  How did the optimization algorithm (prematurely) converge?   mlr  provides methods to generate and plot the data in order to evaluate the effect of\nhyperparameter tuning.", 
            "title": "Evaluating Hyperparameter Tuning"
        }, 
        {
            "location": "/hyperpar_tuning_effects/index.html#generating-hyperparameter-tuning-data", 
            "text": "mlr  separates the generation of the data from the plotting of the data in case the user\nwishes to use the data in a custom way downstream.  The  generateHyperParsEffectData  method takes the tuning result along with 2 additional\narguments:  trafo  and  include.diagnostics . The  trafo  argument will convert the\nhyperparameter data to be on the transformed scale in case a transformation was used when\ncreating the parameter (as in the case below). The  include.diagnostics  argument will tell mlr  whether to include the eol and any error messages from the learner.  Below we perform random search on the  C  parameter for SVM on the famous Pima Indians  dataset.\nWe generate the hyperparameter effect data so that the  C  parameter is on the transformed\nscale and we do not include diagnostic data:  ps = makeParamSet(\n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlRandom(maxit = 100L)\nrdesc = makeResampleDesc( CV , iters = 2L)\nres = tuneParams( classif.ksvm , task = pid.task, control = ctrl,\n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE)\ngenerateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE)\n#  HyperParsEffectData:\n#  Hyperparameters: C\n#  Measures: acc.test.mean,mmce.test.mean\n#  Optimizer: TuneControlRandom\n#  Nested CV Used: FALSE\n#  Snapshot of data:\n#             C acc.test.mean mmce.test.mean iteration exec.time\n#  1  0.3770897     0.7695312      0.2304688         1     0.063\n#  2  3.4829323     0.7526042      0.2473958         2     0.062\n#  3  2.2050176     0.7630208      0.2369792         3     0.061\n#  4 24.9285221     0.7070312      0.2929688         4     0.068\n#  5  0.2092395     0.7539062      0.2460938         5     0.071\n#  6  0.1495099     0.7395833      0.2604167         6     0.067  As a reminder from the  resampling  tutorial, if we wanted to generate data on\nthe training set as well as the validation set, we only need to make a few minor changes:  ps = makeParamSet(\n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlRandom(maxit = 100L)\nrdesc = makeResampleDesc( CV , iters = 2L, predict =  both )\nres = tuneParams( classif.ksvm , task = pid.task, control = ctrl,\n  measures = list(acc, setAggregation(acc, train.mean), mmce, setAggregation(mmce,\n    train.mean)), resampling = rdesc, par.set = ps, show.info = FALSE)\ngenerateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE)\n#  HyperParsEffectData:\n#  Hyperparameters: C\n#  Measures: acc.test.mean,acc.train.mean,mmce.test.mean,mmce.train.mean\n#  Optimizer: TuneControlRandom\n#  Nested CV Used: FALSE\n#  Snapshot of data:\n#             C acc.test.mean acc.train.mean mmce.test.mean mmce.train.mean\n#  1 0.03518875     0.6510417      0.6510417      0.3489583       0.3489583\n#  2 0.17104229     0.7356771      0.7721354      0.2643229       0.2278646\n#  3 4.35326556     0.7304688      0.8828125      0.2695312       0.1171875\n#  4 0.33644238     0.7486979      0.8138021      0.2513021       0.1861979\n#  5 1.28168692     0.7500000      0.8476562      0.2500000       0.1523438\n#  6 7.36607693     0.7239583      0.8932292      0.2760417       0.1067708\n#    iteration exec.time\n#  1         1     0.103\n#  2         2     0.095\n#  3         3     0.088\n#  4         4     0.087\n#  5         5     0.087\n#  6         6     0.087  In the example below, we perform grid search on the  C  parameter for SVM on the Pima Indians\ndataset using nested cross validation. We generate the hyperparameter effect data so that\nthe  C  parameter is on the untransformed scale and we do not include diagnostic data. As\nyou can see below, nested cross validation is supported without any extra work by the user,\nallowing the user to obtain an unbiased estimator for the performance.  ps = makeParamSet(\n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc( CV , iters = 2L)\nlrn = makeTuneWrapper( classif.ksvm , control = ctrl,\n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE)\nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE)\ngenerateHyperParsEffectData(res)\n#  HyperParsEffectData:\n#  Hyperparameters: C\n#  Measures: acc.test.mean,mmce.test.mean\n#  Optimizer: TuneControlGrid\n#  Nested CV Used: TRUE\n#  Snapshot of data:\n#             C acc.test.mean mmce.test.mean iteration exec.time\n#  1 -5.0000000     0.6640625      0.3359375         1     0.046\n#  2 -3.8888889     0.6640625      0.3359375         2     0.044\n#  3 -2.7777778     0.6822917      0.3177083         3     0.044\n#  4 -1.6666667     0.7473958      0.2526042         4     0.045\n#  5 -0.5555556     0.7708333      0.2291667         5     0.044\n#  6  0.5555556     0.7682292      0.2317708         6     0.045\n#    nested_cv_run\n#  1             1\n#  2             1\n#  3             1\n#  4             1\n#  5             1\n#  6             1  After generating the hyperparameter effect data, the next step is to visualize it.  mlr \nhas several methods built-in to visualize the data, meant to support the needs of the\nresearcher and the engineer in industry. The next few sections will walk through the\nvisualization support for several use-cases.", 
            "title": "Generating hyperparameter tuning data"
        }, 
        {
            "location": "/hyperpar_tuning_effects/index.html#visualizing-the-effect-of-a-single-hyperparameter", 
            "text": "In a situation when the user is tuning a single hyperparameter for a learner, the user may\nwish to plot the performance of the learner against the values of the hyperparameter.  In the example below, we tune the number of clusters against the silhouette\nscore on the Pima dataset. We specify the x-axis with the  x  argument and the y-axis with\nthe  y  argument. If the  plot.type  argument is not specified,  mlr  will attempt to plot\na scatterplot by default. Since  plotHyperParsEffect  returns a  ggplot \nobject, we can easily customize it to our liking!  ps = makeParamSet(\n  makeDiscreteParam( centers , values = 3:10)\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc( Holdout )\nres = tuneParams( cluster.kmeans , task = mtcars.task, control = ctrl,\n  measures = silhouette, resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x =  centers , y =  silhouette.test.mean )\n## add our own touches to the plot\nplt + geom_point(colour =  red ) +\n  ggtitle( Evaluating Number of Cluster Centers on mtcars ) +\n  scale_x_continuous(breaks = 3:10) +\n  theme_bw()   In the example below, we tune SVM with the  C  hyperparameter on the Pima dataset. We will\nuse simulated annealing optimizer, so we are interested in seeing if the optimization\nalgorithm actually improves with iterations. By default,  mlr  only plots improvements to\nthe global optimum.  ps = makeParamSet(\n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlGenSA(budget = 100L)\nrdesc = makeResampleDesc( Holdout )\nres = tuneParams( classif.ksvm , task = pid.task, control = ctrl,\n  resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x =  iteration , y =  mmce.test.mean ,\n  plot.type =  line )\nplt + ggtitle( Analyzing convergence of simulated annealing ) +\n  theme_minimal()   In the case of a learner crash,  mlr  will impute the crash with the worst value graphically\nand indicate the point. In the example below, we give the  C  parameter negative values,\nwhich will result in a learner crash for SVM.  ps = makeParamSet(\n  makeDiscreteParam( C , values = c(-1, -0.5, 0.5, 1, 1.5))\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc( CV , iters = 2L)\nres = tuneParams( classif.ksvm , task = pid.task, control = ctrl,\n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x =  C , y =  acc.test.mean )\nplt + ggtitle( SVM learner crashes with negative C ) +\n  theme_bw()   The example below uses  nested cross validation  with an outer loop\nof 2 runs.  mlr  indicates each run within the visualization.  ps = makeParamSet(\n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc( Holdout )\nlrn = makeTuneWrapper( classif.ksvm , control = ctrl,\n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE)\nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplotHyperParsEffect(data, x =  C , y =  acc.test.mean , plot.type =  line )", 
            "title": "Visualizing the effect of a single hyperparameter"
        }, 
        {
            "location": "/hyperpar_tuning_effects/index.html#visualizing-the-effect-of-2-hyperparameters", 
            "text": "In the case of tuning 2 hyperparameters simultaneously,  mlr  provides the ability to plot\na heatmap and contour plot in addition to a scatterplot or line.  In the example below, we tune the  C  and  sigma  parameters for SVM on the Pima dataset. We use\ninterpolation to produce a regular grid for plotting the heatmap. The  interpolation  argument\naccepts any regression learner from  mlr  to perform the interpolation. The  z  argument\nwill be used to fill the heatmap or color lines, depending on the  plot.type  used.  ps = makeParamSet(\n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeNumericParam( sigma , lower = -5, upper = 5, trafo = function(x) 2^x))\nctrl = makeTuneControlRandom(maxit = 100L)\nrdesc = makeResampleDesc( Holdout )\nlearn = makeLearner( classif.ksvm , par.vals = list(kernel =  rbfdot ))\nres = tuneParams(learn, task = pid.task, control = ctrl, measures = acc,\n  resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean ,\n  plot.type =  heatmap , interpolate =  regr.earth )\nmin_plt = min(data$data$acc.test.mean, na.rm = TRUE)\nmax_plt = max(data$data$acc.test.mean, na.rm = TRUE)\nmed_plt = mean(c(min_plt, max_plt))\nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),\n  low =  blue , mid =  white , high =  red , midpoint = med_plt)   We can use the  show.experiments  argument in order to visualize which points were\nspecifically passed to the learner in the original experiment and which points were\ninterpolated by  mlr :  plt = plotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean ,\n  plot.type =  heatmap , interpolate =  regr.earth , show.experiments = TRUE)\nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),\n  low =  blue , mid =  white , high =  red , midpoint = med_plt)   We can also visualize how long the optimizer takes to reach an optima for the same example:  plotHyperParsEffect(data, x =  iteration , y =  acc.test.mean ,\n  plot.type =  line )   In the case where we are tuning 2 hyperparameters and we have a learner crash,  mlr  will\nindicate the respective points and impute them with the worst value. In the example below,\nwe tune  C  and  sigma , forcing  C  to be negative for some instances which will crash SVM.\nWe perform interpolation to get a regular grid in order to plot a heatmap. We can see that\nthe interpolation creates axis parallel lines resulting from the learner crashes.  ps = makeParamSet(\n  makeDiscreteParam( C , values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5)),\n  makeDiscreteParam( sigma , values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5)))\nctrl = makeTuneControlGrid()\nrdesc = makeResampleDesc( Holdout )\nlearn = makeLearner( classif.ksvm , par.vals = list(kernel =  rbfdot ))\nres = tuneParams(learn, task = pid.task, control = ctrl, measures = acc,\n  resampling = rdesc, par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean ,\n  plot.type =  heatmap , interpolate =  regr.earth )   A slightly more complicated example is using nested cross validation while simultaneously\ntuning 2 hyperparameters. In order to plot a heatmap in this case,  mlr  will aggregate\neach of the nested runs by a user-specified function. The default function is  mean . As\nexpected, we can still take advantage of interpolation.  ps = makeParamSet(\n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeNumericParam( sigma , lower = -5, upper = 5, trafo = function(x) 2^x))\nctrl = makeTuneControlRandom(maxit = 100)\nrdesc = makeResampleDesc( Holdout )\nlearn = makeLearner( classif.ksvm , par.vals = list(kernel =  rbfdot ))\nlrn = makeTuneWrapper(learn, control = ctrl, measures = list(acc, mmce),\n  resampling = rdesc, par.set = ps, show.info = FALSE)\nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE)\ndata = generateHyperParsEffectData(res)\nplt = plotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean ,\n  plot.type =  heatmap , interpolate =  regr.earth , show.experiments = TRUE,\n  nested.agg = mean)\nmin_plt = min(plt$data$acc.test.mean, na.rm = TRUE)\nmax_plt = max(plt$data$acc.test.mean, na.rm = TRUE)\nmed_plt = mean(c(min_plt, max_plt))\nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),\n  low =  red , mid =  white , high =  blue , midpoint = med_plt)", 
            "title": "Visualizing the effect of 2 hyperparameters"
        }, 
        {
            "location": "/hyperpar_tuning_effects/index.html#visualizing-the-effects-of-more-than-2-hyperparameters", 
            "text": "In order to visualize the result when tuning 3 or more hyperparameters simultaneously we\ncan take advantage of  partial dependence plots  to show how the\nperformance depends on a one- or two-dimensional subset of the hyperparameters.\nBelow we tune three hyperparameters  C ,  sigma , and  degree  of an SVM with Bessel kernel\nand set the  partial.dep  flag to  TRUE  to indicate that we intend to calculate partial\ndependences.  ps = makeParamSet(\n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeNumericParam( sigma , lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeDiscreteParam( degree , values = 2:5))\nctrl = makeTuneControlRandom(maxit = 100L)\nrdesc = makeResampleDesc( Holdout , predict =  both )\nlearn = makeLearner( classif.ksvm , par.vals = list(kernel =  besseldot ))\nres = tuneParams(learn, task = pid.task, control = ctrl,\n  measures = list(acc, setAggregation(acc, train.mean)), resampling = rdesc,\n  par.set = ps, show.info = FALSE)\ndata = generateHyperParsEffectData(res, partial.dep = TRUE)  You can generate a plot for a single hyperparameter like  C  as shown below.\nThe  partial.dep.learn  can be any regression  Learner  in  mlr  and is used\nto regress the attained performance values on the values of the 3 hyperparameters visited\nduring tuning. The fitted model serves as basis for calculating partial dependences.  plotHyperParsEffect(data, x =  C , y =  acc.test.mean , plot.type =  line ,\n  partial.dep.learn =  regr.randomForest )   We can also look at two hyperparameters simultaneously, for example  C  and  sigma .  plotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean ,\n  plot.type =  heatmap , partial.dep.learn =  regr.randomForest )", 
            "title": "Visualizing the effects of more than 2 hyperparameters"
        }, 
        {
            "location": "/hyperpar_tuning_effects/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  ## ex: create a search space for the C hyperparameter from 0.01 to 0.1 \nps = makeParamSet( \n  makeNumericParam( C , lower = 0.01, upper = 0.1) \n) \n## ex: random search with 100 iterations \nctrl = makeTuneControlRandom(maxit = 100L) \n## ex: 2-fold CV \nrdesc = makeResampleDesc( CV , iters = 2L) \nps = makeParamSet( \n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc( CV , iters = 2L) \nres = tuneParams( classif.ksvm , task = pid.task, control = ctrl, \n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) \ngenerateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE) \nps = makeParamSet( \n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc( CV , iters = 2L, predict =  both ) \nres = tuneParams( classif.ksvm , task = pid.task, control = ctrl, \n  measures = list(acc, setAggregation(acc, train.mean), mmce, setAggregation(mmce, \n    train.mean)), resampling = rdesc, par.set = ps, show.info = FALSE) \ngenerateHyperParsEffectData(res, trafo = T, include.diagnostics = FALSE) \nps = makeParamSet( \n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc( CV , iters = 2L) \nlrn = makeTuneWrapper( classif.ksvm , control = ctrl, \n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) \nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) \ngenerateHyperParsEffectData(res) \nps = makeParamSet( \n  makeDiscreteParam( centers , values = 3:10) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc( Holdout ) \nres = tuneParams( cluster.kmeans , task = mtcars.task, control = ctrl, \n  measures = silhouette, resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x =  centers , y =  silhouette.test.mean ) \n## add our own touches to the plot \nplt + geom_point(colour =  red ) + \n  ggtitle( Evaluating Number of Cluster Centers on mtcars ) + \n  scale_x_continuous(breaks = 3:10) + \n  theme_bw() \nps = makeParamSet( \n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlGenSA(budget = 100L) \nrdesc = makeResampleDesc( Holdout ) \nres = tuneParams( classif.ksvm , task = pid.task, control = ctrl, \n  resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x =  iteration , y =  mmce.test.mean , \n  plot.type =  line ) \nplt + ggtitle( Analyzing convergence of simulated annealing ) + \n  theme_minimal() \nps = makeParamSet( \n  makeDiscreteParam( C , values = c(-1, -0.5, 0.5, 1, 1.5)) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc( CV , iters = 2L) \nres = tuneParams( classif.ksvm , task = pid.task, control = ctrl, \n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x =  C , y =  acc.test.mean ) \nplt + ggtitle( SVM learner crashes with negative C ) + \n  theme_bw() \nps = makeParamSet( \n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x) \n) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc( Holdout ) \nlrn = makeTuneWrapper( classif.ksvm , control = ctrl, \n  measures = list(acc, mmce), resampling = rdesc, par.set = ps, show.info = FALSE) \nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplotHyperParsEffect(data, x =  C , y =  acc.test.mean , plot.type =  line ) \nps = makeParamSet( \n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x), \n  makeNumericParam( sigma , lower = -5, upper = 5, trafo = function(x) 2^x)) \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc( Holdout ) \nlearn = makeLearner( classif.ksvm , par.vals = list(kernel =  rbfdot )) \nres = tuneParams(learn, task = pid.task, control = ctrl, measures = acc, \n  resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean , \n  plot.type =  heatmap , interpolate =  regr.earth ) \nmin_plt = min(data$data$acc.test.mean, na.rm = TRUE) \nmax_plt = max(data$data$acc.test.mean, na.rm = TRUE) \nmed_plt = mean(c(min_plt, max_plt)) \nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5), \n  low =  blue , mid =  white , high =  red , midpoint = med_plt) \nplt = plotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean , \n  plot.type =  heatmap , interpolate =  regr.earth , show.experiments = TRUE) \nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5), \n  low =  blue , mid =  white , high =  red , midpoint = med_plt) \nplotHyperParsEffect(data, x =  iteration , y =  acc.test.mean , \n  plot.type =  line ) \nps = makeParamSet( \n  makeDiscreteParam( C , values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5)), \n  makeDiscreteParam( sigma , values = c(-1, 0.5, 1.5, 1, 0.2, 0.3, 0.4, 5))) \nctrl = makeTuneControlGrid() \nrdesc = makeResampleDesc( Holdout ) \nlearn = makeLearner( classif.ksvm , par.vals = list(kernel =  rbfdot )) \nres = tuneParams(learn, task = pid.task, control = ctrl, measures = acc, \n  resampling = rdesc, par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean , \n  plot.type =  heatmap , interpolate =  regr.earth ) \nps = makeParamSet( \n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x), \n  makeNumericParam( sigma , lower = -5, upper = 5, trafo = function(x) 2^x)) \nctrl = makeTuneControlRandom(maxit = 100) \nrdesc = makeResampleDesc( Holdout ) \nlearn = makeLearner( classif.ksvm , par.vals = list(kernel =  rbfdot )) \nlrn = makeTuneWrapper(learn, control = ctrl, measures = list(acc, mmce), \n  resampling = rdesc, par.set = ps, show.info = FALSE) \nres = resample(lrn, task = pid.task, resampling = cv2, extract = getTuneResult, show.info = FALSE) \ndata = generateHyperParsEffectData(res) \nplt = plotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean , \n  plot.type =  heatmap , interpolate =  regr.earth , show.experiments = TRUE, \n  nested.agg = mean) \nmin_plt = min(plt$data$acc.test.mean, na.rm = TRUE) \nmax_plt = max(plt$data$acc.test.mean, na.rm = TRUE) \nmed_plt = mean(c(min_plt, max_plt)) \nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5), \n  low =  red , mid =  white , high =  blue , midpoint = med_plt) \nps = makeParamSet( \n  makeNumericParam( C , lower = -5, upper = 5, trafo = function(x) 2^x), \n  makeNumericParam( sigma , lower = -5, upper = 5, trafo = function(x) 2^x), \n  makeDiscreteParam( degree , values = 2:5)) \nctrl = makeTuneControlRandom(maxit = 100L) \nrdesc = makeResampleDesc( Holdout , predict =  both ) \nlearn = makeLearner( classif.ksvm , par.vals = list(kernel =  besseldot )) \nres = tuneParams(learn, task = pid.task, control = ctrl, \n  measures = list(acc, setAggregation(acc, train.mean)), resampling = rdesc, \n  par.set = ps, show.info = FALSE) \ndata = generateHyperParsEffectData(res, partial.dep = TRUE) \nplotHyperParsEffect(data, x =  C , y =  acc.test.mean , plot.type =  line , \n  partial.dep.learn =  regr.randomForest ) \nplotHyperParsEffect(data, x =  C , y =  sigma , z =  acc.test.mean , \n  plot.type =  heatmap , partial.dep.learn =  regr.randomForest )", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/out_of_bag_predictions/index.html", 
            "text": "Out-of-Bag Predictions\n\n\nSome learners like random forest use bagging. Bagging means that the learner consists of an\nensemble of several base learners and each base learner is trained with a different random\nsubsample or bootstrap sample from all observations. A prediction made for an observation\nin the original data set using only base learners not trained on this particular observation\nis called out-of-bag (OOB) prediction. These predictions are not prone to overfitting, as\neach prediction is only made by learners that did not use the observation for training.\n\n\nTo get a list of learners that provide OOB predictions, you can call\n\nlistLearners(obj = NA, properties = \"oobpreds\")\n.\n\n\nlistLearners(obj = NA, properties = \noobpreds\n)[c(\nclass\n, \npackage\n)]\n#\n                     class         package\n#\n 1    classif.randomForest    randomForest\n#\n 2 classif.randomForestSRC randomForestSRC\n#\n 3          classif.ranger          ranger\n#\n 4          classif.rFerns          rFerns\n#\n 5       regr.randomForest    randomForest\n#\n 6    regr.randomForestSRC randomForestSRC\n#\n ... (#rows: 8, #cols: 2)\n\n\n\n\nIn \nmlr\n function \ngetOOBPreds\n can be used to extract these observations from the trained\nmodels.\nThese predictions can be used to evaluate the performance of a given learner like in the\nfollowing example.\n\n\nlrn = makeLearner(\nclassif.ranger\n, predict.type = \nprob\n, predict.threshold = 0.6)\nmod = train(lrn, sonar.task)\noob = getOOBPreds(mod, sonar.task)\noob\n#\n Prediction: 208 observations\n#\n predict.type: prob\n#\n threshold: M=0.60,R=0.40\n#\n time: NA\n#\n   id truth    prob.M    prob.R response\n#\n 1  1     R 0.5373385 0.4626615        R\n#\n 2  2     R 0.5971972 0.4028028        R\n#\n 3  3     R 0.5626560 0.4373440        R\n#\n 4  4     R 0.4319901 0.5680099        R\n#\n 5  5     R 0.5417589 0.4582411        R\n#\n 6  6     R 0.4005787 0.5994213        R\n#\n ... (#rows: 208, #cols: 5)\n\nperformance(oob, measures = list(auc, mmce))\n#\n       auc      mmce \n#\n 0.9308071 0.1778846\n\n\n\n\nAs the predictions that are used are out-of-bag, this evaluation strategy is very similar\nto common resampling strategies like 10-fold cross-validation, but much faster, as only one\ntraining instance of the model is required.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nlistLearners(obj = NA, properties = \noobpreds\n)[c(\nclass\n, \npackage\n)] \nlrn = makeLearner(\nclassif.ranger\n, predict.type = \nprob\n, predict.threshold = 0.6) \nmod = train(lrn, sonar.task) \noob = getOOBPreds(mod, sonar.task) \noob \n\nperformance(oob, measures = list(auc, mmce))", 
            "title": "Out-of-Bag Predictions"
        }, 
        {
            "location": "/out_of_bag_predictions/index.html#out-of-bag-predictions", 
            "text": "Some learners like random forest use bagging. Bagging means that the learner consists of an\nensemble of several base learners and each base learner is trained with a different random\nsubsample or bootstrap sample from all observations. A prediction made for an observation\nin the original data set using only base learners not trained on this particular observation\nis called out-of-bag (OOB) prediction. These predictions are not prone to overfitting, as\neach prediction is only made by learners that did not use the observation for training.  To get a list of learners that provide OOB predictions, you can call listLearners(obj = NA, properties = \"oobpreds\") .  listLearners(obj = NA, properties =  oobpreds )[c( class ,  package )]\n#                      class         package\n#  1    classif.randomForest    randomForest\n#  2 classif.randomForestSRC randomForestSRC\n#  3          classif.ranger          ranger\n#  4          classif.rFerns          rFerns\n#  5       regr.randomForest    randomForest\n#  6    regr.randomForestSRC randomForestSRC\n#  ... (#rows: 8, #cols: 2)  In  mlr  function  getOOBPreds  can be used to extract these observations from the trained\nmodels.\nThese predictions can be used to evaluate the performance of a given learner like in the\nfollowing example.  lrn = makeLearner( classif.ranger , predict.type =  prob , predict.threshold = 0.6)\nmod = train(lrn, sonar.task)\noob = getOOBPreds(mod, sonar.task)\noob\n#  Prediction: 208 observations\n#  predict.type: prob\n#  threshold: M=0.60,R=0.40\n#  time: NA\n#    id truth    prob.M    prob.R response\n#  1  1     R 0.5373385 0.4626615        R\n#  2  2     R 0.5971972 0.4028028        R\n#  3  3     R 0.5626560 0.4373440        R\n#  4  4     R 0.4319901 0.5680099        R\n#  5  5     R 0.5417589 0.4582411        R\n#  6  6     R 0.4005787 0.5994213        R\n#  ... (#rows: 208, #cols: 5)\n\nperformance(oob, measures = list(auc, mmce))\n#        auc      mmce \n#  0.9308071 0.1778846  As the predictions that are used are out-of-bag, this evaluation strategy is very similar\nto common resampling strategies like 10-fold cross-validation, but much faster, as only one\ntraining instance of the model is required.", 
            "title": "Out-of-Bag Predictions"
        }, 
        {
            "location": "/out_of_bag_predictions/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  listLearners(obj = NA, properties =  oobpreds )[c( class ,  package )] \nlrn = makeLearner( classif.ranger , predict.type =  prob , predict.threshold = 0.6) \nmod = train(lrn, sonar.task) \noob = getOOBPreds(mod, sonar.task) \noob \n\nperformance(oob, measures = list(auc, mmce))", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/create_learner/index.html", 
            "text": "Integrating Another Learner\n\n\nIn order to integrate a learning algorithm into \nmlr\n some interface code has to be written.\nThree functions are mandatory for each learner.\n\n\n\n\nFirst, define a new learner class with a name, description, capabilities, parameters,\n  and a few other things.\n  (An object of this class can then be generated by \nmakeLearner\n.)\n\n\nSecond, you need to provide a function that calls the learner function and builds the\n  model given data (which makes it possible to invoke training by calling \nmlr\n's \ntrain\n\n  function).\n\n\nFinally, a prediction function that returns predicted values given new data is required\n  (which enables invoking prediction by calling \nmlr\n's \npredict\n\n  function).\n\n\n\n\nTechnically, integrating a learning method means introducing a new S3 \nclass\n\nand implementing the corresponding methods for the generic functions \nmakeRLerner\n,\n\ntrainLearner\n, and \npredictLearner\n.\nTherefore we start with a quick overview of the involved \nclasses\n and\nconstructor functions.\n\n\nClasses, constructors, and naming schemes\n\n\nAs you already know \nmakeLearner\n generates an object of class \nLearner\n.\n\n\nclass(makeLearner(cl = \nclassif.lda\n))\n#\n [1] \nclassif.lda\n     \nRLearnerClassif\n \nRLearner\n        \nLearner\n\n\nclass(makeLearner(cl = \nregr.lm\n))\n#\n [1] \nregr.lm\n      \nRLearnerRegr\n \nRLearner\n     \nLearner\n\n\nclass(makeLearner(cl = \nsurv.coxph\n))\n#\n [1] \nsurv.coxph\n   \nRLearnerSurv\n \nRLearner\n     \nLearner\n\n\nclass(makeLearner(cl = \ncluster.kmeans\n))\n#\n [1] \ncluster.kmeans\n  \nRLearnerCluster\n \nRLearner\n        \nLearner\n\n\nclass(makeLearner(cl = \nmultilabel.rFerns\n))\n#\n [1] \nmultilabel.rFerns\n  \nRLearnerMultilabel\n \nRLearner\n          \n#\n [4] \nLearner\n\n\n\n\n\nThe first element of each \nclass\n attribute vector is the name of the learner\nclass passed to the \ncl\n argument of \nmakeLearner\n.\nObviously, this adheres to the naming conventions\n\n\n\n\n\"classif.\nR_method_name\n\"\n for classification,\n\n\n\"multilabel.\nR_method_name\n\"\n for multilabel classification,\n\n\n\"regr.\nR_method_name\n\"\n for regression,\n\n\n\"surv.\nR_method_name\n\"\n for survival analysis, and\n\n\n\"cluster.\nR_method_name\n\"\n for clustering.\n\n\n\n\nAdditionally, there exist intermediate classes that reflect the type of learning problem, i.e.,\nall classification learners inherit from \nRLearnerClassif\n, all regression learners\nfrom \nRLearnerRegr\n and so on.\nTheir superclasses are \nRLearner\n and finally \nLearner\n.\nFor all these (sub)classes there exist constructor functions \nmakeRLearner\n,\n\nmakeRLearnerClassif\n, \nmakeRLearneRegr\n etc. that are called\ninternally by \nmakeLearner\n.\n\n\nA short side remark:\nAs you might have noticed there does not exist a special learner class for\n\ncost-sensitive classification (costsens)\n with example-specific\ncosts. This type of learning task is currently exclusively handled through \nwrappers\n\nlike \nmakeCostSensWeightedPairsWrapper\n.\n\n\nIn the following we show how to integrate learners for the five types of learning tasks\nmentioned above.\nDefining a completely new type of learner that has special properties and does not fit into\none of the existing schemes is of course possible, but much more advanced and not covered\nhere.\n\n\nWe use a classification example to explain some general principles (so even if you are\ninterested in integrating a learner for another type of learning task you might want to read\nthe following section).\nExamples for other types of learning tasks are shown later on.\n\n\nClassification\n\n\nWe show how the \nLinear Discriminant Analysis\n from\npackage \nMASS\n has been integrated\ninto the classification learner \nclassif.lda\n in \nmlr\n as an example.\n\n\nDefinition of the learner\n\n\nThe minimal information required to define a learner is the \nmlr\n name of the\nlearner, its package, the parameter set, and the set of properties of your\nlearner. In addition, you may provide a human-readable name, a short name and a\nnote with information relevant to users of the learner.\n\n\nFirst, name your learner. According to the naming conventions above the name starts with\n\nclassif.\n and we choose \nclassif.lda\n.\n\n\nSecond, we need to define the parameters of the learner. These are any options\nthat can be set when running it to change how it learns, how input is\ninterpreted, how and what output is generated, and so on. \nmlr\n provides a\nnumber of functions to define parameters, a complete list can be found in the\ndocumentation of \nLearnerParam\n of the\n\nParamHelpers\n package.\n\n\nIn our example, we have discrete and numeric parameters, so we use\n\nmakeDiscreteLearnerParam\n and\n\nmakeNumericLearnerParam\n to incorporate the\ncomplete description of the parameters. We include all possible values for\ndiscrete parameters and lower and upper bounds for numeric parameters. Strictly\nspeaking it is not necessary to provide bounds for all parameters and if this\ninformation is not available they can be estimated, but providing accurate and\nspecific information here makes it possible to tune the learner much better (see\nthe section on \ntuning\n).\n\n\nNext, we add information on the properties of the learner (see also the section\non \nlearners\n). Which types of features are supported (numerics,\nfactors)? Are case weights supported? Are class weights supported? Can the method deal\nwith missing values in the features and deal with NA's in a meaningful way (not \nna.omit\n)?\nAre one-class, two-class, multi-class problems supported? Can the learner predict\nposterior probabilities?\n\n\nIf the learner supports class weights the name of the relevant learner parameter\ncan be specified via argument \nclass.weights.param\n.\n\n\nBelow is the complete code for the definition of the LDA learner. It has one\ndiscrete parameter, \nmethod\n, and two continuous ones, \nnu\n and \ntol\n. It\nsupports classification problems with two or more classes and can deal with\nnumeric and factor explanatory variables. It can predict posterior\nprobabilities.\n\n\nmakeRLearner.classif.lda = function() {\n  makeRLearnerClassif(\n    cl = \nclassif.lda\n,\n    package = \nMASS\n,\n    par.set = makeParamSet(\n      makeDiscreteLearnerParam(id = \nmethod\n, default = \nmoment\n, values = c(\nmoment\n, \nmle\n, \nmve\n, \nt\n)),\n      makeNumericLearnerParam(id = \nnu\n, lower = 2, requires = quote(method == \nt\n)),\n      makeNumericLearnerParam(id = \ntol\n, default = 1e-4, lower = 0),\n      makeDiscreteLearnerParam(id = \npredict.method\n, values = c(\nplug-in\n, \npredictive\n, \ndebiased\n),\n        default = \nplug-in\n, when = \npredict\n),\n      makeLogicalLearnerParam(id = \nCV\n, default = FALSE, tunable = FALSE)\n    ),\n    properties = c(\ntwoclass\n, \nmulticlass\n, \nnumerics\n, \nfactors\n, \nprob\n),\n    name = \nLinear Discriminant Analysis\n,\n    short.name = \nlda\n,\n    note = \nLearner param 'predict.method' maps to 'method' in predict.lda.\n\n  )\n}\n\n\n\n\nCreating the training function of the learner\n\n\nOnce the learner has been defined, we need to tell \nmlr\n how to call it to\ntrain a model. The name of the function has to start with \ntrainLearner.\n,\nfollowed by the \nmlr\n name of the learner as defined above (\nclassif.lda\n\nhere). The prototype of the function looks as follows.\n\n\nfunction(.learner, .task, .subset, .weights = NULL, ...) { }\n\n\n\n\nThis function must fit a model on the data of the task \n.task\n with regard to\nthe subset defined in the integer vector \n.subset\n and the parameters passed\nin the \n...\n arguments. Usually, the data should be extracted from the task\nusing \ngetTaskData\n. This will take care of any subsetting as well. It must\nreturn the fitted model. \nmlr\n assumes no special data type for the return\nvalue -- it will be passed to the predict function we are going to define below,\nso any special code the learner may need can be encapsulated there.\n\n\nFor our example, the definition of the function looks like this. In addition to\nthe data of the task, we also need the formula that describes what to predict.\nWe use the function \ngetTaskFormula\n to extract this from the task.\n\n\ntrainLearner.classif.lda = function(.learner, .task, .subset, .weights = NULL, ...) {\n  f = getTaskFormula(.task)\n  MASS::lda(f, data = getTaskData(.task, .subset), ...)\n}\n\n\n\n\nCreating the prediction method\n\n\nFinally, the prediction function needs to be defined. The name of this function\nstarts with \npredictLearner.\n, followed again by the \nmlr\n name of the\nlearner. The prototype of the function is as follows.\n\n\nfunction(.learner, .model, .newdata, ...) { }\n\n\n\n\nIt must predict for the new observations in the \ndata.frame\n \n.newdata\n with\nthe wrapped model \n.model\n, which is returned from the training function.\nThe actual model the learner built is stored in the \n$learner.model\n member\nand can be accessed simply through \n.model$learner.model\n.\n\n\nFor classification, you have to return a factor of predicted classes if\n\n.learner$predict.type\n is \n\"response\"\n, or a matrix of predicted\nprobabilities if \n.learner$predict.type\n is \n\"prob\"\n and this type of\nprediction is supported by the learner. In the latter case the matrix must have\nthe same number of columns as there are classes in the task and the columns have\nto be named by the class names.\n\n\nThe definition for LDA looks like this. It is pretty much just a straight\npass-through of the arguments to the \npredict\n function and some extraction of\nprediction data depending on the type of prediction requested.\n\n\npredictLearner.classif.lda = function(.learner, .model, .newdata, predict.method = \nplug-in\n, ...) {\n  p = predict(.model$learner.model, newdata = .newdata, method = predict.method, ...)\n  if (.learner$predict.type == \nresponse\n) \n    return(p$class) else return(p$posterior)\n}\n\n\n\n\nRegression\n\n\nThe main difference for regression is that the type of predictions are different\n(numeric instead of labels or probabilities) and that not all of the properties\nare relevant. In particular, whether one-, two-, or multi-class problems and\nposterior probabilities are supported is not applicable.\n\n\nApart from this, everything explained above applies. Below is the definition for\nthe \nearth\n learner from the\n\nearth\n package.\n\n\nmakeRLearner.regr.earth = function() {\n  makeRLearnerRegr(\n    cl = \nregr.earth\n,\n    package = \nearth\n,\n    par.set = makeParamSet(\n      makeLogicalLearnerParam(id = \nkeepxy\n, default = FALSE, tunable = FALSE),\n      makeNumericLearnerParam(id = \ntrace\n, default = 0, upper = 10, tunable = FALSE),\n      makeIntegerLearnerParam(id = \ndegree\n, default = 1L, lower = 1L),\n      makeNumericLearnerParam(id = \npenalty\n),\n      makeIntegerLearnerParam(id = \nnk\n, lower = 0L),\n      makeNumericLearnerParam(id = \nthres\n, default = 0.001),\n      makeIntegerLearnerParam(id = \nminspan\n, default = 0L),\n      makeIntegerLearnerParam(id = \nendspan\n, default = 0L),\n      makeNumericLearnerParam(id = \nnewvar.penalty\n, default = 0),\n      makeIntegerLearnerParam(id = \nfast.k\n, default = 20L, lower = 0L),\n      makeNumericLearnerParam(id = \nfast.beta\n, default = 1),\n      makeDiscreteLearnerParam(id = \npmethod\n, default = \nbackward\n,\n        values = c(\nbackward\n, \nnone\n, \nexhaustive\n, \nforward\n, \nseqrep\n, \ncv\n)),\n      makeIntegerLearnerParam(id = \nnprune\n)\n    ),\n    properties = c(\nnumerics\n, \nfactors\n),\n    name = \nMultivariate Adaptive Regression Splines\n,\n    short.name = \nearth\n,\n    note = \n\n  )\n}\n\n\n\n\ntrainLearner.regr.earth = function(.learner, .task, .subset, .weights = NULL, ...) {\n  f = getTaskFormula(.task)\n  earth::earth(f, data = getTaskData(.task, .subset), ...)\n}\n\n\n\n\npredictLearner.regr.earth = function(.learner, .model, .newdata, ...) {\n  predict(.model$learner.model, newdata = .newdata)[, 1L]\n}\n\n\n\n\nAgain most of the data is passed straight through to/from the train/predict\nfunctions of the learner.\n\n\nSurvival analysis\n\n\nFor survival analysis, you have to return so-called linear predictors in order to compute\nthe default measure for this task type, the \ncindex\n (for\n\n.learner$predict.type\n == \n\"response\"\n). For \n.learner$predict.type\n == \n\"prob\"\n,\nthere is no substantially meaningful measure (yet). You may either ignore this case or return\nsomething like predicted survival curves (cf. example below).\n\n\nThere are three properties that are specific to survival learners:\n\"rcens\", \"lcens\" and \"icens\", defining the type(s) of censoring a learner can handle -- right,\nleft and/or interval censored.\n\n\nLet's have a look at how the \nCox Proportional Hazard Model\n from\npackage \nsurvival\n has been integrated\ninto the survival learner \nsurv.coxph\n in \nmlr\n as an example:\n\n\nmakeRLearner.surv.coxph = function() {\n  makeRLearnerSurv(\n    cl = \nsurv.coxph\n,\n    package = \nsurvival\n,\n    par.set = makeParamSet(\n      makeDiscreteLearnerParam(id = \nties\n, default = \nefron\n, values = c(\nefron\n, \nbreslow\n, \nexact\n)),\n      makeLogicalLearnerParam(id = \nsingular.ok\n, default = TRUE),\n      makeNumericLearnerParam(id = \neps\n, default = 1e-09, lower = 0),\n      makeNumericLearnerParam(id = \ntoler.chol\n, default = .Machine$double.eps^0.75, lower = 0),\n      makeIntegerLearnerParam(id = \niter.max\n, default = 20L, lower = 1L),\n      makeNumericLearnerParam(id = \ntoler.inf\n, default = sqrt(.Machine$double.eps^0.75), lower = 0),\n      makeIntegerLearnerParam(id = \nouter.max\n, default = 10L, lower = 1L),\n      makeLogicalLearnerParam(id = \nmodel\n, default = FALSE, tunable = FALSE),\n      makeLogicalLearnerParam(id = \nx\n, default = FALSE, tunable = FALSE),\n      makeLogicalLearnerParam(id = \ny\n, default = TRUE, tunable = FALSE)\n    ),\n    properties = c(\nmissings\n, \nnumerics\n, \nfactors\n, \nweights\n, \nprob\n, \nrcens\n),\n    name = \nCox Proportional Hazard Model\n,\n    short.name = \ncoxph\n,\n    note = \n\n  )\n}\n\n\n\n\ntrainLearner.surv.coxph = function(.learner, .task, .subset, .weights = NULL, ...) {\n  f = getTaskFormula(.task)\n  data = getTaskData(.task, subset = .subset)\n  if (is.null(.weights)) {\n    survival::coxph(formula = f, data = data, ...)\n  } else {\n    survival::coxph(formula = f, data = data, weights = .weights, ...)\n  }\n}\n\n\n\n\npredictLearner.surv.coxph = function(.learner, .model, .newdata, ...) {\n  predict(.model$learner.model, newdata = .newdata, type = \nlp\n, ...)\n}\n\n\n\n\nClustering\n\n\nFor clustering, you have to return a numeric vector with the IDs of the clusters\nthat the respective datum has been assigned to. The numbering should start at 1.\n\n\nBelow is the definition for the \nFarthestFirst\n learner\nfrom the \nRWeka\n package. Weka\nstarts the IDs of the clusters at 0, so we add 1 to the predicted clusters.\nRWeka has a different way of setting learner parameters; we use the special\n\nWeka_control\n function to do this.\n\n\nmakeRLearner.cluster.FarthestFirst = function() {\n  makeRLearnerCluster(\n    cl = \ncluster.FarthestFirst\n,\n    package = \nRWeka\n,\n    par.set = makeParamSet(\n      makeIntegerLearnerParam(id = \nN\n, default = 2L, lower = 1L),\n      makeIntegerLearnerParam(id = \nS\n, default = 1L, lower = 1L),\n      makeLogicalLearnerParam(id = \noutput-debug-info\n, default = FALSE, tunable = FALSE)\n    ),\n    properties = c(\nnumerics\n),\n    name = \nFarthestFirst Clustering Algorithm\n,\n    short.name = \nfarthestfirst\n\n  )\n}\n\n\n\n\ntrainLearner.cluster.FarthestFirst = function(.learner, .task, .subset, .weights = NULL, ...) {\n  ctrl = RWeka::Weka_control(...)\n  RWeka::FarthestFirst(getTaskData(.task, .subset), control = ctrl)\n}\n\n\n\n\npredictLearner.cluster.FarthestFirst = function(.learner, .model, .newdata, ...) {\n  as.integer(predict(.model$learner.model, .newdata, ...)) + 1L\n}\n\n\n\n\nMultilabel classification\n\n\nAs stated in the \nmultilabel\n section, multilabel classification\nmethods can be divided into problem transformation methods and algorithm adaptation methods.\n\n\nAt this moment the only problem transformation method implemented in \nmlr\n\nis the \nbinary relevance method\n. Integrating more of\nthese methods requires good knowledge of the architecture of the \nmlr\n package.\n\n\nThe integration of an algorithm adaptation multilabel classification learner is easier and\nworks very similar to the normal multiclass-classification.\nIn contrast to the multiclass case, not all of the learner properties are relevant.\nIn particular, whether one-, two-, or multi-class problems are supported is not applicable.\nFurthermore the prediction function output must be a matrix\nwith each prediction of a label in one column and the names of the labels\nas column names. If \n.learner$predict.type\n is \n\"response\"\n the predictions must\nbe logical. If \n.learner$predict.type\n is \n\"prob\"\n and this type of\nprediction is supported by the learner, the matrix must consist of predicted\nprobabilities.\n\n\nBelow is the definition of the \nrFerns\n learner from the\n\nrFerns\n package, which does not support probability predictions.\n\n\nmakeRLearner.multilabel.rFerns = function() {\n  makeRLearnerMultilabel(\n    cl = \nmultilabel.rFerns\n,\n    package = \nrFerns\n,\n    par.set = makeParamSet(\n      makeIntegerLearnerParam(id = \ndepth\n, default = 5L),\n      makeIntegerLearnerParam(id = \nferns\n, default = 1000L)\n    ),\n    properties = c(\nnumerics\n, \nfactors\n, \nordered\n),\n    name = \nRandom ferns\n,\n    short.name = \nrFerns\n,\n    note = \n\n  )\n}\n\n\n\n\ntrainLearner.multilabel.rFerns = function(.learner, .task, .subset, .weights = NULL, ...) {\n  d = getTaskData(.task, .subset, target.extra = TRUE)\n  rFerns::rFerns(x = d$data, y = as.matrix(d$target), ...)\n}\n\n\n\n\npredictLearner.multilabel.rFerns = function(.learner, .model, .newdata, ...) {\n  as.matrix(predict(.model$learner.model, .newdata, ...))\n}\n\n\n\n\nCreating a new method for extracting feature importance values\n\n\nSome learners, for example decision trees and random forests, can calculate feature importance\nvalues, which can be extracted from a \nfitted model\n using function\n\ngetFeatureImportance\n.\n\n\nIf your newly integrated learner supports this you need to\n\n\n\n\nadd \n\"featimp\"\n to the learner properties and\n\n\nimplement a new S3 method for function \ngetFeatureImportanceLearner\n (which later is called\n  internally by \ngetFeatureImportance\n)\n\n\n\n\nin order to make this work.\n\n\nThis method takes the \nLearner\n \n.learner\n, the \nWrappedModel\n \n.model\n\nand potential further arguments and calculates or extracts the feature importance.\nIt must return a named vector of importance values.\n\n\nBelow are two simple examples.\nIn case of \n\"classif.rpart\"\n the feature importance values can be easily extracted from the\nfitted model.\n\n\ngetFeatureImportanceLearner.classif.rpart = function(.learner, .model, ...) {\n  mod = getLearnerModel(.model, more.unwrap = TRUE)\n  mod$variable.importance\n}\n\n\n\n\nFor the \nrandom forest\n from package \nrandomForestSRC\n function\n\nvimp\n is called.\n\n\ngetFeatureImportanceLearner.classif.randomForestSRC = function(.learner, .model, ...) {\n  mod = getLearnerModel(.model, more.unwrap = TRUE)\n  randomForestSRC::vimp(mod, ...)$importance[, \nall\n]\n}\n\n\n\n\nCreating a new method for extracting out-of-bag predictions\n\n\nMany ensemble learners generate out-of-bag predictions (OOB predictions) automatically.\n\nmlr\n provides the function \ngetOOBPreds\n to access these predictions in the \nmlr\n framework.\n\n\nIf your newly integrated learner is able to calculate OOB predictions and you want to be\nable to access them in \nmlr\n via \ngetOOBPreds\n you need to\n\n\n\n\nadd \n\"oobpreds\"\n to the learner properties and\n\n\nimplement a new S3 method for function \ngetOOBPredsLearner\n (which later is called\n  internally by \ngetOOBPreds\n).\n\n\n\n\nThis method takes the \nLearner\n \n.learner\n and the \nWrappedModel\n\n\n.model\n and extracts the OOB predictions.\nIt must return the predictions in the same format as the \npredictLearner\n function.\n\n\ngetOOBPredsLearner.classif.randomForest = function(.learner, .model) {\n  if (.learner$predict.type == \nresponse\n) {\n    m = getLearnerModel(.model, more.unwrap = TRUE)\n    unname(m$predicted)\n  } else {\n    getLearnerModel(.model, more.unwrap = TRUE)$votes\n  }\n}\n\n\n\n\nRegistering your learner\n\n\nIf your interface code to a new learning algorithm exists only locally, i.e., it is not (yet)\nmerged into \nmlr\n or does not live in an extra package with a proper namespace you might want\nto register the new S3 methods to make sure that these are found by, e.g., \nlistLearners\n.\nYou can do this as follows:\n\n\nregisterS3method(\nmakeRLearner\n, \nawesome_new_learner_class\n, makeRLearner.\nawesome_new_learner_class\n)\nregisterS3method(\ntrainLearner\n, \nawesome_new_learner_class\n, trainLearner.\nawesome_new_learner_class\n)\nregisterS3method(\npredictLearner\n, \nawesome_new_learner_class\n, predictLearner.\nawesome_new_learner_class\n)\n\n\n\n\nIf you have written more methods, for example in order to extract feature importance values\nor out-of-bag predictions these also need to be registered in the same manner, for example:\n\n\nregisterS3method(\ngetFeatureImportanceLearner\n, \nawesome_new_learner_class\n,\n  getFeatureImportanceLearner.\nawesome_new_learner_class\n)\n\n\n\n\nFor the new learner to work with parallelization, you may have to export the new\nmethods explicitly:\n\n\nparallelExport(\ntrainLearner.\nawesome_new_learner_class\n, \npredictLearner.\nawesome_new_learner_class\n)\n\n\n\n\nFurther information for developers\n\n\nIf you haven't written a learner interface for private use only, but intend to send a pull\nrequest to have it included in the \nmlr\n package there are a few things to take care of,\nmost importantly unit testing!\n\n\nFor general information about contributing to the package, unit testing, version control setup\nand the like please also read the\n\ncoding guidelines in the mlr Wiki\n.\n\n\n\n\n\n\nThe R file containing the interface code should adhere to the naming convention\n\nRLearner_\ntype\n_\nlearner_name\n.R\n, e.g., \nRLearner_classif_lda.R\n, see for example\n\nhttps://github.com/mlr-org/mlr/blob/master/R/RLearner_classif_lda.R\n and contain the\nnecessary roxygen \n@export\n tags to register the S3 methods in the NAMESPACE.\n\n\n\n\n\n\nThe learner interfaces should work out of the box without requiring any parameters to be set,\ne.g., \ntrain(\"classif.lda\", iris.task)\n should run.\nSometimes, this makes it necessary to change or set some additional defaults as explained above\nand -- very important -- informing the user about this in the \nnote\n.\n\n\n\n\n\n\nThe parameter set of the learner should be as complete as possible.\n\n\n\n\n\n\nEvery learner interface must be unit tested.\n\n\n\n\n\n\nUnit testing\n\n\nThe tests make sure that we get the same results when the learner is invoked through the \nmlr\n\ninterface and when using the original functions.\nIf you are not familiar or want to learn more about unit testing and package \ntestthat\n\nhave a look at \nthe Testing chapter in Hadley Wickham's R packages\n.\n\n\nIn \nmlr\n all unit tests are in the following directory:\n\nhttps://github.com/mlr-org/mlr/tree/master/tests/testthat\n.\nFor each learner interface there is an individual file whose name follows the scheme\n\ntest_\ntype\n_\nlearner_name\n.R\n, for example\n\nhttps://github.com/mlr-org/mlr/blob/master/tests/testthat/test_classif_lda.R\n.\n\n\nBelow is a snippet from the tests of the lda interface\n\nhttps://github.com/mlr-org/mlr/blob/master/tests/testthat/test_classif_lda.R\n.\n\n\ntest_that(\nclassif_lda\n, {\n  requirePackagesOrSkip(\nMASS\n, default.method = \nload\n)\n\n  set.seed(getOption(\nmlr.debug.seed\n))\n  m = MASS::lda(formula = multiclass.formula, data = multiclass.train)\n  set.seed(getOption(\nmlr.debug.seed\n))\n  p = predict(m, newdata = multiclass.test)\n\n  testSimple(\nclassif.lda\n, multiclass.df, multiclass.target, multiclass.train.inds, p$class)\n  testProb(\nclassif.lda\n, multiclass.df, multiclass.target, multiclass.train.inds, p$posterior)\n})\n\n\n\n\nThe tests make use of numerous helper objects and helper functions.\nAll of these are defined in the \nhelper_\n files in\n\nhttps://github.com/mlr-org/mlr/blob/master/tests/testthat/\n.\n\n\nIn the above code the first line just loads package \nMASS\n or skips the test if the package is\nnot available.\nThe objects \nmulticlass.formula\n, \nmulticlass.train\n, \nmulticlass.test\n etc. are defined\nin \nhttps://github.com/mlr-org/mlr/blob/master/tests/testthat/helper_objects.R\n.\nWe tried to choose fairly self-explanatory names:\nFor example \nmulticlass\n indicates a multi-class classification problem, \nmulticlass.train\n\ncontains data for training, \nmulticlass.formula\n a \nformula\n object etc.\n\n\nThe test fits an lda model on the training set and makes predictions on the test set\nusing the original functions \nlda\n and \npredict.lda\n.\nThe helper functions \ntestSimple\n and \ntestProb\n perform training and prediction on the same\ndata using the \nmlr\n interface -- \ntestSimple\n for \npredict.type = \"response\n and \ntestProbs\n for\n\npredict.type = \"prob\"\n -- and check if the predicted class labels and probabilities coincide\nwith the outcomes \np$class\n and \np$posterior\n.\n\n\nIn order to get reproducible results seeding is required for many learners.\nThe \n\"mlr.debug.seed\"\n works as follows:\nWhen invoking the tests the option \n\"mlr.debug.seed\"\n is set\n(see \nhttps://github.com/mlr-org/mlr/blob/master/tests/testthat/helper_zzz.R\n), and\n\nset.seed(getOption(\"mlr.debug.seed\"))\n is used to specify the seed.\nInternally, \nmlr\n's\n\ntrain\n and\n\npredict.WrappedModel\n\nfunctions check if the \n\"mlr.debug.seed\"\n option is set and if yes, also specify the seed.\n\n\nNote that the option \n\"mlr.debug.seed\"\n is only set for testing, so no seeding happens in\nnormal usage of \nmlr\n.\n\n\nLet's look at a second example.\nMany learners have parameters that are commonly changed or tuned and it is important to make\nsure that these are passed through correctly.\nBelow is a snippet from \nhttps://github.com/mlr-org/mlr/blob/master/tests/testthat/test_regr_randomForest.R\n.\n\n\ntest_that(\nregr_randomForest\n, {\n  requirePackagesOrSkip(\nrandomForest\n, default.method = \nload\n)\n\n  parset.list = list(\n    list(),\n    list(ntree = 5, mtry = 2),\n    list(ntree = 5, mtry = 4),\n    list(proximity = TRUE, oob.prox = TRUE),\n    list(nPerm = 3)\n  )\n\n  old.predicts.list = list()\n\n  for (i in 1:length(parset.list)) {\n    parset = parset.list[[i]]\n    pars = list(formula = regr.formula, data = regr.train)\n    pars = c(pars, parset)\n    set.seed(getOption(\nmlr.debug.seed\n))\n    m = do.call(randomForest::randomForest, pars)\n    set.seed(getOption(\nmlr.debug.seed\n))\n    p = predict(m, newdata = regr.test, type = \nresponse\n)\n    old.predicts.list[[i]] = p\n  }\n\n  testSimpleParsets(\nregr.randomForest\n, regr.df, regr.target,\n    regr.train.inds, old.predicts.list, parset.list)\n})\n\n\n\n\nAll tested parameter configurations are collected in the \nparset.list\n.\nIn order to make sure that the default parameter configuration is tested the first element\nof the \nparset.list\n is an empty \nlist\n.\nThen we simply loop over all parameter settings and store the resulting predictions in\n\nold.predicts.list\n.\nAgain the helper function \ntestSimpleParsets\n does the same using the \nmlr\n interface\nand compares the outcomes.\n\n\nAdditional to tests for individual learners we also have general tests that loop through all\nintegrated learners and make for example sure that learners have the correct properties\n(e.g. a learner with property \n\"factors\"\n can cope with \nfactor\n features,\na learner with property \n\"weights\"\n takes observation weights into account properly etc.).\nFor example \nhttps://github.com/mlr-org/mlr/blob/master/tests/testthat/test_learners_all_classif.R\n\nruns through all classification learners.\nSimilar tests exist for all types of learning methods like regression, cluster and survival analysis\nas well as multilabel classification.\n\n\nIn order to run all tests for, e.g., classification learners on your machine you can invoke\nthe tests from within \nR\n by\n\n\ndevtools::test(\nmlr\n, filter = \nclassif\n)\n\n\n\n\nor from the command line using \nMichel's rt tool\n\n\nrtest --filter=classif\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nclass(makeLearner(cl = \nclassif.lda\n)) \n\nclass(makeLearner(cl = \nregr.lm\n)) \n\nclass(makeLearner(cl = \nsurv.coxph\n)) \n\nclass(makeLearner(cl = \ncluster.kmeans\n)) \n\nclass(makeLearner(cl = \nmultilabel.rFerns\n)) \nmakeRLearner.classif.lda = function() { \n  makeRLearnerClassif( \n    cl = \nclassif.lda\n, \n    package = \nMASS\n, \n    par.set = makeParamSet( \n      makeDiscreteLearnerParam(id = \nmethod\n, default = \nmoment\n, values = c(\nmoment\n, \nmle\n, \nmve\n, \nt\n)), \n      makeNumericLearnerParam(id = \nnu\n, lower = 2, requires = quote(method == \nt\n)), \n      makeNumericLearnerParam(id = \ntol\n, default = 1e-4, lower = 0), \n      makeDiscreteLearnerParam(id = \npredict.method\n, values = c(\nplug-in\n, \npredictive\n, \ndebiased\n), \n        default = \nplug-in\n, when = \npredict\n), \n      makeLogicalLearnerParam(id = \nCV\n, default = FALSE, tunable = FALSE) \n    ), \n    properties = c(\ntwoclass\n, \nmulticlass\n, \nnumerics\n, \nfactors\n, \nprob\n), \n    name = \nLinear Discriminant Analysis\n, \n    short.name = \nlda\n, \n    note = \nLearner param 'predict.method' maps to 'method' in predict.lda.\n \n  ) \n} \n## function(.learner, .task, .subset, .weights = NULL, ...) { } \n## trainLearner.classif.lda = function(.learner, .task, .subset, .weights = NULL,  ...) { \n##   f = getTaskFormula(.task) \n##   MASS::lda(f, data = getTaskData(.task, .subset), ...) \n## } \n## function(.learner, .model, .newdata, ...) { } \n## predictLearner.classif.lda = function(.learner, .model, .newdata, predict.method = \nplug-in\n, ...) { \n##   p = predict(.model$learner.model, newdata = .newdata, method = predict.method, ...) \n##   if(.learner$predict.type == \nresponse\n) \n##     return(p$class) \n##   else \n##     return(p$posterior) \n## } \nmakeRLearner.regr.earth = function() { \n  makeRLearnerRegr( \n    cl = \nregr.earth\n, \n    package = \nearth\n, \n    par.set = makeParamSet( \n      makeLogicalLearnerParam(id = \nkeepxy\n, default = FALSE, tunable = FALSE), \n      makeNumericLearnerParam(id = \ntrace\n, default = 0, upper = 10, tunable = FALSE), \n      makeIntegerLearnerParam(id = \ndegree\n, default = 1L, lower = 1L), \n      makeNumericLearnerParam(id = \npenalty\n), \n      makeIntegerLearnerParam(id = \nnk\n, lower = 0L), \n      makeNumericLearnerParam(id = \nthres\n, default = 0.001), \n      makeIntegerLearnerParam(id = \nminspan\n, default = 0L), \n      makeIntegerLearnerParam(id = \nendspan\n, default = 0L), \n      makeNumericLearnerParam(id = \nnewvar.penalty\n, default = 0), \n      makeIntegerLearnerParam(id = \nfast.k\n, default = 20L, lower = 0L), \n      makeNumericLearnerParam(id = \nfast.beta\n, default = 1), \n      makeDiscreteLearnerParam(id = \npmethod\n, default = \nbackward\n, \n        values = c(\nbackward\n, \nnone\n, \nexhaustive\n, \nforward\n, \nseqrep\n, \ncv\n)), \n      makeIntegerLearnerParam(id = \nnprune\n) \n    ), \n    properties = c(\nnumerics\n, \nfactors\n), \n    name = \nMultivariate Adaptive Regression Splines\n, \n    short.name = \nearth\n, \n    note = \n \n  ) \n} \n## trainLearner.regr.earth = function(.learner, .task, .subset, .weights = NULL,  ...) { \n##   f = getTaskFormula(.task) \n##   earth::earth(f, data = getTaskData(.task, .subset), ...) \n## } \n## predictLearner.regr.earth = function(.learner, .model, .newdata, ...) { \n##   predict(.model$learner.model, newdata = .newdata)[, 1L] \n## } \nmakeRLearner.surv.coxph = function() { \n  makeRLearnerSurv( \n    cl = \nsurv.coxph\n, \n    package = \nsurvival\n, \n    par.set = makeParamSet( \n      makeDiscreteLearnerParam(id = \nties\n, default = \nefron\n, values = c(\nefron\n, \nbreslow\n, \nexact\n)), \n      makeLogicalLearnerParam(id = \nsingular.ok\n, default = TRUE), \n      makeNumericLearnerParam(id = \neps\n, default = 1e-09, lower = 0), \n      makeNumericLearnerParam(id = \ntoler.chol\n, default = .Machine$double.eps^0.75, lower = 0), \n      makeIntegerLearnerParam(id = \niter.max\n, default = 20L, lower = 1L), \n      makeNumericLearnerParam(id = \ntoler.inf\n, default = sqrt(.Machine$double.eps^0.75), lower = 0), \n      makeIntegerLearnerParam(id = \nouter.max\n, default = 10L, lower = 1L), \n      makeLogicalLearnerParam(id = \nmodel\n, default = FALSE, tunable = FALSE), \n      makeLogicalLearnerParam(id = \nx\n, default = FALSE, tunable = FALSE), \n      makeLogicalLearnerParam(id = \ny\n, default = TRUE, tunable = FALSE) \n    ), \n    properties = c(\nmissings\n, \nnumerics\n, \nfactors\n, \nweights\n, \nprob\n, \nrcens\n), \n    name = \nCox Proportional Hazard Model\n, \n    short.name = \ncoxph\n, \n    note = \n \n  ) \n} \n## trainLearner.surv.coxph = function(.learner, .task, .subset, .weights = NULL,  ...) { \n##   f = getTaskFormula(.task) \n##   data = getTaskData(.task, subset = .subset) \n##   if (is.null(.weights)) { \n##     mod = survival::coxph(formula = f, data = data, ...) \n##   } else  { \n##     mod = survival::coxph(formula = f, data = data, weights = .weights, ...) \n##   } \n##   if (.learner$predict.type == \nprob\n) \n##     mod = attachTrainingInfo(mod, list(surv.range = range(getTaskTargets(.task)[, 1L]))) \n##   mod \n## } \n## predictLearner.surv.coxph = function(.learner, .model, .newdata, ...) { \n##   if(.learner$predict.type == \nresponse\n) { \n##     predict(.model$learner.model, newdata = .newdata, type = \nlp\n, ...) \n##   } else if (.learner$predict.type == \nprob\n) { \n##     surv.range = getTrainingInfo(.model$learner.model)$surv.range \n##     times = seq(from = surv.range[1L], to = surv.range[2L], length.out = 1000) \n##     t(summary(survival::survfit(.model$learner.model, newdata = .newdata, se.fit = FALSE, conf.int = FALSE), times = times)$surv) \n##   } else { \n##     stop(\nUnknown predict type\n) \n##   } \n## } \nmakeRLearner.cluster.FarthestFirst = function() { \n  makeRLearnerCluster( \n    cl = \ncluster.FarthestFirst\n, \n    package = \nRWeka\n, \n    par.set = makeParamSet( \n      makeIntegerLearnerParam(id = \nN\n, default = 2L, lower = 1L), \n      makeIntegerLearnerParam(id = \nS\n, default = 1L, lower = 1L), \n      makeLogicalLearnerParam(id = \noutput-debug-info\n, default = FALSE, tunable = FALSE) \n    ), \n    properties = c(\nnumerics\n), \n    name = \nFarthestFirst Clustering Algorithm\n, \n    short.name = \nfarthestfirst\n \n  ) \n} \n## trainLearner.cluster.FarthestFirst = function(.learner, .task, .subset, .weights = NULL,  ...) { \n##   ctrl = RWeka::Weka_control(...) \n##   RWeka::FarthestFirst(getTaskData(.task, .subset), control = ctrl) \n## } \n## predictLearner.cluster.FarthestFirst = function(.learner, .model, .newdata, ...) { \n##   # RWeka returns cluster indices (i.e. starting from 0, which some tools don't like \n##   as.integer(predict(.model$learner.model, .newdata, ...)) + 1L \n## } \nmakeRLearner.multilabel.rFerns = function() { \n  makeRLearnerMultilabel( \n    cl = \nmultilabel.rFerns\n, \n    package = \nrFerns\n, \n    par.set = makeParamSet( \n      makeIntegerLearnerParam(id = \ndepth\n, default = 5L), \n      makeIntegerLearnerParam(id = \nferns\n, default = 1000L) \n    ), \n    properties = c(\nnumerics\n, \nfactors\n, \nordered\n), \n    name = \nRandom ferns\n, \n    short.name = \nrFerns\n, \n    note = \n \n  ) \n} \n## trainLearner.multilabel.rFerns = function(.learner, .task, .subset, .weights = NULL, ...) { \n##   d = getTaskData(.task, .subset, target.extra = TRUE) \n##   rFerns::rFerns(x = d$data, y = as.matrix(d$target), ...) \n## } \n## predictLearner.multilabel.rFerns = function(.learner, .model, .newdata, ...) { \n##   as.matrix(predict(.model$learner.model, .newdata, ...)) \n## } \n## getFeatureImportanceLearner.classif.rpart = function(.learner, .model, ...) { \n##   mod = getLearnerModel(.model) \n##   mod$variable.importance \n## } \n## getFeatureImportanceLearner.classif.randomForestSRC = function(.learner, .model, ...) { \n##   mod = getLearnerModel(.model) \n##   randomForestSRC::vimp(mod, ...)$importance[, \nall\n] \n## } \n## NA \n## registerS3method(\nmakeRLearner\n, \nawesome_new_learner_class\n, makeRLearner.\nawesome_new_learner_class\n) \n## registerS3method(\ntrainLearner\n, \nawesome_new_learner_class\n, trainLearner.\nawesome_new_learner_class\n) \n## registerS3method(\npredictLearner\n, \nawesome_new_learner_class\n, predictLearner.\nawesome_new_learner_class\n) \n## registerS3method(\ngetFeatureImportanceLearner\n, \nawesome_new_learner_class\n, \n##   getFeatureImportanceLearner.\nawesome_new_learner_class\n) \n## parallelExport(\ntrainLearner.\nawesome_new_learner_class\n, \npredictLearner.\nawesome_new_learner_class\n) \n## test_that(\nclassif_lda\n, { \n##   requirePackagesOrSkip(\nMASS\n, default.method = \nload\n) \n##  \n##   set.seed(getOption(\nmlr.debug.seed\n)) \n##   m = MASS::lda(formula = multiclass.formula, data = multiclass.train) \n##   set.seed(getOption(\nmlr.debug.seed\n)) \n##   p = predict(m, newdata = multiclass.test) \n##  \n##   testSimple(\nclassif.lda\n, multiclass.df, multiclass.target, multiclass.train.inds, p$class) \n##   testProb(\nclassif.lda\n, multiclass.df, multiclass.target, multiclass.train.inds, p$posterior) \n## }) \n## test_that(\nregr_randomForest\n, { \n##   requirePackagesOrSkip(\nrandomForest\n, default.method = \nload\n) \n##  \n##   parset.list = list( \n##     list(), \n##     list(ntree = 5, mtry = 2), \n##     list(ntree = 5, mtry = 4), \n##     list(proximity = TRUE, oob.prox = TRUE), \n##     list(nPerm = 3) \n##   ) \n##  \n##   old.predicts.list = list() \n##  \n##   for (i in 1:length(parset.list)) { \n##     parset = parset.list[[i]] \n##     pars = list(formula = regr.formula, data = regr.train) \n##     pars = c(pars, parset) \n##     set.seed(getOption(\nmlr.debug.seed\n)) \n##     m = do.call(randomForest::randomForest, pars) \n##     set.seed(getOption(\nmlr.debug.seed\n)) \n##     p = predict(m, newdata = regr.test, type = \nresponse\n) \n##     old.predicts.list[[i]] = p \n##   } \n##  \n##   testSimpleParsets(\nregr.randomForest\n, regr.df, regr.target, \n##     regr.train.inds, old.predicts.list, parset.list) \n## }) \n## devtools::test(\nmlr\n, filter = \nclassif\n)", 
            "title": "Create Custom Learners"
        }, 
        {
            "location": "/create_learner/index.html#integrating-another-learner", 
            "text": "In order to integrate a learning algorithm into  mlr  some interface code has to be written.\nThree functions are mandatory for each learner.   First, define a new learner class with a name, description, capabilities, parameters,\n  and a few other things.\n  (An object of this class can then be generated by  makeLearner .)  Second, you need to provide a function that calls the learner function and builds the\n  model given data (which makes it possible to invoke training by calling  mlr 's  train \n  function).  Finally, a prediction function that returns predicted values given new data is required\n  (which enables invoking prediction by calling  mlr 's  predict \n  function).   Technically, integrating a learning method means introducing a new S3  class \nand implementing the corresponding methods for the generic functions  makeRLerner , trainLearner , and  predictLearner .\nTherefore we start with a quick overview of the involved  classes  and\nconstructor functions.", 
            "title": "Integrating Another Learner"
        }, 
        {
            "location": "/create_learner/index.html#classes-constructors-and-naming-schemes", 
            "text": "As you already know  makeLearner  generates an object of class  Learner .  class(makeLearner(cl =  classif.lda ))\n#  [1]  classif.lda       RLearnerClassif   RLearner          Learner \n\nclass(makeLearner(cl =  regr.lm ))\n#  [1]  regr.lm        RLearnerRegr   RLearner       Learner \n\nclass(makeLearner(cl =  surv.coxph ))\n#  [1]  surv.coxph     RLearnerSurv   RLearner       Learner \n\nclass(makeLearner(cl =  cluster.kmeans ))\n#  [1]  cluster.kmeans    RLearnerCluster   RLearner          Learner \n\nclass(makeLearner(cl =  multilabel.rFerns ))\n#  [1]  multilabel.rFerns    RLearnerMultilabel   RLearner           \n#  [4]  Learner   The first element of each  class  attribute vector is the name of the learner\nclass passed to the  cl  argument of  makeLearner .\nObviously, this adheres to the naming conventions   \"classif. R_method_name \"  for classification,  \"multilabel. R_method_name \"  for multilabel classification,  \"regr. R_method_name \"  for regression,  \"surv. R_method_name \"  for survival analysis, and  \"cluster. R_method_name \"  for clustering.   Additionally, there exist intermediate classes that reflect the type of learning problem, i.e.,\nall classification learners inherit from  RLearnerClassif , all regression learners\nfrom  RLearnerRegr  and so on.\nTheir superclasses are  RLearner  and finally  Learner .\nFor all these (sub)classes there exist constructor functions  makeRLearner , makeRLearnerClassif ,  makeRLearneRegr  etc. that are called\ninternally by  makeLearner .  A short side remark:\nAs you might have noticed there does not exist a special learner class for cost-sensitive classification (costsens)  with example-specific\ncosts. This type of learning task is currently exclusively handled through  wrappers \nlike  makeCostSensWeightedPairsWrapper .  In the following we show how to integrate learners for the five types of learning tasks\nmentioned above.\nDefining a completely new type of learner that has special properties and does not fit into\none of the existing schemes is of course possible, but much more advanced and not covered\nhere.  We use a classification example to explain some general principles (so even if you are\ninterested in integrating a learner for another type of learning task you might want to read\nthe following section).\nExamples for other types of learning tasks are shown later on.", 
            "title": "Classes, constructors, and naming schemes"
        }, 
        {
            "location": "/create_learner/index.html#classification", 
            "text": "We show how the  Linear Discriminant Analysis  from\npackage  MASS  has been integrated\ninto the classification learner  classif.lda  in  mlr  as an example.", 
            "title": "Classification"
        }, 
        {
            "location": "/create_learner/index.html#definition-of-the-learner", 
            "text": "The minimal information required to define a learner is the  mlr  name of the\nlearner, its package, the parameter set, and the set of properties of your\nlearner. In addition, you may provide a human-readable name, a short name and a\nnote with information relevant to users of the learner.  First, name your learner. According to the naming conventions above the name starts with classif.  and we choose  classif.lda .  Second, we need to define the parameters of the learner. These are any options\nthat can be set when running it to change how it learns, how input is\ninterpreted, how and what output is generated, and so on.  mlr  provides a\nnumber of functions to define parameters, a complete list can be found in the\ndocumentation of  LearnerParam  of the ParamHelpers  package.  In our example, we have discrete and numeric parameters, so we use makeDiscreteLearnerParam  and makeNumericLearnerParam  to incorporate the\ncomplete description of the parameters. We include all possible values for\ndiscrete parameters and lower and upper bounds for numeric parameters. Strictly\nspeaking it is not necessary to provide bounds for all parameters and if this\ninformation is not available they can be estimated, but providing accurate and\nspecific information here makes it possible to tune the learner much better (see\nthe section on  tuning ).  Next, we add information on the properties of the learner (see also the section\non  learners ). Which types of features are supported (numerics,\nfactors)? Are case weights supported? Are class weights supported? Can the method deal\nwith missing values in the features and deal with NA's in a meaningful way (not  na.omit )?\nAre one-class, two-class, multi-class problems supported? Can the learner predict\nposterior probabilities?  If the learner supports class weights the name of the relevant learner parameter\ncan be specified via argument  class.weights.param .  Below is the complete code for the definition of the LDA learner. It has one\ndiscrete parameter,  method , and two continuous ones,  nu  and  tol . It\nsupports classification problems with two or more classes and can deal with\nnumeric and factor explanatory variables. It can predict posterior\nprobabilities.  makeRLearner.classif.lda = function() {\n  makeRLearnerClassif(\n    cl =  classif.lda ,\n    package =  MASS ,\n    par.set = makeParamSet(\n      makeDiscreteLearnerParam(id =  method , default =  moment , values = c( moment ,  mle ,  mve ,  t )),\n      makeNumericLearnerParam(id =  nu , lower = 2, requires = quote(method ==  t )),\n      makeNumericLearnerParam(id =  tol , default = 1e-4, lower = 0),\n      makeDiscreteLearnerParam(id =  predict.method , values = c( plug-in ,  predictive ,  debiased ),\n        default =  plug-in , when =  predict ),\n      makeLogicalLearnerParam(id =  CV , default = FALSE, tunable = FALSE)\n    ),\n    properties = c( twoclass ,  multiclass ,  numerics ,  factors ,  prob ),\n    name =  Linear Discriminant Analysis ,\n    short.name =  lda ,\n    note =  Learner param 'predict.method' maps to 'method' in predict.lda. \n  )\n}", 
            "title": "Definition of the learner"
        }, 
        {
            "location": "/create_learner/index.html#creating-the-training-function-of-the-learner", 
            "text": "Once the learner has been defined, we need to tell  mlr  how to call it to\ntrain a model. The name of the function has to start with  trainLearner. ,\nfollowed by the  mlr  name of the learner as defined above ( classif.lda \nhere). The prototype of the function looks as follows.  function(.learner, .task, .subset, .weights = NULL, ...) { }  This function must fit a model on the data of the task  .task  with regard to\nthe subset defined in the integer vector  .subset  and the parameters passed\nin the  ...  arguments. Usually, the data should be extracted from the task\nusing  getTaskData . This will take care of any subsetting as well. It must\nreturn the fitted model.  mlr  assumes no special data type for the return\nvalue -- it will be passed to the predict function we are going to define below,\nso any special code the learner may need can be encapsulated there.  For our example, the definition of the function looks like this. In addition to\nthe data of the task, we also need the formula that describes what to predict.\nWe use the function  getTaskFormula  to extract this from the task.  trainLearner.classif.lda = function(.learner, .task, .subset, .weights = NULL, ...) {\n  f = getTaskFormula(.task)\n  MASS::lda(f, data = getTaskData(.task, .subset), ...)\n}", 
            "title": "Creating the training function of the learner"
        }, 
        {
            "location": "/create_learner/index.html#creating-the-prediction-method", 
            "text": "Finally, the prediction function needs to be defined. The name of this function\nstarts with  predictLearner. , followed again by the  mlr  name of the\nlearner. The prototype of the function is as follows.  function(.learner, .model, .newdata, ...) { }  It must predict for the new observations in the  data.frame   .newdata  with\nthe wrapped model  .model , which is returned from the training function.\nThe actual model the learner built is stored in the  $learner.model  member\nand can be accessed simply through  .model$learner.model .  For classification, you have to return a factor of predicted classes if .learner$predict.type  is  \"response\" , or a matrix of predicted\nprobabilities if  .learner$predict.type  is  \"prob\"  and this type of\nprediction is supported by the learner. In the latter case the matrix must have\nthe same number of columns as there are classes in the task and the columns have\nto be named by the class names.  The definition for LDA looks like this. It is pretty much just a straight\npass-through of the arguments to the  predict  function and some extraction of\nprediction data depending on the type of prediction requested.  predictLearner.classif.lda = function(.learner, .model, .newdata, predict.method =  plug-in , ...) {\n  p = predict(.model$learner.model, newdata = .newdata, method = predict.method, ...)\n  if (.learner$predict.type ==  response ) \n    return(p$class) else return(p$posterior)\n}", 
            "title": "Creating the prediction method"
        }, 
        {
            "location": "/create_learner/index.html#regression", 
            "text": "The main difference for regression is that the type of predictions are different\n(numeric instead of labels or probabilities) and that not all of the properties\nare relevant. In particular, whether one-, two-, or multi-class problems and\nposterior probabilities are supported is not applicable.  Apart from this, everything explained above applies. Below is the definition for\nthe  earth  learner from the earth  package.  makeRLearner.regr.earth = function() {\n  makeRLearnerRegr(\n    cl =  regr.earth ,\n    package =  earth ,\n    par.set = makeParamSet(\n      makeLogicalLearnerParam(id =  keepxy , default = FALSE, tunable = FALSE),\n      makeNumericLearnerParam(id =  trace , default = 0, upper = 10, tunable = FALSE),\n      makeIntegerLearnerParam(id =  degree , default = 1L, lower = 1L),\n      makeNumericLearnerParam(id =  penalty ),\n      makeIntegerLearnerParam(id =  nk , lower = 0L),\n      makeNumericLearnerParam(id =  thres , default = 0.001),\n      makeIntegerLearnerParam(id =  minspan , default = 0L),\n      makeIntegerLearnerParam(id =  endspan , default = 0L),\n      makeNumericLearnerParam(id =  newvar.penalty , default = 0),\n      makeIntegerLearnerParam(id =  fast.k , default = 20L, lower = 0L),\n      makeNumericLearnerParam(id =  fast.beta , default = 1),\n      makeDiscreteLearnerParam(id =  pmethod , default =  backward ,\n        values = c( backward ,  none ,  exhaustive ,  forward ,  seqrep ,  cv )),\n      makeIntegerLearnerParam(id =  nprune )\n    ),\n    properties = c( numerics ,  factors ),\n    name =  Multivariate Adaptive Regression Splines ,\n    short.name =  earth ,\n    note =  \n  )\n}  trainLearner.regr.earth = function(.learner, .task, .subset, .weights = NULL, ...) {\n  f = getTaskFormula(.task)\n  earth::earth(f, data = getTaskData(.task, .subset), ...)\n}  predictLearner.regr.earth = function(.learner, .model, .newdata, ...) {\n  predict(.model$learner.model, newdata = .newdata)[, 1L]\n}  Again most of the data is passed straight through to/from the train/predict\nfunctions of the learner.", 
            "title": "Regression"
        }, 
        {
            "location": "/create_learner/index.html#survival-analysis", 
            "text": "For survival analysis, you have to return so-called linear predictors in order to compute\nthe default measure for this task type, the  cindex  (for .learner$predict.type  ==  \"response\" ). For  .learner$predict.type  ==  \"prob\" ,\nthere is no substantially meaningful measure (yet). You may either ignore this case or return\nsomething like predicted survival curves (cf. example below).  There are three properties that are specific to survival learners:\n\"rcens\", \"lcens\" and \"icens\", defining the type(s) of censoring a learner can handle -- right,\nleft and/or interval censored.  Let's have a look at how the  Cox Proportional Hazard Model  from\npackage  survival  has been integrated\ninto the survival learner  surv.coxph  in  mlr  as an example:  makeRLearner.surv.coxph = function() {\n  makeRLearnerSurv(\n    cl =  surv.coxph ,\n    package =  survival ,\n    par.set = makeParamSet(\n      makeDiscreteLearnerParam(id =  ties , default =  efron , values = c( efron ,  breslow ,  exact )),\n      makeLogicalLearnerParam(id =  singular.ok , default = TRUE),\n      makeNumericLearnerParam(id =  eps , default = 1e-09, lower = 0),\n      makeNumericLearnerParam(id =  toler.chol , default = .Machine$double.eps^0.75, lower = 0),\n      makeIntegerLearnerParam(id =  iter.max , default = 20L, lower = 1L),\n      makeNumericLearnerParam(id =  toler.inf , default = sqrt(.Machine$double.eps^0.75), lower = 0),\n      makeIntegerLearnerParam(id =  outer.max , default = 10L, lower = 1L),\n      makeLogicalLearnerParam(id =  model , default = FALSE, tunable = FALSE),\n      makeLogicalLearnerParam(id =  x , default = FALSE, tunable = FALSE),\n      makeLogicalLearnerParam(id =  y , default = TRUE, tunable = FALSE)\n    ),\n    properties = c( missings ,  numerics ,  factors ,  weights ,  prob ,  rcens ),\n    name =  Cox Proportional Hazard Model ,\n    short.name =  coxph ,\n    note =  \n  )\n}  trainLearner.surv.coxph = function(.learner, .task, .subset, .weights = NULL, ...) {\n  f = getTaskFormula(.task)\n  data = getTaskData(.task, subset = .subset)\n  if (is.null(.weights)) {\n    survival::coxph(formula = f, data = data, ...)\n  } else {\n    survival::coxph(formula = f, data = data, weights = .weights, ...)\n  }\n}  predictLearner.surv.coxph = function(.learner, .model, .newdata, ...) {\n  predict(.model$learner.model, newdata = .newdata, type =  lp , ...)\n}", 
            "title": "Survival analysis"
        }, 
        {
            "location": "/create_learner/index.html#clustering", 
            "text": "For clustering, you have to return a numeric vector with the IDs of the clusters\nthat the respective datum has been assigned to. The numbering should start at 1.  Below is the definition for the  FarthestFirst  learner\nfrom the  RWeka  package. Weka\nstarts the IDs of the clusters at 0, so we add 1 to the predicted clusters.\nRWeka has a different way of setting learner parameters; we use the special Weka_control  function to do this.  makeRLearner.cluster.FarthestFirst = function() {\n  makeRLearnerCluster(\n    cl =  cluster.FarthestFirst ,\n    package =  RWeka ,\n    par.set = makeParamSet(\n      makeIntegerLearnerParam(id =  N , default = 2L, lower = 1L),\n      makeIntegerLearnerParam(id =  S , default = 1L, lower = 1L),\n      makeLogicalLearnerParam(id =  output-debug-info , default = FALSE, tunable = FALSE)\n    ),\n    properties = c( numerics ),\n    name =  FarthestFirst Clustering Algorithm ,\n    short.name =  farthestfirst \n  )\n}  trainLearner.cluster.FarthestFirst = function(.learner, .task, .subset, .weights = NULL, ...) {\n  ctrl = RWeka::Weka_control(...)\n  RWeka::FarthestFirst(getTaskData(.task, .subset), control = ctrl)\n}  predictLearner.cluster.FarthestFirst = function(.learner, .model, .newdata, ...) {\n  as.integer(predict(.model$learner.model, .newdata, ...)) + 1L\n}", 
            "title": "Clustering"
        }, 
        {
            "location": "/create_learner/index.html#multilabel-classification", 
            "text": "As stated in the  multilabel  section, multilabel classification\nmethods can be divided into problem transformation methods and algorithm adaptation methods.  At this moment the only problem transformation method implemented in  mlr \nis the  binary relevance method . Integrating more of\nthese methods requires good knowledge of the architecture of the  mlr  package.  The integration of an algorithm adaptation multilabel classification learner is easier and\nworks very similar to the normal multiclass-classification.\nIn contrast to the multiclass case, not all of the learner properties are relevant.\nIn particular, whether one-, two-, or multi-class problems are supported is not applicable.\nFurthermore the prediction function output must be a matrix\nwith each prediction of a label in one column and the names of the labels\nas column names. If  .learner$predict.type  is  \"response\"  the predictions must\nbe logical. If  .learner$predict.type  is  \"prob\"  and this type of\nprediction is supported by the learner, the matrix must consist of predicted\nprobabilities.  Below is the definition of the  rFerns  learner from the rFerns  package, which does not support probability predictions.  makeRLearner.multilabel.rFerns = function() {\n  makeRLearnerMultilabel(\n    cl =  multilabel.rFerns ,\n    package =  rFerns ,\n    par.set = makeParamSet(\n      makeIntegerLearnerParam(id =  depth , default = 5L),\n      makeIntegerLearnerParam(id =  ferns , default = 1000L)\n    ),\n    properties = c( numerics ,  factors ,  ordered ),\n    name =  Random ferns ,\n    short.name =  rFerns ,\n    note =  \n  )\n}  trainLearner.multilabel.rFerns = function(.learner, .task, .subset, .weights = NULL, ...) {\n  d = getTaskData(.task, .subset, target.extra = TRUE)\n  rFerns::rFerns(x = d$data, y = as.matrix(d$target), ...)\n}  predictLearner.multilabel.rFerns = function(.learner, .model, .newdata, ...) {\n  as.matrix(predict(.model$learner.model, .newdata, ...))\n}", 
            "title": "Multilabel classification"
        }, 
        {
            "location": "/create_learner/index.html#creating-a-new-method-for-extracting-feature-importance-values", 
            "text": "Some learners, for example decision trees and random forests, can calculate feature importance\nvalues, which can be extracted from a  fitted model  using function getFeatureImportance .  If your newly integrated learner supports this you need to   add  \"featimp\"  to the learner properties and  implement a new S3 method for function  getFeatureImportanceLearner  (which later is called\n  internally by  getFeatureImportance )   in order to make this work.  This method takes the  Learner   .learner , the  WrappedModel   .model \nand potential further arguments and calculates or extracts the feature importance.\nIt must return a named vector of importance values.  Below are two simple examples.\nIn case of  \"classif.rpart\"  the feature importance values can be easily extracted from the\nfitted model.  getFeatureImportanceLearner.classif.rpart = function(.learner, .model, ...) {\n  mod = getLearnerModel(.model, more.unwrap = TRUE)\n  mod$variable.importance\n}  For the  random forest  from package  randomForestSRC  function vimp  is called.  getFeatureImportanceLearner.classif.randomForestSRC = function(.learner, .model, ...) {\n  mod = getLearnerModel(.model, more.unwrap = TRUE)\n  randomForestSRC::vimp(mod, ...)$importance[,  all ]\n}", 
            "title": "Creating a new method for extracting feature importance values"
        }, 
        {
            "location": "/create_learner/index.html#creating-a-new-method-for-extracting-out-of-bag-predictions", 
            "text": "Many ensemble learners generate out-of-bag predictions (OOB predictions) automatically. mlr  provides the function  getOOBPreds  to access these predictions in the  mlr  framework.  If your newly integrated learner is able to calculate OOB predictions and you want to be\nable to access them in  mlr  via  getOOBPreds  you need to   add  \"oobpreds\"  to the learner properties and  implement a new S3 method for function  getOOBPredsLearner  (which later is called\n  internally by  getOOBPreds ).   This method takes the  Learner   .learner  and the  WrappedModel  .model  and extracts the OOB predictions.\nIt must return the predictions in the same format as the  predictLearner  function.  getOOBPredsLearner.classif.randomForest = function(.learner, .model) {\n  if (.learner$predict.type ==  response ) {\n    m = getLearnerModel(.model, more.unwrap = TRUE)\n    unname(m$predicted)\n  } else {\n    getLearnerModel(.model, more.unwrap = TRUE)$votes\n  }\n}", 
            "title": "Creating a new method for extracting out-of-bag predictions"
        }, 
        {
            "location": "/create_learner/index.html#registering-your-learner", 
            "text": "If your interface code to a new learning algorithm exists only locally, i.e., it is not (yet)\nmerged into  mlr  or does not live in an extra package with a proper namespace you might want\nto register the new S3 methods to make sure that these are found by, e.g.,  listLearners .\nYou can do this as follows:  registerS3method( makeRLearner ,  awesome_new_learner_class , makeRLearner. awesome_new_learner_class )\nregisterS3method( trainLearner ,  awesome_new_learner_class , trainLearner. awesome_new_learner_class )\nregisterS3method( predictLearner ,  awesome_new_learner_class , predictLearner. awesome_new_learner_class )  If you have written more methods, for example in order to extract feature importance values\nor out-of-bag predictions these also need to be registered in the same manner, for example:  registerS3method( getFeatureImportanceLearner ,  awesome_new_learner_class ,\n  getFeatureImportanceLearner. awesome_new_learner_class )  For the new learner to work with parallelization, you may have to export the new\nmethods explicitly:  parallelExport( trainLearner. awesome_new_learner_class ,  predictLearner. awesome_new_learner_class )", 
            "title": "Registering your learner"
        }, 
        {
            "location": "/create_learner/index.html#further-information-for-developers", 
            "text": "If you haven't written a learner interface for private use only, but intend to send a pull\nrequest to have it included in the  mlr  package there are a few things to take care of,\nmost importantly unit testing!  For general information about contributing to the package, unit testing, version control setup\nand the like please also read the coding guidelines in the mlr Wiki .    The R file containing the interface code should adhere to the naming convention RLearner_ type _ learner_name .R , e.g.,  RLearner_classif_lda.R , see for example https://github.com/mlr-org/mlr/blob/master/R/RLearner_classif_lda.R  and contain the\nnecessary roxygen  @export  tags to register the S3 methods in the NAMESPACE.    The learner interfaces should work out of the box without requiring any parameters to be set,\ne.g.,  train(\"classif.lda\", iris.task)  should run.\nSometimes, this makes it necessary to change or set some additional defaults as explained above\nand -- very important -- informing the user about this in the  note .    The parameter set of the learner should be as complete as possible.    Every learner interface must be unit tested.", 
            "title": "Further information for developers"
        }, 
        {
            "location": "/create_learner/index.html#unit-testing", 
            "text": "The tests make sure that we get the same results when the learner is invoked through the  mlr \ninterface and when using the original functions.\nIf you are not familiar or want to learn more about unit testing and package  testthat \nhave a look at  the Testing chapter in Hadley Wickham's R packages .  In  mlr  all unit tests are in the following directory: https://github.com/mlr-org/mlr/tree/master/tests/testthat .\nFor each learner interface there is an individual file whose name follows the scheme test_ type _ learner_name .R , for example https://github.com/mlr-org/mlr/blob/master/tests/testthat/test_classif_lda.R .  Below is a snippet from the tests of the lda interface https://github.com/mlr-org/mlr/blob/master/tests/testthat/test_classif_lda.R .  test_that( classif_lda , {\n  requirePackagesOrSkip( MASS , default.method =  load )\n\n  set.seed(getOption( mlr.debug.seed ))\n  m = MASS::lda(formula = multiclass.formula, data = multiclass.train)\n  set.seed(getOption( mlr.debug.seed ))\n  p = predict(m, newdata = multiclass.test)\n\n  testSimple( classif.lda , multiclass.df, multiclass.target, multiclass.train.inds, p$class)\n  testProb( classif.lda , multiclass.df, multiclass.target, multiclass.train.inds, p$posterior)\n})  The tests make use of numerous helper objects and helper functions.\nAll of these are defined in the  helper_  files in https://github.com/mlr-org/mlr/blob/master/tests/testthat/ .  In the above code the first line just loads package  MASS  or skips the test if the package is\nnot available.\nThe objects  multiclass.formula ,  multiclass.train ,  multiclass.test  etc. are defined\nin  https://github.com/mlr-org/mlr/blob/master/tests/testthat/helper_objects.R .\nWe tried to choose fairly self-explanatory names:\nFor example  multiclass  indicates a multi-class classification problem,  multiclass.train \ncontains data for training,  multiclass.formula  a  formula  object etc.  The test fits an lda model on the training set and makes predictions on the test set\nusing the original functions  lda  and  predict.lda .\nThe helper functions  testSimple  and  testProb  perform training and prediction on the same\ndata using the  mlr  interface --  testSimple  for  predict.type = \"response  and  testProbs  for predict.type = \"prob\"  -- and check if the predicted class labels and probabilities coincide\nwith the outcomes  p$class  and  p$posterior .  In order to get reproducible results seeding is required for many learners.\nThe  \"mlr.debug.seed\"  works as follows:\nWhen invoking the tests the option  \"mlr.debug.seed\"  is set\n(see  https://github.com/mlr-org/mlr/blob/master/tests/testthat/helper_zzz.R ), and set.seed(getOption(\"mlr.debug.seed\"))  is used to specify the seed.\nInternally,  mlr 's train  and predict.WrappedModel \nfunctions check if the  \"mlr.debug.seed\"  option is set and if yes, also specify the seed.  Note that the option  \"mlr.debug.seed\"  is only set for testing, so no seeding happens in\nnormal usage of  mlr .  Let's look at a second example.\nMany learners have parameters that are commonly changed or tuned and it is important to make\nsure that these are passed through correctly.\nBelow is a snippet from  https://github.com/mlr-org/mlr/blob/master/tests/testthat/test_regr_randomForest.R .  test_that( regr_randomForest , {\n  requirePackagesOrSkip( randomForest , default.method =  load )\n\n  parset.list = list(\n    list(),\n    list(ntree = 5, mtry = 2),\n    list(ntree = 5, mtry = 4),\n    list(proximity = TRUE, oob.prox = TRUE),\n    list(nPerm = 3)\n  )\n\n  old.predicts.list = list()\n\n  for (i in 1:length(parset.list)) {\n    parset = parset.list[[i]]\n    pars = list(formula = regr.formula, data = regr.train)\n    pars = c(pars, parset)\n    set.seed(getOption( mlr.debug.seed ))\n    m = do.call(randomForest::randomForest, pars)\n    set.seed(getOption( mlr.debug.seed ))\n    p = predict(m, newdata = regr.test, type =  response )\n    old.predicts.list[[i]] = p\n  }\n\n  testSimpleParsets( regr.randomForest , regr.df, regr.target,\n    regr.train.inds, old.predicts.list, parset.list)\n})  All tested parameter configurations are collected in the  parset.list .\nIn order to make sure that the default parameter configuration is tested the first element\nof the  parset.list  is an empty  list .\nThen we simply loop over all parameter settings and store the resulting predictions in old.predicts.list .\nAgain the helper function  testSimpleParsets  does the same using the  mlr  interface\nand compares the outcomes.  Additional to tests for individual learners we also have general tests that loop through all\nintegrated learners and make for example sure that learners have the correct properties\n(e.g. a learner with property  \"factors\"  can cope with  factor  features,\na learner with property  \"weights\"  takes observation weights into account properly etc.).\nFor example  https://github.com/mlr-org/mlr/blob/master/tests/testthat/test_learners_all_classif.R \nruns through all classification learners.\nSimilar tests exist for all types of learning methods like regression, cluster and survival analysis\nas well as multilabel classification.  In order to run all tests for, e.g., classification learners on your machine you can invoke\nthe tests from within  R  by  devtools::test( mlr , filter =  classif )  or from the command line using  Michel's rt tool  rtest --filter=classif", 
            "title": "Unit testing"
        }, 
        {
            "location": "/create_learner/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  class(makeLearner(cl =  classif.lda )) \n\nclass(makeLearner(cl =  regr.lm )) \n\nclass(makeLearner(cl =  surv.coxph )) \n\nclass(makeLearner(cl =  cluster.kmeans )) \n\nclass(makeLearner(cl =  multilabel.rFerns )) \nmakeRLearner.classif.lda = function() { \n  makeRLearnerClassif( \n    cl =  classif.lda , \n    package =  MASS , \n    par.set = makeParamSet( \n      makeDiscreteLearnerParam(id =  method , default =  moment , values = c( moment ,  mle ,  mve ,  t )), \n      makeNumericLearnerParam(id =  nu , lower = 2, requires = quote(method ==  t )), \n      makeNumericLearnerParam(id =  tol , default = 1e-4, lower = 0), \n      makeDiscreteLearnerParam(id =  predict.method , values = c( plug-in ,  predictive ,  debiased ), \n        default =  plug-in , when =  predict ), \n      makeLogicalLearnerParam(id =  CV , default = FALSE, tunable = FALSE) \n    ), \n    properties = c( twoclass ,  multiclass ,  numerics ,  factors ,  prob ), \n    name =  Linear Discriminant Analysis , \n    short.name =  lda , \n    note =  Learner param 'predict.method' maps to 'method' in predict.lda.  \n  ) \n} \n## function(.learner, .task, .subset, .weights = NULL, ...) { } \n## trainLearner.classif.lda = function(.learner, .task, .subset, .weights = NULL,  ...) { \n##   f = getTaskFormula(.task) \n##   MASS::lda(f, data = getTaskData(.task, .subset), ...) \n## } \n## function(.learner, .model, .newdata, ...) { } \n## predictLearner.classif.lda = function(.learner, .model, .newdata, predict.method =  plug-in , ...) { \n##   p = predict(.model$learner.model, newdata = .newdata, method = predict.method, ...) \n##   if(.learner$predict.type ==  response ) \n##     return(p$class) \n##   else \n##     return(p$posterior) \n## } \nmakeRLearner.regr.earth = function() { \n  makeRLearnerRegr( \n    cl =  regr.earth , \n    package =  earth , \n    par.set = makeParamSet( \n      makeLogicalLearnerParam(id =  keepxy , default = FALSE, tunable = FALSE), \n      makeNumericLearnerParam(id =  trace , default = 0, upper = 10, tunable = FALSE), \n      makeIntegerLearnerParam(id =  degree , default = 1L, lower = 1L), \n      makeNumericLearnerParam(id =  penalty ), \n      makeIntegerLearnerParam(id =  nk , lower = 0L), \n      makeNumericLearnerParam(id =  thres , default = 0.001), \n      makeIntegerLearnerParam(id =  minspan , default = 0L), \n      makeIntegerLearnerParam(id =  endspan , default = 0L), \n      makeNumericLearnerParam(id =  newvar.penalty , default = 0), \n      makeIntegerLearnerParam(id =  fast.k , default = 20L, lower = 0L), \n      makeNumericLearnerParam(id =  fast.beta , default = 1), \n      makeDiscreteLearnerParam(id =  pmethod , default =  backward , \n        values = c( backward ,  none ,  exhaustive ,  forward ,  seqrep ,  cv )), \n      makeIntegerLearnerParam(id =  nprune ) \n    ), \n    properties = c( numerics ,  factors ), \n    name =  Multivariate Adaptive Regression Splines , \n    short.name =  earth , \n    note =   \n  ) \n} \n## trainLearner.regr.earth = function(.learner, .task, .subset, .weights = NULL,  ...) { \n##   f = getTaskFormula(.task) \n##   earth::earth(f, data = getTaskData(.task, .subset), ...) \n## } \n## predictLearner.regr.earth = function(.learner, .model, .newdata, ...) { \n##   predict(.model$learner.model, newdata = .newdata)[, 1L] \n## } \nmakeRLearner.surv.coxph = function() { \n  makeRLearnerSurv( \n    cl =  surv.coxph , \n    package =  survival , \n    par.set = makeParamSet( \n      makeDiscreteLearnerParam(id =  ties , default =  efron , values = c( efron ,  breslow ,  exact )), \n      makeLogicalLearnerParam(id =  singular.ok , default = TRUE), \n      makeNumericLearnerParam(id =  eps , default = 1e-09, lower = 0), \n      makeNumericLearnerParam(id =  toler.chol , default = .Machine$double.eps^0.75, lower = 0), \n      makeIntegerLearnerParam(id =  iter.max , default = 20L, lower = 1L), \n      makeNumericLearnerParam(id =  toler.inf , default = sqrt(.Machine$double.eps^0.75), lower = 0), \n      makeIntegerLearnerParam(id =  outer.max , default = 10L, lower = 1L), \n      makeLogicalLearnerParam(id =  model , default = FALSE, tunable = FALSE), \n      makeLogicalLearnerParam(id =  x , default = FALSE, tunable = FALSE), \n      makeLogicalLearnerParam(id =  y , default = TRUE, tunable = FALSE) \n    ), \n    properties = c( missings ,  numerics ,  factors ,  weights ,  prob ,  rcens ), \n    name =  Cox Proportional Hazard Model , \n    short.name =  coxph , \n    note =   \n  ) \n} \n## trainLearner.surv.coxph = function(.learner, .task, .subset, .weights = NULL,  ...) { \n##   f = getTaskFormula(.task) \n##   data = getTaskData(.task, subset = .subset) \n##   if (is.null(.weights)) { \n##     mod = survival::coxph(formula = f, data = data, ...) \n##   } else  { \n##     mod = survival::coxph(formula = f, data = data, weights = .weights, ...) \n##   } \n##   if (.learner$predict.type ==  prob ) \n##     mod = attachTrainingInfo(mod, list(surv.range = range(getTaskTargets(.task)[, 1L]))) \n##   mod \n## } \n## predictLearner.surv.coxph = function(.learner, .model, .newdata, ...) { \n##   if(.learner$predict.type ==  response ) { \n##     predict(.model$learner.model, newdata = .newdata, type =  lp , ...) \n##   } else if (.learner$predict.type ==  prob ) { \n##     surv.range = getTrainingInfo(.model$learner.model)$surv.range \n##     times = seq(from = surv.range[1L], to = surv.range[2L], length.out = 1000) \n##     t(summary(survival::survfit(.model$learner.model, newdata = .newdata, se.fit = FALSE, conf.int = FALSE), times = times)$surv) \n##   } else { \n##     stop( Unknown predict type ) \n##   } \n## } \nmakeRLearner.cluster.FarthestFirst = function() { \n  makeRLearnerCluster( \n    cl =  cluster.FarthestFirst , \n    package =  RWeka , \n    par.set = makeParamSet( \n      makeIntegerLearnerParam(id =  N , default = 2L, lower = 1L), \n      makeIntegerLearnerParam(id =  S , default = 1L, lower = 1L), \n      makeLogicalLearnerParam(id =  output-debug-info , default = FALSE, tunable = FALSE) \n    ), \n    properties = c( numerics ), \n    name =  FarthestFirst Clustering Algorithm , \n    short.name =  farthestfirst  \n  ) \n} \n## trainLearner.cluster.FarthestFirst = function(.learner, .task, .subset, .weights = NULL,  ...) { \n##   ctrl = RWeka::Weka_control(...) \n##   RWeka::FarthestFirst(getTaskData(.task, .subset), control = ctrl) \n## } \n## predictLearner.cluster.FarthestFirst = function(.learner, .model, .newdata, ...) { \n##   # RWeka returns cluster indices (i.e. starting from 0, which some tools don't like \n##   as.integer(predict(.model$learner.model, .newdata, ...)) + 1L \n## } \nmakeRLearner.multilabel.rFerns = function() { \n  makeRLearnerMultilabel( \n    cl =  multilabel.rFerns , \n    package =  rFerns , \n    par.set = makeParamSet( \n      makeIntegerLearnerParam(id =  depth , default = 5L), \n      makeIntegerLearnerParam(id =  ferns , default = 1000L) \n    ), \n    properties = c( numerics ,  factors ,  ordered ), \n    name =  Random ferns , \n    short.name =  rFerns , \n    note =   \n  ) \n} \n## trainLearner.multilabel.rFerns = function(.learner, .task, .subset, .weights = NULL, ...) { \n##   d = getTaskData(.task, .subset, target.extra = TRUE) \n##   rFerns::rFerns(x = d$data, y = as.matrix(d$target), ...) \n## } \n## predictLearner.multilabel.rFerns = function(.learner, .model, .newdata, ...) { \n##   as.matrix(predict(.model$learner.model, .newdata, ...)) \n## } \n## getFeatureImportanceLearner.classif.rpart = function(.learner, .model, ...) { \n##   mod = getLearnerModel(.model) \n##   mod$variable.importance \n## } \n## getFeatureImportanceLearner.classif.randomForestSRC = function(.learner, .model, ...) { \n##   mod = getLearnerModel(.model) \n##   randomForestSRC::vimp(mod, ...)$importance[,  all ] \n## } \n## NA \n## registerS3method( makeRLearner ,  awesome_new_learner_class , makeRLearner. awesome_new_learner_class ) \n## registerS3method( trainLearner ,  awesome_new_learner_class , trainLearner. awesome_new_learner_class ) \n## registerS3method( predictLearner ,  awesome_new_learner_class , predictLearner. awesome_new_learner_class ) \n## registerS3method( getFeatureImportanceLearner ,  awesome_new_learner_class , \n##   getFeatureImportanceLearner. awesome_new_learner_class ) \n## parallelExport( trainLearner. awesome_new_learner_class ,  predictLearner. awesome_new_learner_class ) \n## test_that( classif_lda , { \n##   requirePackagesOrSkip( MASS , default.method =  load ) \n##  \n##   set.seed(getOption( mlr.debug.seed )) \n##   m = MASS::lda(formula = multiclass.formula, data = multiclass.train) \n##   set.seed(getOption( mlr.debug.seed )) \n##   p = predict(m, newdata = multiclass.test) \n##  \n##   testSimple( classif.lda , multiclass.df, multiclass.target, multiclass.train.inds, p$class) \n##   testProb( classif.lda , multiclass.df, multiclass.target, multiclass.train.inds, p$posterior) \n## }) \n## test_that( regr_randomForest , { \n##   requirePackagesOrSkip( randomForest , default.method =  load ) \n##  \n##   parset.list = list( \n##     list(), \n##     list(ntree = 5, mtry = 2), \n##     list(ntree = 5, mtry = 4), \n##     list(proximity = TRUE, oob.prox = TRUE), \n##     list(nPerm = 3) \n##   ) \n##  \n##   old.predicts.list = list() \n##  \n##   for (i in 1:length(parset.list)) { \n##     parset = parset.list[[i]] \n##     pars = list(formula = regr.formula, data = regr.train) \n##     pars = c(pars, parset) \n##     set.seed(getOption( mlr.debug.seed )) \n##     m = do.call(randomForest::randomForest, pars) \n##     set.seed(getOption( mlr.debug.seed )) \n##     p = predict(m, newdata = regr.test, type =  response ) \n##     old.predicts.list[[i]] = p \n##   } \n##  \n##   testSimpleParsets( regr.randomForest , regr.df, regr.target, \n##     regr.train.inds, old.predicts.list, parset.list) \n## }) \n## devtools::test( mlr , filter =  classif )", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/create_measure/index.html", 
            "text": "Integrating Another Measure\n\n\nIn some cases, you might want to evaluate a \nPrediction\n or \nResamplePrediction\n with a\n\nMeasure\n which is not yet implemented in \nmlr\n. This could be either a\nperformance measure which is not listed in the \nAppendix\n or a measure that\nuses a misclassification cost matrix.\n\n\nPerformance measures and aggregation schemes\n\n\nPerformance measures in \nmlr\n are objects of class \nMeasure\n.\nFor example the \nmse\n (mean squared error) looks as follows.\n\n\nstr(mse)\n#\n List of 10\n#\n  $ id        : chr \nmse\n\n#\n  $ minimize  : logi TRUE\n#\n  $ properties: chr [1:3] \nregr\n \nreq.pred\n \nreq.truth\n\n#\n  $ fun       :function (task, model, pred, feats, extra.args)  \n#\n  $ extra.args: list()\n#\n  $ best      : num 0\n#\n  $ worst     : num Inf\n#\n  $ name      : chr \nMean of squared errors\n\n#\n  $ note      : chr \nDefined as: mean((response - truth)^2)\n\n#\n  $ aggr      :List of 4\n#\n   ..$ id        : chr \ntest.mean\n\n#\n   ..$ name      : chr \nTest mean\n\n#\n   ..$ fun       :function (task, perf.test, perf.train, measure, group, pred)  \n#\n   ..$ properties: chr \nreq.test\n\n#\n   ..- attr(*, \nclass\n)= chr \nAggregation\n\n#\n  - attr(*, \nclass\n)= chr \nMeasure\n\n\nmse$fun\n#\n function (task, model, pred, feats, extra.args) \n#\n {\n#\n     measureMSE(pred$data$truth, pred$data$response)\n#\n }\n#\n \nbytecode: 0xf18df28\n\n#\n \nenvironment: namespace:mlr\n\n\nmeasureMSE\n#\n function (truth, response) \n#\n {\n#\n     mean((response - truth)^2)\n#\n }\n#\n \nbytecode: 0xd974088\n\n#\n \nenvironment: namespace:mlr\n\n\n\n\n\nSee the \nMeasure\n documentation page for a detailed description of the object\nslots.\n\n\nAt the core is slot \n$fun\n which contains the function that calculates the performance value.\nThe actual work is done by function \nmeasureMSE\n.\nSimilar functions, generally adhering to the naming scheme \nmeasure\n followed by the\ncapitalized measure ID, exist for most performance measures.\nSee the \nmeasures\n help page for a complete list.\n\n\nJust as \nTask\n and \nLearner\n objects each \nMeasure\n has an\nidentifier \n$id\n which is for example used to annotate results and plots.\nFor plots there is also the option to use the longer measure \n$name\n instead. See the\ntutorial page on \nVisualization\n for more information.\n\n\nMoreover, a \nMeasure\n includes a number of \n$properties\n that indicate for\nwhich types of learning problems it is suitable and what information is required to\ncalculate it.\nObviously, most measures need the \nPrediction\n object (\n\"req.pred\"\n) and, for supervised\nproblems, the true values of the target variable(s) (\n\"req.truth\"\n).\nYou can use functions \ngetMeasureProperties\n and\n\nhasMeasureProperties\n to determine the properties of a \nMeasure\n.\nMoreover, \nlistMeasureProperties\n shows all measure properties currently available in \nmlr\n.\n\n\nlistMeasureProperties()\n#\n  [1] \nclassif\n       \nclassif.multi\n \nmultilabel\n    \nregr\n         \n#\n  [5] \nsurv\n          \ncluster\n       \ncostsens\n      \nreq.pred\n     \n#\n  [9] \nreq.truth\n     \nreq.task\n      \nreq.feats\n     \nreq.model\n    \n#\n [13] \nreq.prob\n\n\n\n\n\nAdditional to its properties, each \nMeasure\n knows its extreme values \n$best\n\nand \n$worst\n and if it wants to be minimized or maximized (\n$minimize\n) during \ntuning\n\nor \nfeature selection\n.\n\n\nFor resampling slot \n$aggr\n specifies how the overall performance across all resampling\niterations is calculated. Typically, this is just a matter of aggregating the performance\nvalues obtained on the test sets \nperf.test\n or the training sets \nperf.train\n by a simple\nfunction. The by far most common scheme is \ntest.mean\n, i.e., the unweighted\nmean of the performances on the test sets.\n\n\nstr(test.mean)\n#\n List of 4\n#\n  $ id        : chr \ntest.mean\n\n#\n  $ name      : chr \nTest mean\n\n#\n  $ fun       :function (task, perf.test, perf.train, measure, group, pred)  \n#\n  $ properties: chr \nreq.test\n\n#\n  - attr(*, \nclass\n)= chr \nAggregation\n\n\ntest.mean$fun\n#\n function (task, perf.test, perf.train, measure, group, pred) \n#\n mean(perf.test)\n#\n \nbytecode: 0xa215d50\n\n#\n \nenvironment: namespace:mlr\n\n\n\n\n\nAll aggregation schemes are objects of class \nAggregation\n with the function in slot\n\n$fun\n doing the actual work.\nThe \n$properties\n member indicates if predictions (or performance values) on the training\nor test data sets are required to calculate the aggregation.\n\n\nYou can change the aggregation scheme of a \nMeasure\n via function\n\nsetAggregation\n. See the tutorial page on \nresampling\n for some examples\nand the \naggregations\n help page for all available aggregation schemes.\n\n\nYou can construct your own \nMeasure\n and \nAggregation\n objects via functions\n\nmakeMeasure\n, \nmakeCostMeasure\n, \nmakeCustomResampledMeasure\n and \nmakeAggregation\n.\nSome examples are shown in the following.\n\n\nConstructing a performance measure\n\n\nFunction \nmakeMeasure\n provides a simple way to construct your own performance measure.\n\n\nBelow this is exemplified by re-implementing the mean misclassification error\n(\nmmce\n).\nWe first write a function that computes the measure on the basis of the true and predicted\nclass labels.\nNote that this function must have certain formal arguments listed in the documentation of\n\nmakeMeasure\n.\nThen the \nMeasure\n object is created and we work with it as usual with the\n\nperformance\n function.\n\n\nSee the \nR\n documentation of \nmakeMeasure\n for more details on the various parameters.\n\n\n## Define a function that calculates the misclassification rate\nmy.mmce.fun = function(task, model, pred, feats, extra.args) {\n  tb = table(getPredictionResponse(pred), getPredictionTruth(pred))\n  1 - sum(diag(tb)) / sum(tb)\n}\n\n## Generate the Measure object\nmy.mmce = makeMeasure(\n  id = \nmy.mmce\n, name = \nMy Mean Misclassification Error\n,\n  properties = c(\nclassif\n, \nclassif.multi\n, \nreq.pred\n, \nreq.truth\n),\n  minimize = TRUE, best = 0, worst = 1,\n  fun = my.mmce.fun\n)\n\n## Train a learner and make predictions\nmod = train(\nclassif.lda\n, iris.task)\npred = predict(mod, task = iris.task)\n\n## Calculate the performance using the new measure\nperformance(pred, measures = my.mmce)\n#\n my.mmce \n#\n    0.02\n\n## Apparently the result coincides with the mlr implementation\nperformance(pred, measures = mmce)\n#\n mmce \n#\n 0.02\n\n\n\n\nConstructing a measure for ordinary misclassification costs\n\n\nFor in depth explanations and details see the tutorial page on\n\ncost-sensitive classification\n.\n\n\nTo create a measure that involves ordinary, i.e., class-dependent misclassification costs\nyou can use function \nmakeCostMeasure\n. You first need to define the cost matrix. The rows\nindicate true and the columns predicted classes and the rows and columns have to be named\nby the class labels. The cost matrix can then be wrapped in a \nMeasure\n\nobject and predictions can be evaluated as usual with the \nperformance\n function.\n\n\nSee the \nR\n documentation of function \nmakeCostMeasure\n for details on the various\nparameters.\n\n\n## Create the cost matrix\ncosts = matrix(c(0, 2, 2, 3, 0, 2, 1, 1, 0), ncol = 3)\nrownames(costs) = colnames(costs) = getTaskClassLevels(iris.task)\n\n## Encapsulate the cost matrix in a Measure object\nmy.costs = makeCostMeasure(\n  id = \nmy.costs\n, name = \nMy Costs\n,\n  costs = costs,\n  minimize = TRUE, best = 0, worst = 3\n)\n\n## Train a learner and make a prediction\nmod = train(\nclassif.lda\n, iris.task)\npred = predict(mod, newdata = iris)\n\n## Calculate the average costs\nperformance(pred, measures = my.costs)\n#\n   my.costs \n#\n 0.02666667\n\n\n\n\nCreating an aggregation scheme\n\n\nIt is possible to create your own aggregation scheme using function \nmakeAggregation\n.\nYou need to specify an identifier \nid\n, the \nproperties\n, and write a function that does\nthe actual aggregation. Optionally, you can \nname\n your aggregation scheme.\n\n\nPossible settings for \nproperties\n are \n\"req.test\"\n and \n\"req.train\"\n if predictions\non either the training or test sets are required, and the vector \nc(\"req.train\", \"req.test\")\n\nif both are needed.\n\n\nThe aggregation function must have a certain\nsignature detailed in the documentation of \nmakeAggregation\n.\nUsually, you will only need the performance values on the test sets \nperf.test\n or\nthe training sets \nperf.train\n. In rare cases, e.g., the \nPrediction\n object \npred\n or\ninformation stored in the \nTask\n object might be required to obtain the aggregated performance.\nFor an example have a look at the\n\ndefinition\n of function\n\ntest.join\n.\n\n\nExample: Evaluating the range of measures\n\n\nLet's say you are interested in the range of the performance values obtained on individual\ntest sets.\n\n\nmy.range.aggr = makeAggregation(id = \ntest.range\n, name = \nTest Range\n,\n  properties = \nreq.test\n,\n  fun = function (task, perf.test, perf.train, measure, group, pred)\n    diff(range(perf.test))\n)\n\n\n\n\nperf.train\n and \nperf.test\n are both numerical vectors containing the performances on\nthe train and test data sets.\nIn most cases (unless you are using bootstrap as resampling strategy or have set\n\npredict = \"both\"\n in \nmakeResampleDesc\n) the \nperf.train\n vector is empty.\n\n\nNow we can run a feature selection based on the first measure in the provided\n\nlist\n and see how the other measures turn out.\n\n\n## mmce with default aggregation scheme test.mean\nms1 = mmce\n\n## mmce with new aggregation scheme my.range.aggr\nms2 = setAggregation(ms1, my.range.aggr)\n\n## Minimum and maximum of the mmce over test sets\nms1min = setAggregation(ms1, test.min)\nms1max = setAggregation(ms1, test.max)\n\n## Feature selection\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nres = selectFeatures(\nclassif.rpart\n, iris.task, rdesc, measures = list(ms1, ms2, ms1min, ms1max),\n  control = makeFeatSelControlExhaustive(), show.info = FALSE)\n\n## Optimization path, i.e., performances for the 16 possible feature subsets\nperf.data = as.data.frame(res$opt.path)\nhead(perf.data[1:8])\n#\n   Sepal.Length Sepal.Width Petal.Length Petal.Width mmce.test.mean\n#\n 1            0           0            0           0     0.70666667\n#\n 2            1           0            0           0     0.31333333\n#\n 3            0           1            0           0     0.50000000\n#\n 4            0           0            1           0     0.09333333\n#\n 5            0           0            0           1     0.04666667\n#\n 6            1           1            0           0     0.28666667\n#\n   mmce.test.range mmce.test.min mmce.test.max\n#\n 1            0.16          0.60          0.76\n#\n 2            0.02          0.30          0.32\n#\n 3            0.22          0.36          0.58\n#\n 4            0.10          0.04          0.14\n#\n 5            0.08          0.02          0.10\n#\n 6            0.08          0.24          0.32\n\npd = position_jitter(width = 0.005, height = 0)\np = ggplot(aes(x = mmce.test.range, y = mmce.test.mean, ymax = mmce.test.max, ymin = mmce.test.min,\n  color = as.factor(Sepal.Width), pch = as.factor(Petal.Width)), data = perf.data) +\n  geom_pointrange(position = pd) +\n  coord_flip()\nprint(p)\n\n\n\n\n\n\nThe plot shows the range versus the mean misclassification error. The value on the\ny-axis thus corresponds to the length of the error bars. (Note that the points and error\nbars are jittered in y-direction.)\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nstr(mse) \n\nmse$fun \n\nmeasureMSE \nlistMeasureProperties() \nstr(test.mean) \n\ntest.mean$fun \n## Define a function that calculates the misclassification rate \nmy.mmce.fun = function(task, model, pred, feats, extra.args) { \n  tb = table(getPredictionResponse(pred), getPredictionTruth(pred)) \n  1 - sum(diag(tb)) / sum(tb) \n} \n\n## Generate the Measure object \nmy.mmce = makeMeasure( \n  id = \nmy.mmce\n, name = \nMy Mean Misclassification Error\n, \n  properties = c(\nclassif\n, \nclassif.multi\n, \nreq.pred\n, \nreq.truth\n), \n  minimize = TRUE, best = 0, worst = 1, \n  fun = my.mmce.fun \n) \n\n## Train a learner and make predictions \nmod = train(\nclassif.lda\n, iris.task) \npred = predict(mod, task = iris.task) \n\n## Calculate the performance using the new measure \nperformance(pred, measures = my.mmce) \n\n## Apparently the result coincides with the mlr implementation \nperformance(pred, measures = mmce) \n## Create the cost matrix \ncosts = matrix(c(0, 2, 2, 3, 0, 2, 1, 1, 0), ncol = 3) \nrownames(costs) = colnames(costs) = getTaskClassLevels(iris.task) \n\n## Encapsulate the cost matrix in a Measure object \nmy.costs = makeCostMeasure( \n  id = \nmy.costs\n, name = \nMy Costs\n, \n  costs = costs, \n  minimize = TRUE, best = 0, worst = 3 \n) \n\n## Train a learner and make a prediction \nmod = train(\nclassif.lda\n, iris.task) \npred = predict(mod, newdata = iris) \n\n## Calculate the average costs \nperformance(pred, measures = my.costs) \nmy.range.aggr = makeAggregation(id = \ntest.range\n, name = \nTest Range\n, \n  properties = \nreq.test\n, \n  fun = function (task, perf.test, perf.train, measure, group, pred) \n    diff(range(perf.test)) \n) \n## mmce with default aggregation scheme test.mean \nms1 = mmce \n\n## mmce with new aggregation scheme my.range.aggr \nms2 = setAggregation(ms1, my.range.aggr) \n\n## Minimum and maximum of the mmce over test sets \nms1min = setAggregation(ms1, test.min) \nms1max = setAggregation(ms1, test.max) \n\n## Feature selection \nrdesc = makeResampleDesc(\nCV\n, iters = 3) \nres = selectFeatures(\nclassif.rpart\n, iris.task, rdesc, measures = list(ms1, ms2, ms1min, ms1max), \n  control = makeFeatSelControlExhaustive(), show.info = FALSE) \n\n## Optimization path, i.e., performances for the 16 possible feature subsets \nperf.data = as.data.frame(res$opt.path) \nhead(perf.data[1:8]) \n\npd = position_jitter(width = 0.005, height = 0) \np = ggplot(aes(x = mmce.test.range, y = mmce.test.mean, ymax = mmce.test.max, ymin = mmce.test.min, \n  color = as.factor(Sepal.Width), pch = as.factor(Petal.Width)), data = perf.data) + \n  geom_pointrange(position = pd) + \n  coord_flip() \nprint(p)", 
            "title": "Create Custom Measures"
        }, 
        {
            "location": "/create_measure/index.html#integrating-another-measure", 
            "text": "In some cases, you might want to evaluate a  Prediction  or  ResamplePrediction  with a Measure  which is not yet implemented in  mlr . This could be either a\nperformance measure which is not listed in the  Appendix  or a measure that\nuses a misclassification cost matrix.", 
            "title": "Integrating Another Measure"
        }, 
        {
            "location": "/create_measure/index.html#performance-measures-and-aggregation-schemes", 
            "text": "Performance measures in  mlr  are objects of class  Measure .\nFor example the  mse  (mean squared error) looks as follows.  str(mse)\n#  List of 10\n#   $ id        : chr  mse \n#   $ minimize  : logi TRUE\n#   $ properties: chr [1:3]  regr   req.pred   req.truth \n#   $ fun       :function (task, model, pred, feats, extra.args)  \n#   $ extra.args: list()\n#   $ best      : num 0\n#   $ worst     : num Inf\n#   $ name      : chr  Mean of squared errors \n#   $ note      : chr  Defined as: mean((response - truth)^2) \n#   $ aggr      :List of 4\n#    ..$ id        : chr  test.mean \n#    ..$ name      : chr  Test mean \n#    ..$ fun       :function (task, perf.test, perf.train, measure, group, pred)  \n#    ..$ properties: chr  req.test \n#    ..- attr(*,  class )= chr  Aggregation \n#   - attr(*,  class )= chr  Measure \n\nmse$fun\n#  function (task, model, pred, feats, extra.args) \n#  {\n#      measureMSE(pred$data$truth, pred$data$response)\n#  }\n#   bytecode: 0xf18df28 \n#   environment: namespace:mlr \n\nmeasureMSE\n#  function (truth, response) \n#  {\n#      mean((response - truth)^2)\n#  }\n#   bytecode: 0xd974088 \n#   environment: namespace:mlr   See the  Measure  documentation page for a detailed description of the object\nslots.  At the core is slot  $fun  which contains the function that calculates the performance value.\nThe actual work is done by function  measureMSE .\nSimilar functions, generally adhering to the naming scheme  measure  followed by the\ncapitalized measure ID, exist for most performance measures.\nSee the  measures  help page for a complete list.  Just as  Task  and  Learner  objects each  Measure  has an\nidentifier  $id  which is for example used to annotate results and plots.\nFor plots there is also the option to use the longer measure  $name  instead. See the\ntutorial page on  Visualization  for more information.  Moreover, a  Measure  includes a number of  $properties  that indicate for\nwhich types of learning problems it is suitable and what information is required to\ncalculate it.\nObviously, most measures need the  Prediction  object ( \"req.pred\" ) and, for supervised\nproblems, the true values of the target variable(s) ( \"req.truth\" ).\nYou can use functions  getMeasureProperties  and hasMeasureProperties  to determine the properties of a  Measure .\nMoreover,  listMeasureProperties  shows all measure properties currently available in  mlr .  listMeasureProperties()\n#   [1]  classif         classif.multi   multilabel      regr          \n#   [5]  surv            cluster         costsens        req.pred      \n#   [9]  req.truth       req.task        req.feats       req.model     \n#  [13]  req.prob   Additional to its properties, each  Measure  knows its extreme values  $best \nand  $worst  and if it wants to be minimized or maximized ( $minimize ) during  tuning \nor  feature selection .  For resampling slot  $aggr  specifies how the overall performance across all resampling\niterations is calculated. Typically, this is just a matter of aggregating the performance\nvalues obtained on the test sets  perf.test  or the training sets  perf.train  by a simple\nfunction. The by far most common scheme is  test.mean , i.e., the unweighted\nmean of the performances on the test sets.  str(test.mean)\n#  List of 4\n#   $ id        : chr  test.mean \n#   $ name      : chr  Test mean \n#   $ fun       :function (task, perf.test, perf.train, measure, group, pred)  \n#   $ properties: chr  req.test \n#   - attr(*,  class )= chr  Aggregation \n\ntest.mean$fun\n#  function (task, perf.test, perf.train, measure, group, pred) \n#  mean(perf.test)\n#   bytecode: 0xa215d50 \n#   environment: namespace:mlr   All aggregation schemes are objects of class  Aggregation  with the function in slot $fun  doing the actual work.\nThe  $properties  member indicates if predictions (or performance values) on the training\nor test data sets are required to calculate the aggregation.  You can change the aggregation scheme of a  Measure  via function setAggregation . See the tutorial page on  resampling  for some examples\nand the  aggregations  help page for all available aggregation schemes.  You can construct your own  Measure  and  Aggregation  objects via functions makeMeasure ,  makeCostMeasure ,  makeCustomResampledMeasure  and  makeAggregation .\nSome examples are shown in the following.", 
            "title": "Performance measures and aggregation schemes"
        }, 
        {
            "location": "/create_measure/index.html#constructing-a-performance-measure", 
            "text": "Function  makeMeasure  provides a simple way to construct your own performance measure.  Below this is exemplified by re-implementing the mean misclassification error\n( mmce ).\nWe first write a function that computes the measure on the basis of the true and predicted\nclass labels.\nNote that this function must have certain formal arguments listed in the documentation of makeMeasure .\nThen the  Measure  object is created and we work with it as usual with the performance  function.  See the  R  documentation of  makeMeasure  for more details on the various parameters.  ## Define a function that calculates the misclassification rate\nmy.mmce.fun = function(task, model, pred, feats, extra.args) {\n  tb = table(getPredictionResponse(pred), getPredictionTruth(pred))\n  1 - sum(diag(tb)) / sum(tb)\n}\n\n## Generate the Measure object\nmy.mmce = makeMeasure(\n  id =  my.mmce , name =  My Mean Misclassification Error ,\n  properties = c( classif ,  classif.multi ,  req.pred ,  req.truth ),\n  minimize = TRUE, best = 0, worst = 1,\n  fun = my.mmce.fun\n)\n\n## Train a learner and make predictions\nmod = train( classif.lda , iris.task)\npred = predict(mod, task = iris.task)\n\n## Calculate the performance using the new measure\nperformance(pred, measures = my.mmce)\n#  my.mmce \n#     0.02\n\n## Apparently the result coincides with the mlr implementation\nperformance(pred, measures = mmce)\n#  mmce \n#  0.02", 
            "title": "Constructing a performance measure"
        }, 
        {
            "location": "/create_measure/index.html#constructing-a-measure-for-ordinary-misclassification-costs", 
            "text": "For in depth explanations and details see the tutorial page on cost-sensitive classification .  To create a measure that involves ordinary, i.e., class-dependent misclassification costs\nyou can use function  makeCostMeasure . You first need to define the cost matrix. The rows\nindicate true and the columns predicted classes and the rows and columns have to be named\nby the class labels. The cost matrix can then be wrapped in a  Measure \nobject and predictions can be evaluated as usual with the  performance  function.  See the  R  documentation of function  makeCostMeasure  for details on the various\nparameters.  ## Create the cost matrix\ncosts = matrix(c(0, 2, 2, 3, 0, 2, 1, 1, 0), ncol = 3)\nrownames(costs) = colnames(costs) = getTaskClassLevels(iris.task)\n\n## Encapsulate the cost matrix in a Measure object\nmy.costs = makeCostMeasure(\n  id =  my.costs , name =  My Costs ,\n  costs = costs,\n  minimize = TRUE, best = 0, worst = 3\n)\n\n## Train a learner and make a prediction\nmod = train( classif.lda , iris.task)\npred = predict(mod, newdata = iris)\n\n## Calculate the average costs\nperformance(pred, measures = my.costs)\n#    my.costs \n#  0.02666667", 
            "title": "Constructing a measure for ordinary misclassification costs"
        }, 
        {
            "location": "/create_measure/index.html#creating-an-aggregation-scheme", 
            "text": "It is possible to create your own aggregation scheme using function  makeAggregation .\nYou need to specify an identifier  id , the  properties , and write a function that does\nthe actual aggregation. Optionally, you can  name  your aggregation scheme.  Possible settings for  properties  are  \"req.test\"  and  \"req.train\"  if predictions\non either the training or test sets are required, and the vector  c(\"req.train\", \"req.test\") \nif both are needed.  The aggregation function must have a certain\nsignature detailed in the documentation of  makeAggregation .\nUsually, you will only need the performance values on the test sets  perf.test  or\nthe training sets  perf.train . In rare cases, e.g., the  Prediction  object  pred  or\ninformation stored in the  Task  object might be required to obtain the aggregated performance.\nFor an example have a look at the definition  of function test.join .", 
            "title": "Creating an aggregation scheme"
        }, 
        {
            "location": "/create_measure/index.html#example-evaluating-the-range-of-measures", 
            "text": "Let's say you are interested in the range of the performance values obtained on individual\ntest sets.  my.range.aggr = makeAggregation(id =  test.range , name =  Test Range ,\n  properties =  req.test ,\n  fun = function (task, perf.test, perf.train, measure, group, pred)\n    diff(range(perf.test))\n)  perf.train  and  perf.test  are both numerical vectors containing the performances on\nthe train and test data sets.\nIn most cases (unless you are using bootstrap as resampling strategy or have set predict = \"both\"  in  makeResampleDesc ) the  perf.train  vector is empty.  Now we can run a feature selection based on the first measure in the provided list  and see how the other measures turn out.  ## mmce with default aggregation scheme test.mean\nms1 = mmce\n\n## mmce with new aggregation scheme my.range.aggr\nms2 = setAggregation(ms1, my.range.aggr)\n\n## Minimum and maximum of the mmce over test sets\nms1min = setAggregation(ms1, test.min)\nms1max = setAggregation(ms1, test.max)\n\n## Feature selection\nrdesc = makeResampleDesc( CV , iters = 3)\nres = selectFeatures( classif.rpart , iris.task, rdesc, measures = list(ms1, ms2, ms1min, ms1max),\n  control = makeFeatSelControlExhaustive(), show.info = FALSE)\n\n## Optimization path, i.e., performances for the 16 possible feature subsets\nperf.data = as.data.frame(res$opt.path)\nhead(perf.data[1:8])\n#    Sepal.Length Sepal.Width Petal.Length Petal.Width mmce.test.mean\n#  1            0           0            0           0     0.70666667\n#  2            1           0            0           0     0.31333333\n#  3            0           1            0           0     0.50000000\n#  4            0           0            1           0     0.09333333\n#  5            0           0            0           1     0.04666667\n#  6            1           1            0           0     0.28666667\n#    mmce.test.range mmce.test.min mmce.test.max\n#  1            0.16          0.60          0.76\n#  2            0.02          0.30          0.32\n#  3            0.22          0.36          0.58\n#  4            0.10          0.04          0.14\n#  5            0.08          0.02          0.10\n#  6            0.08          0.24          0.32\n\npd = position_jitter(width = 0.005, height = 0)\np = ggplot(aes(x = mmce.test.range, y = mmce.test.mean, ymax = mmce.test.max, ymin = mmce.test.min,\n  color = as.factor(Sepal.Width), pch = as.factor(Petal.Width)), data = perf.data) +\n  geom_pointrange(position = pd) +\n  coord_flip()\nprint(p)   The plot shows the range versus the mean misclassification error. The value on the\ny-axis thus corresponds to the length of the error bars. (Note that the points and error\nbars are jittered in y-direction.)", 
            "title": "Example: Evaluating the range of measures"
        }, 
        {
            "location": "/create_measure/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  str(mse) \n\nmse$fun \n\nmeasureMSE \nlistMeasureProperties() \nstr(test.mean) \n\ntest.mean$fun \n## Define a function that calculates the misclassification rate \nmy.mmce.fun = function(task, model, pred, feats, extra.args) { \n  tb = table(getPredictionResponse(pred), getPredictionTruth(pred)) \n  1 - sum(diag(tb)) / sum(tb) \n} \n\n## Generate the Measure object \nmy.mmce = makeMeasure( \n  id =  my.mmce , name =  My Mean Misclassification Error , \n  properties = c( classif ,  classif.multi ,  req.pred ,  req.truth ), \n  minimize = TRUE, best = 0, worst = 1, \n  fun = my.mmce.fun \n) \n\n## Train a learner and make predictions \nmod = train( classif.lda , iris.task) \npred = predict(mod, task = iris.task) \n\n## Calculate the performance using the new measure \nperformance(pred, measures = my.mmce) \n\n## Apparently the result coincides with the mlr implementation \nperformance(pred, measures = mmce) \n## Create the cost matrix \ncosts = matrix(c(0, 2, 2, 3, 0, 2, 1, 1, 0), ncol = 3) \nrownames(costs) = colnames(costs) = getTaskClassLevels(iris.task) \n\n## Encapsulate the cost matrix in a Measure object \nmy.costs = makeCostMeasure( \n  id =  my.costs , name =  My Costs , \n  costs = costs, \n  minimize = TRUE, best = 0, worst = 3 \n) \n\n## Train a learner and make a prediction \nmod = train( classif.lda , iris.task) \npred = predict(mod, newdata = iris) \n\n## Calculate the average costs \nperformance(pred, measures = my.costs) \nmy.range.aggr = makeAggregation(id =  test.range , name =  Test Range , \n  properties =  req.test , \n  fun = function (task, perf.test, perf.train, measure, group, pred) \n    diff(range(perf.test)) \n) \n## mmce with default aggregation scheme test.mean \nms1 = mmce \n\n## mmce with new aggregation scheme my.range.aggr \nms2 = setAggregation(ms1, my.range.aggr) \n\n## Minimum and maximum of the mmce over test sets \nms1min = setAggregation(ms1, test.min) \nms1max = setAggregation(ms1, test.max) \n\n## Feature selection \nrdesc = makeResampleDesc( CV , iters = 3) \nres = selectFeatures( classif.rpart , iris.task, rdesc, measures = list(ms1, ms2, ms1min, ms1max), \n  control = makeFeatSelControlExhaustive(), show.info = FALSE) \n\n## Optimization path, i.e., performances for the 16 possible feature subsets \nperf.data = as.data.frame(res$opt.path) \nhead(perf.data[1:8]) \n\npd = position_jitter(width = 0.005, height = 0) \np = ggplot(aes(x = mmce.test.range, y = mmce.test.mean, ymax = mmce.test.max, ymin = mmce.test.min, \n  color = as.factor(Sepal.Width), pch = as.factor(Petal.Width)), data = perf.data) + \n  geom_pointrange(position = pd) + \n  coord_flip() \nprint(p)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/create_imputation/index.html", 
            "text": "Creating an Imputation Method\n\n\nFunction \nmakeImputeMethod\n permits to create your own imputation method.\nFor this purpose you need to specify a \nlearn\n function that extracts the necessary\ninformation and an \nimpute\n function that does the actual imputation.\nThe \nlearn\n and \nimpute\n functions both have at least the following formal arguments:\n\n\n\n\ndata\n is a \ndata.frame\n with missing values in some features.\n\n\ncol\n indicates the feature to be imputed.\n\n\ntarget\n indicates the target variable(s) in a supervised learning task.\n\n\n\n\nExample: Imputation using the mean\n\n\nLet's have a look at function \nimputeMean\n.\n\n\nimputeMean = function() {\n  makeImputeMethod(learn = function(data, target, col) mean(data[[col]], na.rm = TRUE), \n    impute = simpleImpute)\n}\n\n\n\n\nimputeMean\n calls the unexported \nmlr\n function \nsimpleImpute\n which is defined as follows.\n\n\nsimpleImpute = function(data, target, col, const) {\n  if (is.na(const)) \n    stopf(\nError imputing column '%s'. Maybe all input data was missing?\n, col)\n  x = data[[col]]\n  if (is.logical(x) \n !is.logical(const)) {\n    x = as.factor(x)\n  }\n  if (is.factor(x) \n const %nin% levels(x)) {\n    levels(x) = c(levels(x), as.character(const))\n  }\n  replace(x, is.na(x), const)\n}\n\n\n\n\nThe \nlearn\n function calculates the mean of the non-missing observations in column \ncol\n.\nThe mean is passed via argument \nconst\n to the \nimpute\n function that replaces all missing values\nin feature \ncol\n.\n\n\nWriting your own imputation method\n\n\nNow let's write a new imputation method:\nA frequently used simple technique for longitudinal data is \nlast observation\ncarried forward\n (LOCF). Missing values are replaced by the most recent observed value.\n\n\nIn the \nR\n code below the \nlearn\n function determines the last observed value previous to\neach \nNA\n (\nvalues\n) as well as the corresponding number of consecutive \nNA's\n (\ntimes\n).\nThe \nimpute\n function generates a vector by replicating the entries in \nvalues\n\naccording to \ntimes\n and replaces the \nNA's\n in feature \ncol\n.\n\n\nimputeLOCF = function() {\n  makeImputeMethod(\n    learn = function(data, target, col) {\n      x = data[[col]]\n      ind = is.na(x)\n      dind = diff(ind)\n      lastValue = which(dind == 1)  # position of the last observed value previous to NA\n      lastNA = which(dind == -1)    # position of the last of potentially several consecutive NA's\n      values = x[lastValue]         # last observed value previous to NA\n      times = lastNA - lastValue    # number of consecutive NA's\n      return(list(values = values, times = times))\n    },\n    impute = function(data, target, col, values, times) {\n      x = data[[col]]\n      replace(x, is.na(x), rep(values, times))\n    }\n  )\n}\n\n\n\n\nNote that this function is just for demonstration and is lacking some checks for real-world\nusage (for example 'What should happen if the first value in \nx\n is already missing?').\nBelow it is used to impute the missing values in features \nOzone\n and \nSolar.R\n in the\n\nairquality\n data set.\n\n\ndata(airquality)\nimp = impute(airquality, cols = list(Ozone = imputeLOCF(), Solar.R = imputeLOCF()),\n  dummy.cols = c(\nOzone\n, \nSolar.R\n))\nhead(imp$data, 10)\n#\n    Ozone Solar.R Wind Temp Month Day Ozone.dummy Solar.R.dummy\n#\n 1     41     190  7.4   67     5   1       FALSE         FALSE\n#\n 2     36     118  8.0   72     5   2       FALSE         FALSE\n#\n 3     12     149 12.6   74     5   3       FALSE         FALSE\n#\n 4     18     313 11.5   62     5   4       FALSE         FALSE\n#\n 5     18     313 14.3   56     5   5        TRUE          TRUE\n#\n 6     28     313 14.9   66     5   6       FALSE          TRUE\n#\n 7     23     299  8.6   65     5   7       FALSE         FALSE\n#\n 8     19      99 13.8   59     5   8       FALSE         FALSE\n#\n 9      8      19 20.1   61     5   9       FALSE         FALSE\n#\n 10     8     194  8.6   69     5  10        TRUE         FALSE\n\n\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nimputeLOCF = function() { \n  makeImputeMethod( \n    learn = function(data, target, col) { \n      x = data[[col]] \n      ind = is.na(x) \n      dind = diff(ind) \n      lastValue = which(dind == 1)  # position of the last observed value previous to NA \n      lastNA = which(dind == -1)    # position of the last of potentially several consecutive NA's \n      values = x[lastValue]         # last observed value previous to NA \n      times = lastNA - lastValue    # number of consecutive NA's \n      return(list(values = values, times = times)) \n    }, \n    impute = function(data, target, col, values, times) { \n      x = data[[col]] \n      replace(x, is.na(x), rep(values, times)) \n    } \n  ) \n} \ndata(airquality) \nimp = impute(airquality, cols = list(Ozone = imputeLOCF(), Solar.R = imputeLOCF()), \n  dummy.cols = c(\nOzone\n, \nSolar.R\n)) \nhead(imp$data, 10)", 
            "title": "Create Imputation Methods"
        }, 
        {
            "location": "/create_imputation/index.html#creating-an-imputation-method", 
            "text": "Function  makeImputeMethod  permits to create your own imputation method.\nFor this purpose you need to specify a  learn  function that extracts the necessary\ninformation and an  impute  function that does the actual imputation.\nThe  learn  and  impute  functions both have at least the following formal arguments:   data  is a  data.frame  with missing values in some features.  col  indicates the feature to be imputed.  target  indicates the target variable(s) in a supervised learning task.", 
            "title": "Creating an Imputation Method"
        }, 
        {
            "location": "/create_imputation/index.html#example-imputation-using-the-mean", 
            "text": "Let's have a look at function  imputeMean .  imputeMean = function() {\n  makeImputeMethod(learn = function(data, target, col) mean(data[[col]], na.rm = TRUE), \n    impute = simpleImpute)\n}  imputeMean  calls the unexported  mlr  function  simpleImpute  which is defined as follows.  simpleImpute = function(data, target, col, const) {\n  if (is.na(const)) \n    stopf( Error imputing column '%s'. Maybe all input data was missing? , col)\n  x = data[[col]]\n  if (is.logical(x)   !is.logical(const)) {\n    x = as.factor(x)\n  }\n  if (is.factor(x)   const %nin% levels(x)) {\n    levels(x) = c(levels(x), as.character(const))\n  }\n  replace(x, is.na(x), const)\n}  The  learn  function calculates the mean of the non-missing observations in column  col .\nThe mean is passed via argument  const  to the  impute  function that replaces all missing values\nin feature  col .", 
            "title": "Example: Imputation using the mean"
        }, 
        {
            "location": "/create_imputation/index.html#writing-your-own-imputation-method", 
            "text": "Now let's write a new imputation method:\nA frequently used simple technique for longitudinal data is  last observation\ncarried forward  (LOCF). Missing values are replaced by the most recent observed value.  In the  R  code below the  learn  function determines the last observed value previous to\neach  NA  ( values ) as well as the corresponding number of consecutive  NA's  ( times ).\nThe  impute  function generates a vector by replicating the entries in  values \naccording to  times  and replaces the  NA's  in feature  col .  imputeLOCF = function() {\n  makeImputeMethod(\n    learn = function(data, target, col) {\n      x = data[[col]]\n      ind = is.na(x)\n      dind = diff(ind)\n      lastValue = which(dind == 1)  # position of the last observed value previous to NA\n      lastNA = which(dind == -1)    # position of the last of potentially several consecutive NA's\n      values = x[lastValue]         # last observed value previous to NA\n      times = lastNA - lastValue    # number of consecutive NA's\n      return(list(values = values, times = times))\n    },\n    impute = function(data, target, col, values, times) {\n      x = data[[col]]\n      replace(x, is.na(x), rep(values, times))\n    }\n  )\n}  Note that this function is just for demonstration and is lacking some checks for real-world\nusage (for example 'What should happen if the first value in  x  is already missing?').\nBelow it is used to impute the missing values in features  Ozone  and  Solar.R  in the airquality  data set.  data(airquality)\nimp = impute(airquality, cols = list(Ozone = imputeLOCF(), Solar.R = imputeLOCF()),\n  dummy.cols = c( Ozone ,  Solar.R ))\nhead(imp$data, 10)\n#     Ozone Solar.R Wind Temp Month Day Ozone.dummy Solar.R.dummy\n#  1     41     190  7.4   67     5   1       FALSE         FALSE\n#  2     36     118  8.0   72     5   2       FALSE         FALSE\n#  3     12     149 12.6   74     5   3       FALSE         FALSE\n#  4     18     313 11.5   62     5   4       FALSE         FALSE\n#  5     18     313 14.3   56     5   5        TRUE          TRUE\n#  6     28     313 14.9   66     5   6       FALSE          TRUE\n#  7     23     299  8.6   65     5   7       FALSE         FALSE\n#  8     19      99 13.8   59     5   8       FALSE         FALSE\n#  9      8      19 20.1   61     5   9       FALSE         FALSE\n#  10     8     194  8.6   69     5  10        TRUE         FALSE", 
            "title": "Writing your own imputation method"
        }, 
        {
            "location": "/create_imputation/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  imputeLOCF = function() { \n  makeImputeMethod( \n    learn = function(data, target, col) { \n      x = data[[col]] \n      ind = is.na(x) \n      dind = diff(ind) \n      lastValue = which(dind == 1)  # position of the last observed value previous to NA \n      lastNA = which(dind == -1)    # position of the last of potentially several consecutive NA's \n      values = x[lastValue]         # last observed value previous to NA \n      times = lastNA - lastValue    # number of consecutive NA's \n      return(list(values = values, times = times)) \n    }, \n    impute = function(data, target, col, values, times) { \n      x = data[[col]] \n      replace(x, is.na(x), rep(values, times)) \n    } \n  ) \n} \ndata(airquality) \nimp = impute(airquality, cols = list(Ozone = imputeLOCF(), Solar.R = imputeLOCF()), \n  dummy.cols = c( Ozone ,  Solar.R )) \nhead(imp$data, 10)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/create_filter/index.html", 
            "text": "Integrating Another Filter Method\n\n\nA lot of feature filter methods are already integrated in \nmlr\n and a complete list is given\nin the \nAppendix\n or can be obtained using \nlistFilterMethods\n.\nYou can easily add another filter, be it a brand new one or a method which is already\nimplemented in another package, via function \nmakeFilter\n.\n\n\nFilter objects\n\n\nIn \nmlr\n all filter methods are objects of class \nFilter\n and are registered\nin an environment called \n.FilterRegister\n\n(where \nlistFilterMethods\n looks them up to compile the list of available methods).\nTo get to know their structure let's have a closer look at the \n\"rank.correlation\"\n filter\nwhich interfaces function \ncorrels\n in package \nRfast\n.\n\n\nfilters = as.list(mlr:::.FilterRegister)\nfilters$rank.correlation\n#\n Filter: 'rank.correlation'\n#\n Packages: 'Rfast'\n#\n Supported tasks: regr\n#\n Supported features: numerics\n\nstr(filters$rank.correlation)\n#\n List of 6\n#\n  $ name              : chr \nrank.correlation\n\n#\n  $ desc              : chr \nSpearman's correlation between feature and target\n\n#\n  $ pkg               : chr \nRfast\n\n#\n  $ supported.tasks   : chr \nregr\n\n#\n  $ supported.features: chr \nnumerics\n\n#\n  $ fun               :function (task, nselect, ...)  \n#\n  - attr(*, \nclass\n)= chr \nFilter\n\n\nfilters$rank.correlation$fun\n#\n function (task, nselect, ...) \n#\n {\n#\n     d = getTaskData(task, target.extra = TRUE)\n#\n     y = Rfast::correls(d$target, d$data, type = \nspearman\n)\n#\n     for (i in which(is.na(y[, \ncorrelation\n]))) {\n#\n         y[i, \ncorrelation\n] = cor(d$target, d$data[, i], use = \ncomplete.obs\n, \n#\n             method = \nspearman\n)\n#\n     }\n#\n     setNames(abs(y[, \ncorrelation\n]), getTaskFeatureNames(task))\n#\n }\n#\n \nbytecode: 0xe6b1cb8\n\n#\n \nenvironment: namespace:mlr\n\n\n\n\n\nThe core element is \n$fun\n which calculates the feature importance.\nFor the \n\"rank.correlation\"\n filter it just extracts the data and formula from the \ntask\n\nand passes them on to the \ncorrels\n function.\n\n\nAdditionally, each \nFilter\n object has a \n$name\n, which should be short and is\nfor example used to annotate graphics (cp. \nplotFilterValues\n), and a slightly more detailed\ndescription in slot \n$desc\n.\nIf the filter method is implemented by another package its name is given in the \n$pkg\n member.\nMoreover, the supported task types and feature types are listed.\n\n\nWriting a new filter method\n\n\nYou can integrate your own filter method using \nmakeFilter\n. This function generates a\n\nFilter\n object and also registers it in the \n.FilterRegister\n environment.\n\n\nThe arguments of \nmakeFilter\n correspond to the slot names of the \nFilter\n\nobject above.\nCurrently, feature filtering is only supported for supervised learning tasks and possible\nvalues for \nsupported.tasks\n are \n\"regr\"\n, \n\"classif\"\n and \n\"surv\"\n.\n\nsupported.features\n can be \n\"numerics\"\n, \n\"factors\"\n and \n\"ordered\"\n.\n\n\nfun\n must be a \nfunction\n with at least the following formal arguments:\n\n\n\n\ntask\n is a \nmlr\n learning \nTask\n.\n\n\nnselect\n corresponds to the argument of \ngenerateFilterValuesData\n of the same name and\n  specifies the number of features for which to calculate importance scores.\n  Some filter methods have the option to stop after a certain number of top-ranked features\n  have been found in order to save time and ressources when the number of features is high.\n  The majority of filter methods integrated in \nmlr\n doesn't support this and thus\n  \nnselect\n is ignored in most cases.\n  An exception is the minimum redundancy maximum relevance filter from package \nmRMRe\n.\n\n\n...\n for additional arguments.\n\n\n\n\nfun\n must return a named vector of feature importance values.\nBy convention the most important features receive the highest scores.\n\n\nIf you are making use of the \nnselect\n option \nfun\n can either return a vector of \nnselect\n\nscores or a vector as long as the total numbers of features in the task filled with \nNAs\n\nfor all features whose scores weren't calculated.\n\n\nWhen writing \nfun\n many of the getter functions for \nTask\ns come in handy,\nparticularly \ngetTaskData\n, \ngetTaskFormula\n and \ngetTaskFeatureNames\n.\nIt's worth having a closer look at \ngetTaskData\n which provides many options for\nformatting the data and recoding the target variable.\n\n\nAs a short demonstration we write a totally meaningless filter that determines the\nimportance of features according to alphabetical order, i.e., giving highest scores to\nfeatures with names that come first (\ndecreasing = TRUE\n) or last (\ndecreasing = FALSE\n)\nin the alphabet.\n\n\nmakeFilter(\n  name = \nnonsense.filter\n,\n  desc = \nCalculates scores according to alphabetical order of features\n,\n  pkg = \n,\n  supported.tasks = c(\nclassif\n, \nregr\n, \nsurv\n),\n  supported.features = c(\nnumerics\n, \nfactors\n, \nordered\n),\n  fun = function(task, nselect, decreasing = TRUE, ...) {\n    feats = getTaskFeatureNames(task)\n    imp = order(feats, decreasing = decreasing)\n    names(imp) = feats\n    imp\n  }\n)\n#\n Filter: 'nonsense.filter'\n#\n Packages: ''\n#\n Supported tasks: classif,regr,surv\n#\n Supported features: numerics,factors,ordered\n\n\n\n\nThe \nnonsense.filter\n is now registered in \nmlr\n and shown by \nlistFilterMethods\n.\n\n\nlistFilterMethods()$id\n#\n  [1] anova.test                 auc                       \n#\n  [3] carscore                   cforest.importance        \n#\n  [5] chi.squared                gain.ratio                \n#\n  [7] information.gain           kruskal.test              \n#\n  [9] linear.correlation         mrmr                      \n#\n [11] nonsense.filter            oneR                      \n#\n [13] permutation.importance     randomForest.importance   \n#\n [15] randomForestSRC.rfsrc      randomForestSRC.var.select\n#\n [17] rank.correlation           relief                    \n#\n [19] symmetrical.uncertainty    univariate.model.score    \n#\n [21] variance                  \n#\n 24 Levels: anova.test auc carscore cforest.importance ... variance\n\n\n\n\nYou can use it like any other filter method already integrated in \nmlr\n (i.e., via the\n\nmethod\n argument of \ngenerateFilterValuesData\n or the \nfw.method\n argument of\n\nmakeFilterWrapper\n; see also the page on \nfeature selection\n).\n\n\nd = generateFilterValuesData(iris.task, method = c(\nnonsense.filter\n, \nanova.test\n))\nd\n#\n FilterValues:\n#\n Task: iris_example\n#\n           name    type nonsense.filter anova.test\n#\n 1 Sepal.Length numeric               2  119.26450\n#\n 2  Sepal.Width numeric               1   49.16004\n#\n 3 Petal.Length numeric               4 1180.16118\n#\n 4  Petal.Width numeric               3  960.00715\n\nplotFilterValues(d)\n\n\n\n\n\n\niris.task.filtered = filterFeatures(iris.task, method = \nnonsense.filter\n, abs = 2)\niris.task.filtered\n#\n Supervised task: iris_example\n#\n Type: classif\n#\n Target: Species\n#\n Observations: 150\n#\n Features:\n#\n    numerics     factors     ordered functionals \n#\n           2           0           0           0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Is spatial: FALSE\n#\n Classes: 3\n#\n     setosa versicolor  virginica \n#\n         50         50         50 \n#\n Positive class: NA\n\ngetTaskFeatureNames(iris.task.filtered)\n#\n [1] \nPetal.Length\n \nPetal.Width\n\n\n\n\n\nYou might also want to have a look at the\n\nsource code\n\nof the filter methods already integrated in \nmlr\n for some more complex and meaningful\nexamples.\n\n\nComplete code listing\n\n\nThe above code without the output is given below:\n\n\nfilters = as.list(mlr:::.FilterRegister) \nfilters$rank.correlation \n\nstr(filters$rank.correlation) \n\nfilters$rank.correlation$fun \nmakeFilter( \n  name = \nnonsense.filter\n, \n  desc = \nCalculates scores according to alphabetical order of features\n, \n  pkg = \n, \n  supported.tasks = c(\nclassif\n, \nregr\n, \nsurv\n), \n  supported.features = c(\nnumerics\n, \nfactors\n, \nordered\n), \n  fun = function(task, nselect, decreasing = TRUE, ...) { \n    feats = getTaskFeatureNames(task) \n    imp = order(feats, decreasing = decreasing) \n    names(imp) = feats \n    imp \n  } \n) \nlistFilterMethods()$id \nd = generateFilterValuesData(iris.task, method = c(\nnonsense.filter\n, \nanova.test\n)) \nd \n\nplotFilterValues(d) \niris.task.filtered = filterFeatures(iris.task, method = \nnonsense.filter\n, abs = 2) \niris.task.filtered \n\ngetTaskFeatureNames(iris.task.filtered) \nrm(\nnonsense.filter\n, envir = mlr:::.FilterRegister)", 
            "title": "Create Custom Filters"
        }, 
        {
            "location": "/create_filter/index.html#integrating-another-filter-method", 
            "text": "A lot of feature filter methods are already integrated in  mlr  and a complete list is given\nin the  Appendix  or can be obtained using  listFilterMethods .\nYou can easily add another filter, be it a brand new one or a method which is already\nimplemented in another package, via function  makeFilter .", 
            "title": "Integrating Another Filter Method"
        }, 
        {
            "location": "/create_filter/index.html#filter-objects", 
            "text": "In  mlr  all filter methods are objects of class  Filter  and are registered\nin an environment called  .FilterRegister \n(where  listFilterMethods  looks them up to compile the list of available methods).\nTo get to know their structure let's have a closer look at the  \"rank.correlation\"  filter\nwhich interfaces function  correls  in package  Rfast .  filters = as.list(mlr:::.FilterRegister)\nfilters$rank.correlation\n#  Filter: 'rank.correlation'\n#  Packages: 'Rfast'\n#  Supported tasks: regr\n#  Supported features: numerics\n\nstr(filters$rank.correlation)\n#  List of 6\n#   $ name              : chr  rank.correlation \n#   $ desc              : chr  Spearman's correlation between feature and target \n#   $ pkg               : chr  Rfast \n#   $ supported.tasks   : chr  regr \n#   $ supported.features: chr  numerics \n#   $ fun               :function (task, nselect, ...)  \n#   - attr(*,  class )= chr  Filter \n\nfilters$rank.correlation$fun\n#  function (task, nselect, ...) \n#  {\n#      d = getTaskData(task, target.extra = TRUE)\n#      y = Rfast::correls(d$target, d$data, type =  spearman )\n#      for (i in which(is.na(y[,  correlation ]))) {\n#          y[i,  correlation ] = cor(d$target, d$data[, i], use =  complete.obs , \n#              method =  spearman )\n#      }\n#      setNames(abs(y[,  correlation ]), getTaskFeatureNames(task))\n#  }\n#   bytecode: 0xe6b1cb8 \n#   environment: namespace:mlr   The core element is  $fun  which calculates the feature importance.\nFor the  \"rank.correlation\"  filter it just extracts the data and formula from the  task \nand passes them on to the  correls  function.  Additionally, each  Filter  object has a  $name , which should be short and is\nfor example used to annotate graphics (cp.  plotFilterValues ), and a slightly more detailed\ndescription in slot  $desc .\nIf the filter method is implemented by another package its name is given in the  $pkg  member.\nMoreover, the supported task types and feature types are listed.", 
            "title": "Filter objects"
        }, 
        {
            "location": "/create_filter/index.html#writing-a-new-filter-method", 
            "text": "You can integrate your own filter method using  makeFilter . This function generates a Filter  object and also registers it in the  .FilterRegister  environment.  The arguments of  makeFilter  correspond to the slot names of the  Filter \nobject above.\nCurrently, feature filtering is only supported for supervised learning tasks and possible\nvalues for  supported.tasks  are  \"regr\" ,  \"classif\"  and  \"surv\" . supported.features  can be  \"numerics\" ,  \"factors\"  and  \"ordered\" .  fun  must be a  function  with at least the following formal arguments:   task  is a  mlr  learning  Task .  nselect  corresponds to the argument of  generateFilterValuesData  of the same name and\n  specifies the number of features for which to calculate importance scores.\n  Some filter methods have the option to stop after a certain number of top-ranked features\n  have been found in order to save time and ressources when the number of features is high.\n  The majority of filter methods integrated in  mlr  doesn't support this and thus\n   nselect  is ignored in most cases.\n  An exception is the minimum redundancy maximum relevance filter from package  mRMRe .  ...  for additional arguments.   fun  must return a named vector of feature importance values.\nBy convention the most important features receive the highest scores.  If you are making use of the  nselect  option  fun  can either return a vector of  nselect \nscores or a vector as long as the total numbers of features in the task filled with  NAs \nfor all features whose scores weren't calculated.  When writing  fun  many of the getter functions for  Task s come in handy,\nparticularly  getTaskData ,  getTaskFormula  and  getTaskFeatureNames .\nIt's worth having a closer look at  getTaskData  which provides many options for\nformatting the data and recoding the target variable.  As a short demonstration we write a totally meaningless filter that determines the\nimportance of features according to alphabetical order, i.e., giving highest scores to\nfeatures with names that come first ( decreasing = TRUE ) or last ( decreasing = FALSE )\nin the alphabet.  makeFilter(\n  name =  nonsense.filter ,\n  desc =  Calculates scores according to alphabetical order of features ,\n  pkg =  ,\n  supported.tasks = c( classif ,  regr ,  surv ),\n  supported.features = c( numerics ,  factors ,  ordered ),\n  fun = function(task, nselect, decreasing = TRUE, ...) {\n    feats = getTaskFeatureNames(task)\n    imp = order(feats, decreasing = decreasing)\n    names(imp) = feats\n    imp\n  }\n)\n#  Filter: 'nonsense.filter'\n#  Packages: ''\n#  Supported tasks: classif,regr,surv\n#  Supported features: numerics,factors,ordered  The  nonsense.filter  is now registered in  mlr  and shown by  listFilterMethods .  listFilterMethods()$id\n#   [1] anova.test                 auc                       \n#   [3] carscore                   cforest.importance        \n#   [5] chi.squared                gain.ratio                \n#   [7] information.gain           kruskal.test              \n#   [9] linear.correlation         mrmr                      \n#  [11] nonsense.filter            oneR                      \n#  [13] permutation.importance     randomForest.importance   \n#  [15] randomForestSRC.rfsrc      randomForestSRC.var.select\n#  [17] rank.correlation           relief                    \n#  [19] symmetrical.uncertainty    univariate.model.score    \n#  [21] variance                  \n#  24 Levels: anova.test auc carscore cforest.importance ... variance  You can use it like any other filter method already integrated in  mlr  (i.e., via the method  argument of  generateFilterValuesData  or the  fw.method  argument of makeFilterWrapper ; see also the page on  feature selection ).  d = generateFilterValuesData(iris.task, method = c( nonsense.filter ,  anova.test ))\nd\n#  FilterValues:\n#  Task: iris_example\n#            name    type nonsense.filter anova.test\n#  1 Sepal.Length numeric               2  119.26450\n#  2  Sepal.Width numeric               1   49.16004\n#  3 Petal.Length numeric               4 1180.16118\n#  4  Petal.Width numeric               3  960.00715\n\nplotFilterValues(d)   iris.task.filtered = filterFeatures(iris.task, method =  nonsense.filter , abs = 2)\niris.task.filtered\n#  Supervised task: iris_example\n#  Type: classif\n#  Target: Species\n#  Observations: 150\n#  Features:\n#     numerics     factors     ordered functionals \n#            2           0           0           0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Is spatial: FALSE\n#  Classes: 3\n#      setosa versicolor  virginica \n#          50         50         50 \n#  Positive class: NA\n\ngetTaskFeatureNames(iris.task.filtered)\n#  [1]  Petal.Length   Petal.Width   You might also want to have a look at the source code \nof the filter methods already integrated in  mlr  for some more complex and meaningful\nexamples.", 
            "title": "Writing a new filter method"
        }, 
        {
            "location": "/create_filter/index.html#complete-code-listing", 
            "text": "The above code without the output is given below:  filters = as.list(mlr:::.FilterRegister) \nfilters$rank.correlation \n\nstr(filters$rank.correlation) \n\nfilters$rank.correlation$fun \nmakeFilter( \n  name =  nonsense.filter , \n  desc =  Calculates scores according to alphabetical order of features , \n  pkg =  , \n  supported.tasks = c( classif ,  regr ,  surv ), \n  supported.features = c( numerics ,  factors ,  ordered ), \n  fun = function(task, nselect, decreasing = TRUE, ...) { \n    feats = getTaskFeatureNames(task) \n    imp = order(feats, decreasing = decreasing) \n    names(imp) = feats \n    imp \n  } \n) \nlistFilterMethods()$id \nd = generateFilterValuesData(iris.task, method = c( nonsense.filter ,  anova.test )) \nd \n\nplotFilterValues(d) \niris.task.filtered = filterFeatures(iris.task, method =  nonsense.filter , abs = 2) \niris.task.filtered \n\ngetTaskFeatureNames(iris.task.filtered) \nrm( nonsense.filter , envir = mlr:::.FilterRegister)", 
            "title": "Complete code listing"
        }, 
        {
            "location": "/example_tasks/index.html", 
            "text": "Example Tasks\n\n\nFor your convenience \nmlr\n provides pre-defined \nTask\ns for each type of learning problem.\nThese are used throughout this tutorial in order to get shorter and more readable code.\n\n\n\n\n\n\n\n\nType\n\n\nTask\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nclassif\n\n\nbc.task\n\n\nWisconsin Breast Cancer classification task.\n\n\n\n\n\n\n\n\nbc.task.spatial\n\n\nBasque pathogen infection spatial binary classification task.\n\n\n\n\n\n\n\n\ngunpoint.task\n\n\nGunpoint functional data classification task.\n\n\n\n\n\n\n\n\niris.task\n\n\nIris classification task.\n\n\n\n\n\n\n\n\nphoneme.task\n\n\nPhoneme functional data multilabel classification task.\n\n\n\n\n\n\n\n\npid.task\n\n\nPimaIndiansDiabetes classification task.\n\n\n\n\n\n\n\n\nsonar.task\n\n\nSonar classification task.\n\n\n\n\n\n\ncluster\n\n\nagri.task\n\n\nEuropean Union Agricultural Workforces clustering task.\n\n\n\n\n\n\n\n\nmtcars.task\n\n\nMotor Trend Car Road Tests clustering task.\n\n\n\n\n\n\ncostsens\n\n\ncostiris.task\n\n\nIris cost-sensitive classification task.\n\n\n\n\n\n\nmultilabel\n\n\nyeast.task\n\n\nYeast multilabel classification task.\n\n\n\n\n\n\nregr\n\n\nbh.task\n\n\nBoston Housing regression task.\n\n\n\n\n\n\n\n\nfuelsubset.task\n\n\nFuelSubset functional data regression task.\n\n\n\n\n\n\nsurv\n\n\nlung.task\n\n\nNCCTG Lung Cancer survival task.\n\n\n\n\n\n\n\n\nwpbc.task\n\n\nWisonsin Prognostic Breast Cancer (WPBC) survival task.", 
            "title": "Example Tasks"
        }, 
        {
            "location": "/example_tasks/index.html#example-tasks", 
            "text": "For your convenience  mlr  provides pre-defined  Task s for each type of learning problem.\nThese are used throughout this tutorial in order to get shorter and more readable code.     Type  Task  Description      classif  bc.task  Wisconsin Breast Cancer classification task.     bc.task.spatial  Basque pathogen infection spatial binary classification task.     gunpoint.task  Gunpoint functional data classification task.     iris.task  Iris classification task.     phoneme.task  Phoneme functional data multilabel classification task.     pid.task  PimaIndiansDiabetes classification task.     sonar.task  Sonar classification task.    cluster  agri.task  European Union Agricultural Workforces clustering task.     mtcars.task  Motor Trend Car Road Tests clustering task.    costsens  costiris.task  Iris cost-sensitive classification task.    multilabel  yeast.task  Yeast multilabel classification task.    regr  bh.task  Boston Housing regression task.     fuelsubset.task  FuelSubset functional data regression task.    surv  lung.task  NCCTG Lung Cancer survival task.     wpbc.task  Wisonsin Prognostic Breast Cancer (WPBC) survival task.", 
            "title": "Example Tasks"
        }, 
        {
            "location": "/integrated_learners/index.html", 
            "text": "Integrated Learners\n\n\nThis page lists the learning methods already integrated in \nmlr\n.\n\n\nColumns \nNum.\n, \nFac.\n, \nOrd.\n, \nNAs\n, and \nWeights\n indicate if a method can cope with\nnumerical, factor, and ordered factor predictors, if it can deal with missing values in a meaningful way\n(other than simply removing observations with missing values) and if observation\nweights are supported.\n\n\nColumn \nProps\n shows further properties of the learning methods specific to the\ntype of learning task.\nSee also \nRLearner\n for details.\n\n\nClassification (84)\n\n\nFor classification the following additional learner properties are relevant and shown in\ncolumn \nProps\n:\n\n\n\n\nprob\n: The method can predict probabilities,\n\n\noneclass\n, \ntwoclass\n, \nmulticlass\n: One-class, two-class (binary) or multi-class\n  classification problems be handled,\n\n\nclass.weights\n: Class weights can be handled.\n\n\n\n\n\n\n\n\n\n\nClass / Short Name / Name\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nOrd.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nclassif.ada\n \n \nada\n \nada Boosting\n\n\nada\nrpart\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\nxval\n has been set to \n0\n by default for speed.\n\n\n\n\n\n\nclassif.adaboostm1\n \n \nadaboostm1\n \nada Boosting M1\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n.\n\n\n\n\n\n\nclassif.bartMachine\n \n \nbartmachine\n \nBayesian Additive Regression Trees\n\n\nbartMachine\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\n\n\nuse_missing_data\n has been set to \nTRUE\n by default to allow missing data support.\n\n\n\n\n\n\nclassif.binomial\n \n \nbinomial\n \nBinomial Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nDelegates to \nglm\n with freely choosable binomial link function via learner parameter \nlink\n. We set 'model' to FALSE by default to save memory.\n\n\n\n\n\n\nclassif.blackboost\n \n \nblackboost\n \nGradient Boosting With Regression Trees\n\n\nmboost\nparty\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\n\n\nSee \n?ctree_control\n for possible breakage for nominal features with missingness. \nfamily\n has been set to \nBinomial\n by default. For 'family' 'AUC' and 'AdaExp' probabilities cannot be predcited.\n\n\n\n\n\n\nclassif.boosting\n \n \nadabag\n \nAdabag Boosting\n\n\nadabag\nrpart\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\nfeatimp\n\n\nxval\n has been set to \n0\n by default for speed.\n\n\n\n\n\n\nclassif.bst\n \n \nbst\n \nGradient Boosting\n\n\nbst\nrpart\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\n\n\nRenamed parameter \nlearner\n to \nLearner\n due to nameclash with \nsetHyperPars\n. Default changes: \nLearner = \"ls\"\n, \nxval = 0\n, and \nmaxdepth = 1\n.\n\n\n\n\n\n\nclassif.C50\n \n \nC50\n \nC50\n\n\nC50\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.cforest\n \n \ncforest\n \nRandom forest based on conditional inference trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\nfeatimp\n\n\nSee \n?ctree_control\n for possible breakage for nominal features with missingness.\n\n\n\n\n\n\nclassif.clusterSVM\n \n \nclusterSVM\n \nClustered Support Vector Machines\n\n\nSwarmSVM\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\n\n\ncenters\n set to \n2\n by default.\n\n\n\n\n\n\nclassif.ctree\n \n \nctree\n \nConditional Inference Trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\nSee \n?ctree_control\n for possible breakage for nominal features with missingness.\n\n\n\n\n\n\nclassif.cvglmnet\n \n \ncvglmnet\n \nGLM with Lasso or Elasticnet Regularization (Cross Validated Lambda)\n\n\nglmnet\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\nThe family parameter is set to \nbinomial\n for two-class problems and to \nmultinomial\n otherwise. Factors automatically get converted to dummy columns, ordered factors to integer.       glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults       before setting the specified parameters and after training.       If you are setting glmnet.control parameters through glmnet.control,       you need to save and re-set them after running the glmnet learner.\n\n\n\n\n\n\nclassif.dbnDNN\n \n \ndbn.dnn\n \nDeep neural network with weights initialized by DBN\n\n\ndeepnet\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\noutput\n set to \n\"softmax\"\n by default.\n\n\n\n\n\n\nclassif.dcSVM\n \n \ndcSVM\n \nDivided-Conquer Support Vector Machines\n\n\nSwarmSVM\ne1071\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\n\n\n\n\n\n\n\n\nclassif.earth\n \n \nfda\n \nFlexible Discriminant Analysis\n\n\nearth\nstats\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\nThis learner performs flexible discriminant analysis using the earth algorithm. na.action is set to na.fail and only this is supported.\n\n\n\n\n\n\nclassif.evtree\n \n \nevtree\n \nEvolutionary learning of globally optimal trees\n\n\nevtree\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\npmutatemajor\n, \npmutateminor\n, \npcrossover\n, \npsplit\n, and \npprune\n,       are scaled internally to sum to 100.\n\n\n\n\n\n\nclassif.extraTrees\n \n \nextraTrees\n \nExtremely Randomized Trees\n\n\nextraTrees\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.fdausc.glm\n \n \nfdausc.glm\n \nGeneralized Linear Models classification on FDA\n\n\nfda.usc\n\n\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nfunctionals\n\n\nmodel$C[[1]] is set to quote(classif.glm)\n\n\n\n\n\n\nclassif.fdausc.kernel\n \n \nfdausc.kernel\n \nKernel classification on FDA\n\n\nfda.usc\n\n\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nsingle.functional\n\n\nArgument draw=FALSE is used as default.\n\n\n\n\n\n\nclassif.fdausc.knn\n \n \nfdausc.knn\n \nfdausc.knn\n\n\nfda.usc\n\n\n\n\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\nsingle.functional\n\n\nArgument draw=FALSE is used as default.\n\n\n\n\n\n\nclassif.fdausc.np\n \n \nfdausc.np\n \nNonparametric classification on FDA\n\n\nfda.usc\n\n\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nsingle.functional\n\n\nArgument draw=FALSE is used as default. Additionally, mod$C[[1]] is set to quote(classif.np)\n\n\n\n\n\n\nclassif.featureless\n \n \nfeatureless\n \nFeatureless classifier\n\n\nmlr\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\nfunctionals\n\n\n\n\n\n\n\n\nclassif.fnn\n \n \nfnn\n \nFast k-Nearest Neighbour\n\n\nFNN\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.gamboost\n \n \ngamboost\n \nGradient boosting with smooth components\n\n\nmboost\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nfamily\n has been set to \nBinomial()\n by default. For 'family' 'AUC' and 'AdaExp' probabilities cannot be predicted.\n\n\n\n\n\n\nclassif.gaterSVM\n \n \ngaterSVM\n \nMixture of SVMs with Neural Network Gater Function\n\n\nSwarmSVM\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\n\n\nm\n set to \n3\n and \nmax.iter\n set to \n1\n by default.\n\n\n\n\n\n\nclassif.gausspr\n \n \ngausspr\n \nGaussian Processes\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nKernel parameters have to be passed directly and not by using the \nkpar\n list in \ngausspr\n.     Note that \nfit\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nclassif.gbm\n \n \ngbm\n \nGradient Boosting Machine\n\n\ngbm\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\nfeatimp\n\n\nkeep.data\n is set to FALSE to reduce memory requirements. Note on param 'distribution': gbm will select 'bernoulli' by default for 2 classes, and 'multinomial' for       multiclass problems. The latter is the only setting that works for \n 2 classes.\n\n\n\n\n\n\nclassif.geoDA\n \n \ngeoda\n \nGeometric Predictive Discriminant Analysis\n\n\nDiscriMiner\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.glmboost\n \n \nglmboost\n \nBoosting for GLMs\n\n\nmboost\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nfamily\n has been set to \nBinomial\n by default. For 'family' 'AUC' and 'AdaExp' probabilities cannot be predcited.\n\n\n\n\n\n\nclassif.glmnet\n \n \nglmnet\n \nGLM with Lasso or Elasticnet Regularization\n\n\nglmnet\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\nThe family parameter is set to \nbinomial\n for two-class problems and to \nmultinomial\n otherwise.       Factors automatically get converted to dummy columns, ordered factors to integer.       Parameter \ns\n (value of the regularization parameter used for predictions) is set to \n0.1\n by default,       but needs to be tuned by the user.       glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults       before setting the specified parameters and after training.       If you are setting glmnet.control parameters through glmnet.control,       you need to save and re-set them after running the glmnet learner.\n\n\n\n\n\n\nclassif.h2o.deeplearning\n \n \nh2o.dl\n \nh2o.deeplearning\n\n\nh2o\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.h2o.gbm\n \n \nh2o.gbm\n \nh2o.gbm\n\n\nh2o\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\n'distribution' is set automatically to 'gaussian'.\n\n\n\n\n\n\nclassif.h2o.glm\n \n \nh2o.glm\n \nh2o.glm\n\n\nh2o\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\n\n\n'family' is always set to 'binomial' to get a binary classifier.\n\n\n\n\n\n\nclassif.h2o.randomForest\n \n \nh2o.rf\n \nh2o.randomForest\n\n\nh2o\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.IBk\n \n \nibk\n \nk-Nearest Neighbours\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.J48\n \n \nj48\n \nJ48 Decision Trees\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n.\n\n\n\n\n\n\nclassif.JRip\n \n \njrip\n \nPropositional Rule Learner\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n.\n\n\n\n\n\n\nclassif.kknn\n \n \nkknn\n \nk-Nearest Neighbor\n\n\nkknn\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.knn\n \n \nknn\n \nk-Nearest Neighbor\n\n\nclass\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.ksvm\n \n \nksvm\n \nSupport Vector Machines\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nclass.weights\n\n\nKernel parameters have to be passed directly and not by using the \nkpar\n list in \nksvm\n. Note that \nfit\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nclassif.lda\n \n \nlda\n \nLinear Discriminant Analysis\n\n\nMASS\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nLearner parameter \npredict.method\n maps to \nmethod\n in \npredict.lda\n.\n\n\n\n\n\n\nclassif.LiblineaRL1L2SVC\n \n \nliblinl1l2svc\n \nL1-Regularized L2-Loss Support Vector Classification\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\nclass.weights\n\n\n\n\n\n\n\n\nclassif.LiblineaRL1LogReg\n \n \nliblinl1logreg\n \nL1-Regularized Logistic Regression\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nclass.weights\n\n\n\n\n\n\n\n\nclassif.LiblineaRL2L1SVC\n \n \nliblinl2l1svc\n \nL2-Regularized L1-Loss Support Vector Classification\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\nclass.weights\n\n\n\n\n\n\n\n\nclassif.LiblineaRL2LogReg\n \n \nliblinl2logreg\n \nL2-Regularized Logistic Regression\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nclass.weights\n\n\ntype = 0\n (the default) is primal and \ntype = 7\n is dual problem.\n\n\n\n\n\n\nclassif.LiblineaRL2SVC\n \n \nliblinl2svc\n \nL2-Regularized L2-Loss Support Vector Classification\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\nclass.weights\n\n\ntype = 2\n (the default) is primal and \ntype = 1\n is dual problem.\n\n\n\n\n\n\nclassif.LiblineaRMultiClassSVC\n \n \nliblinmulticlasssvc\n \nSupport Vector Classification by Crammer and Singer\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\nclass.weights\n\n\n\n\n\n\n\n\nclassif.linDA\n \n \nlinda\n \nLinear Discriminant Analysis\n\n\nDiscriMiner\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\nSet \nvalidation = NULL\n by default to disable internal test set validation.\n\n\n\n\n\n\nclassif.logreg\n \n \nlogreg\n \nLogistic Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nDelegates to \nglm\n with \nfamily = binomial(link = 'logit')\n. We set 'model' to FALSE by default to save memory.\n\n\n\n\n\n\nclassif.lqa\n \n \nlqa\n \nFitting penalized Generalized Linear Models with the LQA algorithm\n\n\nlqa\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\npenalty\n has been set to \n\"lasso\"\n and \nlambda\n to \n0.1\n by default. The parameters \nlambda\n, \ngamma\n, \nalpha\n, \noscar.c\n, \na\n, \nlambda1\n and \nlambda2\n are the tuning parameters of the \npenalty\n function being used, and correspond to the parameters as named in the respective help files. Parameter \nc\n for penalty method \noscar\n has been named \noscar.c\n. Parameters \nlambda1\n and \nlambda2\n correspond to the parameters named 'lambda_1' and 'lambda_2' of the penalty functions \nenet\n, \nfused.lasso\n, \nicb\n, \nlicb\n, as well as \nweighted.fusion\n.\n\n\n\n\n\n\nclassif.lssvm\n \n \nlssvm\n \nLeast Squares Support Vector Machine\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\nfitted\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nclassif.lvq1\n \n \nlvq1\n \nLearning Vector Quantization\n\n\nclass\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.mda\n \n \nmda\n \nMixture Discriminant Analysis\n\n\nmda\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nkeep.fitted\n has been set to \nFALSE\n by default for speed and we use \nstart.method = \"lvq\"\n for more robust behavior / less technical crashes.\n\n\n\n\n\n\nclassif.mlp\n \n \nmlp\n \nMulti-Layer Perceptron\n\n\nRSNNS\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.multinom\n \n \nmultinom\n \nMultinomial Regression\n\n\nnnet\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.naiveBayes\n \n \nnbayes\n \nNaive Bayes\n\n\ne1071\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.neuralnet\n \n \nneuralnet\n \nNeural Network from neuralnet\n\n\nneuralnet\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\nerr.fct\n has been set to \nce\n and \nlinear.output\n to FALSE to do classification.\n\n\n\n\n\n\nclassif.nnet\n \n \nnnet\n \nNeural Network\n\n\nnnet\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\n\n\nsize\n has been set to \n3\n by default.\n\n\n\n\n\n\nclassif.nnTrain\n \n \nnn.train\n \nTraining Neural Network by Backpropagation\n\n\ndeepnet\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\noutput\n set to \nsoftmax\n by default. \nmax.number.of.layers\n can be set to control and tune the maximal number of layers specified via \nhidden\n.\n\n\n\n\n\n\nclassif.nodeHarvest\n \n \nnodeHarvest\n \nNode Harvest\n\n\nnodeHarvest\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.OneR\n \n \noner\n \n1-R Classifier\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n.\n\n\n\n\n\n\nclassif.pamr\n \n \npamr\n \nNearest shrunken centroid\n\n\npamr\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\nThreshold for prediction (\nthreshold.predict\n) has been set to \n1\n by default.\n\n\n\n\n\n\nclassif.PART\n \n \npart\n \nPART Decision Lists\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n.\n\n\n\n\n\n\nclassif.penalized\n \n \npenalized\n \nPenalized Logistic Regression\n\n\npenalized\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nprob\ntwoclass\n\n\ntrace=FALSE was set by default to disable logging output.\n\n\n\n\n\n\nclassif.plr\n \n \nplr\n \nLogistic Regression with a L2 Penalty\n\n\nstepPlr\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nAIC and BIC penalty types can be selected via the new parameter \ncp.type\n.\n\n\n\n\n\n\nclassif.plsdaCaret\n \n \nplsdacaret\n \nPartial Least Squares (PLS) Discriminant Analysis\n\n\ncaret\npls\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.probit\n \n \nprobit\n \nProbit Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nDelegates to \nglm\n with \nfamily = binomial(link = 'probit')\n. We set 'model' to FALSE by default to save memory.\n\n\n\n\n\n\nclassif.qda\n \n \nqda\n \nQuadratic Discriminant Analysis\n\n\nMASS\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nLearner parameter \npredict.method\n maps to \nmethod\n in \npredict.qda\n.\n\n\n\n\n\n\nclassif.quaDA\n \n \nquada\n \nQuadratic Discriminant Analysis\n\n\nDiscriMiner\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.randomForest\n \n \nrf\n \nRandom Forest\n\n\nrandomForest\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nclass.weights\nfeatimp\noobpreds\n\n\nNote that the rf can freeze the R process if trained on a task with 1 feature which is constant. This can happen in feature forward selection, also due to resampling, and you need to remove such features with removeConstantFeatures.\n\n\n\n\n\n\nclassif.randomForestSRC\n \n \nrfsrc\n \nRandom Forest\n\n\nrandomForestSRC\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\nfeatimp\noobpreds\n\n\nna.action\n has been set to \n\"na.impute\"\n by default to allow missing data support.\n\n\n\n\n\n\nclassif.ranger\n \n \nranger\n \nRandom Forests\n\n\nranger\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\nfeatimp\noobpreds\n\n\nBy default, internal parallelization is switched off (\nnum.threads = 1\n), \nverbose\n output is disabled, \nrespect.unordered.factors\n is set to \norder\n for all splitrules. All settings are changeable. \nmtry.perc\n sets \nmtry\n to \nmtry.perc*getTaskNFeats(.task)\n. Default for \nmtry\n is the floor of square root of number of features in task. Default for \nmin.node.size\n is 1 for classification and 10 for probability estimation.\n\n\n\n\n\n\nclassif.rda\n \n \nrda\n \nRegularized Discriminant Analysis\n\n\nklaR\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nestimate.error\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nclassif.rFerns\n \n \nrFerns\n \nRandom ferns\n\n\nrFerns\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\ntwoclass\nmulticlass\noobpreds\n\n\n\n\n\n\n\n\nclassif.rknn\n \n \nrknn\n \nRandom k-Nearest-Neighbors\n\n\nrknn\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\nk restricted to \n 99 as the code allocates arrays of static size\n\n\n\n\n\n\nclassif.rotationForest\n \n \nrotationForest\n \nRotation Forest\n\n\nrotationForest\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.rpart\n \n \nrpart\n \nDecision Tree\n\n\nrpart\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\nfeatimp\n\n\nxval\n has been set to \n0\n by default for speed.\n\n\n\n\n\n\nclassif.RRF\n \n \nRRF\n \nRegularized Random Forests\n\n\nRRF\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nfeatimp\n\n\n\n\n\n\n\n\nclassif.rrlda\n \n \nrrlda\n \nRobust Regularized Linear Discriminant Analysis\n\n\nrrlda\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.saeDNN\n \n \nsae.dnn\n \nDeep neural network with weights initialized by Stacked AutoEncoder\n\n\ndeepnet\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\noutput\n set to \n\"softmax\"\n by default.\n\n\n\n\n\n\nclassif.sda\n \n \nsda\n \nShrinkage Discriminant Analysis\n\n\nsda\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\n\n\n\n\n\n\nclassif.sparseLDA\n \n \nsparseLDA\n \nSparse Discriminant Analysis\n\n\nsparseLDA\nMASS\nelasticnet\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\n\n\nArguments \nQ\n and \nstop\n are not yet provided as they depend on the task.\n\n\n\n\n\n\nclassif.svm\n \n \nsvm\n \nSupport Vector Machines (libsvm)\n\n\ne1071\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\nmulticlass\nclass.weights\n\n\n\n\n\n\n\n\nclassif.xgboost\n \n \nxgboost\n \neXtreme Gradient Boosting\n\n\nxgboost\n\n\nX\n\n\n\n\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\nmulticlass\nfeatimp\n\n\nAll settings are passed directly, rather than through \nxgboost\n's \nparams\n argument. \nnrounds\n has been set to \n1\n and \nverbose\n to \n0\n by default. \nnum_class\n is set internally, so do not set this manually.\n\n\n\n\n\n\n\n\nRegression (61)\n\n\nAdditional learner properties:\n\n\n\n\nse\n: Standard errors can be predicted.\n\n\n\n\n\n\n\n\n\n\nClass / Short Name / Name\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nOrd.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nregr.bartMachine\n \n \nbartmachine\n \nBayesian Additive Regression Trees\n\n\nbartMachine\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\nuse_missing_data\n has been set to \nTRUE\n by default to allow missing data support.\n\n\n\n\n\n\nregr.bcart\n \n \nbcart\n \nBayesian CART\n\n\ntgp\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.bgp\n \n \nbgp\n \nBayesian Gaussian Process\n\n\ntgp\n\n\nX\n\n\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.bgpllm\n \n \nbgpllm\n \nBayesian Gaussian Process with jumps to the Limiting Linear Model\n\n\ntgp\n\n\nX\n\n\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.blackboost\n \n \nblackboost\n \nGradient Boosting with Regression Trees\n\n\nmboost\nparty\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\nSee \n?ctree_control\n for possible breakage for nominal features with missingness.\n\n\n\n\n\n\nregr.blm\n \n \nblm\n \nBayesian Linear Model\n\n\ntgp\n\n\nX\n\n\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.brnn\n \n \nbrnn\n \nBayesian regularization for feed-forward neural networks\n\n\nbrnn\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.bst\n \n \nbst\n \nGradient Boosting\n\n\nbst\nrpart\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nRenamed parameter \nlearner\n to \nLearner\n due to nameclash with \nsetHyperPars\n. Default changes: \nLearner = \"ls\"\n, \nxval = 0\n, and \nmaxdepth = 1\n.\n\n\n\n\n\n\nregr.btgp\n \n \nbtgp\n \nBayesian Treed Gaussian Process\n\n\ntgp\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.btgpllm\n \n \nbtgpllm\n \nBayesian Treed Gaussian Process with jumps to the Limiting Linear Model\n\n\ntgp\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.btlm\n \n \nbtlm\n \nBayesian Treed Linear Model\n\n\ntgp\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.cforest\n \n \ncforest\n \nRandom Forest Based on Conditional Inference Trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nfeatimp\n\n\nSee \n?ctree_control\n for possible breakage for nominal features with missingness.\n\n\n\n\n\n\nregr.crs\n \n \ncrs\n \nRegression Splines\n\n\ncrs\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nse\n\n\n\n\n\n\n\n\nregr.ctree\n \n \nctree\n \nConditional Inference Trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nSee \n?ctree_control\n for possible breakage for nominal features with missingness.\n\n\n\n\n\n\nregr.cubist\n \n \ncubist\n \nCubist\n\n\nCubist\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nregr.cvglmnet\n \n \ncvglmnet\n \nGLM with Lasso or Elasticnet Regularization (Cross Validated Lambda)\n\n\nglmnet\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer.     glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults     before setting the specified parameters and after training.     If you are setting glmnet.control parameters through glmnet.control,     you need to save and re-set them after running the glmnet learner.\n\n\n\n\n\n\nregr.earth\n \n \nearth\n \nMultivariate Adaptive Regression Splines\n\n\nearth\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.elmNN\n \n \nelmNN\n \nExtreme Learning Machine for Single Hidden Layer Feedforward Neural Networks\n\n\nelmNN\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nnhid\n has been set to \n1\n and \nactfun\n has been set to \n\"sig\"\n by default.\n\n\n\n\n\n\nregr.evtree\n \n \nevtree\n \nEvolutionary learning of globally optimal trees\n\n\nevtree\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\npmutatemajor\n, \npmutateminor\n, \npcrossover\n, \npsplit\n, and \npprune\n,       are scaled internally to sum to 100.\n\n\n\n\n\n\nregr.extraTrees\n \n \nextraTrees\n \nExtremely Randomized Trees\n\n\nextraTrees\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\nregr.FDboost\n \n \nFDboost\n \nFunctional linear array regression boosting\n\n\nFDboost\nmboost\n\n\nX\n\n\n\n\n\n\n\n\n\n\nfunctionals\n\n\nOnly allow one base learner for functional covariate and one base learner for scalar covariate, the parameters for these base learners are the same. Also we currently do not support interaction between scalar covariates\n\n\n\n\n\n\nregr.featureless\n \n \nfeatureless\n \nFeatureless regression\n\n\nmlr\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nfunctionals\n\n\n\n\n\n\n\n\nregr.fnn\n \n \nfnn\n \nFast k-Nearest Neighbor\n\n\nFNN\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.frbs\n \n \nfrbs\n \nFuzzy Rule-based Systems\n\n\nfrbs\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.gamboost\n \n \ngamboost\n \nGradient Boosting with Smooth Components\n\n\nmboost\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\nregr.gausspr\n \n \ngausspr\n \nGaussian Processes\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\nKernel parameters have to be passed directly and not by using the \nkpar\n list in \ngausspr\n.     Note that \nfit\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nregr.gbm\n \n \ngbm\n \nGradient Boosting Machine\n\n\ngbm\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nfeatimp\n\n\nkeep.data\n is set to FALSE to reduce memory requirements, \ndistribution\n has been set to \n\"gaussian\"\n by default.\n\n\n\n\n\n\nregr.glm\n \n \nglm\n \nGeneralized Linear Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nse\n\n\n'family' must be a character and every family has its own link, i.e. family = 'gaussian', link.gaussian = 'identity', which is also the default. We set 'model' to FALSE by default to save memory.\n\n\n\n\n\n\nregr.glmboost\n \n \nglmboost\n \nBoosting for GLMs\n\n\nmboost\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\nregr.glmnet\n \n \nglmnet\n \nGLM with Lasso or Elasticnet Regularization\n\n\nglmnet\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer.       Parameter \ns\n (value of the regularization parameter used for predictions) is set to \n0.1\n by default,       but needs to be tuned by the user.       glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults       before setting the specified parameters and after training.       If you are setting glmnet.control parameters through glmnet.control,       you need to save and re-set them after running the glmnet learner.\n\n\n\n\n\n\nregr.GPfit\n \n \nGPfit\n \nGaussian Process\n\n\nGPfit\n\n\nX\n\n\n\n\n\n\n\n\n\n\nse\n\n\n(1) As the optimization routine assumes that the inputs are scaled to the unit hypercube [0,1]^d,             the input gets scaled for each variable by default. If this is not wanted, scale = FALSE has             to be set. (2) We replace the GPfit parameter 'corr = list(type = 'exponential',power = 1.95)' to be seperate             parameters 'type' and 'power', in the case of  corr = list(type = 'matern', nu = 0.5), the seperate parameters             are 'type' and 'matern_nu_k = 0', and nu is computed by 'nu = (2 * matern_nu_k + 1) / 2 = 0.5'\n\n\n\n\n\n\nregr.h2o.deeplearning\n \n \nh2o.dl\n \nh2o.deeplearning\n\n\nh2o\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\nregr.h2o.gbm\n \n \nh2o.gbm\n \nh2o.gbm\n\n\nh2o\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\n'distribution' is set automatically to 'gaussian'.\n\n\n\n\n\n\nregr.h2o.glm\n \n \nh2o.glm\n \nh2o.glm\n\n\nh2o\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n'family' is always set to 'gaussian'.\n\n\n\n\n\n\nregr.h2o.randomForest\n \n \nh2o.rf\n \nh2o.randomForest\n\n\nh2o\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nregr.IBk\n \n \nibk\n \nK-Nearest Neighbours\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.kknn\n \n \nkknn\n \nK-Nearest-Neighbor regression\n\n\nkknn\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.km\n \n \nkm\n \nKriging\n\n\nDiceKriging\n\n\nX\n\n\n\n\n\n\n\n\n\n\nse\n\n\nIn predict, we currently always use \ntype = \"SK\"\n. The extra parameter \njitter\n (default is \nFALSE\n) enables adding a very small jitter (order 1e-12) to the x-values before prediction, as \npredict.km\n reproduces the exact y-values of the training data points, when you pass them in, even if the nugget effect is turned on.   We further introduced \nnugget.stability\n which sets the \nnugget\n to \nnugget.stability * var(y)\n before each training to improve numerical stability. We recommend a setting of 10^-8\n\n\n\n\n\n\nregr.ksvm\n \n \nksvm\n \nSupport Vector Machines\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\nKernel parameters have to be passed directly and not by using the \nkpar\n list in \nksvm\n. Note that \nfit\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nregr.laGP\n \n \nlaGP\n \nLocal Approximate Gaussian Process\n\n\nlaGP\n\n\nX\n\n\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.LiblineaRL2L1SVR\n \n \nliblinl2l1svr\n \nL2-Regularized L1-Loss Support Vector Regression\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nParameter \nsvr_eps\n has been set to \n0.1\n by default.\n\n\n\n\n\n\nregr.LiblineaRL2L2SVR\n \n \nliblinl2l2svr\n \nL2-Regularized L2-Loss Support Vector Regression\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\ntype = 11\n (the default) is primal and \ntype = 12\n is dual problem. Parameter \nsvr_eps\n has been set to \n0.1\n by default.\n\n\n\n\n\n\nregr.lm\n \n \nlm\n \nSimple Linear Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\nse\n\n\n\n\n\n\n\n\nregr.mars\n \n \nmars\n \nMultivariate Adaptive Regression Splines\n\n\nmda\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.mob\n \n \nmob\n \nModel-based Recursive Partitioning  Yielding a Tree with Fitted Models Associated with each Terminal Node\n\n\nparty\nmodeltools\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\nregr.nnet\n \n \nnnet\n \nNeural Network\n\n\nnnet\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\nsize\n has been set to \n3\n by default.\n\n\n\n\n\n\nregr.nodeHarvest\n \n \nnodeHarvest\n \nNode Harvest\n\n\nnodeHarvest\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.pcr\n \n \npcr\n \nPrincipal Component Regression\n\n\npls\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.penalized\n \n \npenalized\n \nPenalized Regression\n\n\npenalized\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntrace=FALSE was set by default to disable logging output.\n\n\n\n\n\n\nregr.plsr\n \n \nplsr\n \nPartial Least Squares Regression\n\n\npls\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.randomForest\n \n \nrf\n \nRandom Forest\n\n\nrandomForest\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nfeatimp\noobpreds\nse\n\n\nSee \n?regr.randomForest\n for information about se estimation. Note that the rf can freeze the R process if trained on a task with 1 feature which is constant. This can happen in feature forward selection, also due to resampling, and you need to remove such features with removeConstantFeatures. keep.inbag is NULL by default but if predict.type = 'se' and se.method = 'jackknife' (the default) then it is automatically set to TRUE.\n\n\n\n\n\n\nregr.randomForestSRC\n \n \nrfsrc\n \nRandom Forest\n\n\nrandomForestSRC\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nfeatimp\noobpreds\n\n\nna.action\n has been set to \n\"na.impute\"\n by default to allow missing data support.\n\n\n\n\n\n\nregr.ranger\n \n \nranger\n \nRandom Forests\n\n\nranger\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nfeatimp\noobpreds\nse\n\n\nBy default, internal parallelization is switched off (\nnum.threads = 1\n), \nverbose\n output is disabled, \nrespect.unordered.factors\n is set to \norder\n for all splitrules. All settings are changeable. \nmtry.perc\n sets \nmtry\n to \nmtry.perc*getTaskNFeats(.task)\n. Default for \nmtry\n is the floor of square root of number of features in task.\n\n\n\n\n\n\nregr.rknn\n \n \nrknn\n \nRandom k-Nearest-Neighbors\n\n\nrknn\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.rpart\n \n \nrpart\n \nDecision Tree\n\n\nrpart\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nfeatimp\n\n\nxval\n has been set to \n0\n by default for speed.\n\n\n\n\n\n\nregr.RRF\n \n \nRRF\n \nRegularized Random Forests\n\n\nRRF\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nfeatimp\n\n\n\n\n\n\n\n\nregr.rsm\n \n \nrsm\n \nResponse Surface Regression\n\n\nrsm\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nYou select the order of the regression by using \nmodelfun = \"FO\"\n (first order), \n\"TWI\"\n (two-way interactions, this is with 1st oder terms!) and \n\"SO\"\n (full second order).\n\n\n\n\n\n\nregr.rvm\n \n \nrvm\n \nRelevance Vector Machine\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\nKernel parameters have to be passed directly and not by using the \nkpar\n list in \nrvm\n. Note that \nfit\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nregr.slim\n \n \nslim\n \nSparse Linear Regression using Nonsmooth Loss Functions and L1 Regularization\n\n\nflare\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nlambda.idx\n has been set to \n3\n by default.\n\n\n\n\n\n\nregr.svm\n \n \nsvm\n \nSupport Vector Machines (libsvm)\n\n\ne1071\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.xgboost\n \n \nxgboost\n \neXtreme Gradient Boosting\n\n\nxgboost\n\n\nX\n\n\n\n\n\n\nX\n\n\nX\n\n\nfeatimp\n\n\nAll settings are passed directly, rather than through \nxgboost\n's \nparams\n argument. \nnrounds\n has been set to \n1\n and \nverbose\n to \n0\n by default.\n\n\n\n\n\n\n\n\nSurvival analysis (12)\n\n\nAdditional learner properties:\n\n\n\n\nprob\n: Probabilities can be predicted,\n\n\nrcens\n, \nlcens\n, \nicens\n: The learner can handle right, left and/or interval censored data.\n\n\n\n\n\n\n\n\n\n\nClass / Short Name / Name\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nOrd.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nsurv.cforest\n \n \ncrf\n \nRandom Forest based on Conditional Inference Trees\n\n\nparty\nsurvival\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nfeatimp\n\n\nSee \n?ctree_control\n for possible breakage for nominal features with missingness.\n\n\n\n\n\n\nsurv.CoxBoost\n \n \ncoxboost\n \nCox Proportional Hazards Model with Componentwise Likelihood based Boosting\n\n\nCoxBoost\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer.\n\n\n\n\n\n\nsurv.coxph\n \n \ncoxph\n \nCox Proportional Hazard Model\n\n\nsurvival\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\nsurv.cv.CoxBoost\n \n \ncv.CoxBoost\n \nCox Proportional Hazards Model with Componentwise Likelihood based Boosting, tuned for the optimal number of boosting steps\n\n\nCoxBoost\n\n\nX\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer.\n\n\n\n\n\n\nsurv.cvglmnet\n \n \ncvglmnet\n \nGLM with Regularization (Cross Validated Lambda)\n\n\nglmnet\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer.\n\n\n\n\n\n\nsurv.gamboost\n \n \ngamboost\n \nGradient boosting with smooth components\n\n\nsurvival\nmboost\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nfamily\n has been set to \nCoxPH()\n by default.\n\n\n\n\n\n\nsurv.gbm\n \n \ngbm\n \nGradient Boosting Machine\n\n\ngbm\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nfeatimp\n\n\nkeep.data\n is set to FALSE to reduce memory requirements.\n\n\n\n\n\n\nsurv.glmboost\n \n \nglmboost\n \nGradient Boosting with Componentwise Linear Models\n\n\nsurvival\nmboost\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nfamily\n has been set to \nCoxPH()\n by default.\n\n\n\n\n\n\nsurv.glmnet\n \n \nglmnet\n \nGLM with Regularization\n\n\nglmnet\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer.       Parameter \ns\n (value of the regularization parameter used for predictions) is set to \n0.1\n by default,       but needs to be tuned by the user.       glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults       before setting the specified parameters and after training.       If you are setting glmnet.control parameters through glmnet.control,       you need to save and re-set them after running the glmnet learner.\n\n\n\n\n\n\nsurv.randomForestSRC\n \n \nrfsrc\n \nRandom Forest\n\n\nsurvival\nrandomForestSRC\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nfeatimp\noobpreds\n\n\nna.action\n has been set to \n\"na.impute\"\n by default to allow missing data support.\n\n\n\n\n\n\nsurv.ranger\n \n \nranger\n \nRandom Forests\n\n\nranger\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nfeatimp\n\n\nBy default, internal parallelization is switched off (\nnum.threads = 1\n), \nverbose\n output is disabled, \nrespect.unordered.factors\n is set to \norder\n for all splitrules. All settings are changeable.\n\n\n\n\n\n\nsurv.rpart\n \n \nrpart\n \nSurvival Tree\n\n\nrpart\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nfeatimp\n\n\nxval\n has been set to \n0\n by default for speed.\n\n\n\n\n\n\n\n\nCluster analysis (9)\n\n\nAdditional learner properties:\n\n\n\n\nprob\n: Probabilities can be predicted.\n\n\n\n\n\n\n\n\n\n\nClass / Short Name / Name\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nOrd.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\ncluster.cmeans\n \n \ncmeans\n \nFuzzy C-Means Clustering\n\n\ne1071\nclue\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\n\n\nThe \npredict\n method uses \ncl_predict\n from the \nclue\n package to compute the cluster memberships for new data. The default \ncenters = 2\n is added so the method runs without setting parameters, but this must in reality of course be changed by the user.\n\n\n\n\n\n\ncluster.Cobweb\n \n \ncobweb\n \nCobweb Clustering Algorithm\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster.dbscan\n \n \ndbscan\n \nDBScan Clustering\n\n\nfpc\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nA cluster index of NA indicates noise points. Specify \nmethod = 'dist'\n if the data should be interpreted as dissimilarity matrix or object. Otherwise Euclidean distances will be used.\n\n\n\n\n\n\ncluster.EM\n \n \nem\n \nExpectation-Maximization Clustering\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster.FarthestFirst\n \n \nfarthestfirst\n \nFarthestFirst Clustering Algorithm\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster.kkmeans\n \n \nkkmeans\n \nKernel K-Means\n\n\nkernlab\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\ncenters\n has been set to \n2L\n by default. The nearest center in kernel distance determines cluster assignment of new data points. Kernel parameters have to be passed directly and not by using the \nkpar\n list in \nkkmeans\n\n\n\n\n\n\ncluster.kmeans\n \n \nkmeans\n \nK-Means\n\n\nstats\nclue\n\n\nX\n\n\n\n\n\n\n\n\n\n\nprob\n\n\nThe \npredict\n method uses \ncl_predict\n from the \nclue\n package to compute the cluster memberships for new data. The default \ncenters = 2\n is added so the method runs without setting parameters, but this must in reality of course be changed by the user.\n\n\n\n\n\n\ncluster.SimpleKMeans\n \n \nsimplekmeans\n \nK-Means Clustering\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster.XMeans\n \n \nxmeans\n \nXMeans (k-means with automatic determination of k)\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nYou may have to install the XMeans Weka package: \nWPM('install-package', 'XMeans')\n.\n\n\n\n\n\n\n\n\nCost-sensitive classification\n\n\nFor \nordinary misclassification costs\n you can use all the standard classification methods listed\nabove.\n\n\nFor \nexample-dependent costs\n there are several ways to generate cost-sensitive learners from\nordinary regression and classification learners.\nSee section \ncost-sensitive classification\n and the documentation\nof \nmakeCostSensClassifWrapper\n, \nmakeCostSensRegrWrapper\n and \nmakeCostSensWeightedPairsWrapper\n\nfor details.\n\n\nMultilabel classification (3)\n\n\n\n\n\n\n\n\nClass / Short Name / Name\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nOrd.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nmultilabel.cforest\n \n \ncforest\n \nRandom forest based on conditional inference trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nprob\n\n\n\n\n\n\n\n\nmultilabel.randomForestSRC\n \n \nrfsrc\n \nRandom Forest\n\n\nrandomForestSRC\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nprob\n\n\nna.action\n has been set to \nna.impute\n by default to allow missing data support.\n\n\n\n\n\n\nmultilabel.rFerns\n \n \nrFerns\n \nRandom ferns\n\n\nrFerns\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoreover, you can use the binary relevance method to apply ordinary classification learners\nto the multilabel problem. See the documentation of function \nmakeMultilabelBinaryRelevanceWrapper\n\nand the tutorial section on \nmultilabel classification\n for details.", 
            "title": "Integrated Learners"
        }, 
        {
            "location": "/integrated_learners/index.html#integrated-learners", 
            "text": "This page lists the learning methods already integrated in  mlr .  Columns  Num. ,  Fac. ,  Ord. ,  NAs , and  Weights  indicate if a method can cope with\nnumerical, factor, and ordered factor predictors, if it can deal with missing values in a meaningful way\n(other than simply removing observations with missing values) and if observation\nweights are supported.  Column  Props  shows further properties of the learning methods specific to the\ntype of learning task.\nSee also  RLearner  for details.", 
            "title": "Integrated Learners"
        }, 
        {
            "location": "/integrated_learners/index.html#classification-84", 
            "text": "For classification the following additional learner properties are relevant and shown in\ncolumn  Props :   prob : The method can predict probabilities,  oneclass ,  twoclass ,  multiclass : One-class, two-class (binary) or multi-class\n  classification problems be handled,  class.weights : Class weights can be handled.      Class / Short Name / Name  Packages  Num.  Fac.  Ord.  NAs  Weights  Props  Note      classif.ada     ada   ada Boosting  ada rpart  X  X     prob twoclass  xval  has been set to  0  by default for speed.    classif.adaboostm1     adaboostm1   ada Boosting M1  RWeka  X  X     prob twoclass multiclass  NAs are directly passed to WEKA with  na.action = na.pass .    classif.bartMachine     bartmachine   Bayesian Additive Regression Trees  bartMachine  X  X   X   prob twoclass  use_missing_data  has been set to  TRUE  by default to allow missing data support.    classif.binomial     binomial   Binomial Regression  stats  X  X    X  prob twoclass  Delegates to  glm  with freely choosable binomial link function via learner parameter  link . We set 'model' to FALSE by default to save memory.    classif.blackboost     blackboost   Gradient Boosting With Regression Trees  mboost party  X  X   X  X  prob twoclass  See  ?ctree_control  for possible breakage for nominal features with missingness.  family  has been set to  Binomial  by default. For 'family' 'AUC' and 'AdaExp' probabilities cannot be predcited.    classif.boosting     adabag   Adabag Boosting  adabag rpart  X  X   X   prob twoclass multiclass featimp  xval  has been set to  0  by default for speed.    classif.bst     bst   Gradient Boosting  bst rpart  X      twoclass  Renamed parameter  learner  to  Learner  due to nameclash with  setHyperPars . Default changes:  Learner = \"ls\" ,  xval = 0 , and  maxdepth = 1 .    classif.C50     C50   C50  C50  X  X   X  X  prob twoclass multiclass     classif.cforest     cforest   Random forest based on conditional inference trees  party  X  X  X  X  X  prob twoclass multiclass featimp  See  ?ctree_control  for possible breakage for nominal features with missingness.    classif.clusterSVM     clusterSVM   Clustered Support Vector Machines  SwarmSVM LiblineaR  X      twoclass  centers  set to  2  by default.    classif.ctree     ctree   Conditional Inference Trees  party  X  X  X  X  X  prob twoclass multiclass  See  ?ctree_control  for possible breakage for nominal features with missingness.    classif.cvglmnet     cvglmnet   GLM with Lasso or Elasticnet Regularization (Cross Validated Lambda)  glmnet  X  X    X  prob twoclass multiclass  The family parameter is set to  binomial  for two-class problems and to  multinomial  otherwise. Factors automatically get converted to dummy columns, ordered factors to integer.       glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults       before setting the specified parameters and after training.       If you are setting glmnet.control parameters through glmnet.control,       you need to save and re-set them after running the glmnet learner.    classif.dbnDNN     dbn.dnn   Deep neural network with weights initialized by DBN  deepnet  X      prob twoclass multiclass  output  set to  \"softmax\"  by default.    classif.dcSVM     dcSVM   Divided-Conquer Support Vector Machines  SwarmSVM e1071  X      twoclass     classif.earth     fda   Flexible Discriminant Analysis  earth stats  X  X    X  prob twoclass multiclass  This learner performs flexible discriminant analysis using the earth algorithm. na.action is set to na.fail and only this is supported.    classif.evtree     evtree   Evolutionary learning of globally optimal trees  evtree  X  X  X   X  prob twoclass multiclass  pmutatemajor ,  pmutateminor ,  pcrossover ,  psplit , and  pprune ,       are scaled internally to sum to 100.    classif.extraTrees     extraTrees   Extremely Randomized Trees  extraTrees  X     X  prob twoclass multiclass     classif.fdausc.glm     fdausc.glm   Generalized Linear Models classification on FDA  fda.usc       prob twoclass multiclass functionals  model$C[[1]] is set to quote(classif.glm)    classif.fdausc.kernel     fdausc.kernel   Kernel classification on FDA  fda.usc       prob twoclass multiclass single.functional  Argument draw=FALSE is used as default.    classif.fdausc.knn     fdausc.knn   fdausc.knn  fda.usc      X  prob twoclass multiclass single.functional  Argument draw=FALSE is used as default.    classif.fdausc.np     fdausc.np   Nonparametric classification on FDA  fda.usc       prob twoclass multiclass single.functional  Argument draw=FALSE is used as default. Additionally, mod$C[[1]] is set to quote(classif.np)    classif.featureless     featureless   Featureless classifier  mlr  X  X  X  X   prob twoclass multiclass functionals     classif.fnn     fnn   Fast k-Nearest Neighbour  FNN  X      twoclass multiclass     classif.gamboost     gamboost   Gradient boosting with smooth components  mboost  X  X    X  prob twoclass  family  has been set to  Binomial()  by default. For 'family' 'AUC' and 'AdaExp' probabilities cannot be predicted.    classif.gaterSVM     gaterSVM   Mixture of SVMs with Neural Network Gater Function  SwarmSVM  X      twoclass  m  set to  3  and  max.iter  set to  1  by default.    classif.gausspr     gausspr   Gaussian Processes  kernlab  X  X     prob twoclass multiclass  Kernel parameters have to be passed directly and not by using the  kpar  list in  gausspr .     Note that  fit  has been set to  FALSE  by default for speed.    classif.gbm     gbm   Gradient Boosting Machine  gbm  X  X   X  X  prob twoclass multiclass featimp  keep.data  is set to FALSE to reduce memory requirements. Note on param 'distribution': gbm will select 'bernoulli' by default for 2 classes, and 'multinomial' for       multiclass problems. The latter is the only setting that works for   2 classes.    classif.geoDA     geoda   Geometric Predictive Discriminant Analysis  DiscriMiner  X      twoclass multiclass     classif.glmboost     glmboost   Boosting for GLMs  mboost  X  X    X  prob twoclass  family  has been set to  Binomial  by default. For 'family' 'AUC' and 'AdaExp' probabilities cannot be predcited.    classif.glmnet     glmnet   GLM with Lasso or Elasticnet Regularization  glmnet  X  X    X  prob twoclass multiclass  The family parameter is set to  binomial  for two-class problems and to  multinomial  otherwise.       Factors automatically get converted to dummy columns, ordered factors to integer.       Parameter  s  (value of the regularization parameter used for predictions) is set to  0.1  by default,       but needs to be tuned by the user.       glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults       before setting the specified parameters and after training.       If you are setting glmnet.control parameters through glmnet.control,       you need to save and re-set them after running the glmnet learner.    classif.h2o.deeplearning     h2o.dl   h2o.deeplearning  h2o  X  X   X  X  prob twoclass multiclass     classif.h2o.gbm     h2o.gbm   h2o.gbm  h2o  X  X   X   prob twoclass multiclass  'distribution' is set automatically to 'gaussian'.    classif.h2o.glm     h2o.glm   h2o.glm  h2o  X  X   X  X  prob twoclass  'family' is always set to 'binomial' to get a binary classifier.    classif.h2o.randomForest     h2o.rf   h2o.randomForest  h2o  X  X   X   prob twoclass multiclass     classif.IBk     ibk   k-Nearest Neighbours  RWeka  X  X     prob twoclass multiclass     classif.J48     j48   J48 Decision Trees  RWeka  X  X   X   prob twoclass multiclass  NAs are directly passed to WEKA with  na.action = na.pass .    classif.JRip     jrip   Propositional Rule Learner  RWeka  X  X   X   prob twoclass multiclass  NAs are directly passed to WEKA with  na.action = na.pass .    classif.kknn     kknn   k-Nearest Neighbor  kknn  X  X     prob twoclass multiclass     classif.knn     knn   k-Nearest Neighbor  class  X      twoclass multiclass     classif.ksvm     ksvm   Support Vector Machines  kernlab  X  X     prob twoclass multiclass class.weights  Kernel parameters have to be passed directly and not by using the  kpar  list in  ksvm . Note that  fit  has been set to  FALSE  by default for speed.    classif.lda     lda   Linear Discriminant Analysis  MASS  X  X     prob twoclass multiclass  Learner parameter  predict.method  maps to  method  in  predict.lda .    classif.LiblineaRL1L2SVC     liblinl1l2svc   L1-Regularized L2-Loss Support Vector Classification  LiblineaR  X      twoclass multiclass class.weights     classif.LiblineaRL1LogReg     liblinl1logreg   L1-Regularized Logistic Regression  LiblineaR  X      prob twoclass multiclass class.weights     classif.LiblineaRL2L1SVC     liblinl2l1svc   L2-Regularized L1-Loss Support Vector Classification  LiblineaR  X      twoclass multiclass class.weights     classif.LiblineaRL2LogReg     liblinl2logreg   L2-Regularized Logistic Regression  LiblineaR  X      prob twoclass multiclass class.weights  type = 0  (the default) is primal and  type = 7  is dual problem.    classif.LiblineaRL2SVC     liblinl2svc   L2-Regularized L2-Loss Support Vector Classification  LiblineaR  X      twoclass multiclass class.weights  type = 2  (the default) is primal and  type = 1  is dual problem.    classif.LiblineaRMultiClassSVC     liblinmulticlasssvc   Support Vector Classification by Crammer and Singer  LiblineaR  X      twoclass multiclass class.weights     classif.linDA     linda   Linear Discriminant Analysis  DiscriMiner  X      twoclass multiclass  Set  validation = NULL  by default to disable internal test set validation.    classif.logreg     logreg   Logistic Regression  stats  X  X    X  prob twoclass  Delegates to  glm  with  family = binomial(link = 'logit') . We set 'model' to FALSE by default to save memory.    classif.lqa     lqa   Fitting penalized Generalized Linear Models with the LQA algorithm  lqa  X      prob twoclass  penalty  has been set to  \"lasso\"  and  lambda  to  0.1  by default. The parameters  lambda ,  gamma ,  alpha ,  oscar.c ,  a ,  lambda1  and  lambda2  are the tuning parameters of the  penalty  function being used, and correspond to the parameters as named in the respective help files. Parameter  c  for penalty method  oscar  has been named  oscar.c . Parameters  lambda1  and  lambda2  correspond to the parameters named 'lambda_1' and 'lambda_2' of the penalty functions  enet ,  fused.lasso ,  icb ,  licb , as well as  weighted.fusion .    classif.lssvm     lssvm   Least Squares Support Vector Machine  kernlab  X  X     twoclass multiclass  fitted  has been set to  FALSE  by default for speed.    classif.lvq1     lvq1   Learning Vector Quantization  class  X      twoclass multiclass     classif.mda     mda   Mixture Discriminant Analysis  mda  X  X     prob twoclass multiclass  keep.fitted  has been set to  FALSE  by default for speed and we use  start.method = \"lvq\"  for more robust behavior / less technical crashes.    classif.mlp     mlp   Multi-Layer Perceptron  RSNNS  X      prob twoclass multiclass     classif.multinom     multinom   Multinomial Regression  nnet  X  X    X  prob twoclass multiclass     classif.naiveBayes     nbayes   Naive Bayes  e1071  X  X   X   prob twoclass multiclass     classif.neuralnet     neuralnet   Neural Network from neuralnet  neuralnet  X      prob twoclass  err.fct  has been set to  ce  and  linear.output  to FALSE to do classification.    classif.nnet     nnet   Neural Network  nnet  X  X    X  prob twoclass multiclass  size  has been set to  3  by default.    classif.nnTrain     nn.train   Training Neural Network by Backpropagation  deepnet  X      prob twoclass multiclass  output  set to  softmax  by default.  max.number.of.layers  can be set to control and tune the maximal number of layers specified via  hidden .    classif.nodeHarvest     nodeHarvest   Node Harvest  nodeHarvest  X  X     prob twoclass     classif.OneR     oner   1-R Classifier  RWeka  X  X   X   prob twoclass multiclass  NAs are directly passed to WEKA with  na.action = na.pass .    classif.pamr     pamr   Nearest shrunken centroid  pamr  X      prob twoclass  Threshold for prediction ( threshold.predict ) has been set to  1  by default.    classif.PART     part   PART Decision Lists  RWeka  X  X   X   prob twoclass multiclass  NAs are directly passed to WEKA with  na.action = na.pass .    classif.penalized     penalized   Penalized Logistic Regression  penalized  X  X  X    prob twoclass  trace=FALSE was set by default to disable logging output.    classif.plr     plr   Logistic Regression with a L2 Penalty  stepPlr  X  X    X  prob twoclass  AIC and BIC penalty types can be selected via the new parameter  cp.type .    classif.plsdaCaret     plsdacaret   Partial Least Squares (PLS) Discriminant Analysis  caret pls  X      prob twoclass     classif.probit     probit   Probit Regression  stats  X  X    X  prob twoclass  Delegates to  glm  with  family = binomial(link = 'probit') . We set 'model' to FALSE by default to save memory.    classif.qda     qda   Quadratic Discriminant Analysis  MASS  X  X     prob twoclass multiclass  Learner parameter  predict.method  maps to  method  in  predict.qda .    classif.quaDA     quada   Quadratic Discriminant Analysis  DiscriMiner  X      twoclass multiclass     classif.randomForest     rf   Random Forest  randomForest  X  X  X    prob twoclass multiclass class.weights featimp oobpreds  Note that the rf can freeze the R process if trained on a task with 1 feature which is constant. This can happen in feature forward selection, also due to resampling, and you need to remove such features with removeConstantFeatures.    classif.randomForestSRC     rfsrc   Random Forest  randomForestSRC  X  X  X  X  X  prob twoclass multiclass featimp oobpreds  na.action  has been set to  \"na.impute\"  by default to allow missing data support.    classif.ranger     ranger   Random Forests  ranger  X  X  X   X  prob twoclass multiclass featimp oobpreds  By default, internal parallelization is switched off ( num.threads = 1 ),  verbose  output is disabled,  respect.unordered.factors  is set to  order  for all splitrules. All settings are changeable.  mtry.perc  sets  mtry  to  mtry.perc*getTaskNFeats(.task) . Default for  mtry  is the floor of square root of number of features in task. Default for  min.node.size  is 1 for classification and 10 for probability estimation.    classif.rda     rda   Regularized Discriminant Analysis  klaR  X  X     prob twoclass multiclass  estimate.error  has been set to  FALSE  by default for speed.    classif.rFerns     rFerns   Random ferns  rFerns  X  X  X    twoclass multiclass oobpreds     classif.rknn     rknn   Random k-Nearest-Neighbors  rknn  X   X    twoclass multiclass  k restricted to   99 as the code allocates arrays of static size    classif.rotationForest     rotationForest   Rotation Forest  rotationForest  X  X  X    prob twoclass     classif.rpart     rpart   Decision Tree  rpart  X  X  X  X  X  prob twoclass multiclass featimp  xval  has been set to  0  by default for speed.    classif.RRF     RRF   Regularized Random Forests  RRF  X  X     prob twoclass multiclass featimp     classif.rrlda     rrlda   Robust Regularized Linear Discriminant Analysis  rrlda  X      twoclass multiclass     classif.saeDNN     sae.dnn   Deep neural network with weights initialized by Stacked AutoEncoder  deepnet  X      prob twoclass multiclass  output  set to  \"softmax\"  by default.    classif.sda     sda   Shrinkage Discriminant Analysis  sda  X      prob twoclass multiclass     classif.sparseLDA     sparseLDA   Sparse Discriminant Analysis  sparseLDA MASS elasticnet  X      prob twoclass multiclass  Arguments  Q  and  stop  are not yet provided as they depend on the task.    classif.svm     svm   Support Vector Machines (libsvm)  e1071  X  X     prob twoclass multiclass class.weights     classif.xgboost     xgboost   eXtreme Gradient Boosting  xgboost  X    X  X  prob twoclass multiclass featimp  All settings are passed directly, rather than through  xgboost 's  params  argument.  nrounds  has been set to  1  and  verbose  to  0  by default.  num_class  is set internally, so do not set this manually.", 
            "title": "Classification (84)"
        }, 
        {
            "location": "/integrated_learners/index.html#regression-61", 
            "text": "Additional learner properties:   se : Standard errors can be predicted.      Class / Short Name / Name  Packages  Num.  Fac.  Ord.  NAs  Weights  Props  Note      regr.bartMachine     bartmachine   Bayesian Additive Regression Trees  bartMachine  X  X   X    use_missing_data  has been set to  TRUE  by default to allow missing data support.    regr.bcart     bcart   Bayesian CART  tgp  X  X     se     regr.bgp     bgp   Bayesian Gaussian Process  tgp  X      se     regr.bgpllm     bgpllm   Bayesian Gaussian Process with jumps to the Limiting Linear Model  tgp  X      se     regr.blackboost     blackboost   Gradient Boosting with Regression Trees  mboost party  X  X   X  X   See  ?ctree_control  for possible breakage for nominal features with missingness.    regr.blm     blm   Bayesian Linear Model  tgp  X      se     regr.brnn     brnn   Bayesian regularization for feed-forward neural networks  brnn  X  X         regr.bst     bst   Gradient Boosting  bst rpart  X       Renamed parameter  learner  to  Learner  due to nameclash with  setHyperPars . Default changes:  Learner = \"ls\" ,  xval = 0 , and  maxdepth = 1 .    regr.btgp     btgp   Bayesian Treed Gaussian Process  tgp  X  X     se     regr.btgpllm     btgpllm   Bayesian Treed Gaussian Process with jumps to the Limiting Linear Model  tgp  X  X     se     regr.btlm     btlm   Bayesian Treed Linear Model  tgp  X  X     se     regr.cforest     cforest   Random Forest Based on Conditional Inference Trees  party  X  X  X  X  X  featimp  See  ?ctree_control  for possible breakage for nominal features with missingness.    regr.crs     crs   Regression Splines  crs  X  X    X  se     regr.ctree     ctree   Conditional Inference Trees  party  X  X  X  X  X   See  ?ctree_control  for possible breakage for nominal features with missingness.    regr.cubist     cubist   Cubist  Cubist  X  X   X       regr.cvglmnet     cvglmnet   GLM with Lasso or Elasticnet Regularization (Cross Validated Lambda)  glmnet  X  X    X   Factors automatically get converted to dummy columns, ordered factors to integer.     glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults     before setting the specified parameters and after training.     If you are setting glmnet.control parameters through glmnet.control,     you need to save and re-set them after running the glmnet learner.    regr.earth     earth   Multivariate Adaptive Regression Splines  earth  X  X         regr.elmNN     elmNN   Extreme Learning Machine for Single Hidden Layer Feedforward Neural Networks  elmNN  X       nhid  has been set to  1  and  actfun  has been set to  \"sig\"  by default.    regr.evtree     evtree   Evolutionary learning of globally optimal trees  evtree  X  X  X   X   pmutatemajor ,  pmutateminor ,  pcrossover ,  psplit , and  pprune ,       are scaled internally to sum to 100.    regr.extraTrees     extraTrees   Extremely Randomized Trees  extraTrees  X     X      regr.FDboost     FDboost   Functional linear array regression boosting  FDboost mboost  X      functionals  Only allow one base learner for functional covariate and one base learner for scalar covariate, the parameters for these base learners are the same. Also we currently do not support interaction between scalar covariates    regr.featureless     featureless   Featureless regression  mlr  X  X  X  X   functionals     regr.fnn     fnn   Fast k-Nearest Neighbor  FNN  X          regr.frbs     frbs   Fuzzy Rule-based Systems  frbs  X          regr.gamboost     gamboost   Gradient Boosting with Smooth Components  mboost  X  X    X      regr.gausspr     gausspr   Gaussian Processes  kernlab  X  X     se  Kernel parameters have to be passed directly and not by using the  kpar  list in  gausspr .     Note that  fit  has been set to  FALSE  by default for speed.    regr.gbm     gbm   Gradient Boosting Machine  gbm  X  X   X  X  featimp  keep.data  is set to FALSE to reduce memory requirements,  distribution  has been set to  \"gaussian\"  by default.    regr.glm     glm   Generalized Linear Regression  stats  X  X    X  se  'family' must be a character and every family has its own link, i.e. family = 'gaussian', link.gaussian = 'identity', which is also the default. We set 'model' to FALSE by default to save memory.    regr.glmboost     glmboost   Boosting for GLMs  mboost  X  X    X      regr.glmnet     glmnet   GLM with Lasso or Elasticnet Regularization  glmnet  X  X  X   X   Factors automatically get converted to dummy columns, ordered factors to integer.       Parameter  s  (value of the regularization parameter used for predictions) is set to  0.1  by default,       but needs to be tuned by the user.       glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults       before setting the specified parameters and after training.       If you are setting glmnet.control parameters through glmnet.control,       you need to save and re-set them after running the glmnet learner.    regr.GPfit     GPfit   Gaussian Process  GPfit  X      se  (1) As the optimization routine assumes that the inputs are scaled to the unit hypercube [0,1]^d,             the input gets scaled for each variable by default. If this is not wanted, scale = FALSE has             to be set. (2) We replace the GPfit parameter 'corr = list(type = 'exponential',power = 1.95)' to be seperate             parameters 'type' and 'power', in the case of  corr = list(type = 'matern', nu = 0.5), the seperate parameters             are 'type' and 'matern_nu_k = 0', and nu is computed by 'nu = (2 * matern_nu_k + 1) / 2 = 0.5'    regr.h2o.deeplearning     h2o.dl   h2o.deeplearning  h2o  X  X   X  X      regr.h2o.gbm     h2o.gbm   h2o.gbm  h2o  X  X   X    'distribution' is set automatically to 'gaussian'.    regr.h2o.glm     h2o.glm   h2o.glm  h2o  X  X   X  X   'family' is always set to 'gaussian'.    regr.h2o.randomForest     h2o.rf   h2o.randomForest  h2o  X  X   X       regr.IBk     ibk   K-Nearest Neighbours  RWeka  X  X         regr.kknn     kknn   K-Nearest-Neighbor regression  kknn  X  X         regr.km     km   Kriging  DiceKriging  X      se  In predict, we currently always use  type = \"SK\" . The extra parameter  jitter  (default is  FALSE ) enables adding a very small jitter (order 1e-12) to the x-values before prediction, as  predict.km  reproduces the exact y-values of the training data points, when you pass them in, even if the nugget effect is turned on.   We further introduced  nugget.stability  which sets the  nugget  to  nugget.stability * var(y)  before each training to improve numerical stability. We recommend a setting of 10^-8    regr.ksvm     ksvm   Support Vector Machines  kernlab  X  X      Kernel parameters have to be passed directly and not by using the  kpar  list in  ksvm . Note that  fit  has been set to  FALSE  by default for speed.    regr.laGP     laGP   Local Approximate Gaussian Process  laGP  X      se     regr.LiblineaRL2L1SVR     liblinl2l1svr   L2-Regularized L1-Loss Support Vector Regression  LiblineaR  X       Parameter  svr_eps  has been set to  0.1  by default.    regr.LiblineaRL2L2SVR     liblinl2l2svr   L2-Regularized L2-Loss Support Vector Regression  LiblineaR  X       type = 11  (the default) is primal and  type = 12  is dual problem. Parameter  svr_eps  has been set to  0.1  by default.    regr.lm     lm   Simple Linear Regression  stats  X  X    X  se     regr.mars     mars   Multivariate Adaptive Regression Splines  mda  X          regr.mob     mob   Model-based Recursive Partitioning  Yielding a Tree with Fitted Models Associated with each Terminal Node  party modeltools  X  X    X      regr.nnet     nnet   Neural Network  nnet  X  X    X   size  has been set to  3  by default.    regr.nodeHarvest     nodeHarvest   Node Harvest  nodeHarvest  X  X         regr.pcr     pcr   Principal Component Regression  pls  X  X         regr.penalized     penalized   Penalized Regression  penalized  X  X      trace=FALSE was set by default to disable logging output.    regr.plsr     plsr   Partial Least Squares Regression  pls  X  X         regr.randomForest     rf   Random Forest  randomForest  X  X  X    featimp oobpreds se  See  ?regr.randomForest  for information about se estimation. Note that the rf can freeze the R process if trained on a task with 1 feature which is constant. This can happen in feature forward selection, also due to resampling, and you need to remove such features with removeConstantFeatures. keep.inbag is NULL by default but if predict.type = 'se' and se.method = 'jackknife' (the default) then it is automatically set to TRUE.    regr.randomForestSRC     rfsrc   Random Forest  randomForestSRC  X  X  X  X  X  featimp oobpreds  na.action  has been set to  \"na.impute\"  by default to allow missing data support.    regr.ranger     ranger   Random Forests  ranger  X  X  X    featimp oobpreds se  By default, internal parallelization is switched off ( num.threads = 1 ),  verbose  output is disabled,  respect.unordered.factors  is set to  order  for all splitrules. All settings are changeable.  mtry.perc  sets  mtry  to  mtry.perc*getTaskNFeats(.task) . Default for  mtry  is the floor of square root of number of features in task.    regr.rknn     rknn   Random k-Nearest-Neighbors  rknn  X   X        regr.rpart     rpart   Decision Tree  rpart  X  X  X  X  X  featimp  xval  has been set to  0  by default for speed.    regr.RRF     RRF   Regularized Random Forests  RRF  X  X  X    featimp     regr.rsm     rsm   Response Surface Regression  rsm  X       You select the order of the regression by using  modelfun = \"FO\"  (first order),  \"TWI\"  (two-way interactions, this is with 1st oder terms!) and  \"SO\"  (full second order).    regr.rvm     rvm   Relevance Vector Machine  kernlab  X  X      Kernel parameters have to be passed directly and not by using the  kpar  list in  rvm . Note that  fit  has been set to  FALSE  by default for speed.    regr.slim     slim   Sparse Linear Regression using Nonsmooth Loss Functions and L1 Regularization  flare  X       lambda.idx  has been set to  3  by default.    regr.svm     svm   Support Vector Machines (libsvm)  e1071  X  X         regr.xgboost     xgboost   eXtreme Gradient Boosting  xgboost  X    X  X  featimp  All settings are passed directly, rather than through  xgboost 's  params  argument.  nrounds  has been set to  1  and  verbose  to  0  by default.", 
            "title": "Regression (61)"
        }, 
        {
            "location": "/integrated_learners/index.html#survival-analysis-12", 
            "text": "Additional learner properties:   prob : Probabilities can be predicted,  rcens ,  lcens ,  icens : The learner can handle right, left and/or interval censored data.      Class / Short Name / Name  Packages  Num.  Fac.  Ord.  NAs  Weights  Props  Note      surv.cforest     crf   Random Forest based on Conditional Inference Trees  party survival  X  X  X  X  X  featimp  See  ?ctree_control  for possible breakage for nominal features with missingness.    surv.CoxBoost     coxboost   Cox Proportional Hazards Model with Componentwise Likelihood based Boosting  CoxBoost  X  X  X   X   Factors automatically get converted to dummy columns, ordered factors to integer.    surv.coxph     coxph   Cox Proportional Hazard Model  survival  X  X    X      surv.cv.CoxBoost     cv.CoxBoost   Cox Proportional Hazards Model with Componentwise Likelihood based Boosting, tuned for the optimal number of boosting steps  CoxBoost  X  X    X   Factors automatically get converted to dummy columns, ordered factors to integer.    surv.cvglmnet     cvglmnet   GLM with Regularization (Cross Validated Lambda)  glmnet  X  X  X   X   Factors automatically get converted to dummy columns, ordered factors to integer.    surv.gamboost     gamboost   Gradient boosting with smooth components  survival mboost  X  X  X   X   family  has been set to  CoxPH()  by default.    surv.gbm     gbm   Gradient Boosting Machine  gbm  X  X   X  X  featimp  keep.data  is set to FALSE to reduce memory requirements.    surv.glmboost     glmboost   Gradient Boosting with Componentwise Linear Models  survival mboost  X  X  X   X   family  has been set to  CoxPH()  by default.    surv.glmnet     glmnet   GLM with Regularization  glmnet  X  X  X   X   Factors automatically get converted to dummy columns, ordered factors to integer.       Parameter  s  (value of the regularization parameter used for predictions) is set to  0.1  by default,       but needs to be tuned by the user.       glmnet uses a global control object for its parameters. mlr resets all control parameters to their defaults       before setting the specified parameters and after training.       If you are setting glmnet.control parameters through glmnet.control,       you need to save and re-set them after running the glmnet learner.    surv.randomForestSRC     rfsrc   Random Forest  survival randomForestSRC  X  X  X  X  X  featimp oobpreds  na.action  has been set to  \"na.impute\"  by default to allow missing data support.    surv.ranger     ranger   Random Forests  ranger  X  X  X    featimp  By default, internal parallelization is switched off ( num.threads = 1 ),  verbose  output is disabled,  respect.unordered.factors  is set to  order  for all splitrules. All settings are changeable.    surv.rpart     rpart   Survival Tree  rpart  X  X  X  X  X  featimp  xval  has been set to  0  by default for speed.", 
            "title": "Survival analysis (12)"
        }, 
        {
            "location": "/integrated_learners/index.html#cluster-analysis-9", 
            "text": "Additional learner properties:   prob : Probabilities can be predicted.      Class / Short Name / Name  Packages  Num.  Fac.  Ord.  NAs  Weights  Props  Note      cluster.cmeans     cmeans   Fuzzy C-Means Clustering  e1071 clue  X      prob  The  predict  method uses  cl_predict  from the  clue  package to compute the cluster memberships for new data. The default  centers = 2  is added so the method runs without setting parameters, but this must in reality of course be changed by the user.    cluster.Cobweb     cobweb   Cobweb Clustering Algorithm  RWeka  X          cluster.dbscan     dbscan   DBScan Clustering  fpc  X       A cluster index of NA indicates noise points. Specify  method = 'dist'  if the data should be interpreted as dissimilarity matrix or object. Otherwise Euclidean distances will be used.    cluster.EM     em   Expectation-Maximization Clustering  RWeka  X          cluster.FarthestFirst     farthestfirst   FarthestFirst Clustering Algorithm  RWeka  X          cluster.kkmeans     kkmeans   Kernel K-Means  kernlab  X       centers  has been set to  2L  by default. The nearest center in kernel distance determines cluster assignment of new data points. Kernel parameters have to be passed directly and not by using the  kpar  list in  kkmeans    cluster.kmeans     kmeans   K-Means  stats clue  X      prob  The  predict  method uses  cl_predict  from the  clue  package to compute the cluster memberships for new data. The default  centers = 2  is added so the method runs without setting parameters, but this must in reality of course be changed by the user.    cluster.SimpleKMeans     simplekmeans   K-Means Clustering  RWeka  X          cluster.XMeans     xmeans   XMeans (k-means with automatic determination of k)  RWeka  X       You may have to install the XMeans Weka package:  WPM('install-package', 'XMeans') .", 
            "title": "Cluster analysis (9)"
        }, 
        {
            "location": "/integrated_learners/index.html#cost-sensitive-classification", 
            "text": "For  ordinary misclassification costs  you can use all the standard classification methods listed\nabove.  For  example-dependent costs  there are several ways to generate cost-sensitive learners from\nordinary regression and classification learners.\nSee section  cost-sensitive classification  and the documentation\nof  makeCostSensClassifWrapper ,  makeCostSensRegrWrapper  and  makeCostSensWeightedPairsWrapper \nfor details.", 
            "title": "Cost-sensitive classification"
        }, 
        {
            "location": "/integrated_learners/index.html#multilabel-classification-3", 
            "text": "Class / Short Name / Name  Packages  Num.  Fac.  Ord.  NAs  Weights  Props  Note      multilabel.cforest     cforest   Random forest based on conditional inference trees  party  X  X  X  X  X  prob     multilabel.randomForestSRC     rfsrc   Random Forest  randomForestSRC  X  X   X  X  prob  na.action  has been set to  na.impute  by default to allow missing data support.    multilabel.rFerns     rFerns   Random ferns  rFerns  X  X  X         Moreover, you can use the binary relevance method to apply ordinary classification learners\nto the multilabel problem. See the documentation of function  makeMultilabelBinaryRelevanceWrapper \nand the tutorial section on  multilabel classification  for details.", 
            "title": "Multilabel classification (3)"
        }, 
        {
            "location": "/measures/index.html", 
            "text": "Implemented Performance Measures\n\n\nThis page shows the performance measures available for the different types of\nlearning problems as well as general performance measures in alphabetical order.\n(See also the documentation about \nmeasures\n and \nmakeMeasure\n for available measures and\ntheir properties.)\n\n\nIf you find that a measure is missing, you can either \nopen an issue\n\nor try \nto implement a measure yourself\n.\n\n\nColumn \nMinim.\n indicates if the measure is minimized during, e.g., tuning or\nfeature selection.\n\nBest\n and \nWorst\n show the best and worst values the performance measure can attain.\nFor \nclassification\n, column \nMulti\n indicates if a measure is suitable for\nmulti-class problems. If not, the measure can only be used for binary classification problems.\n\n\nThe next six columns refer to information required to calculate the performance measure.\n\n\n\n\nPred.\n: The \nPrediction\n object.\n\n\nTruth\n: The true values of the response variable(s) (for supervised learning).\n\n\nProbs\n: The predicted probabilities (might be needed for classification).\n\n\nModel\n: The \nWrappedModel\n (e.g., for calculating the training time).\n\n\nTask\n: The \nTask\n (relevant for cost-sensitive classification).\n\n\nFeats\n: The predicted data (relevant for clustering).\n\n\n\n\nAggr.\n shows the default \naggregation method\n tied to the measure.\n\n\nClassification\n\n\n\n\n\n\n\n\nID / Name\n\n\nMinim.\n\n\nBest\n\n\nWorst\n\n\nMulti\n\n\nPred.\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggr.\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nacc\n \nAccuracy\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: mean(response == truth)\n\n\n\n\n\n\nauc\n \nArea under the curve\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nIntegral over the graph that results from computing fpr and tpr for many different thresholds.\n\n\n\n\n\n\nbac\n \nBalanced accuracy\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nMean of true positive rate and true negative rate.\n\n\n\n\n\n\nber\n \nBalanced error rate\n\n\nX\n\n\n0\n\n\n1\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nMean of misclassification error rates on all individual classes.\n\n\n\n\n\n\nbrier\n \nBrier score\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nThe Brier score is defined as the quadratic difference between the probability and the value (1,0) for the class.   That means we use the numeric representation 1 and 0 for our target classes. It is similiar to the mean squared error in regression.   multiclass.brier is the sum over all one vs. all comparisons and for a binary classifcation 2 * brier.\n\n\n\n\n\n\nbrier.scaled\n \nBrier scaled\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nBrier score scaled to [0,1], see \nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/\n.\n\n\n\n\n\n\nf1\n \nF1 measure\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: 2 * tp/ (sum(truth == positive) + sum(response == positive))\n\n\n\n\n\n\nfdr\n \nFalse discovery rate\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: fp / (tp + fp).\n\n\n\n\n\n\nfn\n \nFalse negatives\n\n\nX\n\n\n0\n\n\nInf\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nSum of misclassified observations in the negative class. Also called misses.\n\n\n\n\n\n\nfnr\n \nFalse negative rate\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nPercentage of misclassified observations in the negative class.\n\n\n\n\n\n\nfp\n \nFalse positives\n\n\nX\n\n\n0\n\n\nInf\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nSum of misclassified observations in the positive class. Also called false alarms.\n\n\n\n\n\n\nfpr\n \nFalse positive rate\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nPercentage of misclassified observations in the positive class. Also called false alarm rate or fall-out.\n\n\n\n\n\n\ngmean\n \nG-mean\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nGeometric mean of recall and specificity.\n\n\n\n\n\n\ngpr\n \nGeometric mean of precision and recall.\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: sqrt(ppv * tpr)\n\n\n\n\n\n\nkappa\n \nCohen's kappa\n\n\n\n\n1\n\n\n-1\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: 1 - (1 - p0) / (1 - pe). With: p0 = 'observed frequency of     agreement' and pe = 'expected agremeent frequency under independence\n\n\n\n\n\n\nlogloss\n \nLogarithmic loss\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: -mean(log(p_i)), where p_i is the predicted probability of the true class of observation i. Inspired by \nhttps://www.kaggle.com/wiki/MultiClassLogLoss\n.\n\n\n\n\n\n\nlsr\n \nLogarithmic Scoring Rule\n\n\n\n\n0\n\n\n-Inf\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: mean(log(p_i)), where p_i is the predicted probability of the true class of observation i.   This scoring rule is the same as the negative logloss, self-information or surprisal.   See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2), 49-65.\n\n\n\n\n\n\nmcc\n \nMatthews correlation coefficient\n\n\n\n\n1\n\n\n-1\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as (tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)), denominator set to 1 if 0\n\n\n\n\n\n\nmmce\n \nMean misclassification error\n\n\nX\n\n\n0\n\n\n1\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: mean(response != truth)\n\n\n\n\n\n\nmulticlass.au1p\n \nWeighted average 1 vs. 1 multiclass AUC\n\n\n\n\n1\n\n\n0.5\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nComputes AUC of c(c - 1) binary classifiers while considering the a priori distribution of the classes. See Ferri et al.: \nhttps://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf\n.\n\n\n\n\n\n\nmulticlass.au1u\n \nAverage 1 vs. 1 multiclass AUC\n\n\n\n\n1\n\n\n0.5\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nComputes AUC of c(c - 1) binary classifiers (all possible pairwise combinations) while considering uniform distribution of the classes. See Ferri et al.: \nhttps://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf\n.\n\n\n\n\n\n\nmulticlass.aunp\n \nWeighted average 1 vs. rest multiclass AUC\n\n\n\n\n1\n\n\n0.5\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nComputes the AUC treating a c-dimensional classifier as c two-dimensional classifiers, taking into account the prior probability of each class. See Ferri et al.: \nhttps://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf\n.\n\n\n\n\n\n\nmulticlass.aunu\n \nAverage 1 vs. rest multiclass AUC\n\n\n\n\n1\n\n\n0.5\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nComputes the AUC treating a c-dimensional classifier as c two-dimensional classifiers, where classes are assumed to have uniform distribution, in order to have a measure which is independent of class distribution change. See Ferri et al.: \nhttps://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf\n.\n\n\n\n\n\n\nmulticlass.brier\n \nMulticlass Brier score\n\n\nX\n\n\n0\n\n\n2\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: (1/n) sum_i sum_j (y_ij - p_ij)^2, where y_ij = 1 if observation i has class j (else 0), and p_ij is the predicted probability of observation i for class j. From \nhttp://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf\n.\n\n\n\n\n\n\nnpv\n \nNegative predictive value\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: tn / (tn + fn).\n\n\n\n\n\n\nppv\n \nPositive predictive value\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: tp / (tp + fp). Also called precision. If the denominator is 0, PPV is set to be either 1 or 0 depending on whether the highest probability prediction is positive (1) or negative (0).\n\n\n\n\n\n\nqsr\n \nQuadratic Scoring Rule\n\n\n\n\n1\n\n\n-1\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: 1 - (1/n) sum_i sum_j (y_ij - p_ij)^2, where y_ij = 1 if observation i has class j (else 0), and p_ij is the predicted probablity of observation i for class j.   This scoring rule is the same as 1 - multiclass.brier.   See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2), 49-65.\n\n\n\n\n\n\nssr\n \nSpherical Scoring Rule\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: mean(p_i(sum_j(p_ij))), where p_i is the predicted probability of the true class of observation i and p_ij is the predicted probablity of observation i for class j.   See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2), 49-65.\n\n\n\n\n\n\ntn\n \nTrue negatives\n\n\n\n\nInf\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nSum of correctly classified observations in the negative class. Also called correct rejections.\n\n\n\n\n\n\ntnr\n \nTrue negative rate\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nPercentage of correctly classified observations in the negative class. Also called specificity.\n\n\n\n\n\n\ntp\n \nTrue positives\n\n\n\n\nInf\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nSum of all correctly classified observations in the positive class.\n\n\n\n\n\n\ntpr\n \nTrue positive rate\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nPercentage of correctly classified observations in the positive class. Also called hit rate or recall or sensitivity.\n\n\n\n\n\n\nwkappa\n \nMean quadratic weighted kappa\n\n\n\n\n1\n\n\n-1\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: 1 - sum(weights * conf.mat) / sum(weights * expected.mat),     the weight matrix measures seriousness of disagreement with the squared euclidean metric.\n\n\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\nID / Name\n\n\nMinim.\n\n\nBest\n\n\nWorst\n\n\nPred.\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggr.\n\n\nNote\n\n\n\n\n\n\n\n\n\n\narsq\n \nAdjusted coefficient of determination\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: 1 - (1 - rsq) * (p / (n - p - 1L)). Adjusted R-squared is only defined for normal linear regression.\n\n\n\n\n\n\nexpvar\n \nExplained variance\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nSimilar to measure rsq (R-squared). Defined as explained_sum_of_squares / total_sum_of_squares.\n\n\n\n\n\n\nkendalltau\n \nKendall's tau\n\n\n\n\n1\n\n\n-1\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: Kendall's tau correlation between truth and response. Only looks at the order.   See Rosset et al.: \nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.1398\nrep=rep1\ntype=pdf\n.\n\n\n\n\n\n\nmae\n \nMean of absolute errors\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: mean(abs(response - truth))\n\n\n\n\n\n\nmape\n \nMean absolute percentage error\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as the abs(truth_i - response_i) / truth_i. Won't work if any truth value is equal to zero. In this case the output will be NA.\n\n\n\n\n\n\nmedae\n \nMedian of absolute errors\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: median(abs(response - truth)).\n\n\n\n\n\n\nmedse\n \nMedian of squared errors\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: median((response - truth)^2).\n\n\n\n\n\n\nmse\n \nMean of squared errors\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: mean((response - truth)^2)\n\n\n\n\n\n\nmsle\n \nMean squared logarithmic error\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: mean((log(response + 1, exp(1)) - log(truth + 1, exp(1)))^2).   This measure is mostly used for count data, note that all predicted and actual target values must be greater or equal '-1'   to compute the measure.\n\n\n\n\n\n\nrae\n \nRelative absolute error\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as sum_of_absolute_errors / mean_absolute_deviation. Undefined for single instances and when every truth value is identical. In this case the output will be NA.\n\n\n\n\n\n\nrmse\n \nRoot mean squared error\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.rmse\n\n\nThe RMSE is aggregated as sqrt(mean(rmse.vals.on.test.sets^2)). If you don't want that, you could also use \ntest.mean\n.\n\n\n\n\n\n\nrmsle\n \nRoot mean squared logarithmic error\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: sqrt(msle). Definition taken from:   Definition taken from: https: / /www.kaggle.com / wiki / RootMeanSquaredLogarithmicError.   This measure is mostly used for count data, note that all predicted and actual target values   must be greater or equal '-1' to compute the measure.\n\n\n\n\n\n\nrrse\n \nRoot relative squared error\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as sqrt (sum_of_squared_errors / total_sum_of_squares). Undefined for single instances and when every truth value is identical. In this case the output will be NA.\n\n\n\n\n\n\nrsq\n \nCoefficient of determination\n\n\n\n\n1\n\n\n-Inf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nAlso called R-squared, which is 1 - residual_sum_of_squares / total_sum_of_squares.\n\n\n\n\n\n\nsae\n \nSum of absolute errors\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: sum(abs(response - truth))\n\n\n\n\n\n\nspearmanrho\n \nSpearman's rho\n\n\n\n\n1\n\n\n-1\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: Spearman's rho correlation between truth and response. Only looks at the order.   See Rosset et al.: \nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.1398\nrep=rep1\ntype=pdf\n.\n\n\n\n\n\n\nsse\n \nSum of squared errors\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nDefined as: sum((response - truth)^2)\n\n\n\n\n\n\n\n\nSurvival analysis\n\n\n\n\n\n\n\n\nID / Name\n\n\nMinim.\n\n\nBest\n\n\nWorst\n\n\nPred.\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggr.\n\n\nNote\n\n\n\n\n\n\n\n\n\n\ncindex\n \nHarrell's Concordance index\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nFraction of all pairs of subjects whose predicted survival times are correctly ordered among all subjects that can actually be ordered. In other words, it is the probability of concordance between the predicted and the observed survival.\n\n\n\n\n\n\ncindex.uno\n \nUno's Concordance index\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\ntest.mean\n\n\nFraction of all pairs of subjects whose predicted survival times are correctly ordered among all subjects that can actually be ordered. In other words, it is the probability of concordance between the predicted and the observed survival. Corrected by weighting with IPCW as suggested by Uno. Implemented in survAUC::UnoC.\n\n\n\n\n\n\niauc.uno\n \nUno's estimator of cumulative AUC for right censored time-to-event data\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\ntest.mean\n\n\nTo set an upper time limit, set argument max.time (defaults to max time in complete task). Implemented in survAUC::AUC.uno.\n\n\n\n\n\n\nibrier\n \nIntegrated brier score using Kaplan-Meier estimator for weighting\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\ntest.mean\n\n\nTo set an upper time limit, set argument max.time (defaults to max time in test data). Implemented in pec::pec\n\n\n\n\n\n\n\n\nCluster analysis\n\n\n\n\n\n\n\n\nID / Name\n\n\nMinim.\n\n\nBest\n\n\nWorst\n\n\nPred.\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggr.\n\n\nNote\n\n\n\n\n\n\n\n\n\n\ndb\n \nDavies-Bouldin cluster separation measure\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\nRatio of the within cluster scatter, to the between cluster separation, averaged over the clusters. See \n?clusterSim::index.DB\n.\n\n\n\n\n\n\ndunn\n \nDunn index\n\n\n\n\nInf\n\n\n0\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\nDefined as the ratio of the smallest distance between observations not in the same cluster to the largest intra-cluster distance. See \n?clValid::dunn\n.\n\n\n\n\n\n\nG1\n \nCalinski-Harabasz pseudo F statistic\n\n\n\n\nInf\n\n\n0\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\nDefined as ratio of between-cluster variance to within cluster variance. See \n?clusterSim::index.G1\n.\n\n\n\n\n\n\nG2\n \nBaker and Hubert adaptation of Goodman-Kruskal's gamma statistic\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\nDefined as: (number of concordant comparisons - number of discordant comparisons) / (number of concordant comparisons + number of discordant comparisons). See \n?clusterSim::index.G2\n.\n\n\n\n\n\n\nsilhouette\n \nRousseeuw's silhouette internal cluster quality index\n\n\n\n\nInf\n\n\n0\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\nSilhouette value of an observation is a measure of how similar an object is to its own cluster compared to other clusters. The measure is calculated as the average of all silhouette values. See \n?clusterSim::index.S\n.\n\n\n\n\n\n\n\n\nCost-sensitive classification\n\n\n\n\n\n\n\n\nID / Name\n\n\nMinim.\n\n\nBest\n\n\nWorst\n\n\nPred.\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggr.\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nmcp\n \nMisclassification penalty\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\n\n\ntest.mean\n\n\nAverage difference between costs of oracle and model prediction.\n\n\n\n\n\n\nmeancosts\n \nMean costs of the predicted choices\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\n\n\ntest.mean\n\n\nDefined as: mean(y), where y is the vector of costs for the predicted classes.\n\n\n\n\n\n\n\n\nNote that in case of \nordinary misclassification costs\n you can also generate performance\nmeasures from cost matrices by function \nmakeCostMeasure\n.\nFor details see the tutorial page on \ncost-sensitive classification\n\nand also the page on \ncustom performance measures\n.\n\n\nMultilabel classification\n\n\n\n\n\n\n\n\nID / Name\n\n\nMinim.\n\n\nBest\n\n\nWorst\n\n\nPred.\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggr.\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nmultilabel.acc\n \nAccuracy (multilabel)\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nAveraged proportion of correctly predicted labels with respect to the total number of labels for each instance,   following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.   Fractions where the denominator becomes 0 are replaced with 1 before computing the average across all instances.\n\n\n\n\n\n\nmultilabel.f1\n \nF1 measure (multilabel)\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nHarmonic mean of precision and recall on a per instance basis (Micro-F1), following the   definition by Montanes et al.: http: / /www.sciencedirect.com / science / article / pii / S0031320313004019.   Fractions where the denominator becomes 0 are replaced with 1 before computing the average across all instances.\n\n\n\n\n\n\nmultilabel.hamloss\n \nHamming loss\n\n\nX\n\n\n0\n\n\n1\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nProportion of labels that are predicted incorrectly, following the definition   by Charte and Charte: \nhttps://journal.r-project.org/archive/2015-2/charte-charte.pdf\n.\n\n\n\n\n\n\nmultilabel.ppv\n \nPositive predictive value (multilabel)\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nAlso called precision. Averaged ratio of correctly predicted labels for each instance,   following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.   Fractions where the denominator becomes 0 are ignored in the average calculation.\n\n\n\n\n\n\nmultilabel.subset01\n \nSubset-0-1 loss\n\n\nX\n\n\n0\n\n\n1\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nProportion of observations where the complete multilabel set (all 0-1-labels) is predicted incorrectly,   following the definition by Charte and Charte: \nhttps://journal.r-project.org/archive/2015-2/charte-charte.pdf\n.\n\n\n\n\n\n\nmultilabel.tpr\n \nTPR (multilabel)\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\nAlso called recall. Averaged proportion of predicted labels which are relevant for each instance,   following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.   Fractions where the denominator becomes 0 are ignored in the average calculation.\n\n\n\n\n\n\n\n\nGeneral performance measures\n\n\n\n\n\n\n\n\nID / Name\n\n\nMinim.\n\n\nBest\n\n\nWorst\n\n\nPred.\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggr.\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nfeatperc\n \nPercentage of original features used for model\n\n\nX\n\n\n0\n\n\n1\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\ntest.mean\n\n\nUseful for feature selection.\n\n\n\n\n\n\ntimeboth\n \ntimetrain + timepredict\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\n\n\ntimepredict\n \nTime of predicting test set\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\n\n\ntimetrain\n \nTime of fitting the model\n\n\nX\n\n\n0\n\n\nInf\n\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\ntest.mean", 
            "title": "Implemented Performance Measures"
        }, 
        {
            "location": "/measures/index.html#implemented-performance-measures", 
            "text": "This page shows the performance measures available for the different types of\nlearning problems as well as general performance measures in alphabetical order.\n(See also the documentation about  measures  and  makeMeasure  for available measures and\ntheir properties.)  If you find that a measure is missing, you can either  open an issue \nor try  to implement a measure yourself .  Column  Minim.  indicates if the measure is minimized during, e.g., tuning or\nfeature selection. Best  and  Worst  show the best and worst values the performance measure can attain.\nFor  classification , column  Multi  indicates if a measure is suitable for\nmulti-class problems. If not, the measure can only be used for binary classification problems.  The next six columns refer to information required to calculate the performance measure.   Pred. : The  Prediction  object.  Truth : The true values of the response variable(s) (for supervised learning).  Probs : The predicted probabilities (might be needed for classification).  Model : The  WrappedModel  (e.g., for calculating the training time).  Task : The  Task  (relevant for cost-sensitive classification).  Feats : The predicted data (relevant for clustering).   Aggr.  shows the default  aggregation method  tied to the measure.", 
            "title": "Implemented Performance Measures"
        }, 
        {
            "location": "/measures/index.html#classification", 
            "text": "ID / Name  Minim.  Best  Worst  Multi  Pred.  Truth  Probs  Model  Task  Feats  Aggr.  Note      acc   Accuracy   1  0  X  X  X      test.mean  Defined as: mean(response == truth)    auc   Area under the curve   1  0   X  X  X     test.mean  Integral over the graph that results from computing fpr and tpr for many different thresholds.    bac   Balanced accuracy   1  0   X  X      test.mean  Mean of true positive rate and true negative rate.    ber   Balanced error rate  X  0  1  X  X  X      test.mean  Mean of misclassification error rates on all individual classes.    brier   Brier score  X  0  1   X  X  X     test.mean  The Brier score is defined as the quadratic difference between the probability and the value (1,0) for the class.   That means we use the numeric representation 1 and 0 for our target classes. It is similiar to the mean squared error in regression.   multiclass.brier is the sum over all one vs. all comparisons and for a binary classifcation 2 * brier.    brier.scaled   Brier scaled   1  0   X  X  X     test.mean  Brier score scaled to [0,1], see  http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/ .    f1   F1 measure   1  0   X  X      test.mean  Defined as: 2 * tp/ (sum(truth == positive) + sum(response == positive))    fdr   False discovery rate  X  0  1   X  X      test.mean  Defined as: fp / (tp + fp).    fn   False negatives  X  0  Inf   X  X      test.mean  Sum of misclassified observations in the negative class. Also called misses.    fnr   False negative rate  X  0  1   X  X      test.mean  Percentage of misclassified observations in the negative class.    fp   False positives  X  0  Inf   X  X      test.mean  Sum of misclassified observations in the positive class. Also called false alarms.    fpr   False positive rate  X  0  1   X  X      test.mean  Percentage of misclassified observations in the positive class. Also called false alarm rate or fall-out.    gmean   G-mean   1  0   X  X      test.mean  Geometric mean of recall and specificity.    gpr   Geometric mean of precision and recall.   1  0   X  X      test.mean  Defined as: sqrt(ppv * tpr)    kappa   Cohen's kappa   1  -1  X  X  X      test.mean  Defined as: 1 - (1 - p0) / (1 - pe). With: p0 = 'observed frequency of     agreement' and pe = 'expected agremeent frequency under independence    logloss   Logarithmic loss  X  0  Inf  X   X  X     test.mean  Defined as: -mean(log(p_i)), where p_i is the predicted probability of the true class of observation i. Inspired by  https://www.kaggle.com/wiki/MultiClassLogLoss .    lsr   Logarithmic Scoring Rule   0  -Inf  X   X  X     test.mean  Defined as: mean(log(p_i)), where p_i is the predicted probability of the true class of observation i.   This scoring rule is the same as the negative logloss, self-information or surprisal.   See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2), 49-65.    mcc   Matthews correlation coefficient   1  -1   X  X      test.mean  Defined as (tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)), denominator set to 1 if 0    mmce   Mean misclassification error  X  0  1  X  X  X      test.mean  Defined as: mean(response != truth)    multiclass.au1p   Weighted average 1 vs. 1 multiclass AUC   1  0.5  X  X  X  X     test.mean  Computes AUC of c(c - 1) binary classifiers while considering the a priori distribution of the classes. See Ferri et al.:  https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf .    multiclass.au1u   Average 1 vs. 1 multiclass AUC   1  0.5  X  X  X  X     test.mean  Computes AUC of c(c - 1) binary classifiers (all possible pairwise combinations) while considering uniform distribution of the classes. See Ferri et al.:  https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf .    multiclass.aunp   Weighted average 1 vs. rest multiclass AUC   1  0.5  X  X  X  X     test.mean  Computes the AUC treating a c-dimensional classifier as c two-dimensional classifiers, taking into account the prior probability of each class. See Ferri et al.:  https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf .    multiclass.aunu   Average 1 vs. rest multiclass AUC   1  0.5  X  X  X  X     test.mean  Computes the AUC treating a c-dimensional classifier as c two-dimensional classifiers, where classes are assumed to have uniform distribution, in order to have a measure which is independent of class distribution change. See Ferri et al.:  https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf .    multiclass.brier   Multiclass Brier score  X  0  2  X  X  X  X     test.mean  Defined as: (1/n) sum_i sum_j (y_ij - p_ij)^2, where y_ij = 1 if observation i has class j (else 0), and p_ij is the predicted probability of observation i for class j. From  http://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf .    npv   Negative predictive value   1  0   X  X      test.mean  Defined as: tn / (tn + fn).    ppv   Positive predictive value   1  0   X  X      test.mean  Defined as: tp / (tp + fp). Also called precision. If the denominator is 0, PPV is set to be either 1 or 0 depending on whether the highest probability prediction is positive (1) or negative (0).    qsr   Quadratic Scoring Rule   1  -1  X   X  X     test.mean  Defined as: 1 - (1/n) sum_i sum_j (y_ij - p_ij)^2, where y_ij = 1 if observation i has class j (else 0), and p_ij is the predicted probablity of observation i for class j.   This scoring rule is the same as 1 - multiclass.brier.   See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2), 49-65.    ssr   Spherical Scoring Rule   1  0  X   X  X     test.mean  Defined as: mean(p_i(sum_j(p_ij))), where p_i is the predicted probability of the true class of observation i and p_ij is the predicted probablity of observation i for class j.   See: Bickel, J. E. (2007). Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2), 49-65.    tn   True negatives   Inf  0   X  X      test.mean  Sum of correctly classified observations in the negative class. Also called correct rejections.    tnr   True negative rate   1  0   X  X      test.mean  Percentage of correctly classified observations in the negative class. Also called specificity.    tp   True positives   Inf  0   X  X      test.mean  Sum of all correctly classified observations in the positive class.    tpr   True positive rate   1  0   X  X      test.mean  Percentage of correctly classified observations in the positive class. Also called hit rate or recall or sensitivity.    wkappa   Mean quadratic weighted kappa   1  -1  X  X  X      test.mean  Defined as: 1 - sum(weights * conf.mat) / sum(weights * expected.mat),     the weight matrix measures seriousness of disagreement with the squared euclidean metric.", 
            "title": "Classification"
        }, 
        {
            "location": "/measures/index.html#regression", 
            "text": "ID / Name  Minim.  Best  Worst  Pred.  Truth  Probs  Model  Task  Feats  Aggr.  Note      arsq   Adjusted coefficient of determination   1  0  X  X      test.mean  Defined as: 1 - (1 - rsq) * (p / (n - p - 1L)). Adjusted R-squared is only defined for normal linear regression.    expvar   Explained variance   1  0  X  X      test.mean  Similar to measure rsq (R-squared). Defined as explained_sum_of_squares / total_sum_of_squares.    kendalltau   Kendall's tau   1  -1  X  X      test.mean  Defined as: Kendall's tau correlation between truth and response. Only looks at the order.   See Rosset et al.:  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.1398 rep=rep1 type=pdf .    mae   Mean of absolute errors  X  0  Inf  X  X      test.mean  Defined as: mean(abs(response - truth))    mape   Mean absolute percentage error  X  0  Inf  X  X      test.mean  Defined as the abs(truth_i - response_i) / truth_i. Won't work if any truth value is equal to zero. In this case the output will be NA.    medae   Median of absolute errors  X  0  Inf  X  X      test.mean  Defined as: median(abs(response - truth)).    medse   Median of squared errors  X  0  Inf  X  X      test.mean  Defined as: median((response - truth)^2).    mse   Mean of squared errors  X  0  Inf  X  X      test.mean  Defined as: mean((response - truth)^2)    msle   Mean squared logarithmic error  X  0  Inf  X  X      test.mean  Defined as: mean((log(response + 1, exp(1)) - log(truth + 1, exp(1)))^2).   This measure is mostly used for count data, note that all predicted and actual target values must be greater or equal '-1'   to compute the measure.    rae   Relative absolute error  X  0  Inf  X  X      test.mean  Defined as sum_of_absolute_errors / mean_absolute_deviation. Undefined for single instances and when every truth value is identical. In this case the output will be NA.    rmse   Root mean squared error  X  0  Inf  X  X      test.rmse  The RMSE is aggregated as sqrt(mean(rmse.vals.on.test.sets^2)). If you don't want that, you could also use  test.mean .    rmsle   Root mean squared logarithmic error  X  0  Inf  X  X      test.mean  Defined as: sqrt(msle). Definition taken from:   Definition taken from: https: / /www.kaggle.com / wiki / RootMeanSquaredLogarithmicError.   This measure is mostly used for count data, note that all predicted and actual target values   must be greater or equal '-1' to compute the measure.    rrse   Root relative squared error  X  0  Inf  X  X      test.mean  Defined as sqrt (sum_of_squared_errors / total_sum_of_squares). Undefined for single instances and when every truth value is identical. In this case the output will be NA.    rsq   Coefficient of determination   1  -Inf  X  X      test.mean  Also called R-squared, which is 1 - residual_sum_of_squares / total_sum_of_squares.    sae   Sum of absolute errors  X  0  Inf  X  X      test.mean  Defined as: sum(abs(response - truth))    spearmanrho   Spearman's rho   1  -1  X  X      test.mean  Defined as: Spearman's rho correlation between truth and response. Only looks at the order.   See Rosset et al.:  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.1398 rep=rep1 type=pdf .    sse   Sum of squared errors  X  0  Inf  X  X      test.mean  Defined as: sum((response - truth)^2)", 
            "title": "Regression"
        }, 
        {
            "location": "/measures/index.html#survival-analysis", 
            "text": "ID / Name  Minim.  Best  Worst  Pred.  Truth  Probs  Model  Task  Feats  Aggr.  Note      cindex   Harrell's Concordance index   1  0  X  X      test.mean  Fraction of all pairs of subjects whose predicted survival times are correctly ordered among all subjects that can actually be ordered. In other words, it is the probability of concordance between the predicted and the observed survival.    cindex.uno   Uno's Concordance index   1  0  X  X   X    test.mean  Fraction of all pairs of subjects whose predicted survival times are correctly ordered among all subjects that can actually be ordered. In other words, it is the probability of concordance between the predicted and the observed survival. Corrected by weighting with IPCW as suggested by Uno. Implemented in survAUC::UnoC.    iauc.uno   Uno's estimator of cumulative AUC for right censored time-to-event data   1  0  X  X   X  X   test.mean  To set an upper time limit, set argument max.time (defaults to max time in complete task). Implemented in survAUC::AUC.uno.    ibrier   Integrated brier score using Kaplan-Meier estimator for weighting  X  0  1   X   X  X   test.mean  To set an upper time limit, set argument max.time (defaults to max time in test data). Implemented in pec::pec", 
            "title": "Survival analysis"
        }, 
        {
            "location": "/measures/index.html#cluster-analysis", 
            "text": "ID / Name  Minim.  Best  Worst  Pred.  Truth  Probs  Model  Task  Feats  Aggr.  Note      db   Davies-Bouldin cluster separation measure  X  0  Inf  X      X  test.mean  Ratio of the within cluster scatter, to the between cluster separation, averaged over the clusters. See  ?clusterSim::index.DB .    dunn   Dunn index   Inf  0  X      X  test.mean  Defined as the ratio of the smallest distance between observations not in the same cluster to the largest intra-cluster distance. See  ?clValid::dunn .    G1   Calinski-Harabasz pseudo F statistic   Inf  0  X      X  test.mean  Defined as ratio of between-cluster variance to within cluster variance. See  ?clusterSim::index.G1 .    G2   Baker and Hubert adaptation of Goodman-Kruskal's gamma statistic   1  0  X      X  test.mean  Defined as: (number of concordant comparisons - number of discordant comparisons) / (number of concordant comparisons + number of discordant comparisons). See  ?clusterSim::index.G2 .    silhouette   Rousseeuw's silhouette internal cluster quality index   Inf  0  X      X  test.mean  Silhouette value of an observation is a measure of how similar an object is to its own cluster compared to other clusters. The measure is calculated as the average of all silhouette values. See  ?clusterSim::index.S .", 
            "title": "Cluster analysis"
        }, 
        {
            "location": "/measures/index.html#cost-sensitive-classification", 
            "text": "ID / Name  Minim.  Best  Worst  Pred.  Truth  Probs  Model  Task  Feats  Aggr.  Note      mcp   Misclassification penalty  X  0  Inf  X     X   test.mean  Average difference between costs of oracle and model prediction.    meancosts   Mean costs of the predicted choices  X  0  Inf  X     X   test.mean  Defined as: mean(y), where y is the vector of costs for the predicted classes.     Note that in case of  ordinary misclassification costs  you can also generate performance\nmeasures from cost matrices by function  makeCostMeasure .\nFor details see the tutorial page on  cost-sensitive classification \nand also the page on  custom performance measures .", 
            "title": "Cost-sensitive classification"
        }, 
        {
            "location": "/measures/index.html#multilabel-classification", 
            "text": "ID / Name  Minim.  Best  Worst  Pred.  Truth  Probs  Model  Task  Feats  Aggr.  Note      multilabel.acc   Accuracy (multilabel)   1  0  X  X      test.mean  Averaged proportion of correctly predicted labels with respect to the total number of labels for each instance,   following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.   Fractions where the denominator becomes 0 are replaced with 1 before computing the average across all instances.    multilabel.f1   F1 measure (multilabel)   1  0  X  X      test.mean  Harmonic mean of precision and recall on a per instance basis (Micro-F1), following the   definition by Montanes et al.: http: / /www.sciencedirect.com / science / article / pii / S0031320313004019.   Fractions where the denominator becomes 0 are replaced with 1 before computing the average across all instances.    multilabel.hamloss   Hamming loss  X  0  1  X  X      test.mean  Proportion of labels that are predicted incorrectly, following the definition   by Charte and Charte:  https://journal.r-project.org/archive/2015-2/charte-charte.pdf .    multilabel.ppv   Positive predictive value (multilabel)   1  0  X  X      test.mean  Also called precision. Averaged ratio of correctly predicted labels for each instance,   following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.   Fractions where the denominator becomes 0 are ignored in the average calculation.    multilabel.subset01   Subset-0-1 loss  X  0  1  X  X      test.mean  Proportion of observations where the complete multilabel set (all 0-1-labels) is predicted incorrectly,   following the definition by Charte and Charte:  https://journal.r-project.org/archive/2015-2/charte-charte.pdf .    multilabel.tpr   TPR (multilabel)   1  0  X  X      test.mean  Also called recall. Averaged proportion of predicted labels which are relevant for each instance,   following the definition by Charte and Charte: https: / /journal.r-project.org / archive / 2015 - 2 / charte-charte.pdf.   Fractions where the denominator becomes 0 are ignored in the average calculation.", 
            "title": "Multilabel classification"
        }, 
        {
            "location": "/measures/index.html#general-performance-measures", 
            "text": "ID / Name  Minim.  Best  Worst  Pred.  Truth  Probs  Model  Task  Feats  Aggr.  Note      featperc   Percentage of original features used for model  X  0  1  X    X    test.mean  Useful for feature selection.    timeboth   timetrain + timepredict  X  0  Inf  X    X    test.mean     timepredict   Time of predicting test set  X  0  Inf  X       test.mean     timetrain   Time of fitting the model  X  0  Inf     X    test.mean", 
            "title": "General performance measures"
        }, 
        {
            "location": "/filter_methods/index.html", 
            "text": "Integrated Filter Methods\n\n\nThe following table shows the available methods for calculating the feature importance.\nColumns \nClassif\n, \nRegr\n and \nSurv\n indicate if classification, regression or survival\nanalysis problems are supported.\nColumns \nFac.\n, \nNum.\n and \nOrd.\n show if a particular method can deal with\n\nfactor\n, \nnumeric\n and \nordered factor\n\nfeatures.\n\n\nCurrent methods\n\n\n\n\n\n\n\n\nMethod\n\n\nPackage\n\n\nDescription\n\n\nClassif\n\n\nRegr\n\n\nSurv\n\n\nFac.\n\n\nNum.\n\n\nOrd.\n\n\n\n\n\n\n\n\n\n\nanova.test\n\n\nRfast\n\n\nANOVA Test for binary and multiclass classification tasks\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\nauc\n\n\n\n\nAUC filter for binary classification tasks\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\ncarscore\n\n\ncare\n\n\nCAR scores\n\n\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\ncforest.importance\n\n\nparty\n\n\nPermutation importance of random forest fitted in package 'party'\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nchi.squared\n\n\nFSelector\n\n\nChi-squared statistic of independence between feature and target\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ngain.ratio\n\n\nFSelector\n\n\nEntropy-based gain ratio between feature and target\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ninformation.gain\n\n\nFSelector\n\n\nEntropy-based information gain between feature and target\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nkruskal.test\n\n\n\n\nKruskal Test for binary and multiclass classification tasks\n\n\nX\n\n\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nlinear.correlation\n\n\nRfast\n\n\nPearson correlation between feature and target\n\n\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\nmrmr\n\n\nmRMRe\n\n\nMinimum redundancy, maximum relevance filter\n\n\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\noneR\n\n\nFSelector\n\n\noneR association rule\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\npermutation.importance\n\n\n\n\nAggregated difference between feature permuted and unpermuted predictions\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nrandomForest.importance\n\n\nrandomForest\n\n\nImportance based on OOB-accuracy or node inpurity of random forest fitted in package 'randomForest'.\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nrandomForestSRC.rfsrc\n\n\nrandomForestSRC\n\n\nImportance of random forests fitted in package 'randomForestSRC'. Importance is calculated using argument 'permute'.\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nrandomForestSRC.var.select\n\n\nrandomForestSRC\n\n\nMinimal depth of / variable hunting via method var.select on random forests fitted in package 'randomForestSRC'.\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nrank.correlation\n\n\nRfast\n\n\nSpearman's correlation between feature and target\n\n\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\nrelief\n\n\nFSelector\n\n\nRELIEF algorithm\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nsymmetrical.uncertainty\n\n\nFSelector\n\n\nEntropy-based symmetrical uncertainty between feature and target\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nunivariate.model.score\n\n\n\n\nResamples an mlr learner for each input feature individually. The resampling performance is used as filter score, with rpart as default learner.\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nvariance\n\n\n\n\nA simple variance filter\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\nDeprecated methods\n\n\n\n\n\n\n\n\nMethod\n\n\nPackage\n\n\nDescription\n\n\nClassif\n\n\nRegr\n\n\nSurv\n\n\nFac.\n\n\nNum.\n\n\nOrd.\n\n\n\n\n\n\n\n\n\n\nrf.importance\n\n\nrandomForestSRC\n\n\nImportance of random forests fitted in package 'randomForestSRC'. Importance is calculated using argument 'permute'. (DEPRECATED)\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nrf.min.depth\n\n\nrandomForestSRC\n\n\nMinimal depth of random forest fitted in package 'randomForestSRC. (DEPRECATED)\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nunivariate\n\n\n\n\nResamples an mlr learner for each input feature individually. The resampling performance is used as filter score, with rpart as default learner. (DEPRECATED)\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX", 
            "title": "Integrated Filter Methods"
        }, 
        {
            "location": "/filter_methods/index.html#integrated-filter-methods", 
            "text": "The following table shows the available methods for calculating the feature importance.\nColumns  Classif ,  Regr  and  Surv  indicate if classification, regression or survival\nanalysis problems are supported.\nColumns  Fac. ,  Num.  and  Ord.  show if a particular method can deal with factor ,  numeric  and  ordered factor \nfeatures.", 
            "title": "Integrated Filter Methods"
        }, 
        {
            "location": "/filter_methods/index.html#current-methods", 
            "text": "Method  Package  Description  Classif  Regr  Surv  Fac.  Num.  Ord.      anova.test  Rfast  ANOVA Test for binary and multiclass classification tasks  X     X     auc   AUC filter for binary classification tasks  X     X     carscore  care  CAR scores   X    X     cforest.importance  party  Permutation importance of random forest fitted in package 'party'  X  X  X  X  X  X    chi.squared  FSelector  Chi-squared statistic of independence between feature and target  X  X   X  X     gain.ratio  FSelector  Entropy-based gain ratio between feature and target  X  X   X  X     information.gain  FSelector  Entropy-based information gain between feature and target  X  X   X  X     kruskal.test   Kruskal Test for binary and multiclass classification tasks  X    X  X     linear.correlation  Rfast  Pearson correlation between feature and target   X    X     mrmr  mRMRe  Minimum redundancy, maximum relevance filter   X  X   X  X    oneR  FSelector  oneR association rule  X  X   X  X     permutation.importance   Aggregated difference between feature permuted and unpermuted predictions  X  X  X  X  X  X    randomForest.importance  randomForest  Importance based on OOB-accuracy or node inpurity of random forest fitted in package 'randomForest'.  X  X   X  X     randomForestSRC.rfsrc  randomForestSRC  Importance of random forests fitted in package 'randomForestSRC'. Importance is calculated using argument 'permute'.  X  X  X  X  X  X    randomForestSRC.var.select  randomForestSRC  Minimal depth of / variable hunting via method var.select on random forests fitted in package 'randomForestSRC'.  X  X  X  X  X  X    rank.correlation  Rfast  Spearman's correlation between feature and target   X    X     relief  FSelector  RELIEF algorithm  X  X   X  X     symmetrical.uncertainty  FSelector  Entropy-based symmetrical uncertainty between feature and target  X  X   X  X     univariate.model.score   Resamples an mlr learner for each input feature individually. The resampling performance is used as filter score, with rpart as default learner.  X  X  X  X  X  X    variance   A simple variance filter  X  X  X   X", 
            "title": "Current methods"
        }, 
        {
            "location": "/filter_methods/index.html#deprecated-methods", 
            "text": "Method  Package  Description  Classif  Regr  Surv  Fac.  Num.  Ord.      rf.importance  randomForestSRC  Importance of random forests fitted in package 'randomForestSRC'. Importance is calculated using argument 'permute'. (DEPRECATED)  X  X  X  X  X  X    rf.min.depth  randomForestSRC  Minimal depth of random forest fitted in package 'randomForestSRC. (DEPRECATED)  X  X  X  X  X  X    univariate   Resamples an mlr learner for each input feature individually. The resampling performance is used as filter score, with rpart as default learner. (DEPRECATED)  X  X  X  X  X  X", 
            "title": "Deprecated methods"
        }
    ]
}