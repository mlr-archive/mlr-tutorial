<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Imbalanced Classification Problems - mlr tutorial</title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">
        <link href="../css/custom_mlr.css" rel="stylesheet">
        <link href="../css/custom_highlight.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../js/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../index.html">mlr tutorial</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../index.html">Home</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Basics <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../task/index.html">Tasks</a>
</li>
                            
<li >
    <a href="../learner/index.html">Learners</a>
</li>
                            
<li >
    <a href="../train/index.html">Train</a>
</li>
                            
<li >
    <a href="../predict/index.html">Predict</a>
</li>
                            
<li >
    <a href="../performance/index.html">Performance</a>
</li>
                            
<li >
    <a href="../resample/index.html">Resampling</a>
</li>
                            
<li >
    <a href="../tune/index.html">Tuning</a>
</li>
                            
<li >
    <a href="../benchmark_experiments/index.html">Benchmark Experiments</a>
</li>
                            
<li >
    <a href="../parallelization/index.html">Parallelization</a>
</li>
                            
<li >
    <a href="../visualization/index.html">Visualization</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Advanced <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../configureMlr/index.html">Configuration</a>
</li>
                            
<li >
    <a href="../wrapper/index.html">Wrapped Learners</a>
</li>
                            
<li >
    <a href="../preproc/index.html">Preprocessing</a>
</li>
                            
<li >
    <a href="../impute/index.html">Imputation</a>
</li>
                            
<li >
    <a href="../bagging/index.html">Bagging</a>
</li>
                            
<li >
    <a href="../advanced_tune/index.html">Advanced Tuning</a>
</li>
                            
<li >
    <a href="../feature_selection/index.html">Feature Selection</a>
</li>
                            
<li >
    <a href="../nested_resampling/index.html">Nested Resampling</a>
</li>
                            
<li >
    <a href="../cost_sensitive_classif/index.html">Cost-Sensitive Classification</a>
</li>
                            
<li class="active">
    <a href="index.html">Imbalanced Classification Problems</a>
</li>
                            
<li >
    <a href="../roc_analysis/index.html">ROC Analysis</a>
</li>
                            
<li >
    <a href="../multilabel/index.html">Multilabel Classification</a>
</li>
                            
<li >
    <a href="../learning_curve/index.html">Learning Curves</a>
</li>
                            
<li >
    <a href="../partial_dependence/index.html">Partial Dependence Plots</a>
</li>
                            
<li >
    <a href="../classifier_calibration/index.html">Classifier Calibration Plots</a>
</li>
                            
<li >
    <a href="../hyperpar_tuning_effects/index.html">Hyperparameter Tuning Effects</a>
</li>
                            
<li >
    <a href="../out_of_bag_predictions/index.html">Out-of-Bag Predictions</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Extend <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../create_learner/index.html">Create Custom Learners</a>
</li>
                            
<li >
    <a href="../create_measure/index.html">Create Custom Measures</a>
</li>
                            
<li >
    <a href="../create_imputation/index.html">Create Imputation Methods</a>
</li>
                            
<li >
    <a href="../create_filter/index.html">Create Custom Filters</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Appendix <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../example_tasks/index.html">Example Tasks</a>
</li>
                            
<li >
    <a href="../integrated_learners/index.html">Integrated Learners</a>
</li>
                            
<li >
    <a href="../measures/index.html">Implemented Performance Measures</a>
</li>
                            
<li >
    <a href="../filter_methods/index.html">Integrated Filter Methods</a>
</li>
                        </ul>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../cost_sensitive_classif/index.html">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../roc_analysis/index.html">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/mlr-org/mlr/edit/master/docs/over_and_undersampling.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#imbalanced-classification-problems">Imbalanced Classification Problems</a></li>
            <li><a href="#sampling-based-approaches">Sampling-based approaches</a></li>
            <li><a href="#cost-based-approaches">Cost-based approaches</a></li>
            <li><a href="#complete-code-listing">Complete code listing</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="imbalanced-classification-problems">Imbalanced Classification Problems</h1>
<p>In case of <em>binary classification</em> strongly imbalanced classes
often lead to unsatisfactory results regarding the prediction of new
observations, especially for the small class.
In this context <em>imbalanced classes</em> simply means that the number of
observations of one class (usu. positive or majority class) by far exceeds
the number of observations of the other class (usu. negative or minority class).
This setting can be observed fairly often in practice and in various disciplines
like credit scoring, fraud detection, medical diagnostics or churn management.</p>
<p>Most classification methods work best when the number of observations per
class are roughly equal. The problem with <em>imbalanced classes</em> is that because
of the dominance of the majority class classifiers tend to ignore cases of
the minority class as noise and therefore predict the majority class far more
often. In order to lay more weight on the cases of the minority class, there are
numerous correction methods which tackle the <em>imbalanced classification problem</em>.
These methods can generally be divided into <em>cost- and sampling-based approaches</em>.
Below all methods supported by <a href="http://www.rdocumentation.org/packages/mlr/">mlr</a> are introduced.</p>
<h2 id="sampling-based-approaches">Sampling-based approaches</h2>
<p>The basic idea of <em>sampling methods</em> is to simply adjust the proportion of
the classes in order to increase the weight of the minority class observations
within the model.</p>
<p>The <em>sampling-based approaches</em> can be divided further into three different categories:</p>
<ol>
<li>
<p><strong>Undersampling methods</strong>:
   Elimination of randomly chosen cases of the majority class to decrease their
   effect on the classifier. All cases of the minority class are kept.</p>
</li>
<li>
<p><strong>Oversampling methods</strong>:
   Generation of additional cases (copies, artificial observations) of the minority
   class to increase their effect on the classifier. All cases of the majority
   class are kept.</p>
</li>
<li>
<p><strong>Hybrid methods</strong>:
   Mixture of under- and oversampling strategies.</p>
</li>
</ol>
<p>All these methods directly access the underlying data and "rearrange" it.
In this way the sampling is done as part of the <em>preprocesssing</em> and can therefore
be combined with every appropriate classifier.</p>
<p><a href="http://www.rdocumentation.org/packages/mlr/">mlr</a> currently supports the first two approaches.</p>
<h3 id="simple-over-and-undersampling">(Simple) over- and undersampling</h3>
<p>As mentioned above <em>undersampling</em> always refers to the majority class, while
<em>oversampling</em> affects the minority class. By the use of <em>undersampling</em>, randomly
chosen observations of the majority class are eliminated. Through (simple)
<em>oversampling</em> all observations of the minority class are considered at least
once when fitting the model. In addition, exact copies of minority class cases are created
by random sampling with repetitions.</p>
<p>First, let's take a look at the effect for a classification <a href="../task/index.html">task</a>.
Based on a simulated <a href="http://www.rdocumentation.org/packages/mlr/functions/Task.html">ClassifTask</a> with imbalanced classes two new
tasks (<code>task.over</code>, <code>task.under</code>) are created via <a href="http://www.rdocumentation.org/packages/mlr/">mlr</a> functions
<a href="http://www.rdocumentation.org/packages/mlr/functions/oversample.html">oversample</a> and <a href="http://www.rdocumentation.org/packages/mlr/functions/oversample.html">undersample</a>, respectively.</p>
<pre><code class="r">data.imbal.train = rbind(
  data.frame(x = rnorm(100, mean = 1), class = &quot;A&quot;),
  data.frame(x = rnorm(5000, mean = 2), class = &quot;B&quot;)
)
task = makeClassifTask(data = data.imbal.train, target = &quot;class&quot;)
task.over = oversample(task, rate = 8)
task.under = undersample(task, rate = 1/8)

table(getTaskTargets(task))
#&gt; 
#&gt;    A    B 
#&gt;  100 5000

table(getTaskTargets(task.over))
#&gt; 
#&gt;    A    B 
#&gt;  800 5000

table(getTaskTargets(task.under))
#&gt; 
#&gt;   A   B 
#&gt; 100 625
</code></pre>

<p>Please note that the <em>undersampling rate</em> has to be between 0 and 1, where 1 means
no undersampling and 0.5 implies a reduction of the majority class size to 50 percent.
Correspondingly, the <em>oversampling rate</em> must be greater or equal to 1,
where 1 means no oversampling and 2 would result in doubling the minority
class size.</p>
<p>As a result the <a href="../performance/index.html">performance</a> should improve if the model is applied to new data.</p>
<pre><code class="r">lrn = makeLearner(&quot;classif.rpart&quot;, predict.type = &quot;prob&quot;)
mod = train(lrn, task)
mod.over = train(lrn, task.over)
mod.under = train(lrn, task.under)
data.imbal.test = rbind(
  data.frame(x = rnorm(10, mean = 1), class = &quot;A&quot;),
  data.frame(x = rnorm(500, mean = 2), class = &quot;B&quot;)
)

performance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc))
#&gt;       mmce        ber        auc 
#&gt; 0.01960784 0.50000000 0.50000000

performance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc))
#&gt;       mmce        ber        auc 
#&gt; 0.04509804 0.41500000 0.58500000

performance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc))
#&gt;       mmce        ber        auc 
#&gt; 0.05098039 0.41800000 0.70550000
</code></pre>

<p>In this case the <em>performance measure</em> has to be considered very carefully.
As the <em>misclassification rate</em> (<a href="../measures/index.html">mmce</a>) evaluates the overall
accuracy of the predictions, the <em>balanced error rate</em> (<a href="../measures/index.html">ber</a>) and
<em>area under the ROC Curve</em> (<a href="../measures/index.html">auc</a>) 
might be more suitable here, as the misclassifications within each class
are separately taken into account.</p>
<h3 id="over-and-undersampling-wrappers">Over- and undersampling wrappers</h3>
<p>Alternatively, <a href="http://www.rdocumentation.org/packages/mlr/">mlr</a> also offers the integration of over- and undersampling
via a <a href="../wrapper/index.html">wrapper approach</a>. This way
over- and undersampling can be applied to already existing <a href="../learner/index.html">learners</a>
to extend their functionality. </p>
<p>The example given above is repeated once again, but this time with extended
learners instead of modified tasks (see <a href="http://www.rdocumentation.org/packages/mlr/functions/makeUndersampleWrapper.html">makeOversampleWrapper</a>
and <a href="http://www.rdocumentation.org/packages/mlr/functions/makeUndersampleWrapper.html">makeUndersampleWrapper</a>).
Just like before the <em>undersampling rate</em> has to be between 0 and 1, while the
<em>oversampling rate</em> has a lower boundary of 1.</p>
<pre><code class="r">lrn.over = makeOversampleWrapper(lrn, osw.rate = 8)
lrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8)
mod = train(lrn, task)
mod.over = train(lrn.over, task)
mod.under = train(lrn.under, task)

performance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc))
#&gt;       mmce        ber        auc 
#&gt; 0.01960784 0.50000000 0.50000000

performance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc))
#&gt;       mmce        ber        auc 
#&gt; 0.03333333 0.40900000 0.72020000

performance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc))
#&gt;       mmce        ber        auc 
#&gt; 0.04509804 0.41500000 0.71660000
</code></pre>

<h3 id="extensions-to-oversampling">Extensions to oversampling</h3>
<p>Two extensions to (simple) oversampling are available in <a href="http://www.rdocumentation.org/packages/mlr/">mlr</a>.</p>
<h4 id="1-smote-synthetic-minority-oversampling-technique">1. SMOTE (Synthetic Minority Oversampling Technique)</h4>
<p>As the duplicating of the minority class observations can lead to overfitting,
within <em>SMOTE</em> the "new cases" are constructed in a different way. For each
new observation, one randomly chosen minority class observation as well as
one of its <em>randomly chosen next neighbours</em> are interpolated, so that finally
a new <em>artificial observation</em> of the minority class is created.
The <a href="http://www.rdocumentation.org/packages/mlr/functions/smote.html">smote</a> function in <a href="http://www.rdocumentation.org/packages/mlr/">mlr</a> handles numeric as well as factor features, as
the gower distance is used for nearest neighbour calculation. The factor level
of the new artificial case is sampled from the given levels of the two
input observations.</p>
<p>Analogous to oversampling, <em>SMOTE preprocessing</em> is possible via modification
of the task.</p>
<pre><code class="r">task.smote = smote(task, rate = 8, nn = 5)
table(getTaskTargets(task))
#&gt; 
#&gt;    A    B 
#&gt;  100 5000

table(getTaskTargets(task.smote))
#&gt; 
#&gt;    A    B 
#&gt;  800 5000
</code></pre>

<p>Alternatively, a new wrapped learner can be created via <a href="http://www.rdocumentation.org/packages/mlr/functions/makeSMOTEWrapper.html">makeSMOTEWrapper</a>.</p>
<pre><code class="r">lrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5)
mod.smote = train(lrn.smote, task)
performance(predict(mod.smote, newdata = data.imbal.test), measures = list(mmce, ber, auc))
#&gt;       mmce        ber        auc 
#&gt; 0.04509804 0.41500000 0.71660000
</code></pre>

<p>By default the number of nearest neighbours considered within the algorithm is
set to 5.</p>
<h4 id="2-overbagging">2. Overbagging</h4>
<p>Another extension of oversampling consists in the combination of sampling with
the <a href="../bagging/index.html">bagging approach</a>. For each iteration of the bagging process,
minority class observations are oversampled with a given rate in <code>obw.rate</code>.
The majority class cases can either all be taken into account for each
iteration (<code>obw.maxcl = "all"</code>) or bootstrapped with replacement to increase
variability between training data sets during iterations (<code>obw.maxcl = "boot"</code>).</p>
<p>The construction of the <strong>Overbagging Wrapper</strong> works similar
to <a href="http://www.rdocumentation.org/packages/mlr/functions/makeBaggingWrapper.html">makeBaggingWrapper</a>.
First an existing <a href="http://www.rdocumentation.org/packages/mlr/">mlr</a> learner has to be passed to <a href="http://www.rdocumentation.org/packages/mlr/functions/makeOverBaggingWrapper.html">makeOverBaggingWrapper</a>.
The number of iterations or fitted models can be set via <code>obw.iters</code>.</p>
<pre><code class="r">lrn = makeLearner(&quot;classif.rpart&quot;, predict.type = &quot;response&quot;)
obw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)
</code></pre>

<p>For <em>binary classification</em> the prediction is based on majority voting to create
a discrete label. Corresponding probabilities are predicted by considering
the proportions of all the predicted labels.
Please note that the benefit of the sampling process is <em>highly dependent</em>
on the specific learner as shown in the following example.</p>
<p>First, let's take a look at the tree learner with and without overbagging:</p>
<pre><code class="r">lrn = setPredictType(lrn, &quot;prob&quot;)
rdesc = makeResampleDesc(&quot;CV&quot;, iters = 5)
r1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE,
  measures = list(mmce, ber, auc))
r1$aggr
#&gt; mmce.test.mean  ber.test.mean  auc.test.mean 
#&gt;     0.01960784     0.50000000     0.50000000

obw.lrn = setPredictType(obw.lrn, &quot;prob&quot;)
r2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,
  measures = list(mmce, ber, auc))
r2$aggr
#&gt; mmce.test.mean  ber.test.mean  auc.test.mean 
#&gt;     0.04470588     0.43611719     0.58535862
</code></pre>

<p>Now let's consider a <em>random forest</em> as initial learner:</p>
<pre><code class="r">lrn = makeLearner(&quot;classif.randomForest&quot;)
obw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)

lrn = setPredictType(lrn, &quot;prob&quot;)
r1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE,
  measures = list(mmce, ber, auc))
r1$aggr
#&gt; mmce.test.mean  ber.test.mean  auc.test.mean 
#&gt;     0.03509804     0.46089748     0.58514212

obw.lrn = setPredictType(obw.lrn, &quot;prob&quot;)
r2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,
  measures = list(mmce, ber, auc))
r2$aggr
#&gt; mmce.test.mean  ber.test.mean  auc.test.mean 
#&gt;     0.04098039     0.45961754     0.54926842
</code></pre>

<p>While <em>overbagging</em> slighty improves the performance of the <em>decision tree</em>,
the auc decreases in the second example when additional overbagging is applied.
As the <em>random forest</em> itself is already a strong learner (and a bagged one
as well), a further bagging step isn't very helpful here and usually won't
improve the model.</p>
<h2 id="cost-based-approaches">Cost-based approaches</h2>
<p>In contrast to sampling, <em>cost-based approaches</em> usually require particular 
learners, which can deal with different <em>class-dependent costs</em>
(<a href="../cost_sensitive_classif/index.html">Cost-Sensitive Classification</a>).</p>
<h3 id="weighted-classes-wrapper">Weighted classes wrapper</h3>
<p>Another approach independent of the underlying classifier is to 
assign the costs as <em>class weights</em>, so that each observation receives a weight,
depending on the class it belongs to. Similar to the sampling-based approaches,
the effect of the minority class observations is thereby increased simply by a
higher weight of these instances and vice versa for majority class observations.</p>
<p>In this way every learner which supports weights can be extended through
the <a href="../wrapper/index.html">wrapper approach</a>.
If the learner does not have a direct parameter for class weights,
but supports observation weights, the weights depending on the class are
internally set in the wrapper.</p>
<pre><code class="r">lrn = makeLearner(&quot;classif.logreg&quot;)
wcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)
</code></pre>

<p>For binary classification, the single number passed to the classifier corresponds
to the weight of the positive / majority class, while the negative / minority 
class receives a weight of 1. So actually, no real costs are used within
this approach, but the cost ratio is taken into account.</p>
<p>If the underlying learner already has a parameter for class weighting (e.g.,
<code>class.weights</code> in <code>"classif.ksvm"</code>), the <code>wcw.weight</code> is basically passed
to the specific class weighting parameter.</p>
<pre><code class="r">lrn = makeLearner(&quot;classif.ksvm&quot;)
wcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)
</code></pre>

<h2 id="complete-code-listing">Complete code listing</h2>
<p>The above code without the output is given below:</p>
<pre><code>data.imbal.train = rbind( 
  data.frame(x = rnorm(100, mean = 1), class = &quot;A&quot;), 
  data.frame(x = rnorm(5000, mean = 2), class = &quot;B&quot;) 
) 
task = makeClassifTask(data = data.imbal.train, target = &quot;class&quot;) 
task.over = oversample(task, rate = 8) 
task.under = undersample(task, rate = 1/8) 

table(getTaskTargets(task)) 

table(getTaskTargets(task.over)) 

table(getTaskTargets(task.under)) 
lrn = makeLearner(&quot;classif.rpart&quot;, predict.type = &quot;prob&quot;) 
mod = train(lrn, task) 
mod.over = train(lrn, task.over) 
mod.under = train(lrn, task.under) 
data.imbal.test = rbind( 
  data.frame(x = rnorm(10, mean = 1), class = &quot;A&quot;), 
  data.frame(x = rnorm(500, mean = 2), class = &quot;B&quot;) 
) 

performance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc)) 

performance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc)) 

performance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc)) 
lrn.over = makeOversampleWrapper(lrn, osw.rate = 8) 
lrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8) 
mod = train(lrn, task) 
mod.over = train(lrn.over, task) 
mod.under = train(lrn.under, task) 

performance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc)) 

performance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc)) 

performance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc)) 
task.smote = smote(task, rate = 8, nn = 5) 
table(getTaskTargets(task)) 

table(getTaskTargets(task.smote)) 
lrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5) 
mod.smote = train(lrn.smote, task) 
performance(predict(mod.smote, newdata = data.imbal.test), measures = list(mmce, ber, auc)) 
lrn = makeLearner(&quot;classif.rpart&quot;, predict.type = &quot;response&quot;) 
obw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3) 
lrn = setPredictType(lrn, &quot;prob&quot;) 
rdesc = makeResampleDesc(&quot;CV&quot;, iters = 5) 
r1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, 
  measures = list(mmce, ber, auc)) 
r1$aggr 

obw.lrn = setPredictType(obw.lrn, &quot;prob&quot;) 
r2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE, 
  measures = list(mmce, ber, auc)) 
r2$aggr 
lrn = makeLearner(&quot;classif.randomForest&quot;) 
obw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3) 

lrn = setPredictType(lrn, &quot;prob&quot;) 
r1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, 
  measures = list(mmce, ber, auc)) 
r1$aggr 

obw.lrn = setPredictType(obw.lrn, &quot;prob&quot;) 
r2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE, 
  measures = list(mmce, ber, auc)) 
r2$aggr 
lrn = makeLearner(&quot;classif.logreg&quot;) 
wcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01) 
lrn = makeLearner(&quot;classif.ksvm&quot;) 
wcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)
</code></pre></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '..';</script>
        <script src="../js/base.js"></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="../search/require.js"></script>
        <script src="../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>
