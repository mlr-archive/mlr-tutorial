# Exploring Learner Predictions

Learners use features to make predictions but how those features are used is often not apparent. [%mlr] can estimate the dependence of a learned function on a subset of the feature space using [generatePartialPredictionData](&generatePartialPredictionData).

Partial prediction plots, also referred to as partial dependence plots, reduce the potentially high dimensional function estimated by the learner, and display a marginalized version of this function in a lower dimensional space. For example suppose $Y = f(X) + \epsilon$, where $\mathbb{E}[\epsilon|X] = 0$. With $(X, Y)$ pairs drawn independently from this statistical model, a learner may estimate $\hat{f}$, which, if $X$ is high dimensional can be uninterpretable. Suppose we want to approximate the relationship between some subset of $X$ (lower dimensional than $X$: possibly unidimensional) We partition $X$ into two sets, $X_s$ and $X_c$ such that $X = X_s \cup X_c$, where $X_s$ is a subset of $X$ of interest.

The partial dependence of $f$ on $X_c$, $f_{X_s} = \mathbb{E}_{X_s}f(X_s, X_c)$: $X_c$ is integrated out. We use the following estimator:

$$\hat{f}_{X_s} = \frac{1}{N} \sum_{i = 1}^N \hat{f}(X_s, x_{iC})$$.

The algorithm works for any supervised learner.

## Usage

### Generating Partial Predictions

Our implementation, following the [generate-plot](plot.md) pattern, consists of the above mentioned function [generatePartialPredictionData](&generatePartialPredictionData), as well as two visualization functions, [plotPartialPrediction](&plotPartialPrediction) and [plotPartialPredictionGGVIS](&plotPartialPredictionGGVIS). The former generates input (objects of class `partialPredictionData`) for the latter.

The first step executed by [generatePartialPredictionData](&generatePartialPredictionData) is to generate a feature grid for every element of the character vector `features` passed, which must be a column name in the `data` argument, which is usually the training data. The feature grid can be generated in several ways. A uniformly spaced grid of length `gridsize` (default 10) from the empirical minimum to the empirical maximum is created by default, but arguments `fmin` and `fmax` may be used to override the empirical default (both the lengths of `fmin` and `fmax` must match the length of `features`). Alternatively the feature data can be resampled, either by using a bootstrap or by subsampling.

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
fit = train(lrn, iris.task)
iris = getTaskData(iris.task)
pd = generatePartialPredictionData(fit, iris, "Petal.Width")
pd
```

As noted above, $X_s$ may not be unidimensional. If it is not, the `interaction` flag must be set to `TRUE`. Then the individual feature grids are combined using the Cartesian product, and the estimator above is applied, producing a partial prediction for every combination of unique feature values. If the `interaction` flag is `FALSE` (the default) then by default $X_s$ is assumed unidimensional, and partial predictions are generate for each feature separately. The resulting output when `interaction = FALSE` has a column for each feature, and `NA` where the feature was not used in generating partial predictions.

```{r}
pd.lst = generatePartialPredictionData(fit, iris, c("Petal.Width", "Petal.Length"), FALSE)
head(pd.lst$data)
tail(pd.lst$data)
```

```{r}
pd.int = generatePartialPredictionData(fit, iris, c("Petal.Width", "Petal.Length"), TRUE)
head(pd.int$data)
```

At each step in the estimation of $\hat{f}_{X_s}$ a set of predictions of length $N$ is generated. By default the mean prediction is used. For classification where `predict.type = "prob"` this entails the mean class probabilities. However, other summaries of the predictions may be used. For regression and survival tasks the function used here must either return one number or three, and, if the latter, the numbers must be sorted lowest to highest. For classification tasks tasks the function must return a number for each level of the target feature.

```{r}
lrn = makeLearner("regr.rpart")
fit = train(lrn, bh.task)
bh = getTaskData(bh.task)
pd.regr = generatePartialPredictionData(fit, bh, "crim")
pd.regr
```

```{r}
lrn = makeLearner("regr.rpart")
fit = train(lrn, bh.task)
bh = getTaskData(bh.task)
pd.ci = generatePartialPredictionData(fit, bh, "crim", fun = function(x) quantile(x, c(.25, .5, .75)))
pd.ci
```

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
fit = train(lrn, iris.task)
pd.classif = generatePartialPredictionData(fit, iris, "Petal.Width", fun = median)
pd.classif
```

### Plotting Partial Predictions

Results from [generatePartialPredictionData](&generatePartialPredictionData) can be visualized with [plotPartialPrediction](&plotPartialPrediction) and [plotPartialPredictionGGVIS](&plotPartialPredictionGGVIS).

With one feature and a regression task the output is a line plot, with a point for each point in the corresponding feature's grid.

```{r}
plotPartialPrediction(pd.regr)
```

With a classification task, a line is drawn for each class, which gives the estimated partial probability of that class for a particular point in the feature grid.

```{r}
plotPartialPrediction(pd.classif)
```

For regression tasks, when the `fun` argument of [generatePartialPredictionData](&generatePartialPredictionData) is used, the bounds will automatically be displayed using a gray ribbon.

```{r}
plotPartialPrediction(pd.ci)
```

When multiple features are passed to [generatePartialPredictionData](&generatePartialPredictionData) but `interaction = FALSE`, facetting is used to display each estimated bivariate relationship.

```{r}
plotPartialPrediction(pd.lst)
```

When `interaction = TRUE` in the call to [generatePartialPredictionData](&generatePartialPredictionData), one variable must be chosen to be used for facetting, and a subplot for each value in the chosen feature's grid is created, wherein the other feature's partial predictions within the facetting feature's value are shown. Note that this type of plot is limited to two features.

```{r}
plotPartialPrediction(pd.int, facet = "Petal.Length")
```

[plotPartialPredictionGGVIS](&plotPartialPredictionGGVIS) can be used similarly, however, since [ggvis](%ggvis) currently lacks subplotting/facetting capabilities, the argument `interaction` maps one feature to an interactive sidebar where the user can select a value of one feature.

```{r, eval = FALSE}
plotPartialPredictionGGVIS(pd.int, interaction = "Petal.Length")
```
