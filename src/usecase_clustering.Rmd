---
output:
  pdf_document: default
  html_document: default
---

# Clustering

```{r, echo = FALSE}
set.seed(1234)
```

This is a use case for clustering with the [%mlr] package. We consider the [agriculture](&cluster::agriculture) dataset that contains observations about $n=12$ countries including  

* the GNP (Gross National Product) per head (\texttt{x}) ,
* the percentage in agriculture (\texttt{y}).

So let's have a look at the data first.

```{r, fig.asp = 0.8}
data(agriculture, package = "cluster")

plot(y ~ x, data = agriculture)
```


We aim to group the observations into clusters that contain similar objects. We will 

* define the learning task  ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/task/index.html)),
* select a learning method ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/learner/index.html)),
* train the learner ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/train/index.html)), 
* evaluate the performance of the model ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/performance/index.html)) and
* tune the model ([here](http://mlr-org.github.io/mlr-tutorial/devel/html/tune/index.html)).

### Defining a task

We now have to define a clustering task. Notice that a clustering task doesn't have a target variable. 

```{r}
agri.task = makeClusterTask(data = agriculture)
agri.task
```

Printing the task shows us some basic information as the number of observations, the data types of the features or if there are still some missing values, that should have been preprocessed.

### Defining a learner

We generate the learner by calling [&makeLearner] and specifying the learning method, and, if needed, hyperparameters.

An overview over all learners can be found  [here](integrated_learners.md). You can also call the [&listLearners] command for our specific task.


```{r eval = FALSE}
listLearners(obj = agri.task)
```

We will apply the $k$-means algorithm with $3$ centers for the moment

```{r}
cluster.lrn = makeLearner("cluster.kmeans", centers = 3)
cluster.lrn
```
### Train the model

The next step is to train our learner by feeding it with our data.

```{r}
agri.mod = train(learner = cluster.lrn, task = agri.task)
```

We can extract the model and have a look at it. 

```{r}
getLearnerModel(agri.mod)
```

### Prediction

Now, we can predict the target values, our cluster labels.  

```{r}
agri.pred = predict(agri.mod, task = agri.task)
agri.pred
```

### Performance 

Since the data given to the learner is unlabeled, there is no objective evaluation of the accuracy of our model. We have to consider other criteria in unsupervised learning. 

An overview over all performance measures can be found [here](measures.md). You can also call the [&listMeasures] command for our specific task.

```{r}
listMeasures(agri.task)
```

Let's have a look at the silhouette coefficient and the Davies-Boulding index.

```{r}
performance(agri.pred, measures = list(silhouette, db), task = agri.task)
```

### Tuning

It's hard to say if our clustering is good since up to now we have nothing to compare to. Could we have done better by choosing a different number of centers? 

Tuning will address the question of choosing the best hyperparameters for our problem.

We first create a search space for the number of clusters $k$, e. g. $k \in \lbrace 2, 3, 4, 5 \rbrace$. Further we define an optimization algorithm and a [resampling strategy](resample.md). Here we use grid search and 3-fold cross validation.

Finally, by combining all the previous pieces, we can tune the parameter $k$ by calling [&tuneParams]. We will use discrete_ps with grid search and the silhouette coefficient as optimization criterion:

```{r}
discrete_ps = makeParamSet(makeDiscreteParam("centers", values = c(2, 3, 4, 5)))
ctrl = makeTuneControlGrid()
res = tuneParams(cluster.lrn, agri.task, measures = silhouette, resampling = cv3,
  par.set = discrete_ps, control = ctrl)
```

Setting $k=2$ yields the best results for our clustering problem. 
So let's generate a learner with the optimal hyperparameter $k=2$. 

```{r}
tuned.lrn = setHyperPars(cluster.lrn, par.vals = res$x)
```

We have to train the tuned learner again and predict the results.
```{r}
tuned.mod = train(tuned.lrn, agri.task)
tuned.pred = predict(tuned.mod, task = agri.task)
tuned.pred
```

This is our final clustering for our problem.

```{r, fig.asp = 0.8}
plot(y ~ x, col = getPredictionResponse(tuned.pred), data = agriculture)
```







